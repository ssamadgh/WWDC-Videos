1
00:00:13,046 --> 00:00:14,086
>> Good morning everyone.


2
00:00:14,436 --> 00:00:17,996
And welcome to Session 501,
"What's New with Core Audio".


3
00:00:18,026 --> 00:00:19,766
I'm the first emcee
on the mic today.


4
00:00:19,766 --> 00:00:20,556
My name is Torrey.


5
00:00:20,996 --> 00:00:23,246
And we have been very busy.


6
00:00:23,246 --> 00:00:25,456
We have a lot of interesting
things to share with you today.


7
00:00:26,026 --> 00:00:29,056
We're going to start off by
talking about some enhancements


8
00:00:29,056 --> 00:00:30,516
that we've made to
Core MIDI and how


9
00:00:30,516 --> 00:00:32,555
that affects you and your apps.


10
00:00:32,946 --> 00:00:34,856
Then we'll move on to
Inter-App Audio views,


11
00:00:35,426 --> 00:00:38,576
and then we will have a large
section on the new and improved


12
00:00:38,576 --> 00:00:41,316
and Enhanced AV Foundation
audio.


13
00:00:41,316 --> 00:00:44,246
And that will include a talk
about the Audio Unit Manager,


14
00:00:44,766 --> 00:00:47,606
AVAudioSession, some
Utility classes,


15
00:00:47,896 --> 00:00:50,086
and that last bullet point
there, AVAudioEngine,


16
00:00:50,086 --> 00:00:53,706
is such a large topic
that it gets a session all


17
00:00:53,706 --> 00:00:55,426
to itself directly
following this one


18
00:00:55,426 --> 00:00:58,676
in the same room
starting at 10:15 a.m.


19
00:00:59,036 --> 00:01:00,516
So without further ado,


20
00:01:01,566 --> 00:01:03,436
let's talk about what's
new with Core MIDI.


21
00:01:03,846 --> 00:01:08,146
If you have a studio, a
music studio, that you use


22
00:01:08,146 --> 00:01:11,116
to make music, it may
look something like this.


23
00:01:11,836 --> 00:01:15,096
So maybe there's a Mac in the
center of it, or an iOS device,


24
00:01:15,096 --> 00:01:17,946
they're also very capable to
be the center of your studio.


25
00:01:18,476 --> 00:01:21,066
And connected to it may be
several USB MIDI devices,


26
00:01:21,216 --> 00:01:23,796
controllers, break out
boxes that are connected


27
00:01:24,136 --> 00:01:29,146
by a 5-pin DIN to legacy
equipment, musical instruments,


28
00:01:29,206 --> 00:01:31,506
and then also you may have
a network session going.


29
00:01:32,296 --> 00:01:35,896
Well, beginning in iOS
8 and Mac OS X Yosemite,


30
00:01:36,096 --> 00:01:38,176
your studio can start
to look like this.


31
00:01:38,956 --> 00:01:42,216
So imagine after making a very
quick Bluetooth connection


32
00:01:42,216 --> 00:01:44,436
and sitting down on a couch
on the other side of the room


33
00:01:44,436 --> 00:01:46,836
of your studio and
controlling all of your music.


34
00:01:47,226 --> 00:01:49,966
That's what you'll be able to
do with MIDI over Bluetooth.


35
00:01:51,026 --> 00:01:54,986
So starting in iOS 8 and in Mac
OS X Yosemite, you'll be able


36
00:01:54,986 --> 00:01:57,466
to send and receive MIDI data


37
00:01:57,466 --> 00:02:01,666
over Bluetooth Low Energy
connections on any device or Mac


38
00:02:01,736 --> 00:02:03,906
that has native Bluetooth
Low Energy support.


39
00:02:05,206 --> 00:02:07,176
The connections you
established is secure meaning


40
00:02:07,176 --> 00:02:08,446
that pairing is enforced.


41
00:02:08,675 --> 00:02:10,175
No one can connect
to your devices


42
00:02:10,175 --> 00:02:11,426
without your explicit consent,


43
00:02:12,266 --> 00:02:15,706
and after the connection is
established, it just appears


44
00:02:15,706 --> 00:02:18,426
as an ordinary MIDI device that
any application that knows how


45
00:02:18,426 --> 00:02:20,536
to communicate with a
MIDI device can talk to.


46
00:02:20,536 --> 00:02:23,996
So to talk a little bit more
about how this connection works


47
00:02:23,996 --> 00:02:27,216
over Bluetooth, I want to talk
about the two key roles involved


48
00:02:27,216 --> 00:02:28,086
in a Bluetooth connection.


49
00:02:28,916 --> 00:02:30,716
There's the Central
and the Peripheral.


50
00:02:31,256 --> 00:02:33,586
You already have some
familiarity with this.


51
00:02:33,746 --> 00:02:35,166
Maybe not with these names.


52
00:02:35,516 --> 00:02:37,906
You can view your Central
as like your iPhone


53
00:02:37,946 --> 00:02:40,036
and your Peripheral as like
your Bluetooth earpiece.


54
00:02:40,706 --> 00:02:44,716
The Peripheral's job is to
become discoverable and say,


55
00:02:44,716 --> 00:02:45,526
"Hey, I can do something.


56
00:02:45,526 --> 00:02:46,256
You can connect to me."


57
00:02:47,076 --> 00:02:48,596
So for Bluetooth MIDI,


58
00:02:48,796 --> 00:02:53,416
the peripheral side will
advertise the MIDI capabilities.


59
00:02:53,416 --> 00:02:54,536
It'll say, "Hey, I can do MIDI.


60
00:02:54,536 --> 00:02:55,586
You can connect to me now."


61
00:02:55,886 --> 00:02:57,076
And then that side waits.


62
00:02:57,716 --> 00:03:00,316
The Central can scan
for a device


63
00:03:00,516 --> 00:03:03,626
that says they can do MIDI and
then establish a connection.


64
00:03:04,756 --> 00:03:07,216
After that Bluetooth
connection has been established,


65
00:03:07,676 --> 00:03:10,656
MIDI data can be
shuttled bi-directionally


66
00:03:10,656 --> 00:03:11,616
between both of these.


67
00:03:12,476 --> 00:03:14,296
Now in order to have a
Bluetooth connection you have


68
00:03:14,296 --> 00:03:16,706
to have one Central, and you
have to have one Peripheral.


69
00:03:16,976 --> 00:03:20,806
And we allow Macs and iOS
devices to play either role.


70
00:03:20,866 --> 00:03:23,716
So you can connect Mac
to Mac, iOS to iOS,


71
00:03:24,026 --> 00:03:25,736
Mac to iOS, and vice versa.


72
00:03:27,306 --> 00:03:29,396
So what does this mean for
you and your application?


73
00:03:29,806 --> 00:03:32,586
If you are writing a
Mac OS X application,


74
00:03:32,586 --> 00:03:37,116
the good news is you are
already ready, already ready.


75
00:03:37,866 --> 00:03:40,466
This is the MIDI Studio
Panel from Audio MIDI Setup,


76
00:03:40,466 --> 00:03:42,076
which I'm sure you're
all familiar with.


77
00:03:42,316 --> 00:03:44,806
If you look there
you'll see a new icon,


78
00:03:45,196 --> 00:03:46,746
the Bluetooth Configuration
icon.


79
00:03:47,216 --> 00:03:48,876
If you double click that icon,


80
00:03:49,006 --> 00:03:50,496
you are going to
get a new window.


81
00:03:51,156 --> 00:03:55,116
And this window will allow
you to play either the Central


82
00:03:55,116 --> 00:03:56,156
or the Peripheral role.


83
00:03:56,406 --> 00:03:58,706
If you look at kind of the
top third of the window,


84
00:03:58,706 --> 00:04:00,846
you'll see where there's a
button that says Advertise.


85
00:04:00,846 --> 00:04:04,026
And click Advertise to become
discoverable as Fresh Air.


86
00:04:04,026 --> 00:04:05,386
That's a name that
you can modify.


87
00:04:05,696 --> 00:04:08,516
Fresh Air is the name of my
MacBook Air because it's fresh.


88
00:04:09,446 --> 00:04:13,076
Then the bottom two thirds
of it is the central view.


89
00:04:13,456 --> 00:04:16,016
If someone is advertising, "Hey,
I can do MIDI," it will show


90
00:04:16,016 --> 00:04:17,836
up in the bottom,
you click Connect


91
00:04:17,836 --> 00:04:18,986
to establish the connection.


92
00:04:19,416 --> 00:04:20,386
The pairing will happen,


93
00:04:20,386 --> 00:04:22,766
and then a new MIDI device
will appear in the setup


94
00:04:22,766 --> 00:04:25,496
that any application that
uses MIDI devices can see


95
00:04:25,636 --> 00:04:26,346
and communicate will.


96
00:04:27,276 --> 00:04:30,016
Now on iOS, there is
no audio MIDI setup.


97
00:04:30,126 --> 00:04:32,306
So how do you manage your
Bluetooth MIDI connections?


98
00:04:32,786 --> 00:04:35,776
You'll be using new
CoreAudioKit View Controllers.


99
00:04:36,626 --> 00:04:39,486
There are 2 new CoreAudioKit
View Controllers


100
00:04:39,486 --> 00:04:40,756
that you can add to
your application.


101
00:04:41,056 --> 00:04:43,636
One of them that allows you to
play the role of the Central,


102
00:04:43,636 --> 00:04:46,616
which means you scan and connect
and another that allows you


103
00:04:46,616 --> 00:04:47,746
to play the role of Peripheral,


104
00:04:47,816 --> 00:04:49,386
which means you advertise
and wait.


105
00:04:49,386 --> 00:04:52,936
If you establish a
connection between 2 devices


106
00:04:52,936 --> 00:04:55,656
over Bluetooth MIDI, and they're
not communicating for a while


107
00:04:55,656 --> 00:04:57,186
and they are unused
by the application,


108
00:04:57,356 --> 00:04:59,506
after several minutes we
will terminate the Bluetooth


109
00:04:59,506 --> 00:05:01,036
connection to save power.


110
00:05:02,106 --> 00:05:04,716
So what does all of this
look like in practice?


111
00:05:05,286 --> 00:05:07,216
I'm going to show
you a short UI demo


112
00:05:08,136 --> 00:05:11,846
of how users would use
this in their studios.


113
00:05:12,326 --> 00:05:14,896
OK. I've got my demo
machine ready here.


114
00:05:15,406 --> 00:05:18,306
And what I'm going
to do is I'm going


115
00:05:18,306 --> 00:05:21,266
to launch audio MIDI setup.


116
00:05:23,836 --> 00:05:25,706
This is the audio window.


117
00:05:25,706 --> 00:05:28,506
We'll close this, and we
will go to the MIDI window.


118
00:05:28,506 --> 00:05:31,626
Now if you'll notice here


119
00:05:31,626 --> 00:05:33,146
in the MIDI window
there's this new Bluetooth


120
00:05:33,146 --> 00:05:34,336
Configuration panel.


121
00:05:34,666 --> 00:05:37,046
So if I double click
this, then I will see


122
00:05:37,366 --> 00:05:40,186
that there are currently
no advertising Bluetooth


123
00:05:40,186 --> 00:05:40,886
MIDI devices.


124
00:05:42,356 --> 00:05:44,326
I want my Mac to play
the role of Central.


125
00:05:44,536 --> 00:05:46,056
So I'm going to wait for someone


126
00:05:46,056 --> 00:05:47,336
to become available
to connect to.


127
00:05:48,406 --> 00:05:51,606
And I'm going to use
my iPad for that.


128
00:05:54,016 --> 00:05:55,396
So here's my iPad.


129
00:05:55,496 --> 00:05:59,146
And this is a little test
application that we wrote


130
00:05:59,146 --> 00:06:02,146
to implement the
CoreAudioKit View Controllers


131
00:06:02,146 --> 00:06:03,086
that I talked about earlier.


132
00:06:03,286 --> 00:06:05,236
I am going to go to
Advertisement Setup,


133
00:06:05,236 --> 00:06:06,786
and this will give me
the Peripheral view.


134
00:06:07,166 --> 00:06:09,606
If you look here at the
top, you see the name


135
00:06:09,606 --> 00:06:12,246
of this iPad Air
is iPad Air MIDI.


136
00:06:12,696 --> 00:06:14,716
If I want to change this
name I could tap the "i",


137
00:06:15,496 --> 00:06:16,476
but I'm OK with that name.


138
00:06:17,226 --> 00:06:19,526
And then I will say
Advertise the MIDI Service.


139
00:06:20,386 --> 00:06:22,836
Now after I'm advertising
the MIDI service,


140
00:06:23,836 --> 00:06:27,376
back on the Mac OS X machine
you'll see iPad Air MIDI has


141
00:06:27,376 --> 00:06:28,086
shown up here.


142
00:06:28,086 --> 00:06:32,866
If I click connect, after a few
minutes you'll see a new device


143
00:06:32,866 --> 00:06:33,886
appear in the MIDI setup.


144
00:06:34,556 --> 00:06:38,636
I'm going to launch Main Stage


145
00:06:38,636 --> 00:06:43,476
because Main Stage can receive
MIDI notes and play back audio.


146
00:06:46,316 --> 00:06:47,936
Go into Performance
Mode [music playing].


147
00:06:48,436 --> 00:06:53,816
OK. So a big confession
here, I don't play keys.


148
00:06:54,326 --> 00:06:55,856
But I do have an application


149
00:06:55,856 --> 00:06:59,476
that plays keys really well
called Arpeggionome Pro.


150
00:06:59,806 --> 00:07:03,496
So I'm going to launch that,
and I'm going to use it


151
00:07:03,496 --> 00:07:11,996
to send MIDI data over
to the Main Stage 3.


152
00:07:13,206 --> 00:07:15,066
OK. Now one thing I want


153
00:07:15,066 --> 00:07:18,656
to do really quickly here is
check my connection status


154
00:07:18,656 --> 00:07:21,196
because I left it inactive
for quite a while here.


155
00:07:21,406 --> 00:07:27,916
So I'm going to go back and make
myself advertise one more time.


156
00:07:28,516 --> 00:07:30,546
[ Music Playing ]


157
00:07:31,046 --> 00:07:40,096
So now this is live MIDI data
being sent over Bluetooth.


158
00:07:40,096 --> 00:07:43,916
If I could get that volume
a little louder please.


159
00:07:45,426 --> 00:07:47,596
Thank you.


160
00:07:48,726 --> 00:07:52,606
So if I wanted to do this
preset, it's called Epic Fall.


161
00:07:54,336 --> 00:07:56,386
And it is epic.


162
00:07:57,066 --> 00:08:00,646
So that's Bluetooth
being sent over MIDI.


163
00:08:00,886 --> 00:08:03,656
And also this sends not
only the controller data,


164
00:08:04,006 --> 00:08:07,836
but it also sends the SISX
data that you may have


165
00:08:07,906 --> 00:08:09,256
or any other type of MIDI.


166
00:08:09,846 --> 00:08:13,066
A few final words before
I turn the mic over.


167
00:08:13,766 --> 00:08:16,426
This Bluetooth, being
able to connect


168
00:08:16,846 --> 00:08:21,086
with Bluetooth MIDI connections
will work on both OS X Yosemite


169
00:08:21,086 --> 00:08:24,166
and iOS 8 using those
view controllers


170
00:08:24,166 --> 00:08:25,886
that I told you about.


171
00:08:25,886 --> 00:08:27,616
And it will work
on any Mac, iPhone,


172
00:08:27,616 --> 00:08:30,066
or iPad that has native
Bluetooth Low Energy support.


173
00:08:30,496 --> 00:08:31,996
So now I'm going to tell
you which ones those are.


174
00:08:31,996 --> 00:08:36,626
For Macs, any Mac that was
manufactured in 2012 or later


175
00:08:36,946 --> 00:08:38,586
and a mixed bag of
Macs that were released


176
00:08:38,586 --> 00:08:41,116
in 2011 also have native
Bluetooth Low Energy support.


177
00:08:41,956 --> 00:08:45,086
For the iPhone, the iPhone 4S
and greater have Bluetooth LE.


178
00:08:45,186 --> 00:08:48,916
For the iPad, the first iPad
with the Retina display has LE


179
00:08:48,916 --> 00:08:50,396
and the ones from that point


180
00:08:50,806 --> 00:08:53,826
and all iPad Minis have native
Bluetooth Low Energy support.


181
00:08:54,336 --> 00:08:56,036
So this will work on
all of those systems.


182
00:08:56,886 --> 00:08:59,636
Also, the connection
is really low latency.


183
00:08:59,636 --> 00:09:00,516
It's very sensitive.


184
00:09:01,356 --> 00:09:05,676
And the Bluetooth LE bandwidth
greatly exceeds the minimum MIDI


185
00:09:05,676 --> 00:09:09,546
bandwidth requirement for MIDI
of 3,125 bytes per second.


186
00:09:10,116 --> 00:09:11,806
Standardization is in the works.


187
00:09:11,806 --> 00:09:14,286
We're working with standards
bodies to standardize this


188
00:09:14,286 --> 00:09:16,316
so more people can get in on it.


189
00:09:16,316 --> 00:09:17,686
And the key takeaway for you is


190
00:09:17,686 --> 00:09:19,896
if you're making your
iOS applications,


191
00:09:20,296 --> 00:09:24,246
please start adding these
Bluetooth UI View controllers


192
00:09:24,666 --> 00:09:26,206
immediately to your applications


193
00:09:26,236 --> 00:09:30,256
so that users can manage
Bluetooth MIDI connections using


194
00:09:30,256 --> 00:09:31,126
your app.


195
00:09:31,436 --> 00:09:32,896
And the person who is
going to show you how to do


196
00:09:32,896 --> 00:09:36,296
that is my colleague and
homeboy Michael Hopkins.


197
00:09:36,526 --> 00:09:37,696
And I'll turn the
mic over to him.


198
00:09:38,436 --> 00:09:39,556
>> Thank you very much, Torrey.


199
00:09:40,196 --> 00:09:43,806
I'd like to talk to you this
morning about a new framework


200
00:09:44,186 --> 00:09:46,636
for iOS called CoreAudioKit.


201
00:09:47,096 --> 00:09:50,996
This framework provides
standardized user interface


202
00:09:50,996 --> 00:09:55,416
elements for you to add to
your application to do things


203
00:09:55,416 --> 00:09:59,136
like show the MIDI
over Bluetooth LE UI


204
00:09:59,136 --> 00:10:03,536
that Torrey just demonstrated as
well as some new views for those


205
00:10:03,536 --> 00:10:05,306
of you that are doing
Inter-App Audio.


206
00:10:06,436 --> 00:10:09,746
We've designed these so that
we do all the heavy lifting


207
00:10:09,746 --> 00:10:12,236
so that you don't have to worry
about rolling your own UI,


208
00:10:12,236 --> 00:10:15,566
and you can just concentrate
on what makes your app unique.


209
00:10:16,166 --> 00:10:18,856
Therefore, they are very easy
to adopt with a minimal amount


210
00:10:18,856 --> 00:10:24,126
of source code, and they
work on both iPhone and iPad.


211
00:10:26,006 --> 00:10:29,736
Looking specifically about these
interface elements for MIDI


212
00:10:29,736 --> 00:10:37,366
over Bluetooth LE, as Torrey
showed you we have separated


213
00:10:37,366 --> 00:10:39,436
these into two different
view controllers


214
00:10:39,436 --> 00:10:41,766
so that you can choose
which one is appropriate


215
00:10:41,766 --> 00:10:44,606
for your own application
or you can use both.


216
00:10:45,046 --> 00:10:48,466
For example, if you use the UI
split view controller you can


217
00:10:48,466 --> 00:10:50,536
have those both visible
at the same time.


218
00:10:51,136 --> 00:10:53,806
The first one is
the CABTMIDILocal


219
00:10:53,806 --> 00:10:55,096
PeripheralViewController.


220
00:10:55,226 --> 00:10:57,116
That's quite a mouthful
this early in the morning.


221
00:10:58,636 --> 00:11:01,646
If you want to advertise
your iOS device


222
00:11:01,646 --> 00:11:04,486
as a Peripheral,
you use this class.


223
00:11:06,076 --> 00:11:09,676
The source code for adopting
this is very straightforward.


224
00:11:10,306 --> 00:11:12,536
You create a new instance
of that view controller,


225
00:11:13,266 --> 00:11:18,556
get the navigation controller
object for your app and push


226
00:11:18,556 --> 00:11:20,966
that view controller
onto the stack.


227
00:11:22,856 --> 00:11:27,696
The CABTMIDICentral
ViewController is required


228
00:11:27,696 --> 00:11:28,926
if you want to discover


229
00:11:28,926 --> 00:11:31,326
and connect two Bluetooth
Peripherals.


230
00:11:32,566 --> 00:11:34,486
And you use that
in the same way.


231
00:11:34,566 --> 00:11:37,456
You create the view
controller and push it


232
00:11:37,456 --> 00:11:38,986
onto your view controller stack.


233
00:11:40,026 --> 00:11:43,386
Now I'd like to switch over
and talk about Inter-App Audio.


234
00:11:44,086 --> 00:11:47,576
For those of you that weren't
present last year at WWDC,


235
00:11:47,576 --> 00:11:51,086
we had a session talking
about this new technology


236
00:11:51,086 --> 00:11:52,916
that we released with iOS 7.


237
00:11:53,976 --> 00:11:57,526
In review, Inter-App Audio
allows you to stream audio


238
00:11:57,526 --> 00:11:59,986
between one or more
apps in real time.


239
00:12:00,556 --> 00:12:05,766
A host application can discover
available node apps even


240
00:12:05,766 --> 00:12:06,676
if they are not running.


241
00:12:08,046 --> 00:12:10,356
And please refer to
last year's session,


242
00:12:11,516 --> 00:12:15,026
"What's New in Core Audio"
Session 206 for further details.


243
00:12:16,506 --> 00:12:18,656
But looking at how this
works with the Host App


244
00:12:18,656 --> 00:12:22,086
and a connected Node App, the
Node App can be an instrument,


245
00:12:22,086 --> 00:12:23,816
an effect, or a generator.


246
00:12:24,546 --> 00:12:27,526
And the Host App and Node App
can send audio back and forth.


247
00:12:28,036 --> 00:12:30,846
In the case of an instrument,


248
00:12:30,846 --> 00:12:34,336
the Host App can also send a
MIDI to that instrument app


249
00:12:34,916 --> 00:12:36,306
and receive audio back.


250
00:12:36,916 --> 00:12:43,176
The two user interface
elements that we provide


251
00:12:43,416 --> 00:12:48,206
in iOS 8 are firstly the
Inter-App Audio switch review,


252
00:12:48,706 --> 00:12:52,186
which provides an easy way to
see all the Inter-App Audio apps


253
00:12:52,186 --> 00:12:55,046
that are connected
together and switch


254
00:12:55,046 --> 00:12:57,176
between them using a
simple tap gesture.


255
00:12:59,516 --> 00:13:03,766
We also provide an Inter-App
Audio Host Transport view.


256
00:13:04,576 --> 00:13:07,346
This displays the transport
of the host you're connected


257
00:13:07,346 --> 00:13:09,696
to in your Node App
and allows you


258
00:13:09,696 --> 00:13:13,136
to control the transport
playback, rewind,


259
00:13:13,806 --> 00:13:16,376
and record in addition
to displaying


260
00:13:16,376 --> 00:13:21,156
where in the Host Transport you
are via that numeric time code.


261
00:13:21,656 --> 00:13:25,026
And I'd like to show a
demo of this in action.


262
00:13:25,746 --> 00:13:28,086
I have 3 different
applications here


263
00:13:28,086 --> 00:13:31,106
that we'll be using together in
our Inter-App Audio Scenario.


264
00:13:31,616 --> 00:13:33,616
The first of which
is GarageBand,


265
00:13:33,616 --> 00:13:35,876
which is the current
version of that application


266
00:13:35,876 --> 00:13:38,046
that I've downloaded
from the iTunes store.


267
00:13:39,086 --> 00:13:43,596
I also have a Delay
application and a Sampler.


268
00:13:44,256 --> 00:13:46,186
Let's take a look at
the Sampler first.


269
00:13:47,336 --> 00:13:51,166
This allows me to trigger sample
playback via the keyboard.


270
00:13:52,516 --> 00:13:56,706
[ Music ]


271
00:13:57,206 --> 00:13:59,616
So now let's go ahead and
connect this to GarageBand.


272
00:13:59,806 --> 00:14:01,226
I'm going to launch GarageBand.


273
00:14:07,796 --> 00:14:11,236
I'm going to connect
to that Sampler app,


274
00:14:11,366 --> 00:14:13,876
and now this is connected
to GarageBand.


275
00:14:14,556 --> 00:14:17,096
So the first thing I'd like to
demonstrate is the Inter-App


276
00:14:17,096 --> 00:14:19,136
Audio Switch Review in action,


277
00:14:19,726 --> 00:14:21,946
which this application
has implemented


278
00:14:22,136 --> 00:14:24,286
as visible via a button.


279
00:14:25,076 --> 00:14:27,016
I press that, and
you can see now


280
00:14:27,016 --> 00:14:29,746
that we have two Nodes shown.


281
00:14:30,116 --> 00:14:33,176
The Host, as well as
our current application.


282
00:14:33,176 --> 00:14:36,816
And I can switch over to
GarageBand simply by tapping.


283
00:14:37,856 --> 00:14:38,826
I'm going to add


284
00:14:38,826 --> 00:14:44,426
in an additional Inter-App
Audio App, the Delay effect.


285
00:14:44,426 --> 00:14:51,106
And now if I was to switch
over to this application


286
00:14:51,106 --> 00:14:53,996
without using the Switch
Review, I could double tap


287
00:14:53,996 --> 00:14:58,476
on my Home key, look, and
try to find that application.


288
00:14:58,476 --> 00:14:59,246
Where is it?


289
00:14:59,246 --> 00:15:00,616
It's difficult to find.


290
00:15:00,996 --> 00:15:04,906
And that's why we've provided
the Inter-App Switch Review.


291
00:15:05,466 --> 00:15:07,066
In this application, the Delay,


292
00:15:07,066 --> 00:15:09,016
you can see that on the
lower right-hand corner.


293
00:15:09,506 --> 00:15:13,726
And now that is showing
our Host, the Sampler,


294
00:15:13,726 --> 00:15:15,486
as well as our current effect.


295
00:15:16,226 --> 00:15:20,766
So it's very easy to
switch back and forth.


296
00:15:20,766 --> 00:15:22,846
And you can see that it
just showed up there.


297
00:15:23,156 --> 00:15:25,166
So that's the first view
I'd like to demonstrate.


298
00:15:25,796 --> 00:15:27,606
And if I play back
on my keyboard,


299
00:15:29,416 --> 00:15:31,346
we can hear that we're
now getting that Delay.


300
00:15:31,346 --> 00:15:36,136
And this is interesting because
we have, we're sending audio


301
00:15:36,136 --> 00:15:39,536
from the host to our Sampler,
and then through an effect


302
00:15:39,796 --> 00:15:41,326
to playing that delay,
and then back.


303
00:15:42,246 --> 00:15:44,896
Now the second view, the
Transport view, you'll see just


304
00:15:44,896 --> 00:15:48,826
above that view, let me hide
that for you, and that allows me


305
00:15:48,826 --> 00:15:50,626
to control the transport of
the Host [music playing].


306
00:15:50,626 --> 00:15:55,976
I can do recording.


307
00:15:56,516 --> 00:15:59,786
[ Music Playing ]


308
00:16:00,286 --> 00:16:01,506
Sorry. I'm no Dr. Dre.


309
00:16:01,506 --> 00:16:04,476
It's too early in the morning
for that, but you get the idea.


310
00:16:04,476 --> 00:16:06,366
And these are the views


311
00:16:06,366 --> 00:16:08,346
that we're providing
for your benefit.


312
00:16:08,346 --> 00:16:09,396
So please adopt those


313
00:16:09,396 --> 00:16:11,556
to add this functionality
to your application.


314
00:16:12,416 --> 00:16:15,836
OK. So the goal between these
user interface elements are


315
00:16:15,836 --> 00:16:19,936
to provide a consistent
experience for your customers.


316
00:16:20,156 --> 00:16:22,586
You do have some flexibility
in controlling some


317
00:16:22,586 --> 00:16:25,416
of the visual appearances
of those controls.


318
00:16:25,946 --> 00:16:28,066
They support a number
of different sizes.


319
00:16:28,066 --> 00:16:31,526
So if you want a ginormous
UI you can have that,


320
00:16:31,526 --> 00:16:33,666
or if you want them very
small you can do that.


321
00:16:34,126 --> 00:16:36,656
The source code, as I'm going
to show you, is very easy


322
00:16:36,656 --> 00:16:37,926
to add to your application.


323
00:16:38,336 --> 00:16:41,846
And because these are subclasses
of UIView, you can choose


324
00:16:41,846 --> 00:16:44,366
to create a view controller
if you want to add them


325
00:16:44,366 --> 00:16:48,176
to a UI popover view
on your iPad, or if,


326
00:16:48,176 --> 00:16:51,046
the example demonstrated it, if
you want to embed that directly


327
00:16:51,046 --> 00:16:53,486
in the content of your app
you can do that as well.


328
00:16:54,116 --> 00:16:56,216
Let's take a look at the code.


329
00:16:56,566 --> 00:16:59,006
We import the umbrella header.


330
00:16:59,436 --> 00:17:01,326
In this case, I'm
demonstrating how


331
00:17:01,326 --> 00:17:03,856
to add the Switcher
View from a nib file.


332
00:17:03,856 --> 00:17:06,906
So you go into IV,
drag out your UI view,


333
00:17:07,016 --> 00:17:10,715
assign that to be the class of
the CAInterAppAudioSwitcherView,


334
00:17:11,146 --> 00:17:13,106
create an outlet for that view,


335
00:17:13,215 --> 00:17:19,296
and then in the viewDidLoad
method we specified a visual


336
00:17:19,296 --> 00:17:20,685
appearance of that view.


337
00:17:20,866 --> 00:17:25,406
And then we need to associate
an audio unit with that view


338
00:17:25,616 --> 00:17:28,046
so that it can automatically
find the other apps


339
00:17:28,046 --> 00:17:28,986
that are connected.


340
00:17:29,616 --> 00:17:34,026
And that's all there is to it.


341
00:17:34,026 --> 00:17:36,686
And creating the Transport
view programmatically,


342
00:17:36,686 --> 00:17:41,556
as this example shows, we create
the view, specify initial size


343
00:17:41,556 --> 00:17:42,956
and location of that view,


344
00:17:43,956 --> 00:17:46,546
configure it's visual
appearances,


345
00:17:47,046 --> 00:17:49,886
associate an output
audio unit with a view,


346
00:17:49,936 --> 00:17:53,106
and then finally we
add that transport view


347
00:17:53,106 --> 00:17:57,806
as a subview of our
main content.


348
00:17:58,576 --> 00:18:01,936
OK. Now I'd like to switch
gears a little bit now back


349
00:18:01,936 --> 00:18:03,406
to AV Foundation.


350
00:18:03,826 --> 00:18:06,766
The rest of my presenters
including myself will be


351
00:18:06,856 --> 00:18:10,016
focusing on this framework and
some of the new enhancements


352
00:18:10,016 --> 00:18:12,076
and abilities that
we've added for you


353
00:18:12,356 --> 00:18:13,726
to add to your application.


354
00:18:14,386 --> 00:18:17,706
The first new feature is
for Audio Unit Management.


355
00:18:17,706 --> 00:18:21,336
And that's the
AVAudioUnitComponentManager.


356
00:18:22,416 --> 00:18:28,416
This is a Mac OS X Yosemite
API, Objective C-based.


357
00:18:28,416 --> 00:18:33,446
And it's primarily designed for
Audio Unit host applications.


358
00:18:33,726 --> 00:18:34,746
However, as you'll see,


359
00:18:34,746 --> 00:18:37,106
we do have some end-user
features as well.


360
00:18:37,836 --> 00:18:40,496
We provide a number of
different querying methods,


361
00:18:40,496 --> 00:18:43,706
which enable your host
to find the Audio Units


362
00:18:43,706 --> 00:18:46,506
on the system given some
criteria, for example,


363
00:18:46,506 --> 00:18:47,866
number of supported channels.


364
00:18:48,816 --> 00:18:51,356
We have a simple API
for getting information


365
00:18:51,356 --> 00:18:53,206
about each individual
Audio Unit.


366
00:18:53,926 --> 00:18:55,926
We have some new
tagging facilities


367
00:18:55,926 --> 00:18:57,606
that I'll demonstrate
in a moment.


368
00:18:58,146 --> 00:19:01,676
And finally we have a
centralized Audio Unit cache


369
00:19:02,086 --> 00:19:04,816
so that if you have
multiple host applications


370
00:19:04,816 --> 00:19:08,686
on your system, once one
host has scanned Audio Units,


371
00:19:08,686 --> 00:19:11,396
and for a lot of people they
have a large number of them


372
00:19:11,396 --> 00:19:14,306
so this can take quite some
time, all the other hosts


373
00:19:14,306 --> 00:19:18,396
on the system share that
information so they don't have


374
00:19:18,396 --> 00:19:20,576
to perform that exhaustive
scan again.


375
00:19:21,126 --> 00:19:25,106
Let's take a look at
the API in more detail.


376
00:19:25,876 --> 00:19:28,486
As I said, these are in AV
Foundation, and they're new.


377
00:19:29,446 --> 00:19:32,736
The first class is the
AVAudioUnitComponentManager.


378
00:19:33,036 --> 00:19:36,176
And this provides three
different search mechanisms


379
00:19:36,176 --> 00:19:37,976
for finding Audio Units.


380
00:19:38,006 --> 00:19:41,036
The first of which is
based on the NSPredicates.


381
00:19:41,616 --> 00:19:45,296
We can use a SQL-based
language to provide strings,


382
00:19:45,296 --> 00:19:49,376
which I'll show you in a
source code example later


383
00:19:49,746 --> 00:19:52,436
for finding audio units
matching the given criteria.


384
00:19:53,156 --> 00:19:55,176
We also have a block-based
mechanism


385
00:19:55,246 --> 00:19:57,136
for finer programmatic control.


386
00:19:57,536 --> 00:20:00,246
And for those of you
using older host apps


387
00:20:00,626 --> 00:20:02,696
with our current
audio component API,


388
00:20:03,246 --> 00:20:05,626
we have a backwards-compatible
mode as well.


389
00:20:06,806 --> 00:20:12,266
Each of these search
methodologies return an NSArray


390
00:20:12,266 --> 00:20:14,186
of AVAudioUnitComponents.


391
00:20:14,556 --> 00:20:15,866
And that class can be used


392
00:20:15,866 --> 00:20:19,376
to get information
about the audio unit.


393
00:20:20,016 --> 00:20:23,076
Now using our prior API,
if I wanted to do something


394
00:20:23,076 --> 00:20:27,966
like find all stereo effects
that support two-channel input


395
00:20:27,966 --> 00:20:32,706
and two-channel output, I'd have
to write a great deal of code.


396
00:20:33,026 --> 00:20:33,876
That's OK.


397
00:20:33,876 --> 00:20:38,386
But now with this new API we can
reduce all that to this simple,


398
00:20:38,386 --> 00:20:39,836
elegant four lines of code.


399
00:20:40,496 --> 00:20:42,846
The first of which is
retrieving an instance


400
00:20:42,846 --> 00:20:45,076
of the sharedAudioUnitManager.


401
00:20:45,806 --> 00:20:50,246
And here I'm using the
block-based search mechanism


402
00:20:50,246 --> 00:20:54,086
to find all components
that pass a specific test.


403
00:20:54,776 --> 00:20:58,726
And in this block I'm checking
to see if the type name


404
00:20:58,766 --> 00:21:00,406
of that audio unit is equal


405
00:21:00,406 --> 00:21:05,606
to the preset string
AVAudioUnitTypeEffect.


406
00:21:05,606 --> 00:21:08,326
And then furthermore
we're checking to see


407
00:21:08,326 --> 00:21:11,886
if that Audio Unit supports
stereo input and output.


408
00:21:12,976 --> 00:21:16,146
You'll notice there is a stop
parameter, so if you wanted


409
00:21:16,146 --> 00:21:19,806
to return only the
first instance


410
00:21:19,806 --> 00:21:22,396
of the audio component
matching this criteria,


411
00:21:22,756 --> 00:21:25,686
you could return yes
and the stop would,


412
00:21:25,686 --> 00:21:27,126
and it would stop immediately.


413
00:21:30,366 --> 00:21:33,436
OK. Now I'd like to move
on to talk about tagging.


414
00:21:34,156 --> 00:21:36,646
A lot of people,
especially Dr. Dre


415
00:21:36,646 --> 00:21:39,876
in his studio has a large
number of Audio Units.


416
00:21:39,876 --> 00:21:43,456
So finding the right one
can be a bit challenging


417
00:21:43,866 --> 00:21:46,146
because they're sorted
alphabetically


418
00:21:46,146 --> 00:21:48,216
or by manufacturer.


419
00:21:48,636 --> 00:21:51,376
And there's a lot
easier way for users


420
00:21:51,406 --> 00:21:53,556
to find these Audio
Units now with tagging.


421
00:21:53,556 --> 00:21:56,596
It's very similar to what
we have done with a finder


422
00:21:56,596 --> 00:21:58,386
of the previous Mac
OS X release.


423
00:21:59,036 --> 00:22:03,606
Users can now specify their own
tags with an audio unit in order


424
00:22:03,606 --> 00:22:07,376
to create broad categories
or even specific categories


425
00:22:07,736 --> 00:22:10,736
of how they want to
organize their audio units.


426
00:22:11,336 --> 00:22:14,986
They can apply one or more tags
in two different categories.


427
00:22:14,986 --> 00:22:16,806
The first of which
is a system tag.


428
00:22:17,736 --> 00:22:21,416
This is defined by the
creator of the audio unit.


429
00:22:21,416 --> 00:22:25,286
And, for example, in Mavericks,
excuse me, in Yosemite,


430
00:22:25,286 --> 00:22:28,556
I have to get that name in my
head, I personally liked Weed,


431
00:22:28,556 --> 00:22:32,266
but I didn't get to vote.


432
00:22:32,376 --> 00:22:35,576
The system tags are
defined by the creator.


433
00:22:35,576 --> 00:22:39,076
And we at Apple have
added standard tags


434
00:22:39,076 --> 00:22:41,486
to all the Audio Units that
we feel would be useful


435
00:22:41,486 --> 00:22:42,716
to most of our users.


436
00:22:42,796 --> 00:22:45,666
You can also have user tags.


437
00:22:46,066 --> 00:22:49,366
These are specified by each
individual user on the system.


438
00:22:49,366 --> 00:22:52,216
So if you have three users they
can each have their own set


439
00:22:52,216 --> 00:22:52,716
of tags.


440
00:22:53,426 --> 00:22:58,156
A tag is a localized string
in the user's own language.


441
00:22:58,636 --> 00:23:00,246
Swedish, Swahili,
it doesn't matter.


442
00:23:00,336 --> 00:23:03,676
They can be arbitrary, or they
can be a pre-defined type.


443
00:23:03,676 --> 00:23:04,506
And these are all


444
00:23:04,506 --> 00:23:09,576
in AudioComponent.h. They can
be either based on the type


445
00:23:09,576 --> 00:23:12,966
of Audio Unit, for example a
filter or a distortion effect,


446
00:23:13,506 --> 00:23:16,036
or they can be based
on the intended usage,


447
00:23:16,326 --> 00:23:21,046
for example an audio unit useful
in a guitar or vocal track.


448
00:23:21,766 --> 00:23:27,796
Now I'd like to show
a demo of tagging


449
00:23:27,796 --> 00:23:31,986
in action using a
modified version of AU Lab.


450
00:23:32,296 --> 00:23:37,676
So in AU Lab we can look
at all the tags associated


451
00:23:37,676 --> 00:23:39,366
with all the built-in
audio units.


452
00:23:39,926 --> 00:23:41,886
And here you see
that, for example,


453
00:23:41,886 --> 00:23:46,716
the AU time pitch
has two standard tags


454
00:23:46,906 --> 00:23:49,306
that are associated with
it, time effect and pitch.


455
00:23:49,626 --> 00:23:51,146
And those are defined by us.


456
00:23:51,736 --> 00:23:55,006
In addition you can see


457
00:23:55,006 --> 00:23:59,646
that this distortion effect has
two user tags, one specifying


458
00:23:59,646 --> 00:24:01,706
that it's useful
for a drum track


459
00:24:01,706 --> 00:24:03,306
and another one for
a guitar track.


460
00:24:04,256 --> 00:24:08,376
The API also provides developers
the ability to get a list


461
00:24:08,376 --> 00:24:12,576
of all the system-defined
tags localized in the language


462
00:24:12,576 --> 00:24:15,486
of the running host
as you can see here.


463
00:24:15,986 --> 00:24:19,176
And I can also see
all of the user tags


464
00:24:19,246 --> 00:24:23,216
that the users assigned to all
the Audio Units on this system.


465
00:24:25,216 --> 00:24:28,216
Adding tags are as simple
as typing a new one.


466
00:24:28,966 --> 00:24:33,796
Now that's been added
to that Audio Unit.


467
00:24:33,796 --> 00:24:38,956
And I can do a search
using this predicate-based


468
00:24:38,956 --> 00:24:40,356
and other search mechanisms.


469
00:24:41,856 --> 00:24:43,796
And it will search all
the audience looking


470
00:24:43,796 --> 00:24:45,086
for that particular tag.


471
00:24:45,676 --> 00:24:47,446
So this is something
that is really exciting,


472
00:24:47,446 --> 00:24:52,256
and we hope that you'll use this
API to add tagging functionality


473
00:24:52,256 --> 00:24:53,736
to your own host application.


474
00:24:54,276 --> 00:24:56,126
Let's take a look
now at the API.


475
00:24:57,416 --> 00:24:59,936
To find an Audio Unit
with a specific tag,


476
00:25:00,176 --> 00:25:01,366
in this example I'm going


477
00:25:01,366 --> 00:25:05,606
to use the NSPredicate
filtering mechanism.


478
00:25:05,956 --> 00:25:07,376
Here I'm defining a predicate.


479
00:25:07,376 --> 00:25:12,336
It says that the component has


480
00:25:12,336 --> 00:25:16,556
to have the old tag name's
property containing a particular


481
00:25:16,556 --> 00:25:18,736
string, in this case,
"My Favorite Tag",


482
00:25:19,236 --> 00:25:21,156
and this is the identical
searching


483
00:25:21,156 --> 00:25:22,696
that you just saw in my demo.


484
00:25:24,166 --> 00:25:26,606
Once you've defined the
predicate you get an instance


485
00:25:26,606 --> 00:25:28,926
of the shared AU Manager,


486
00:25:30,426 --> 00:25:32,606
and then call
componentsMatchingPredicate,


487
00:25:32,606 --> 00:25:33,986
which returns an array.


488
00:25:37,816 --> 00:25:42,036
To get a list of
the tags associated


489
00:25:42,036 --> 00:25:44,666
with this particular
AVAudioUnitComponent,


490
00:25:44,666 --> 00:25:46,616
you use the userTags
named property.


491
00:25:47,766 --> 00:25:49,446
You can assign to that as well.


492
00:25:49,446 --> 00:25:53,486
And in this example I'm adding
two tags to the audio Unit.


493
00:25:54,256 --> 00:25:57,946
We could get all tags
for a specific component,


494
00:25:57,946 --> 00:26:01,196
and these will include the user
tags as well as the system tags.


495
00:26:02,106 --> 00:26:03,426
All tagNames property.


496
00:26:05,016 --> 00:26:08,356
We can get a localized list of
all the standard system tags


497
00:26:08,356 --> 00:26:09,896
by getting the Component Manager


498
00:26:09,896 --> 00:26:12,466
and then calling the
standardLocalizedTagNames


499
00:26:12,496 --> 00:26:13,006
property.


500
00:26:13,786 --> 00:26:16,846
This is what I was displaying
in the pop up in my demo.


501
00:26:17,746 --> 00:26:21,246
And finally I can get a list
of all the localized tags


502
00:26:21,246 --> 00:26:22,766
that this user has assigned


503
00:26:22,766 --> 00:26:24,736
across all the audio
units on the system.


504
00:26:25,226 --> 00:26:26,926
And that, again,
you saw in my demo.


505
00:26:27,626 --> 00:26:32,976
For those of you that ship
Audio Units, and you want


506
00:26:32,976 --> 00:26:35,486
to add your own built-in
tags to those Audio Units,


507
00:26:35,866 --> 00:26:39,336
you need to go into your
AudioComponent bundle.


508
00:26:39,336 --> 00:26:43,486
And in your info.plist, look at
your Audio Component Dictionary


509
00:26:43,486 --> 00:26:45,066
and add a tag section.


510
00:26:46,076 --> 00:26:50,626
These first two items are
examples of using standard tags,


511
00:26:51,286 --> 00:26:53,806
and the third item
is a custom tag.


512
00:26:54,186 --> 00:26:57,206
So you can have that
be something meaningful


513
00:26:57,206 --> 00:26:59,316
to your own company,
for example,


514
00:26:59,316 --> 00:27:02,606
if you have like the
Silver Effect Package,


515
00:27:02,606 --> 00:27:03,646
you could add that tag.


516
00:27:05,076 --> 00:27:08,336
If you do so, you can
also localize that tag


517
00:27:08,406 --> 00:27:10,746
by adding an
AudioUnitsTag.strings file


518
00:27:10,746 --> 00:27:14,926
into your bundle and then adding
localizations for each language


519
00:27:14,926 --> 00:27:16,266
that you wish to support.


520
00:27:16,766 --> 00:27:20,136
And please do not localize any
of our standard system tags.


521
00:27:20,136 --> 00:27:21,466
We've already done so for you.


522
00:27:22,746 --> 00:27:27,706
So, in summary, if you're a
host developer please adopt the


523
00:27:27,706 --> 00:27:29,896
AVAudioComponentManager API,


524
00:27:29,896 --> 00:27:33,606
so your users can tag
all their Audio Units.


525
00:27:33,666 --> 00:27:35,846
And if you're an
Audio Unit developer,


526
00:27:35,846 --> 00:27:38,346
please add system tags
to your audio units.


527
00:27:38,876 --> 00:27:41,976
So without further
ado I'd like to turn


528
00:27:41,976 --> 00:27:43,946
over this session
to Eric Johnson.


529
00:27:43,946 --> 00:27:47,056
He'll be discussing tips and
tricks and new functionality


530
00:27:47,056 --> 00:27:48,176
in the AVAudioSession.


531
00:27:48,796 --> 00:27:48,976
Eric?


532
00:27:49,516 --> 00:27:54,126
[ Applause ]


533
00:27:54,626 --> 00:27:55,006
>> Good morning.


534
00:27:55,006 --> 00:27:58,086
So I'll be continuing on with
the AV Foundation framework.


535
00:27:58,166 --> 00:28:02,376
This time we're on iOS
only with AVAudioSession.


536
00:28:02,866 --> 00:28:05,756
So today we're just going to
spend a few minutes talking


537
00:28:05,756 --> 00:28:08,396
about some best practices
focusing


538
00:28:08,396 --> 00:28:11,196
on managing your
session's activation state,


539
00:28:11,746 --> 00:28:13,456
and then also talking
about just a little bit


540
00:28:13,456 --> 00:28:15,066
of new things in iOS 8.


541
00:28:16,676 --> 00:28:19,126
Before we dive in I wanted
to call your attention


542
00:28:19,166 --> 00:28:22,016
to an updated Audio Session
Programming Guide that's


543
00:28:22,016 --> 00:28:25,686
available on
developer.apple.com.


544
00:28:25,686 --> 00:28:28,256
Since we saw you all
last year at this time,


545
00:28:28,256 --> 00:28:31,976
this guide has been updated
so that it has been rewritten


546
00:28:32,436 --> 00:28:35,296
in terms of AVAudioSession,
so it's no longer referring


547
00:28:35,296 --> 00:28:36,856
to the deprecated C API.


548
00:28:37,806 --> 00:28:39,066
That's a really great update.


549
00:28:39,356 --> 00:28:42,356
And for those of you who
are maybe not that familiar


550
00:28:42,356 --> 00:28:46,466
with Audio Session, there
was a talk from two years ago


551
00:28:46,696 --> 00:28:49,476
where Torrey talked
about Audio Session


552
00:28:49,476 --> 00:28:51,546
and also Multi-Route
Audio in iOS.


553
00:28:53,126 --> 00:28:53,916
All right.


554
00:28:53,916 --> 00:28:55,646
So let's dive into talking


555
00:28:55,646 --> 00:28:58,606
about managing your
session's activation state.


556
00:28:59,636 --> 00:29:02,296
So there's your application
state.


557
00:29:02,466 --> 00:29:04,596
And then there's
Audio Session state.


558
00:29:04,596 --> 00:29:05,866
And they're separate things.


559
00:29:05,866 --> 00:29:07,276
They're managed independently
of each other.


560
00:29:08,126 --> 00:29:09,726
So if you've been
doing development


561
00:29:09,886 --> 00:29:12,716
for iOS you are probably
familiar with app states.


562
00:29:12,716 --> 00:29:15,926
So this is whether your app is
running or not, whether it's


563
00:29:15,926 --> 00:29:17,576
in the foreground
or the background,


564
00:29:17,696 --> 00:29:18,596
if it's been suspended.


565
00:29:19,806 --> 00:29:22,626
Your Audio Session
activation state is binary.


566
00:29:23,206 --> 00:29:25,266
It's either inactive or active.


567
00:29:26,366 --> 00:29:29,016
Once you've made your
session active you do need


568
00:29:29,016 --> 00:29:31,976
to be prepared to handle
interruptions, and we'll talk


569
00:29:31,976 --> 00:29:32,866
about what that means.


570
00:29:33,726 --> 00:29:35,736
So let's look at an example


571
00:29:35,736 --> 00:29:39,446
of how an Audio Session
state changes over time.


572
00:29:39,836 --> 00:29:42,386
So here we're on an iPhone.


573
00:29:42,556 --> 00:29:45,506
We have our application
on top, our Audio Session.


574
00:29:46,096 --> 00:29:48,586
Let's say that we're
developing a game app.


575
00:29:48,586 --> 00:29:51,036
And then on the bottom we have
the phone's Audio Session.


576
00:29:51,746 --> 00:29:54,546
And right now the user
is not in a phone call,


577
00:29:54,856 --> 00:29:56,426
and they haven't
launched their app yet,


578
00:29:56,426 --> 00:29:59,476
so both sessions
are idle, inactive.


579
00:29:59,606 --> 00:30:02,286
So now the user launches
our app.


580
00:30:03,796 --> 00:30:06,796
When we first come into the
foreground our Audio Session is


581
00:30:06,796 --> 00:30:07,586
still inactive.


582
00:30:08,316 --> 00:30:11,296
And because we're a game app, we
want to make our session active


583
00:30:11,556 --> 00:30:12,556
when we're in the foreground


584
00:30:12,556 --> 00:30:13,966
so that we can be
ready to play audio.


585
00:30:13,966 --> 00:30:14,696
So we'll do that.


586
00:30:15,806 --> 00:30:17,556
And we're going to
just play some music,


587
00:30:17,556 --> 00:30:20,136
so we're now happily playing
music in the foreground


588
00:30:20,136 --> 00:30:21,286
with an active Audio Session.


589
00:30:23,056 --> 00:30:27,506
So then the phone
starts ringing.


590
00:30:27,506 --> 00:30:29,246
We get interrupted by,


591
00:30:29,246 --> 00:30:32,406
the system sends us
an interruption event.


592
00:30:33,356 --> 00:30:36,066
The phone's Audio
Session becomes active


593
00:30:36,066 --> 00:30:37,306
and plays the ringtone.


594
00:30:38,086 --> 00:30:40,726
And the user decides
to accept the call.


595
00:30:41,436 --> 00:30:43,596
So the phone's Audio
Session stays active,


596
00:30:43,996 --> 00:30:47,616
and our Audio Session has been
interrupted, so we're inactive.


597
00:30:47,616 --> 00:30:52,236
And then the user ends the
call, hangs up, says goodbye,


598
00:30:53,456 --> 00:30:54,636
and now the system is going


599
00:30:54,636 --> 00:30:57,876
to deliver an end interruption
event to our Audio Session.


600
00:30:57,876 --> 00:31:01,576
And we're going to
use that as a signal


601
00:31:01,576 --> 00:31:04,426
to make our session active
again and presume playback.


602
00:31:05,866 --> 00:31:07,086
And we continue in this state.


603
00:31:07,686 --> 00:31:11,666
So this is a typical
example of how something


604
00:31:11,666 --> 00:31:14,346
like a game application
interacts with the phones,


605
00:31:14,726 --> 00:31:16,816
the phone app's Audio
Session on an iPhone.


606
00:31:19,096 --> 00:31:20,776
So the way that you need


607
00:31:20,776 --> 00:31:25,176
to manage your application's
Audio Session state is actually


608
00:31:25,176 --> 00:31:26,816
going to depend on
how you use audio.


609
00:31:27,446 --> 00:31:30,266
We've identified a number of
different types of applications


610
00:31:30,526 --> 00:31:33,046
that commonly use audio on iOS.


611
00:31:33,686 --> 00:31:36,376
And we don't have time to talk
about all of these this morning,


612
00:31:36,376 --> 00:31:38,076
and you'd probably be
bored to death if we did.


613
00:31:38,436 --> 00:31:42,576
So we're just going to
talk about a few of these.


614
00:31:42,576 --> 00:31:44,006
So let's continue
on with the idea


615
00:31:44,006 --> 00:31:45,696
that we're developing
a game app.


616
00:31:46,136 --> 00:31:49,756
So for game apps usually what
we recommend is that when you're


617
00:31:49,756 --> 00:31:51,106
in the foreground, you'll want


618
00:31:51,106 --> 00:31:53,806
to have your Audio
Session active.


619
00:31:53,806 --> 00:31:57,246
So a good place to make
your Audio Session active is


620
00:31:57,246 --> 00:31:59,836
in the app delegate's
applicationDidBecomeActive


621
00:31:59,876 --> 00:32:00,236
method.


622
00:32:00,816 --> 00:32:03,446
So that will cover the case
when you're being launched.


623
00:32:03,826 --> 00:32:06,636
If you're coming from the
background into the foreground,


624
00:32:07,246 --> 00:32:08,956
or if you are already
in the foreground


625
00:32:09,186 --> 00:32:12,676
and the user had swiped
up the control panel


626
00:32:12,676 --> 00:32:15,196
and then dismissed it, you'll be
covered in each of those cases.


627
00:32:16,336 --> 00:32:19,466
So once you've made your session
active you can leave it active,


628
00:32:19,806 --> 00:32:22,746
but you do need to be prepared
to deal with interruptions.


629
00:32:23,866 --> 00:32:25,966
So if you get a begin
interruption event,


630
00:32:26,336 --> 00:32:28,446
you should update
your internal state


631
00:32:28,446 --> 00:32:29,956
so that you know
that you're paused.


632
00:32:31,266 --> 00:32:33,196
And then if you get an
end interruption event,


633
00:32:33,956 --> 00:32:36,166
that's your opportunity to
make your session active


634
00:32:36,446 --> 00:32:37,626
and to resume audio playback.


635
00:32:37,716 --> 00:32:40,446
And this is just like the
example that we looked


636
00:32:40,446 --> 00:32:44,156
at just a few minutes ago.


637
00:32:44,226 --> 00:32:45,476
Media playback apps need


638
00:32:45,476 --> 00:32:48,116
to manage their Audio Session
state a little bit differently.


639
00:32:48,776 --> 00:32:50,536
So I'm talking about
applications


640
00:32:50,536 --> 00:32:53,856
like the built-in music app
or podcast or streaming radio.


641
00:32:53,896 --> 00:32:55,726
And these are the
types of applications


642
00:32:55,726 --> 00:32:58,206
that we usually have
a play/pause button,


643
00:32:58,326 --> 00:33:01,396
and they're what we refer
to as non-mixable meaning


644
00:33:02,236 --> 00:33:03,796
that they'll interrupt the audio


645
00:33:03,796 --> 00:33:06,296
of other non-mixable
Audio Sessions.


646
00:33:07,416 --> 00:33:09,926
So for these types of
applications we recommend


647
00:33:10,086 --> 00:33:12,906
that instead of making your
session active immediately


648
00:33:12,906 --> 00:33:14,556
when you enter the
foreground that you wait


649
00:33:14,886 --> 00:33:16,516
until a user presses
thePplay button.


650
00:33:17,336 --> 00:33:18,586
And the reason that we give you


651
00:33:18,586 --> 00:33:21,446
that advice is sometimes
the user brings your app


652
00:33:21,526 --> 00:33:22,696
into the foreground just to see


653
00:33:22,696 --> 00:33:25,786
if they have a particular
podcast episode downloaded


654
00:33:25,786 --> 00:33:27,556
or to see if they have
a song in their library.


655
00:33:27,556 --> 00:33:28,936
And they don't necessarily want


656
00:33:28,936 --> 00:33:31,096
to interrupt other
audio that was playing.


657
00:33:31,496 --> 00:33:33,016
So it's good to wait
until they press Play.


658
00:33:34,666 --> 00:33:37,306
So like in the case of a game
app once you've made your


659
00:33:37,306 --> 00:33:39,536
session active you
can leave it active.


660
00:33:40,236 --> 00:33:41,936
But, again, you need
to be prepared


661
00:33:41,936 --> 00:33:42,856
to handle interruptions.


662
00:33:44,076 --> 00:33:45,896
So if you get a begin
interruption event,


663
00:33:46,396 --> 00:33:47,816
you should update your UI.


664
00:33:47,816 --> 00:33:50,176
So if you have a play/pause
button it's a good time


665
00:33:50,176 --> 00:33:53,026
to change that state
and also keep track


666
00:33:53,026 --> 00:33:54,966
of your internal states so that
you know that you're paused.


667
00:33:56,286 --> 00:33:58,596
One thing you do not need
to do is you do not need


668
00:33:58,596 --> 00:33:59,836
to make your session inactive


669
00:34:00,056 --> 00:34:02,476
because the system has
already done that for you.


670
00:34:03,006 --> 00:34:04,236
That's what the interruption is.


671
00:34:06,006 --> 00:34:08,496
So then if you get an
end interruption event,


672
00:34:09,036 --> 00:34:12,646
we ask that you honor
the ShouldResume option.


673
00:34:13,045 --> 00:34:16,386
So if this option is part of
the info dictionary that's part


674
00:34:16,386 --> 00:34:19,775
of that notification, that's
the system giving you a hint


675
00:34:19,826 --> 00:34:21,516
that it's OK to make
your session active


676
00:34:21,616 --> 00:34:23,216
and to immediately
resume playback.


677
00:34:23,216 --> 00:34:26,936
If you don't see that option
as part of the notification,


678
00:34:26,936 --> 00:34:28,946
then you should wait for the
user to press play again.


679
00:34:31,616 --> 00:34:35,416
OK. So we talked about for game
apps and media playback apps


680
00:34:35,815 --> 00:34:37,226
when you would make
your session active.


681
00:34:37,315 --> 00:34:38,916
What about making
your session inactive.


682
00:34:40,436 --> 00:34:44,426
So if you are something like
a navigation or a fitness app,


683
00:34:44,946 --> 00:34:49,376
you're typically going to be
playing short prompts of audio.


684
00:34:50,045 --> 00:34:54,646
And you're going to be using
the duck others category option


685
00:34:55,085 --> 00:34:56,406
which will lower the volume


686
00:34:56,516 --> 00:34:58,606
of other audio applications
on the system.


687
00:34:58,896 --> 00:35:02,186
So it's important when you're
done playing your short prompts


688
00:35:02,236 --> 00:35:03,676
that you make your
session inactive


689
00:35:04,236 --> 00:35:06,946
so that the other audio is
able to resume at full volume.


690
00:35:08,076 --> 00:35:14,376
If you're a voiceover IP app
or a chat app or maybe one


691
00:35:14,376 --> 00:35:19,446
of these apps that has like
kind of like a browser view


692
00:35:19,446 --> 00:35:21,006
where you're playing
short videos,


693
00:35:21,706 --> 00:35:25,356
then you are usually going to be
what we refer to as non-mixable,


694
00:35:25,356 --> 00:35:27,586
meaning that you're going
to interrupt other audio.


695
00:35:28,536 --> 00:35:30,906
And so it's important that
when you're done playing audio


696
00:35:30,906 --> 00:35:32,866
that you make your
session inactive


697
00:35:33,456 --> 00:35:36,146
so that other sessions
are able to resume.


698
00:35:37,866 --> 00:35:42,346
And it's a good idea to use
the NotifyOthersOnDeactivation


699
00:35:42,466 --> 00:35:44,446
option when you make
your session inactive.


700
00:35:44,646 --> 00:35:47,086
And that way the system is able


701
00:35:47,086 --> 00:35:50,536
to tell an interrupted
Audio Session


702
00:35:50,536 --> 00:35:52,246
that it's OK for them to resume.


703
00:35:52,246 --> 00:35:52,966
All right.


704
00:35:53,936 --> 00:35:58,416
So now let's shift gears
a little bit and talk


705
00:35:58,416 --> 00:36:02,946
about managing your secondary
audio in response to other audio


706
00:36:02,946 --> 00:36:03,896
on the system playing.


707
00:36:04,696 --> 00:36:06,966
So first let me explain
what I mean


708
00:36:06,966 --> 00:36:08,886
by secondary audio
and primary audio.


709
00:36:09,886 --> 00:36:12,356
So let's say we're
developing a game application.


710
00:36:12,876 --> 00:36:16,426
Our primary audio is going
to be our sound effects,


711
00:36:16,426 --> 00:36:19,936
our explosions, beeps and
bloops, short bits of dialog.


712
00:36:20,586 --> 00:36:23,576
And it's the kind of audio that
really enhances the gameplay.


713
00:36:24,366 --> 00:36:28,356
And it's also the kind of audio
that, if the user was listening


714
00:36:28,356 --> 00:36:30,926
to music when they launched your
app, you still want it to play.


715
00:36:31,336 --> 00:36:35,576
And it's OK that it mixes
in with the other music.


716
00:36:35,626 --> 00:36:38,196
By secondary audio, I am really
talking about your soundtrack.


717
00:36:38,856 --> 00:36:41,776
This is the audio where it
also enhances the gameplay,


718
00:36:42,326 --> 00:36:45,436
but if the user was previously
listening to their music


719
00:36:45,436 --> 00:36:46,966
or their podcast, you'd just


720
00:36:46,966 --> 00:36:48,556
as soon have your
soundtrack be muted.


721
00:36:49,586 --> 00:36:53,366
And then if the user
stops their music playback


722
00:36:53,366 --> 00:36:55,986
on their podcast then you'd like
to have your soundtrack resume.


723
00:36:57,146 --> 00:37:01,876
So in iOS 8 we've added a bit
of new API to help you do this.


724
00:37:02,336 --> 00:37:04,626
We've added a new property
and a new notification.


725
00:37:05,686 --> 00:37:08,166
The property is called
secondaryAudio


726
00:37:08,166 --> 00:37:09,356
ShouldBeSilencedHint.


727
00:37:09,356 --> 00:37:13,456
As the name implies, it's a hint
that the system is giving you


728
00:37:13,966 --> 00:37:16,736
that it's a good time to
mute your secondary audio.


729
00:37:17,746 --> 00:37:20,806
So this is meant to be used by
apps that are in the foreground.


730
00:37:21,976 --> 00:37:24,606
And we recommend that you
would check this property


731
00:37:24,606 --> 00:37:26,656
in applicationDidBecomeActive.


732
00:37:27,286 --> 00:37:30,366
Going along with the
new property is our


733
00:37:30,366 --> 00:37:31,336
new notification.


734
00:37:32,506 --> 00:37:35,886
This is the AVAudioSession
SilenceSecondary


735
00:37:35,886 --> 00:37:37,086
AudioHintNotification.


736
00:37:37,206 --> 00:37:39,546
Another mouthful for this
early in the morning.


737
00:37:40,256 --> 00:37:43,506
So this notification will be
delivered to apps that are


738
00:37:43,506 --> 00:37:45,436
in the foreground with
active Audio Sessions.


739
00:37:46,406 --> 00:37:47,776
And it's kind of similar


740
00:37:47,776 --> 00:37:50,696
to our interruption notification
that it's two-sided.


741
00:37:50,786 --> 00:37:54,586
There's a begin event, there's
an end event all wrapped


742
00:37:54,586 --> 00:37:56,056
up in the same notification.


743
00:37:57,066 --> 00:38:00,656
So when you get a
begin SilenceSecondary


744
00:38:00,656 --> 00:38:03,446
AudioHintNotification that
means that it's a good time


745
00:38:03,446 --> 00:38:04,856
to mute your secondary audio.


746
00:38:05,266 --> 00:38:07,326
And if you get the end
event it's a good time


747
00:38:07,326 --> 00:38:08,696
to resume your soundtrack.


748
00:38:09,526 --> 00:38:11,306
So let's look at what
this looks like in action.


749
00:38:12,696 --> 00:38:15,716
So on the far right we have
the built-in music app,


750
00:38:15,716 --> 00:38:17,546
and it's currently
in the background.


751
00:38:17,546 --> 00:38:18,506
It's not playing audio.


752
00:38:19,826 --> 00:38:22,676
On the far left we have our
game app that we're developing.


753
00:38:23,346 --> 00:38:26,126
So we're playing our primary
audio, the sound effects,


754
00:38:26,126 --> 00:38:27,616
and we're also playing
our soundtrack


755
00:38:27,906 --> 00:38:29,736
because there was no
other music playing.


756
00:38:30,816 --> 00:38:33,776
And in the middle we have iOS
helping to negotiate things.


757
00:38:34,886 --> 00:38:38,736
So the user has his
headphones plugged in,


758
00:38:38,736 --> 00:38:40,306
and he presses that
middle button.


759
00:38:41,026 --> 00:38:43,556
And the music app responds
to remote control events.


760
00:38:43,556 --> 00:38:46,556
So it uses that as a
signal to begin playback.


761
00:38:47,906 --> 00:38:50,556
The music app also informs iOS


762
00:38:50,556 --> 00:38:52,556
that it's using its
audio output.


763
00:38:53,496 --> 00:38:57,376
And so then the system is able
to send a begin notification


764
00:38:57,376 --> 00:38:58,856
to our app that's
in the foreground.


765
00:38:59,606 --> 00:39:02,076
And in response to that we
can mute our soundtrack.


766
00:39:02,846 --> 00:39:04,946
So our app is still
in the foreground.


767
00:39:04,946 --> 00:39:06,426
The only thing that's
really changed is


768
00:39:06,426 --> 00:39:11,246
that the user used their middle
button to play their music.


769
00:39:12,656 --> 00:39:14,816
And we've responded
to the notification


770
00:39:14,816 --> 00:39:15,746
that we got from the system.


771
00:39:17,576 --> 00:39:18,816
So now some time passes.


772
00:39:18,896 --> 00:39:20,246
We're in this state for a while,


773
00:39:20,246 --> 00:39:22,416
and the user presses
the middle button again.


774
00:39:22,856 --> 00:39:26,916
So the music app responds
by pausing its playback


775
00:39:27,216 --> 00:39:29,866
and telling the system that
it's pausing its audio output.


776
00:39:29,866 --> 00:39:33,906
And then the system is able
to send the end notification


777
00:39:33,906 --> 00:39:35,966
to our app that's still
in the foreground.


778
00:39:35,966 --> 00:39:39,066
And in response to that,
we resume our soundtrack.


779
00:39:40,046 --> 00:39:45,286
So hopefully this will
be pretty easy to use.


780
00:39:45,286 --> 00:39:48,596
There's one new property and
then the two-sided notification


781
00:39:48,596 --> 00:39:50,186
that you can use to
manage your soundtrack.


782
00:39:51,876 --> 00:39:56,976
So kind of on a similar thread,
in the past we've given advice


783
00:39:57,056 --> 00:40:00,916
about how you could manage
your secondary audio based


784
00:40:00,916 --> 00:40:03,016
on the isOtherAudioPlaying
property.


785
00:40:03,796 --> 00:40:07,066
And we had given
advice about choosing


786
00:40:07,066 --> 00:40:10,536
between the ambient category
or solo ambient based


787
00:40:10,536 --> 00:40:12,246
on the state of this property.


788
00:40:13,046 --> 00:40:15,846
What we're recommending now
is that if you're this type


789
00:40:15,846 --> 00:40:18,336
of application, that you
just use the ambient category


790
00:40:18,336 --> 00:40:21,396
and then use the new property
and the new notification


791
00:40:21,556 --> 00:40:22,586
to manage your soundtrack.


792
00:40:23,246 --> 00:40:23,396
All right.


793
00:40:23,396 --> 00:40:25,206
I'm going to hand things
over to Doug Wyatt.


794
00:40:25,256 --> 00:40:27,146
He's going to tell us about
some new utility classes


795
00:40:27,146 --> 00:40:27,896
in AV Foundation.


796
00:40:28,686 --> 00:40:29,276
>> Thank you.


797
00:40:29,276 --> 00:40:29,796
Good morning.


798
00:40:29,796 --> 00:40:30,506
I'm Doug Wyatt.


799
00:40:30,506 --> 00:40:32,186
I'm an engineer in
the Core Audio Group,


800
00:40:32,816 --> 00:40:35,276
and I'd like to talk to you
about some new audio classes


801
00:40:35,326 --> 00:40:37,016
in the AV Foundation framework.


802
00:40:38,596 --> 00:40:41,986
I'll give you, we'll start
out with some background


803
00:40:41,986 --> 00:40:43,676
and tell you what
we're up to and why.


804
00:40:44,856 --> 00:40:47,266
Then we'll start looking through
these classes one by one.


805
00:40:47,356 --> 00:40:49,846
And I'll tie things up at
the end with an example.


806
00:40:50,666 --> 00:40:54,596
So in the past our CoreAudio
and AudioToolbox APIs,


807
00:40:54,596 --> 00:40:57,746
they're very powerful, but
they're not always easy


808
00:40:57,856 --> 00:41:01,066
for developers to get their
hands around at the beginning.


809
00:41:01,706 --> 00:41:03,086
We've tried to work around this


810
00:41:03,166 --> 00:41:07,206
by providing some C++
utility classes in our SDK,


811
00:41:07,206 --> 00:41:09,756
and that's helped
to some extent,


812
00:41:10,356 --> 00:41:13,786
but example code
gets copied around.


813
00:41:13,786 --> 00:41:14,996
It evolves over time.


814
00:41:15,536 --> 00:41:18,256
And we think it's best in
the long run if we sort


815
00:41:18,256 --> 00:41:20,956
of solidify these things
in the form of API,


816
00:41:20,956 --> 00:41:23,746
and that's what we're
providing now with these classes


817
00:41:23,746 --> 00:41:26,006
in the AV Foundation
framework starting


818
00:41:26,006 --> 00:41:30,766
with Mac OS X 10.10 and iOS 8.


819
00:41:31,496 --> 00:41:34,676
So our goals here, we don't want


820
00:41:34,676 --> 00:41:36,366
to make a complete
break with the past.


821
00:41:36,616 --> 00:41:40,916
We want to build on
what we've already got.


822
00:41:41,106 --> 00:41:44,716
So we're going to,
in many cases,


823
00:41:44,716 --> 00:41:47,596
wrap our existing
low-level C structures inside


824
00:41:47,596 --> 00:41:48,966
Objective-C objects.


825
00:41:49,576 --> 00:41:53,006
And in doing so, these lower
level C structures become easier


826
00:41:53,006 --> 00:41:53,446
to build.


827
00:41:53,446 --> 00:41:57,326
But we can also extract them
from our Objective-C objects


828
00:41:57,326 --> 00:42:00,726
and pass them to the low-level
APIs we might already be using.


829
00:42:02,006 --> 00:42:05,786
And this is a philosophy we used
also with the AVAudioEngine,


830
00:42:06,386 --> 00:42:09,866
which we'll be examining in more
detail in the next session here.


831
00:42:10,496 --> 00:42:13,486
And I should also mention an
overriding goal here is for us


832
00:42:13,516 --> 00:42:14,956
to stay real-time safe,


833
00:42:14,956 --> 00:42:17,046
which isn't always
easy with Objective-C.


834
00:42:17,476 --> 00:42:21,336
We can't access methods
for properties


835
00:42:21,646 --> 00:42:23,146
on the audio rendering thread.


836
00:42:23,826 --> 00:42:25,726
So we've taken some
great care to do


837
00:42:25,726 --> 00:42:29,476
that in our implementations and
as we go I'll give you a couple


838
00:42:29,476 --> 00:42:33,726
of examples of places where
you need to do this to be aware


839
00:42:33,726 --> 00:42:35,966
of real-time issues when
you're using these classes.


840
00:42:36,536 --> 00:42:40,386
OK. So here are the
classes we'll be looking


841
00:42:40,386 --> 00:42:42,756
at today in this session.


842
00:42:43,286 --> 00:42:47,026
At the bottom in green
we've got AVAudioFormat,


843
00:42:47,766 --> 00:42:49,606
which has an
AVAudioChannelLayout.


844
00:42:51,556 --> 00:42:53,706
In blue we have
AVAudioPCMBuffer,


845
00:42:53,706 --> 00:42:55,316
which has an audio format.


846
00:42:55,316 --> 00:42:58,186
Every buffer has a
format describing it.


847
00:42:58,666 --> 00:43:01,346
And finally we'll be
talking about AVAudioFile,


848
00:43:01,536 --> 00:43:04,276
which uses AVAudioPCMBuffer
for I/O


849
00:43:04,736 --> 00:43:07,916
and as you would expect the
file also has format objects


850
00:43:08,056 --> 00:43:10,586
describing the file's
data format.


851
00:43:11,176 --> 00:43:16,056
So first let's look
at AVAudioFormat.


852
00:43:17,186 --> 00:43:21,266
This class describes the actual
format of data you might find


853
00:43:21,266 --> 00:43:25,476
in an audio file or stream
and also the audio you,


854
00:43:25,476 --> 00:43:28,446
the format of the audio
you might be passing


855
00:43:28,446 --> 00:43:29,766
to and from APIs.


856
00:43:30,326 --> 00:43:32,386
So our low-level structure here


857
00:43:32,386 --> 00:43:34,476
for describing an
audio format is an


858
00:43:34,476 --> 00:43:36,166
AudioStreamBasicDescription,


859
00:43:36,686 --> 00:43:39,486
which in retrospect might have
been called "audio stream not


860
00:43:39,486 --> 00:43:41,976
so basic" or "audio stream
complete description"


861
00:43:42,476 --> 00:43:44,036
because there's a
lot of fields there,


862
00:43:44,036 --> 00:43:46,536
and it can be a little
challenging to get them all set


863
00:43:46,536 --> 00:43:49,066
up consistently especially
for PCM formats.


864
00:43:49,606 --> 00:43:52,166
But, you know, the beauty
of this structure is


865
00:43:52,226 --> 00:43:57,546
that it describes just about
everything we would want to use


866
00:43:57,596 --> 00:43:59,206
to describe an audio format.


867
00:43:59,856 --> 00:44:01,576
But, again, it's a
little challenging.


868
00:44:02,066 --> 00:44:05,476
But, in any case, you can
always create an AVAudioFormat


869
00:44:05,736 --> 00:44:07,526
from an
AudioStreamBasicDescription,


870
00:44:07,526 --> 00:44:09,556
which you might have
obtained from a low-level API.


871
00:44:10,246 --> 00:44:12,846
And you can always access
a stream description


872
00:44:13,246 --> 00:44:14,636
from an AVAudioFormat.


873
00:44:15,346 --> 00:44:18,666
But now we can move
on to other ways


874
00:44:18,726 --> 00:44:20,396
to interact with AVAudioFormat.


875
00:44:22,416 --> 00:44:28,976
So in the past we've had this
concept of canonical formats.


876
00:44:29,616 --> 00:44:32,426
And this concept goes
back to about 2002


877
00:44:32,426 --> 00:44:34,956
in Mac OS 10.0 or 10.1 or so.


878
00:44:35,766 --> 00:44:39,736
So this format was
floating-point, 32-bit,


879
00:44:40,036 --> 00:44:43,116
de-interleaved, but then
we got along to iOS,


880
00:44:43,356 --> 00:44:46,696
and we couldn't really
recommend using float everywhere


881
00:44:46,696 --> 00:44:48,496
because we didn't have the
greatest floating-point


882
00:44:48,496 --> 00:44:49,576
performance initially.


883
00:44:50,026 --> 00:44:54,866
So for a while canonical
was 8.24 fixed-point.


884
00:44:55,986 --> 00:44:58,646
But because of that
schism we want to reunite


885
00:44:58,646 --> 00:45:00,166
under something new now.


886
00:45:00,456 --> 00:45:03,266
We've deprecated the
concept of canonical formats.


887
00:45:03,666 --> 00:45:05,516
Now we have what we
call a standard format.


888
00:45:05,856 --> 00:45:09,776
We're back to non-interleaved
32-bit floats on both platforms.


889
00:45:10,136 --> 00:45:12,556
So this is the simplest way


890
00:45:12,556 --> 00:45:15,296
to construct an AVAudioFormat
now is with,


891
00:45:16,166 --> 00:45:17,856
you can create a standard format


892
00:45:17,906 --> 00:45:20,316
by specifying just a sample
rate and a channel count.


893
00:45:21,776 --> 00:45:24,926
You can also query any
AVAudioFormat you might come


894
00:45:24,926 --> 00:45:28,416
across and find out if it is
a standard format using the


895
00:45:28,416 --> 00:45:29,346
standard property.


896
00:45:32,166 --> 00:45:38,336
We've also provided for
using Common Formats


897
00:45:38,816 --> 00:45:40,276
with AVAudioFormat.


898
00:45:40,406 --> 00:45:43,726
And we define Common Formats
as formats you would often use


899
00:45:43,726 --> 00:45:46,726
in signal processing
such as 16-bit integers


900
00:45:47,236 --> 00:45:50,216
if you've been using that
on iOS or other platforms.


901
00:45:50,786 --> 00:45:52,866
We also provide for
64-bit floats.


902
00:45:53,126 --> 00:45:56,436
And it's very easy to create
an AVAudioFormat in one


903
00:45:56,436 --> 00:45:59,646
of these formats by
specifying which one you want,


904
00:45:59,776 --> 00:46:01,936
the sample rate channel count,
and whether it's inter-leaved.


905
00:46:02,606 --> 00:46:05,936
You can query any format to
see if it is some common format


906
00:46:06,356 --> 00:46:10,246
or something else using
the Common Format property.


907
00:46:10,916 --> 00:46:15,826
OK. So that's AVAudioFormat.


908
00:46:15,826 --> 00:46:17,586
Let's look at
AVAudioChannelLayout.


909
00:46:19,866 --> 00:46:22,936
Briefly here, this describes
the ordering or the roles


910
00:46:22,936 --> 00:46:25,376
of multiple channels, which
is especially important,


911
00:46:25,606 --> 00:46:27,006
for example, in surround sound.


912
00:46:27,416 --> 00:46:30,246
You might have left, right,
center, or you might have left,


913
00:46:30,246 --> 00:46:32,376
center, right, and so on.


914
00:46:32,376 --> 00:46:34,866
It's important to know the
actual order of the channels.


915
00:46:35,446 --> 00:46:39,226
So every AVAudioFormat may
have an AVAudioChannelLayout.


916
00:46:39,226 --> 00:46:42,216
And, in fact, when
constructing the AVAudioFormat,


917
00:46:42,806 --> 00:46:45,556
if you were describing three
or more channels you have


918
00:46:45,586 --> 00:46:46,966
to tell us what the layout is.


919
00:46:47,356 --> 00:46:51,096
So it becomes unambiguous to
anyplace else in the system


920
00:46:51,096 --> 00:46:53,386
that sees that AVAudioFormat
what the order


921
00:46:53,386 --> 00:46:54,546
of the channels are.


922
00:46:55,656 --> 00:46:59,126
So the underlying
AudioChannelLayout object is


923
00:46:59,316 --> 00:47:00,966
pretty much exposed
the way it is here.


924
00:47:00,966 --> 00:47:03,386
You can go look at that
in the CoreAudioTypes.h,


925
00:47:03,476 --> 00:47:04,976
but we have wrapped that up


926
00:47:04,976 --> 00:47:07,276
in the AVAudioChannelLayout
for you.


927
00:47:08,006 --> 00:47:14,466
OK. Moving on let's look
at AVAudioPCMBuffer.


928
00:47:20,026 --> 00:47:24,106
So buffer can be a sort of
funny term when we're dealing


929
00:47:24,106 --> 00:47:25,376
with de-interleaved audio


930
00:47:25,376 --> 00:47:27,686
because of the audioBufferList
structure,


931
00:47:28,196 --> 00:47:32,076
but that aside you can
think of it simply as memory


932
00:47:32,136 --> 00:47:33,476
for storing your audio data


933
00:47:33,566 --> 00:47:36,336
in any format including
non-interleaved formats.


934
00:47:36,646 --> 00:47:40,816
And here at the low-level
structures, which these ones


935
00:47:40,816 --> 00:47:43,856
in particular can also be
a bit of a bother to deal


936
00:47:43,856 --> 00:47:46,836
with because AudioBufferList
is variable length.


937
00:47:47,396 --> 00:47:50,226
So you can simply create
an AVAudioPCMBuffer.


938
00:47:50,506 --> 00:47:54,256
It'll create an audioBufferList
for you of the right size.


939
00:47:54,906 --> 00:47:58,186
And you can always fetch it back
out of the AVAudioPCMBuffer.


940
00:47:58,776 --> 00:48:02,526
Here's the initializer.


941
00:48:03,096 --> 00:48:06,206
So to create a buffer
you specify the format


942
00:48:06,476 --> 00:48:09,016
and a capacity in
audio sample frames.


943
00:48:10,166 --> 00:48:14,186
You can always fetch back the
buffer's format and the capacity


944
00:48:14,186 --> 00:48:15,586
with which it was constructed.


945
00:48:16,506 --> 00:48:21,086
And unlike audioBufferList,
which has a simple byte size


946
00:48:21,216 --> 00:48:24,496
for every buffer, here
we've separated the concept


947
00:48:24,496 --> 00:48:25,686
of capacity and length.


948
00:48:26,206 --> 00:48:28,916
So there's the fixed
capacity it was created with


949
00:48:29,456 --> 00:48:32,006
and the frame length,
which expresses the number


950
00:48:32,006 --> 00:48:36,226
of currently valid
frames in the buffer.


951
00:48:36,776 --> 00:48:38,486
Some more methods here.


952
00:48:38,646 --> 00:48:42,516
To get to the underlying samples
we provide these simple type


953
00:48:42,516 --> 00:48:43,856
safe assessors.


954
00:48:45,566 --> 00:48:49,326
And this is a good
time now to say a word


955
00:48:49,326 --> 00:48:52,376
about real-time safety
because these are properties.


956
00:48:52,376 --> 00:48:55,026
And as useful as they may be for
actually getting to the data,


957
00:48:55,426 --> 00:48:58,196
since they're properties they
may involve a method lookup,


958
00:48:58,866 --> 00:49:04,006
which can, in principle,
take a miss on the lookup


959
00:49:04,416 --> 00:49:06,456
and cause you to block.


960
00:49:06,676 --> 00:49:09,986
So if you're going to be
using AVAudioPCMBuffers


961
00:49:09,986 --> 00:49:13,966
on audio real-time threads,
it's best to cache these members


962
00:49:14,536 --> 00:49:17,286
in some safe context when you're
first looking at the buffer


963
00:49:18,136 --> 00:49:22,526
and use those cached members
on the real-time thread.


964
00:49:23,416 --> 00:49:27,316
OK. That's PCM Buffer.


965
00:49:27,316 --> 00:49:28,776
Now we can look at AudioFile,


966
00:49:28,816 --> 00:49:31,216
which wraps all these
other classes together.


967
00:49:33,376 --> 00:49:35,886
So here we let you
read and write files


968
00:49:35,886 --> 00:49:37,866
of any CoreAudio
supported format.


969
00:49:37,906 --> 00:49:42,546
This ranges from .M4A,
.MP4, .WAV, .CAF, .AIFF,


970
00:49:42,546 --> 00:49:45,816
and more I can't
think of right now.


971
00:49:46,106 --> 00:49:51,266
So in accessing the file, here
we give you a single way to read


972
00:49:51,266 --> 00:49:54,396
and write the file
completely independent


973
00:49:54,396 --> 00:49:56,176
of the file's actual
data format.


974
00:49:56,586 --> 00:50:00,766
So if it's an encoded format
like AAC or Apple Lossless


975
00:50:00,766 --> 00:50:04,406
or MP3, if there's a
codec on the system,


976
00:50:04,406 --> 00:50:07,766
and in most cases there is,
we will, transparently to you,


977
00:50:07,766 --> 00:50:11,206
decode from that format
as you read the file.


978
00:50:12,676 --> 00:50:15,806
Similarly when you're writing
an audio file we will encode


979
00:50:15,806 --> 00:50:19,106
from PCM into that encoded
format if we have an encoder.


980
00:50:21,146 --> 00:50:23,696
So to do this, the
file has this concept


981
00:50:23,696 --> 00:50:25,146
of the processing format.


982
00:50:25,696 --> 00:50:28,766
And the processing format
is simply the PCM format


983
00:50:29,086 --> 00:50:31,236
with which you will
interact with the file.


984
00:50:32,076 --> 00:50:35,736
So you specify the PCM format
when you create the file,


985
00:50:35,816 --> 00:50:38,646
and it has to be either a
standard or common format.


986
00:50:39,676 --> 00:50:41,066
The only limitation here is


987
00:50:41,156 --> 00:50:44,046
that we don't permit sample
rate conversion as you read


988
00:50:44,046 --> 00:50:45,826
from or write to a file.


989
00:50:46,076 --> 00:50:47,716
Your processing format
needs to be


990
00:50:47,716 --> 00:50:50,466
at the same sample rate
as the file itself.


991
00:50:51,006 --> 00:50:53,996
Now, if you're familiar with
the Audio Toolbox Extended Audio


992
00:50:53,996 --> 00:50:56,426
File API, this is
functionally very similar,


993
00:50:56,766 --> 00:50:59,906
and it's just a bit
simpler to use.


994
00:51:00,556 --> 00:51:04,446
So I'm looking now
at the initializers


995
00:51:05,046 --> 00:51:07,776
and some assessors
for AVAudioFile.


996
00:51:07,896 --> 00:51:10,366
Here's the initializer
for reading from a file.


997
00:51:10,626 --> 00:51:14,246
If you don't specify a
processing format you simply


998
00:51:14,246 --> 00:51:18,436
are, you get the
default behavior,


999
00:51:18,436 --> 00:51:20,456
which is that your
processing format will be a


1000
00:51:20,456 --> 00:51:21,466
standard format.


1001
00:51:23,796 --> 00:51:27,476
Very similarly to creating
an AVAudioFile for writing,


1002
00:51:27,536 --> 00:51:29,236
the only extra information
you need


1003
00:51:29,236 --> 00:51:30,916
to give us is a settings
dictionary.


1004
00:51:31,326 --> 00:51:33,516
This is the same
settings dictionary passed


1005
00:51:33,516 --> 00:51:35,006
to AV Audio Recorder.


1006
00:51:35,536 --> 00:51:37,226
And in there there are keys,


1007
00:51:37,646 --> 00:51:41,366
which specify the file format
you want to use, and in the case


1008
00:51:41,366 --> 00:51:45,036
of example, for example AAC
you can specify the bit rate


1009
00:51:45,446 --> 00:51:46,966
and any other encoder settings.


1010
00:51:47,016 --> 00:51:48,836
Those are in the
settings dictionary.


1011
00:51:50,776 --> 00:51:55,726
So once you've built a file
you can always access back the


1012
00:51:55,726 --> 00:51:57,826
actual file format on disk.


1013
00:51:58,186 --> 00:52:03,026
So that might be, for example,
AAC, 44 kHz, two channels.


1014
00:52:03,396 --> 00:52:06,446
But you can also query
the processing format


1015
00:52:07,056 --> 00:52:08,476
with which you created the file.


1016
00:52:08,756 --> 00:52:12,726
And in the case of the
two simplest initializers,


1017
00:52:13,446 --> 00:52:16,716
this would be floating-point,
32-bit,


1018
00:52:16,866 --> 00:52:18,316
same sample rate as the file.


1019
00:52:18,446 --> 00:52:20,176
Same channel count as the file.


1020
00:52:22,726 --> 00:52:24,826
OK. So to read and write


1021
00:52:24,886 --> 00:52:28,506
from AVAudioFiles there's a
simple method, readIntoBuffer,


1022
00:52:28,656 --> 00:52:31,596
and that will simply
fill the AVAudioPCMBuffer


1023
00:52:31,686 --> 00:52:33,806
to its capacity assuming
you have,


1024
00:52:33,876 --> 00:52:34,996
you don't hit the end of file.


1025
00:52:36,986 --> 00:52:39,256
writeFromBuffer is a
little in that it looks


1026
00:52:39,256 --> 00:52:43,546
like the buffer is frame length
rather than the capacity,


1027
00:52:43,546 --> 00:52:45,716
so it writes all
of the valid frames


1028
00:52:46,346 --> 00:52:47,996
from that buffer to the file.


1029
00:52:49,886 --> 00:52:53,816
And you can do random access I/O
when reading from audio files.


1030
00:52:54,486 --> 00:52:57,496
So this is like the
standard C-library's seek


1031
00:52:57,496 --> 00:53:00,946
and tell functions,
F seek and F tell.


1032
00:53:01,786 --> 00:53:04,346
You can query the frame
position to see where you are


1033
00:53:04,346 --> 00:53:05,586
when reading an audio file.


1034
00:53:06,126 --> 00:53:09,946
And you can also seek to a
different position in the file


1035
00:53:09,946 --> 00:53:13,596
by setting the frame position
pointer before you read.


1036
00:53:14,026 --> 00:53:17,416
And the next read will proceed
sequentially from that point.


1037
00:53:18,216 --> 00:53:24,846
OK. I'd like to tie all
these classes together now


1038
00:53:24,846 --> 00:53:25,896
with this short example.


1039
00:53:26,186 --> 00:53:27,526
And I've got four screens here.


1040
00:53:27,526 --> 00:53:31,476
We'll see what it's like
to open an audio file,


1041
00:53:31,476 --> 00:53:34,866
extract some basic
information from it and read


1042
00:53:34,866 --> 00:53:36,576
through every sample
in the file.


1043
00:53:36,966 --> 00:53:39,406
So here we have initForReading.


1044
00:53:39,956 --> 00:53:41,396
We simply pass the URL.


1045
00:53:41,936 --> 00:53:44,626
I'm using the variant
here that's explicit,


1046
00:53:44,626 --> 00:53:47,416
but I'm passing PCM
Float 32 always.


1047
00:53:47,786 --> 00:53:50,736
I could have left those off
and gotten a standard format.


1048
00:53:53,676 --> 00:53:56,296
I'm going to fetch some basic
information from the file


1049
00:53:56,296 --> 00:54:00,356
and print it, including
the files on disk format


1050
00:54:00,496 --> 00:54:01,926
and the processing format.


1051
00:54:03,056 --> 00:54:05,326
I can query the audio
file's length


1052
00:54:05,326 --> 00:54:07,326
and frames, sample frames.


1053
00:54:08,166 --> 00:54:12,306
And I can convert that length in
frames to a duration by dividing


1054
00:54:12,306 --> 00:54:13,596
by the file's sample rate.


1055
00:54:14,006 --> 00:54:17,016
OK. Next I'm going to create
a PCM Buffer to read from.


1056
00:54:17,676 --> 00:54:20,286
Since the file might be
large, I don't want to try


1057
00:54:20,286 --> 00:54:21,906
to read it all into
memory at once.


1058
00:54:22,046 --> 00:54:25,546
So I'm going to loop through it
128K sample frames at a time.


1059
00:54:26,336 --> 00:54:28,656
So I'm going to create a
buffer with that capacity.


1060
00:54:30,216 --> 00:54:31,176
And notice I'm just going


1061
00:54:31,176 --> 00:54:33,646
to pass the audio files
processing format.


1062
00:54:34,596 --> 00:54:36,856
When allocating this
buffer, and that ensures


1063
00:54:36,856 --> 00:54:38,616
that the buffer is
the same format


1064
00:54:38,616 --> 00:54:42,036
that the file will be giving me.


1065
00:54:43,226 --> 00:54:47,276
And here I'm ready to start
reading through the file.


1066
00:54:47,516 --> 00:54:51,346
And I'm going to read one buffer
at a time until I get to the end


1067
00:54:51,876 --> 00:54:55,066
so I can query the current frame
position and to see if it's less


1068
00:54:55,166 --> 00:54:57,516
than the length I
discovered earlier.


1069
00:54:58,476 --> 00:55:00,016
I can read into buffer,


1070
00:55:00,016 --> 00:55:02,706
which will again fill
the buffer to capacity.


1071
00:55:03,896 --> 00:55:06,456
I can double check to
see if I'm done by seeing


1072
00:55:06,456 --> 00:55:07,946
if I got a zero length buffer.


1073
00:55:10,306 --> 00:55:15,766
And this is a lot of code, but
it boils down to two for loops.


1074
00:55:15,996 --> 00:55:19,316
The outer one is walking
through all of the channels


1075
00:55:19,316 --> 00:55:21,146
in the buffer if it's
a multichannel file.


1076
00:55:22,676 --> 00:55:25,426
And then the inner-loop
will look


1077
00:55:25,426 --> 00:55:27,576
at every sample in that buffer.


1078
00:55:29,456 --> 00:55:32,856
So given every sample, I can
look at its absolute level


1079
00:55:32,856 --> 00:55:35,826
and see if it's the
loudest, or if it's louder


1080
00:55:35,826 --> 00:55:38,266
than the loudest sample I've
found so far, and if so,


1081
00:55:38,266 --> 00:55:41,066
I can record that level and
where I found it in the file.


1082
00:55:41,736 --> 00:55:45,056
So there, in about four screens
of code, I opened an audio file.


1083
00:55:45,056 --> 00:55:49,356
I read through the whole
thing one sample at a time.


1084
00:55:49,776 --> 00:55:53,686
OK. So moving on I'd like to
just sort of foreshadow the uses


1085
00:55:53,686 --> 00:55:59,796
of these classes in the
AVAudioEngine session,


1086
00:55:59,996 --> 00:56:01,636
which will follow this one.


1087
00:56:02,266 --> 00:56:04,706
So at the bottom
we see AVAudioFile


1088
00:56:04,706 --> 00:56:05,606
and AVAudioPCMBuffer.


1089
00:56:05,916 --> 00:56:06,936
And those are both used


1090
00:56:06,976 --> 00:56:09,266
by something called
AVAudioPlayerNode,


1091
00:56:09,846 --> 00:56:11,676
which will be your
basic mechanism


1092
00:56:11,726 --> 00:56:13,686
for scheduling audio
to play back.


1093
00:56:14,846 --> 00:56:17,696
If the AudioPlayerNode
is a subclass


1094
00:56:18,116 --> 00:56:22,056
of a more generic AVAudioNode
class, which is some unit


1095
00:56:22,056 --> 00:56:26,976
of audio processing, and we'll
see how AVAudioFormats are used


1096
00:56:26,976 --> 00:56:29,606
when describing how to
connect AVAudioNodes.


1097
00:56:32,696 --> 00:56:35,366
So that brings us to the end
of my section of this talk.


1098
00:56:35,676 --> 00:56:38,016
We saw the AVAudioFormat
ChannelLayout,


1099
00:56:38,256 --> 00:56:40,156
PCM Buffer and file classes.


1100
00:56:40,616 --> 00:56:44,206
You can use these without
AVAudioEngine using your


1101
00:56:44,206 --> 00:56:46,906
existing code with the
Core Audio, Audio Toolbox,


1102
00:56:46,906 --> 00:56:48,516
and Audio Unit C APIs.


1103
00:56:48,996 --> 00:56:51,176
If you're careful, just
do real-time saves,


1104
00:56:51,626 --> 00:56:53,496
and you can use those
assessor methods


1105
00:56:53,956 --> 00:56:57,226
to extract the low
level C structures.


1106
00:56:57,996 --> 00:57:00,786
And, again, we'll be seeing how
these are used in more detail


1107
00:57:00,786 --> 00:57:03,136
in the next session
on AVAudioEngine.


1108
00:57:03,996 --> 00:57:06,806
And that's the end
of our hour here.


1109
00:57:07,196 --> 00:57:08,696
We've looked at MIDI
over Bluetooth,


1110
00:57:09,086 --> 00:57:12,366
the Inter-App Audio UI
Views, lots of features


1111
00:57:12,366 --> 00:57:16,676
of AV Foundation audio,
and we hope you'll stick


1112
00:57:16,676 --> 00:57:18,686
around for the next
session on AVAudioEngine.


1113
00:57:21,496 --> 00:57:24,076
If you need more information,
Filip is our Evangelist,


1114
00:57:24,076 --> 00:57:25,586
and there are the
developer forums.


1115
00:57:27,686 --> 00:57:30,566
Here's the next session
I keep talking about.

