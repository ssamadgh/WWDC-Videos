1
00:00:27,156 --> 00:00:28,336
>> ERIC BIENVILLE: Good morning.


2
00:00:29,516 --> 00:00:32,746
[ Applause ]


3
00:00:33,246 --> 00:00:35,716
Welcome to the vector
numerics group session.


4
00:00:36,116 --> 00:00:38,426
My name is Eric Bienville
and I'm


5
00:00:38,536 --> 00:00:40,726
with the vector numerics group.


6
00:00:41,166 --> 00:00:46,656
Our group is providing
the accelerate framework


7
00:00:46,696 --> 00:00:51,726
which you already know and
some libraries in the system.


8
00:00:53,386 --> 00:00:56,176
In the accelerate framework,
you will find the image


9
00:00:56,416 --> 00:01:03,496
which is a collection
of hundreds of functions


10
00:01:04,166 --> 00:01:06,866
for image manipulation.


11
00:01:07,306 --> 00:01:11,456
You will also find vDSP
inside accelerate which is


12
00:01:11,456 --> 00:01:14,436
for signal processing
and three layers


13
00:01:14,436 --> 00:01:17,116
of linear algebra libraries.


14
00:01:18,106 --> 00:01:24,416
Outside of accelerate, we also
maintain the Math library, LibM,


15
00:01:25,076 --> 00:01:27,966
and the string functions.


16
00:01:28,656 --> 00:01:33,136
Last year we also introduced
SIMD which is a set of headers


17
00:01:33,136 --> 00:01:37,406
and functions providing
vector types directly mapping


18
00:01:37,406 --> 00:01:43,516
to CPU vector units
near SSC and AVX.


19
00:01:43,706 --> 00:01:47,796
Today we present you three new
additions to these functions.


20
00:01:48,246 --> 00:01:54,106
The first one, compression, is
a library for data compression;


21
00:01:54,896 --> 00:01:58,076
then Steve will present
addition to the SIMD library,


22
00:01:59,146 --> 00:02:01,296
and finally Luke will
present sparse BLAS,


23
00:02:02,196 --> 00:02:04,206
linear algebra for
sparse matrices.


24
00:02:05,376 --> 00:02:07,116
Let's start with compression.


25
00:02:08,256 --> 00:02:09,246
What is that?


26
00:02:10,596 --> 00:02:14,666
Compression is a new
library providing a unified


27
00:02:14,766 --> 00:02:17,766
and simple API for
lossless data compression.


28
00:02:18,426 --> 00:02:22,406
Why would you need such a
thing where if you already try


29
00:02:22,406 --> 00:02:25,496
to use compression in
your app, you first have


30
00:02:25,546 --> 00:02:30,346
to pick a compression
somewhere, read the manual,


31
00:02:30,346 --> 00:02:32,146
write the code, test it.


32
00:02:32,146 --> 00:02:35,276
If it doesn't work, you need


33
00:02:35,276 --> 00:02:38,036
to pick another one,
write the code again.


34
00:02:38,486 --> 00:02:39,446
That's a lot of trouble.


35
00:02:39,946 --> 00:02:44,286
And also sometimes, well,
actually usually you would have


36
00:02:44,346 --> 00:02:46,786
to include the compression
curve inside your app


37
00:02:47,156 --> 00:02:48,996
which is a maintenance
nightmare.


38
00:02:49,646 --> 00:02:51,326
So the idea is to
provide a wrapper


39
00:02:51,326 --> 00:02:57,406
to common compression algorithms
and their unified API and also


40
00:02:57,406 --> 00:02:59,726
with some benefits
like optimized code


41
00:03:00,596 --> 00:03:02,966
and easy switch between
algorithms.


42
00:03:03,606 --> 00:03:06,306
Let's start with the
algorithms we put inside.


43
00:03:07,676 --> 00:03:11,616
When we speak about compression
algorithms there are two metrics


44
00:03:11,616 --> 00:03:12,126
to consider.


45
00:03:12,126 --> 00:03:15,446
The first one is
obviously compression ratio.


46
00:03:15,896 --> 00:03:18,336
That is the ratio
between raw data size


47
00:03:18,856 --> 00:03:23,226
and the compressed payload
size, and then we are interested


48
00:03:23,226 --> 00:03:26,266
in the encoding speed
and the decoding speed.


49
00:03:27,086 --> 00:03:32,386
So to select algorithm, we
put them in one single graph.


50
00:03:34,096 --> 00:03:37,096
This graph, so on the X axis
you have the compression ratio,


51
00:03:38,136 --> 00:03:40,896
and on the Y axis,
the encoding speed.


52
00:03:40,896 --> 00:03:46,716
And in the middle there is a
reference, reference compressor.


53
00:03:46,906 --> 00:03:49,596
So all of the algorithms are
compared to this reference.


54
00:03:51,146 --> 00:03:53,236
So if you are on the
right, you compress better,


55
00:03:53,356 --> 00:03:56,776
if you are on top, you
compress faster, and that's it.


56
00:03:56,996 --> 00:03:59,056
So let's put some
algorithms inside here.


57
00:04:00,016 --> 00:04:02,206
The center one is zlib.


58
00:04:02,306 --> 00:04:04,456
That's the most used.


59
00:04:05,316 --> 00:04:06,646
And so what did we do?


60
00:04:06,646 --> 00:04:11,896
We chose the best compression,
this one, right, this one, LZMA,


61
00:04:11,896 --> 00:04:15,936
and the fastest one LZ4.


62
00:04:15,936 --> 00:04:22,266
You will see the gray line is
to show how it becomes more


63
00:04:22,266 --> 00:04:25,076
and more harder when you
want more compression.


64
00:04:25,736 --> 00:04:27,896
So more compression means slower


65
00:04:28,116 --> 00:04:32,646
and less compression means
faster, exponentially so.


66
00:04:33,586 --> 00:04:36,596
And if you look at that closely,
there is a first point here


67
00:04:37,886 --> 00:04:42,196
which is above zlib, that means
faster and a bit to the right,


68
00:04:42,196 --> 00:04:45,926
that means compresses better
that's LZFSE that's a new


69
00:04:45,926 --> 00:04:47,656
compressor we're
introducing today


70
00:04:49,356 --> 00:04:53,496
and a good alternative to zlib.


71
00:04:53,496 --> 00:04:56,506
I will talk about that later.


72
00:04:56,736 --> 00:04:58,356
These are the four
compressors we put


73
00:04:58,436 --> 00:05:00,136
in the compression library.


74
00:05:00,136 --> 00:05:05,606
And the decode looks like
that so that's the same thing,


75
00:05:05,696 --> 00:05:08,146
if you compress more,
you will need more time


76
00:05:08,146 --> 00:05:09,826
to decompress also.


77
00:05:11,136 --> 00:05:15,476
And our four compressors are
at the same position here.


78
00:05:16,036 --> 00:05:18,816
So these are the four
algorithms we selected to put


79
00:05:18,816 --> 00:05:20,306
in the compression library.


80
00:05:21,166 --> 00:05:25,446
LZMA for high compression,
but it's low.


81
00:05:25,806 --> 00:05:29,926
LZ4 for very fast compression,
but it doesn't compress


82
00:05:29,926 --> 00:05:35,596
that much, and in the middle you
have zlib and LZFSE for balance


83
00:05:35,596 --> 00:05:37,556
between compression
ratio and speed.


84
00:05:38,256 --> 00:05:42,116
So I'm matching different
use cases


85
00:05:42,116 --> 00:05:44,586
like for software distribution
you want to use LZMA


86
00:05:44,586 --> 00:05:48,166
because you will compress
once on the server and ship


87
00:05:48,166 --> 00:05:51,436
that so you want the shipping
part to be as small as possible.


88
00:05:52,096 --> 00:05:54,656
And then what did we do?


89
00:05:54,986 --> 00:05:58,326
We optimized some of
them for Apple hardware.


90
00:05:58,766 --> 00:05:59,926
What does that mean?


91
00:06:00,716 --> 00:06:01,176
Like that.


92
00:06:01,176 --> 00:06:04,726
So, for example, we
optimized zlib decoder,


93
00:06:04,726 --> 00:06:08,736
and the performance of the
zlib decoder we provide is


94
00:06:08,736 --> 00:06:15,246
compression is almost 1.6X
faster than the normal zlib.


95
00:06:16,006 --> 00:06:17,176
And the same goes with the LZ4.


96
00:06:17,176 --> 00:06:20,926
So that's another benefit.


97
00:06:20,926 --> 00:06:23,166
And you will also benefit
from security of data


98
00:06:23,166 --> 00:06:27,496
and performance update without
having to change your code.


99
00:06:27,956 --> 00:06:31,576
Let me talk briefly about
LZFSE, our new compressor.


100
00:06:32,306 --> 00:06:37,216
Why would would you
need a new compressor?


101
00:06:37,216 --> 00:06:39,816
First, its fun.


102
00:06:40,426 --> 00:06:44,006
And then, so why are
we optimizing zlib?


103
00:06:44,056 --> 00:06:49,656
We came to the conclusion in
the entropy part of the zlib,


104
00:06:50,076 --> 00:06:54,816
it is a bottle neck and there
is not much we can do to solve


105
00:06:54,816 --> 00:06:58,526
that except replacing
it with something else.


106
00:06:58,526 --> 00:06:59,436
So that's what we did.


107
00:07:00,126 --> 00:07:03,816
We took the new technology
called finer state entropy


108
00:07:03,936 --> 00:07:06,796
and replaced the part
of zlib with that.


109
00:07:07,326 --> 00:07:13,136
And the good news is that
LZFSE is able to map better


110
00:07:13,136 --> 00:07:17,356
on modern CPU architecture,
much better.


111
00:07:18,686 --> 00:07:24,166
And the design goal of that was,
match the zlib compression ratio


112
00:07:24,166 --> 00:07:26,016
and make this thing
as fast as we can.


113
00:07:26,796 --> 00:07:30,146
So let me show you
numbers about that.


114
00:07:30,546 --> 00:07:32,166
That's compression ratio.


115
00:07:32,256 --> 00:07:35,266
So by design it's
the same thing.


116
00:07:37,076 --> 00:07:42,276
This is energy efficiency,
both encode and decode, LZFSE.


117
00:07:43,366 --> 00:07:46,606
As you can see we are
2-point something times faster


118
00:07:46,606 --> 00:07:50,956
for encode and 2.5
times faster for decode,


119
00:07:50,956 --> 00:07:52,186
not faster, more efficient.


120
00:07:52,306 --> 00:07:53,486
What does that mean?


121
00:07:54,076 --> 00:07:55,686
Suppose you have a
full battery charge


122
00:07:55,686 --> 00:07:58,116
and nothing is compressing.


123
00:07:59,236 --> 00:08:01,096
With zlib you will
compress, let's say,


124
00:08:01,096 --> 00:08:03,936
5 terabytes of data before
the battery is empty.


125
00:08:04,436 --> 00:08:08,746
With LZFSE you would be able to
compress 12 terabytes with data


126
00:08:08,956 --> 00:08:09,976
with the same battery charge.


127
00:08:10,216 --> 00:08:12,376
That's what energy
efficiency means.


128
00:08:13,896 --> 00:08:15,596
Actually speed is
not bad either,


129
00:08:16,096 --> 00:08:20,086
because we can reach 2-point
something times faster


130
00:08:20,086 --> 00:08:22,496
for encoding and three
times faster for decoding


131
00:08:23,026 --> 00:08:24,216
with the same compression ratio.


132
00:08:25,676 --> 00:08:28,366
So that was about LZFSE.


133
00:08:28,366 --> 00:08:31,376
Now, let me show
you to use this API.


134
00:08:31,566 --> 00:08:32,576
There are two APIs.


135
00:08:32,576 --> 00:08:36,395
There is a buffer API to use
when you have all of the data


136
00:08:36,395 --> 00:08:37,866
at once in one single buffer.


137
00:08:38,626 --> 00:08:41,596
And there is streaming
API to use


138
00:08:41,596 --> 00:08:44,776
when receive the data in PCs.


139
00:08:46,166 --> 00:08:47,706
So buffer API is quite simple.


140
00:08:47,706 --> 00:08:52,896
It's like a super mem copy
thing, you give it a buffer


141
00:08:52,896 --> 00:08:55,976
and the number of bytes
inside, destination buffer


142
00:08:55,976 --> 00:08:59,886
and the capacity of it,
and you just give it also


143
00:09:00,096 --> 00:09:03,666
which algorithms
it needs to use.


144
00:09:03,836 --> 00:09:04,876
Here it says LZFSE.


145
00:09:05,446 --> 00:09:09,046
You see that switching between
compressors is changing this


146
00:09:09,046 --> 00:09:11,036
constant, you don't have
any code to rewrite.


147
00:09:11,036 --> 00:09:15,486
The new code is compression
encode buffer function,


148
00:09:15,866 --> 00:09:18,036
and it will return the
number of bytes put


149
00:09:18,036 --> 00:09:21,666
in the destination buffer
of 0 if something went wrong


150
00:09:21,666 --> 00:09:25,246
or there was not enough room
in the destination buffer.


151
00:09:26,366 --> 00:09:28,666
Decoding, that is
the same story.


152
00:09:29,276 --> 00:09:32,856
So you will pass a buffer
containing compressed payload


153
00:09:33,516 --> 00:09:36,766
and a buffer to receive
the decoded data.


154
00:09:38,726 --> 00:09:43,526
The difference here is it will
return the number of bytes put


155
00:09:43,936 --> 00:09:45,526
in the output buffer


156
00:09:46,026 --> 00:09:50,706
and if something fails actually
it will fill up the buffer.


157
00:09:50,706 --> 00:09:55,046
So it will just truncate
the output.


158
00:09:55,046 --> 00:09:56,496
Now, let me show
you the stream API,


159
00:09:56,586 --> 00:09:58,946
which is a little more complex.


160
00:10:00,146 --> 00:10:02,976
Because you will need to have
some state between the calls.


161
00:10:03,056 --> 00:10:06,776
So we will call the library
multiple times with new pieces


162
00:10:06,776 --> 00:10:08,486
of raw data to compress.


163
00:10:08,526 --> 00:10:10,556
It's like a candy factory.


164
00:10:10,556 --> 00:10:13,306
You send sugar inside and
get candies at the output.


165
00:10:13,436 --> 00:10:19,236
So you will have the
stream object to initialize


166
00:10:19,236 --> 00:10:21,416
and then cause a process
function many times


167
00:10:22,046 --> 00:10:26,176
and at the end you will
have to call a function


168
00:10:26,176 --> 00:10:29,036
to free the resources
used by the stream object.


169
00:10:30,216 --> 00:10:32,536
So let me show you
in detail with code.


170
00:10:32,536 --> 00:10:35,796
So first, we initialize the
stream object, you say okay,


171
00:10:35,796 --> 00:10:39,056
I want to encode, I want
to use LZFSE, initialize.


172
00:10:39,756 --> 00:10:43,746
Then you will have
to call process.


173
00:10:44,496 --> 00:10:49,746
Before calling process usually
in other compression libraries,


174
00:10:50,356 --> 00:10:53,316
you will have to tell it where
are the bytes to compress


175
00:10:53,316 --> 00:10:55,466
and where to put the output.


176
00:10:56,226 --> 00:11:00,606
And then you call process,
and it will, as the consumer,


177
00:11:00,606 --> 00:11:02,366
input or fill out the output


178
00:11:03,316 --> 00:11:05,856
and it will update
these fields for you.


179
00:11:06,786 --> 00:11:09,996
Then at the end, you need
to tell it, okay, I'm done,


180
00:11:10,046 --> 00:11:12,076
no more sugar, finalize.


181
00:11:13,196 --> 00:11:15,836
And after that, you are not
allowed to send more data


182
00:11:15,906 --> 00:11:18,186
to compress, but you may
need to call it several times


183
00:11:18,216 --> 00:11:20,666
to empty the pipes
and get the output.


184
00:11:21,616 --> 00:11:25,696
And at that point it will return
end, and which means in the end,


185
00:11:26,546 --> 00:11:29,516
and you will need to call
compression stream destroy


186
00:11:29,516 --> 00:11:32,336
to free the resources.


187
00:11:32,336 --> 00:11:32,916
So encode.


188
00:11:32,916 --> 00:11:34,926
Decode is actually
a bit simpler.


189
00:11:35,236 --> 00:11:38,306
You need the same
initialization code.


190
00:11:39,086 --> 00:11:42,456
This time that's with
decode instead of encode.


191
00:11:43,806 --> 00:11:47,246
Then you send the data and
it will figure out by itself


192
00:11:47,246 --> 00:11:48,676
if it's the end of
the stream or not.


193
00:11:48,676 --> 00:11:51,786
So at some point it will return
end, which means you got all


194
00:11:51,786 --> 00:11:55,266
of the data, and you
need to destroy again.


195
00:11:55,266 --> 00:11:56,446
And that's it.


196
00:11:56,996 --> 00:11:58,206
We are done with compression.


197
00:11:59,326 --> 00:12:03,496
So let me wrap up, that's a new
library providing a simplified


198
00:12:03,496 --> 00:12:06,036
and unified API on top
of several compressors.


199
00:12:06,036 --> 00:12:09,976
LZMA, LZ4, zlib, and LZFSE,


200
00:12:10,026 --> 00:12:14,186
which is to be preferred
over zlib.


201
00:12:15,486 --> 00:12:18,336
And LZFSE which is our new
high performance compressor.


202
00:12:18,876 --> 00:12:24,286
Thank you and let me call
Steve, who will tell us


203
00:12:24,286 --> 00:12:26,996
about improvements
to the SIMD library.


204
00:12:28,136 --> 00:12:28,456
Thank you.


205
00:12:29,516 --> 00:12:34,046
[ Applause ]


206
00:12:34,546 --> 00:12:34,966
>> STEVE CANON: Thanks, Eric.


207
00:12:35,726 --> 00:12:36,666
My name is Steve Canon.


208
00:12:37,166 --> 00:12:39,116
I'm an engineer on the vector
numerics group with Eric,


209
00:12:39,706 --> 00:12:42,476
and today I will talk about
SIMD, which is a library


210
00:12:42,476 --> 00:12:43,926
for two dimensional,
three dimensional,


211
00:12:43,926 --> 00:12:47,066
and four dimensional
vector math.


212
00:12:47,066 --> 00:12:52,496
We introduced SIMD last year in
Yosemite, and iOS 8 and it works


213
00:12:52,496 --> 00:12:54,906
in C, Objective-C, and C++


214
00:12:55,286 --> 00:12:57,556
and it closely mirrors the
Metal shading language.


215
00:12:58,036 --> 00:13:01,486
That means it's really easy
to write code that will run


216
00:13:01,486 --> 00:13:04,646
on the GPU in Metal and use
the same data structures


217
00:13:04,646 --> 00:13:06,906
and the same code on
the CPU using SIMD.


218
00:13:06,906 --> 00:13:11,276
It closely hews to the tradition
after shading languages


219
00:13:11,276 --> 00:13:14,156
and the way they work
with vectors and matrices.


220
00:13:14,536 --> 00:13:15,636
So what's new this year is


221
00:13:16,616 --> 00:13:17,946
that we have support
for Swift as well.


222
00:13:18,516 --> 00:13:22,956
[ Applause ]


223
00:13:23,456 --> 00:13:25,816
So everything from here on
in the talk or in my piece


224
00:13:25,816 --> 00:13:27,106
of the talk is going
to be Swift,


225
00:13:27,176 --> 00:13:28,186
all of the examples are Swift.


226
00:13:28,686 --> 00:13:32,596
For the most part, everything
is about exactly the same


227
00:13:32,826 --> 00:13:34,986
in C, Objective-C and C++.


228
00:13:35,476 --> 00:13:39,406
If you want to see in depth the
story for those languages check


229
00:13:39,406 --> 00:13:42,366
out last year's video session
and there are a few things


230
00:13:42,366 --> 00:13:43,876
where there are important
differences I will call


231
00:13:43,876 --> 00:13:45,106
out explicitly in the talk,


232
00:13:45,236 --> 00:13:48,266
but all of the examples
will be Swift.


233
00:13:48,576 --> 00:13:50,996
Why did we need to introduce
a new vector library?


234
00:13:51,166 --> 00:13:53,736
There were a bunch of vector
libraries on the platform


235
00:13:53,736 --> 00:13:55,886
and I have worked
on a bunch of them,


236
00:13:55,886 --> 00:13:57,136
so why do we add a new one?


237
00:13:57,166 --> 00:13:59,786
I will show you examples
of a few things


238
00:13:59,786 --> 00:14:01,246
that were a little
bit inconvenient


239
00:14:01,506 --> 00:14:02,846
with other vector libraries


240
00:14:02,846 --> 00:14:04,716
and how they are much
nicer using SIMD.


241
00:14:06,196 --> 00:14:07,956
BLAS is a great vector library.


242
00:14:08,106 --> 00:14:09,056
It's part of the accelerate.


243
00:14:09,056 --> 00:14:10,486
It's one the first
things I worked on.


244
00:14:10,596 --> 00:14:15,206
Here is an example of
multiplying a vector by a scaler


245
00:14:15,206 --> 00:14:17,296
and adding it to
another vector in BLAS.


246
00:14:17,956 --> 00:14:20,316
So we create two Swift arrays.


247
00:14:20,676 --> 00:14:21,976
Those will be our vectors.


248
00:14:22,656 --> 00:14:27,966
We can call this C BLAS Saxpy
function, which will multiply X


249
00:14:27,966 --> 00:14:30,686
by 2 and add to Y and
store the result in Y.


250
00:14:31,456 --> 00:14:35,406
There is a bunch of other
information we need to pass


251
00:14:35,506 --> 00:14:38,116
because this is an API
that takes raw coiners,


252
00:14:38,116 --> 00:14:40,486
it doesn't know anything
about lengths or strides;


253
00:14:40,856 --> 00:14:42,176
we need to provide
that information.


254
00:14:42,686 --> 00:14:44,816
There is also inefficiency
that comes


255
00:14:44,816 --> 00:14:46,956
from making an explicit
call to do this work.


256
00:14:46,956 --> 00:14:49,936
We will look at GLKit, which
is another great library,


257
00:14:49,936 --> 00:14:50,666
I love GLKit.


258
00:14:51,456 --> 00:14:53,746
It has explicit vector
types, so you don't just have


259
00:14:53,776 --> 00:14:56,696
to use raw arrays, but the
functions are very verbose.


260
00:14:56,696 --> 00:15:00,696
You have to call it GLK
vector 3 multiply scaler


261
00:15:01,216 --> 00:15:03,956
to do your arithmetic
and that's too wordy.


262
00:15:04,316 --> 00:15:06,046
Here is what it looks
like using SIMD.


263
00:15:09,476 --> 00:15:10,786
This is a lot nicer, right?


264
00:15:13,196 --> 00:15:16,956
You write down the arithmetic
you want to do and it works,


265
00:15:16,956 --> 00:15:18,526
you don't need to
call functions.


266
00:15:18,886 --> 00:15:19,596
Life is nice.


267
00:15:20,656 --> 00:15:21,866
So this is really nice.


268
00:15:22,206 --> 00:15:23,106
What else can you do?


269
00:15:24,936 --> 00:15:28,916
We have vectors of floats,
doubles, and 32 bit integers


270
00:15:28,966 --> 00:15:30,236
of lengths 2, 3, and 4.


271
00:15:31,126 --> 00:15:35,106
In C, Objective-C and C++
there are other vector types


272
00:15:35,106 --> 00:15:39,476
available, for now we
have the subset in Swift,


273
00:15:39,966 --> 00:15:42,796
and the reason we chose this
subset is it's what you have


274
00:15:42,826 --> 00:15:44,886
to use most often
to interoperate


275
00:15:45,096 --> 00:15:47,396
with other libraries for the
Metal programs you are going


276
00:15:47,396 --> 00:15:49,566
to write when you want to
use model I/O, when you want


277
00:15:49,566 --> 00:15:51,556
to do graphics stuff, these
are the types you want


278
00:15:51,556 --> 00:15:52,106
to have available.


279
00:15:53,606 --> 00:15:54,786
What can do you with
these types?


280
00:15:55,096 --> 00:15:56,716
You can create them, first off.


281
00:15:56,826 --> 00:15:58,986
We have a bunch of initializers
that do nice things,


282
00:15:58,986 --> 00:16:00,036
you can create the zero vector,


283
00:16:00,636 --> 00:16:03,356
you can explicitly specify
the elements of the vector,


284
00:16:03,676 --> 00:16:06,966
you can have a vector with
all of the components equal.


285
00:16:06,966 --> 00:16:08,646
There are lots of
nice initializers.


286
00:16:09,406 --> 00:16:10,656
You can do arithmetic.


287
00:16:10,896 --> 00:16:12,226
You have element-wise
arithmetic,


288
00:16:12,256 --> 00:16:13,356
the operators just work.


289
00:16:13,356 --> 00:16:16,926
So if I multiply two vectors I
get a new vector, each element


290
00:16:16,926 --> 00:16:20,096
of which, the value in
that element, is the result


291
00:16:20,096 --> 00:16:22,616
of multiplying the corresponding
elements of the input vectors.


292
00:16:23,006 --> 00:16:26,346
Divide, same thing, I
can multiply by a scaler.


293
00:16:26,896 --> 00:16:29,506
I can do a dot product,
cross product, et cetera.


294
00:16:29,556 --> 00:16:31,356
I already showed
you one example.


295
00:16:31,466 --> 00:16:33,876
I will show you another
simple example right now.


296
00:16:34,236 --> 00:16:36,316
This is a reflect
function that I might use


297
00:16:36,316 --> 00:16:37,466
in graphics frequently.


298
00:16:37,916 --> 00:16:41,216
It takes 1 vector
X and it reflects X


299
00:16:41,526 --> 00:16:43,936
through the plain perpendicular
to the normal vector N,


300
00:16:45,046 --> 00:16:47,556
and to write this we write down
the mathematical expression.


301
00:16:47,966 --> 00:16:52,986
Write X minus twice the dot
product of X and N times N,


302
00:16:53,936 --> 00:16:56,796
but you shouldn't actually have
to write this function, right?


303
00:16:56,796 --> 00:16:58,476
It should be available
to you already.


304
00:16:59,296 --> 00:17:00,226
And it is.


305
00:17:00,286 --> 00:17:02,466
We have a bunch of
geometry, shader,


306
00:17:02,466 --> 00:17:03,686
and math functions available.


307
00:17:04,286 --> 00:17:07,175
So we have dot product, cross
product, reflect, refract,


308
00:17:07,356 --> 00:17:10,226
distances, et cetera, all of
the stuff you want to use.


309
00:17:10,226 --> 00:17:12,296
If you have written
shader programs before,


310
00:17:12,425 --> 00:17:14,056
you have used these
functions a lot.


311
00:17:14,056 --> 00:17:17,236
This is just a standard
stuff you would use in Metal


312
00:17:17,236 --> 00:17:21,596
or open CL or GLSL or whatever
your favorite shader language


313
00:17:21,596 --> 00:17:25,796
is, and we also have a variety
of math functions available


314
00:17:25,796 --> 00:17:29,826
from accelerate for the float
4 type so you can use V sign F,


315
00:17:29,896 --> 00:17:33,396
VCosf, do Math functions
with these types.


316
00:17:34,156 --> 00:17:36,216
We have matrices
as well as vectors.


317
00:17:37,646 --> 00:17:40,516
Matrix types are float N
by N, and double N by M.


318
00:17:41,206 --> 00:17:43,996
So N is the number of columns
and M is the number of rows,


319
00:17:44,066 --> 00:17:46,406
if you are a mathematician,
this is weird.


320
00:17:46,406 --> 00:17:49,176
If you are a graphics
programmer, this is natural.


321
00:17:49,346 --> 00:17:52,506
So you guys will
feel right at home.


322
00:17:53,076 --> 00:17:55,006
N and M can be two, three,
or four, they don't need


323
00:17:55,006 --> 00:17:56,906
to be square matrixes
so, for instance,


324
00:17:56,906 --> 00:18:02,236
a float 2 by 3 is a matrix with
two columns and three rows.


325
00:18:03,486 --> 00:18:06,686
Again, we have a variety
of initializers for you,


326
00:18:07,196 --> 00:18:08,686
so you can create a zero matrix,


327
00:18:08,686 --> 00:18:10,136
you can create the
identity matrix,


328
00:18:10,266 --> 00:18:11,486
you can create a
diagonal matrix,


329
00:18:11,486 --> 00:18:14,516
by specifying the elements
of the diagonal if you want,


330
00:18:14,736 --> 00:18:18,246
all the elements if you want,
either as an array of arrays


331
00:18:18,636 --> 00:18:19,866
or as an array of vectors.


332
00:18:23,296 --> 00:18:24,286
All sorts of nice things.


333
00:18:25,116 --> 00:18:29,456
And I will show you a little
arithmetic example using


334
00:18:29,456 --> 00:18:30,306
matrices as well.


335
00:18:31,336 --> 00:18:33,866
Let's create a matrix
with 2s on the diagonal.


336
00:18:33,966 --> 00:18:37,386
This is a matrix where when you
multiply it will scale a vector


337
00:18:37,386 --> 00:18:37,806
by 2.


338
00:18:38,426 --> 00:18:42,306
Let's modify the last column
to put some values there.


339
00:18:42,306 --> 00:18:46,616
This is a transformation
matrix that scales vectors by 2


340
00:18:46,616 --> 00:18:51,286
and applies an offset as
well, it translates them.


341
00:18:52,166 --> 00:18:54,926
We can apply this to
a vector of all 1s.


342
00:18:59,066 --> 00:19:00,856
And we can also undo
the transform


343
00:19:00,856 --> 00:19:03,056
by using the inverse
property of our matrix


344
00:19:03,056 --> 00:19:05,486
to get the inverse
transformation to get back


345
00:19:05,526 --> 00:19:06,336
to the original vector.


346
00:19:07,746 --> 00:19:09,586
When you want to
interoperate between languages,


347
00:19:09,586 --> 00:19:12,406
you might have Objective-C
API like model I/O APIs


348
00:19:12,406 --> 00:19:13,666
that you want to call.


349
00:19:15,436 --> 00:19:19,266
The Swift vector types
are layout compatible


350
00:19:19,476 --> 00:19:20,986
with the Objective-C
vector types.


351
00:19:21,176 --> 00:19:22,276
What I mean by that is they,


352
00:19:22,276 --> 00:19:24,226
have exactly the same
representation in memory,


353
00:19:24,626 --> 00:19:26,386
and the compiler knows
that they are the same.


354
00:19:27,466 --> 00:19:32,056
So you don't need to do anything
to convert a Swift vector type


355
00:19:32,056 --> 00:19:34,596
into an Objective-C vector type
or an Objective-C vector type


356
00:19:34,636 --> 00:19:35,556
into a Swift vector type.


357
00:19:35,556 --> 00:19:38,706
So here I have an Objective-C
API that returns a vector,


358
00:19:38,966 --> 00:19:42,716
a SIMD vector, and I can
use it immediately in Swift.


359
00:19:43,066 --> 00:19:46,636
For matrices I have to
initialize a Swift matrix


360
00:19:47,066 --> 00:19:49,106
from the Objective-C
matrix that I get.


361
00:19:49,496 --> 00:19:51,476
It's a really cheap
operation, basically a copy


362
00:19:51,476 --> 00:19:53,726
because the layout is
the same, but I do need


363
00:19:53,726 --> 00:19:54,556
to call the initializer.


364
00:19:55,826 --> 00:19:57,876
When I want to pass Swift
types to Objective-C,


365
00:19:58,176 --> 00:19:59,506
the story is exactly the same.


366
00:20:00,216 --> 00:20:01,696
I can pass the vectors


367
00:20:02,076 --> 00:20:04,336
for matrices I use
the C matrix property


368
00:20:04,636 --> 00:20:07,686
to get an Objective-C
matrix that I can pass


369
00:20:07,756 --> 00:20:08,956
to my Objective-C API.


370
00:20:10,546 --> 00:20:12,356
So that's SIMD.


371
00:20:12,886 --> 00:20:14,266
SIMD is great for
when you want to work


372
00:20:14,266 --> 00:20:16,116
with small vectors and matrixes.


373
00:20:16,246 --> 00:20:19,136
Sometimes you want to work with
bigger vectors and matrixes too.


374
00:20:20,076 --> 00:20:22,436
So I will talk to
you about LAPACK,


375
00:20:22,566 --> 00:20:23,716
BLAS, and linear algebra.


376
00:20:24,096 --> 00:20:27,026
This is the fun part of
the talk; we have a break


377
00:20:27,026 --> 00:20:28,186
from the math nerdery.


378
00:20:30,616 --> 00:20:35,206
So LAPACK and BLAS are industry
standard math libraries.


379
00:20:35,796 --> 00:20:41,306
These are some of the oldest
APIs on the platform actually,


380
00:20:41,496 --> 00:20:43,996
which means they are a
little bit cryptic looking


381
00:20:44,506 --> 00:20:46,446
but there is tons of
documentation for them online


382
00:20:46,446 --> 00:20:48,096
because they have been
around for 40 years,


383
00:20:49,746 --> 00:20:53,286
and a lot of times you might
have some code from a library


384
00:20:53,286 --> 00:20:55,406
that you get that depends
on having these available,


385
00:20:55,636 --> 00:20:57,846
just link against
accelerate, it works fine.


386
00:20:58,156 --> 00:20:59,226
It's pretty easy to use.


387
00:20:59,856 --> 00:21:03,566
Linear algebra is a new
interface we introduced last


388
00:21:03,566 --> 00:21:06,536
year that has much simpler APIs


389
00:21:06,586 --> 00:21:09,016
for doing the most common
linear algebra operations.


390
00:21:09,396 --> 00:21:12,806
So this is exactly the same
operation on the last slide,


391
00:21:12,896 --> 00:21:15,306
it's solving a linear
system and rather


392
00:21:15,336 --> 00:21:18,296
than calling a cryptic
function with eight parameters,


393
00:21:18,296 --> 00:21:21,346
you call LA solve and
you give it the vector


394
00:21:21,446 --> 00:21:22,906
and matrix you want
to solve for.


395
00:21:24,446 --> 00:21:25,626
So that's very nice.


396
00:21:26,486 --> 00:21:31,546
The past few years we have done
a little talk about LINPACK


397
00:21:32,506 --> 00:21:34,346
and how fast we can
do linear algebra.


398
00:21:34,936 --> 00:21:38,486
The LINPACK benchmark says how
fast can you solve a system


399
00:21:38,486 --> 00:21:39,406
of linear equations?


400
00:21:39,446 --> 00:21:43,226
It's exactly the operation
we saw in that last slide.


401
00:21:44,486 --> 00:21:48,096
And historically, there have
been three different variations


402
00:21:48,096 --> 00:21:48,936
on this benchmark.


403
00:21:49,406 --> 00:21:52,766
It started out as how fast can
you solve a 100 by 100 system?


404
00:21:54,146 --> 00:21:56,836
But as computers became
more powerful and faster,


405
00:21:57,256 --> 00:22:00,166
it wasn't possible to actually
show how fast you were anymore


406
00:22:00,306 --> 00:22:01,436
with such a small problem.


407
00:22:01,436 --> 00:22:03,956
So it became a 1,000
by 1,000 system.


408
00:22:05,326 --> 00:22:07,196
And today when people
talk about LINPACK,


409
00:22:07,306 --> 00:22:09,426
they actually mean
no holds barred.


410
00:22:09,426 --> 00:22:12,136
You get to pick however
big a matrix you need


411
00:22:12,136 --> 00:22:13,196
to show off how fast you are.


412
00:22:13,836 --> 00:22:16,446
When you see supercomputer
rankings, they are talking


413
00:22:16,446 --> 00:22:18,326
about ratings that are
millions by millions


414
00:22:18,326 --> 00:22:20,106
when they give you
LINPACK scores.


415
00:22:20,106 --> 00:22:23,336
You just pick whatever
makes you fastest.


416
00:22:23,606 --> 00:22:25,496
So I was reading the
internet a few weeks ago


417
00:22:26,036 --> 00:22:27,236
and saw someone talking


418
00:22:27,346 --> 00:22:30,426
about now amazingly fast
their iPad Air 2 was.


419
00:22:30,806 --> 00:22:35,846
It got 1.8 gigaflops on LINPACK,
which sounds pretty impressive.


420
00:22:36,756 --> 00:22:41,396
It's 1.8 billion floating-point
operations per second.


421
00:22:41,396 --> 00:22:43,386
I know that this number is low.


422
00:22:43,806 --> 00:22:49,296
So someone had written a simple
C language routine to solve it


423
00:22:49,296 --> 00:22:51,146
and just taken whatever
the compiler gave them


424
00:22:51,746 --> 00:22:53,886
and that was what the
LINPACK score was.


425
00:22:54,426 --> 00:22:55,596
So I looked around
a little bit more.


426
00:22:55,596 --> 00:22:59,436
I found someone who had
carefully optimized their


427
00:22:59,436 --> 00:23:01,846
LINPACK routines; they
had written vector code,


428
00:23:01,996 --> 00:23:04,776
they had cache tiled, they
had done multithreading,


429
00:23:04,886 --> 00:23:08,466
and they got 5.6
gigaflops LINPACK.


430
00:23:08,996 --> 00:23:12,196
It's three times
faster, nice improvement,


431
00:23:13,176 --> 00:23:15,696
but rather than doing
all of that work,


432
00:23:16,676 --> 00:23:18,376
you could just call
accelerate instead.


433
00:23:18,566 --> 00:23:19,986
I showed you that D get RS


434
00:23:20,266 --> 00:23:22,426
and LA solve function
a few slides ago.


435
00:23:22,926 --> 00:23:27,396
You can write one line of code


436
00:23:29,826 --> 00:23:35,246
and if you do that,
you get 25 gigaflops.


437
00:23:36,516 --> 00:23:41,546
[ Applause ]


438
00:23:42,046 --> 00:23:43,926
So this is what we
really want to do.


439
00:23:43,926 --> 00:23:46,086
And this is what we want
to make easy for you guys.


440
00:23:46,086 --> 00:23:49,526
We want to make it so you can
write one line of code rather


441
00:23:49,526 --> 00:23:51,326
than having to hand
optimize everything,


442
00:23:52,116 --> 00:23:55,626
and get great energy usage
and great performance


443
00:23:55,626 --> 00:23:57,046
without needing to
do a lot of work.


444
00:23:58,036 --> 00:24:01,036
So now I'm going to hand you
over to Luke, who will tell you


445
00:24:01,036 --> 00:24:03,256
about what you can do when
your matrixes are even


446
00:24:03,256 --> 00:24:03,546
bigger [applause].


447
00:24:06,756 --> 00:24:09,356
>> LUKE CHANG: Thank you,
Steve, my name's Luke Chang.


448
00:24:09,636 --> 00:24:12,206
I'm an engineer on the
vector numerics group.


449
00:24:12,556 --> 00:24:15,106
I'm here today to talk
about Sparse BLAS.


450
00:24:17,406 --> 00:24:20,796
BLAS stands for basic linear
algebra solve programs.


451
00:24:21,556 --> 00:24:24,826
And sparse BLAS is BLAS
for sparse matrices.


452
00:24:25,656 --> 00:24:29,746
It's a new library
in iOS 9 and OS X.11.


453
00:24:31,016 --> 00:24:33,836
It's designed with simple
API and good performance.


454
00:24:34,296 --> 00:24:36,466
It supports both single
and double precision.


455
00:24:37,106 --> 00:24:40,476
Why do we need sparse BLAS?


456
00:24:41,266 --> 00:24:44,756
Can I use a dense BLAS
on the sparse matrix


457
00:24:45,126 --> 00:24:47,816
that I already know
how it works.


458
00:24:47,816 --> 00:24:50,326
Yes, it can given
that the dimension


459
00:24:50,326 --> 00:24:52,046
of your matrix is not large,


460
00:24:53,086 --> 00:24:56,576
but using Sparse BLAS is
better in so many ways.


461
00:24:57,626 --> 00:24:58,836
In order to demonstrate that,


462
00:24:59,806 --> 00:25:02,446
let me show you a
typical Sparse matrix.


463
00:25:03,156 --> 00:25:05,606
This is a typical matrix
from a machine learning.


464
00:25:06,036 --> 00:25:10,326
It has more than a million
rows and 200,000 columns.


465
00:25:11,046 --> 00:25:14,646
That gives us more than
300 billion entries.


466
00:25:15,566 --> 00:25:19,506
However, the density of
the matrix is just 0.05%.


467
00:25:20,726 --> 00:25:23,186
Here is a picture to
visualize the sparse matrix.


468
00:25:24,156 --> 00:25:26,316
The darker the blue,
the higher the density.


469
00:25:27,096 --> 00:25:28,856
As you can see there
is a lot of area


470
00:25:28,856 --> 00:25:29,916
in this picture that is white.


471
00:25:30,536 --> 00:25:31,486
They are just zeros.


472
00:25:31,986 --> 00:25:34,056
We don't want to waste
memory on those zeros.


473
00:25:35,216 --> 00:25:37,986
If you were to store
this sparse matrix


474
00:25:38,046 --> 00:25:39,596
in a regular matrix format


475
00:25:40,166 --> 00:25:42,446
so that you can use
the regular dense BLAS,


476
00:25:43,176 --> 00:25:47,476
assuming just single precision
point numbers it takes more


477
00:25:47,476 --> 00:25:51,386
than one terabyte of
memory, which is unfeasible.


478
00:25:51,526 --> 00:25:55,636
You don't want to do that
on your phone, on your iPad.


479
00:25:57,106 --> 00:26:03,796
So it saves memory, besides
sparse BLAS is also energy


480
00:26:03,796 --> 00:26:05,576
efficient and faster.


481
00:26:06,176 --> 00:26:12,436
We measure sparse BLAS
performance on a MacBook Pro,


482
00:26:12,436 --> 00:26:18,326
13 inch at density 0.5%,
sparse BLAS is more


483
00:26:18,326 --> 00:26:22,176
than 18 times more energy
efficient than dense BLAS.


484
00:26:23,396 --> 00:26:29,926
Performance is 15 times faster.


485
00:26:29,926 --> 00:26:32,976
As the density goes down,
performance goes up.


486
00:26:33,716 --> 00:26:39,476
At 0.05%, which is the density
we saw in the previous matrix,


487
00:26:40,046 --> 00:26:43,026
both energy efficiency
and performance are more


488
00:26:43,026 --> 00:26:47,116
than 100 times better than
using a regular dense BLAS.


489
00:26:48,516 --> 00:26:50,476
So now you see there are
many compelling reasons


490
00:26:50,476 --> 00:26:51,636
to use sparse BLAS.


491
00:26:51,636 --> 00:26:54,686
It saves memory, it's
energy efficient, faster.


492
00:26:55,516 --> 00:26:57,586
What are available
in sparse BLAS?


493
00:26:58,876 --> 00:27:03,306
We have products, triangular
solves, we can calculate norm,


494
00:27:03,696 --> 00:27:07,186
L1, L2, L infinity
norms, et cetera.


495
00:27:07,996 --> 00:27:10,646
We encounter trace, which
is the sum of diagonal.


496
00:27:11,376 --> 00:27:15,866
It can permute rows and
columns, add new values a matrix


497
00:27:16,006 --> 00:27:18,416
and extract rows and
columns from a matrix.


498
00:27:19,676 --> 00:27:22,396
Before I talk about
the operations,


499
00:27:22,686 --> 00:27:25,356
let's see how we
store a sparse vector.


500
00:27:26,606 --> 00:27:28,936
Like I said before, we
don't want to waste memory


501
00:27:28,936 --> 00:27:31,666
on the zero values, so we store


502
00:27:31,666 --> 00:27:34,006
in the non-zero values
like this.


503
00:27:34,526 --> 00:27:39,346
And then we need to know where
those non-zero values come from,


504
00:27:40,016 --> 00:27:42,586
so we store the indices
of non-zero values.


505
00:27:43,636 --> 00:27:45,896
Lastly we need to know
how many we stored


506
00:27:46,016 --> 00:27:48,036
so that's the number
of non-zero values.


507
00:27:49,146 --> 00:27:52,316
Those are the three things we
need to store a sparse vector.


508
00:27:52,916 --> 00:27:58,446
In order to make the conversion
from regular dense vector


509
00:27:58,446 --> 00:28:02,396
to a sparse vector, Sparse
BLAS provides utility functions


510
00:28:02,686 --> 00:28:03,906
to help you with the conversion.


511
00:28:04,086 --> 00:28:07,436
If you want to convert from
dense to sparse, you call pack.


512
00:28:08,156 --> 00:28:09,996
The other way around
you call unpack.


513
00:28:10,786 --> 00:28:12,966
If you just want to
know the non-zero count,


514
00:28:12,966 --> 00:28:16,116
you can call get
vector non-zero count.


515
00:28:16,376 --> 00:28:18,506
Now, we can look
at a sparse matrix.


516
00:28:18,506 --> 00:28:19,386
How do we store it?


517
00:28:20,396 --> 00:28:24,286
We can view the sparse matrix
as a collection of rows,


518
00:28:24,676 --> 00:28:26,426
or a collection of columns.


519
00:28:26,726 --> 00:28:30,146
So we have compressed sparse
row, compressed sparse columns,


520
00:28:30,976 --> 00:28:33,166
or we just forget
about rows and columns.


521
00:28:33,166 --> 00:28:33,946
Those are messy.


522
00:28:34,266 --> 00:28:37,586
I want to see the matrix as
a list of non-zero values,


523
00:28:37,766 --> 00:28:40,976
so for each non-zero value
we store the column index


524
00:28:40,976 --> 00:28:42,806
and row index.


525
00:28:43,766 --> 00:28:47,726
There are many other more
storage formats, and each one


526
00:28:47,726 --> 00:28:49,756
of them has its own
pros and cons.


527
00:28:50,596 --> 00:28:52,776
And there might be a
best storage format


528
00:28:52,776 --> 00:28:56,136
for a particular
operation you want to do.


529
00:28:56,376 --> 00:28:58,306
We decided to make
things easier for you.


530
00:28:59,516 --> 00:29:01,716
We defined a sparse
matrix data type.


531
00:29:02,386 --> 00:29:05,856
It is opaque pointer, so
if you want to use it,


532
00:29:05,856 --> 00:29:08,566
you have to first create it
using the create function,


533
00:29:09,146 --> 00:29:11,936
and you can operate on
it; after you are done,


534
00:29:12,146 --> 00:29:13,586
you have to destroy it.


535
00:29:14,206 --> 00:29:15,676
The benefit of doing this is


536
00:29:15,676 --> 00:29:18,006
that we manage the
memory for you.


537
00:29:18,346 --> 00:29:20,606
You don't have to worry
about allocating new memory


538
00:29:20,606 --> 00:29:22,916
when you are trying to add
new values to the matrix,


539
00:29:23,366 --> 00:29:27,046
or resize the buffer, or
freeing the buffer at the end.


540
00:29:27,266 --> 00:29:28,116
We will do that.


541
00:29:29,616 --> 00:29:33,046
Even better, we will pick
the best storage format.


542
00:29:33,696 --> 00:29:36,966
For example, if you
insert a bunch of rows


543
00:29:36,966 --> 00:29:40,206
into the sparse matrix, we
will store it as sparse row.


544
00:29:40,746 --> 00:29:44,876
And we will store it as sparse
columns when you insert a bunch


545
00:29:44,876 --> 00:29:49,256
of columns, and if you want
to do a particular operation


546
00:29:49,256 --> 00:29:52,076
and there is a best storage
format, we will convert it


547
00:29:52,186 --> 00:29:54,306
to that format automatically.


548
00:29:54,916 --> 00:29:58,756
So lastly, this enables us
to hide dimension details


549
00:29:58,756 --> 00:30:01,636
in our API, that
makes our API cleaner.


550
00:30:02,706 --> 00:30:04,696
When you call a sparse function,


551
00:30:04,696 --> 00:30:06,376
you don't have passing
the dimension


552
00:30:06,376 --> 00:30:09,516
of the sparse matrix every time.


553
00:30:09,776 --> 00:30:10,796
So here is an example.


554
00:30:12,366 --> 00:30:14,386
First, you create
a sparse matrix.


555
00:30:14,956 --> 00:30:18,986
And then you can insert a
value to a sparse matrix.


556
00:30:19,566 --> 00:30:23,856
Or you can insert sparse
vectors as rows or as columns.


557
00:30:25,126 --> 00:30:27,676
Then you commit the changes
to the sparse matrix.


558
00:30:27,896 --> 00:30:29,836
I will talk about
commit function


559
00:30:29,836 --> 00:30:31,306
in the next slide
in more detail.


560
00:30:32,196 --> 00:30:34,036
After you are done,
you call destroy.


561
00:30:35,306 --> 00:30:37,576
Okay. Why do we need
a commit function?


562
00:30:38,356 --> 00:30:42,936
Data insertion is expensive
because we store values


563
00:30:42,936 --> 00:30:44,156
in a compressed format.


564
00:30:44,766 --> 00:30:47,616
Every time you want to add a
value to a compressed storage


565
00:30:48,016 --> 00:30:49,866
that involves data movement,


566
00:30:50,256 --> 00:30:53,756
even more memory
allocation, that is expensive.


567
00:30:54,806 --> 00:30:56,986
So we want to delay all
of the data insertion


568
00:30:56,986 --> 00:30:58,546
so we can do them in batch.


569
00:30:59,106 --> 00:31:04,736
Good news is, even you forget
to call the commit function,


570
00:31:05,216 --> 00:31:07,556
the commit function will
be triggered automatically


571
00:31:08,026 --> 00:31:10,046
when you do an operation
on the matrix.


572
00:31:10,716 --> 00:31:13,186
Now, you may ask, then why
do I need a commit function?


573
00:31:13,456 --> 00:31:16,926
If you want to control the
performance of your code,


574
00:31:17,566 --> 00:31:20,116
you will want to use the
commit function explicitly.


575
00:31:20,666 --> 00:31:24,376
Let's say you want to get your
sparse matrix ready during the


576
00:31:24,376 --> 00:31:27,586
startup time of your app,
so your app can respond


577
00:31:27,586 --> 00:31:29,836
to the user input as
quickly as possible.


578
00:31:30,716 --> 00:31:34,626
Then you add the commit
function into your startup code.


579
00:31:35,106 --> 00:31:35,866
That will do the trick.


580
00:31:36,856 --> 00:31:37,466
All right.


581
00:31:37,896 --> 00:31:40,076
So in the interest of
time, I will only talk


582
00:31:40,076 --> 00:31:42,916
about two most common
operations in sparse BLAS.


583
00:31:42,916 --> 00:31:46,186
The first is product,
C equals A times B.


584
00:31:46,846 --> 00:31:49,766
We support vector inner
product, vector outer product,


585
00:31:50,306 --> 00:31:52,956
matrix vector products and
matrix-matrix products.


586
00:31:54,026 --> 00:31:55,896
What are the types for A, B, C?


587
00:31:56,236 --> 00:31:59,356
For inner product, A is
sparse, B is either sparse


588
00:31:59,356 --> 00:32:01,566
or dense, C is a single value.


589
00:32:02,486 --> 00:32:05,556
For outer product, A
is dense, B is sparse,


590
00:32:05,826 --> 00:32:08,176
and the result C
is a sparse matrix.


591
00:32:08,966 --> 00:32:11,366
For matrix vector and
matrix-matrix product,


592
00:32:11,496 --> 00:32:14,046
A is sparse, B is
dense, C is dense.


593
00:32:15,126 --> 00:32:18,546
It's very rare you need a
product of two sparse matrices


594
00:32:18,606 --> 00:32:19,576
so it's not supported.


595
00:32:20,126 --> 00:32:24,766
Here is the function
prototype of matrix product.


596
00:32:26,156 --> 00:32:28,136
The naming convention
of sparse function is,


597
00:32:28,406 --> 00:32:30,526
we start with sparse
underscore followed


598
00:32:30,526 --> 00:32:33,786
by the operation we want to do,
in this case is matrix multiply.


599
00:32:34,896 --> 00:32:38,226
Dense means B and C are
dense, float is the data type.


600
00:32:39,186 --> 00:32:42,156
And you will return
success or an error code.


601
00:32:42,966 --> 00:32:45,946
The argument of this function
is just like regular BLAS.


602
00:32:46,026 --> 00:32:49,546
You specify the order of B and
C, column major or row major.


603
00:32:49,756 --> 00:32:51,996
You say you want to
transpose on A or not,


604
00:32:52,786 --> 00:32:54,236
number of columns of B and C.


605
00:32:54,426 --> 00:32:55,966
The rest is just like BLAS.


606
00:32:56,936 --> 00:33:00,496
The next operation
is triangular solve.


607
00:33:01,146 --> 00:33:03,106
You solve the triangular
system of equations,


608
00:33:03,756 --> 00:33:08,136
so T has to be an upper or
lower triangular matrix.


609
00:33:09,186 --> 00:33:14,096
We support both dense and,
dense vector or matrix for B,


610
00:33:15,296 --> 00:33:17,756
one note to keep
in mind that upper


611
00:33:17,756 --> 00:33:21,296
or lower triangular property
must be set before you do


612
00:33:21,296 --> 00:33:22,126
data insertion.


613
00:33:23,036 --> 00:33:24,646
Here is the code
to highlight that.


614
00:33:25,636 --> 00:33:28,446
Set matrix property is
called before you do any


615
00:33:28,446 --> 00:33:29,296
data insertion.


616
00:33:29,796 --> 00:33:32,336
After data insertion, you
can call triangular solve,


617
00:33:32,826 --> 00:33:35,686
and the argument,
again, is just like BLAS.


618
00:33:36,256 --> 00:33:42,316
In summary, sparse BLAS we
design it with simple API,


619
00:33:42,616 --> 00:33:45,166
it has a wide range
of operations,


620
00:33:45,636 --> 00:33:46,886
and it has good performance.


621
00:33:48,196 --> 00:33:50,836
Okay. Now it's time to
wrap up our session.


622
00:33:51,776 --> 00:33:53,936
We talked about three
new libraries today,


623
00:33:54,646 --> 00:33:58,876
compression with our own
new compressor LZFSE.


624
00:33:59,686 --> 00:34:01,456
Now, you can use SIMD on Swift,


625
00:34:02,096 --> 00:34:04,156
and there is a sparse
BLAS library.


626
00:34:05,446 --> 00:34:07,226
They share the same
design goals.


627
00:34:07,836 --> 00:34:11,606
They are fast, energy
efficient, and easy to use.


628
00:34:12,166 --> 00:34:14,016
We encourage you
to give them a try


629
00:34:14,016 --> 00:34:15,466
and let us know what you think.


630
00:34:16,476 --> 00:34:18,726
We would love to hear from you.


631
00:34:18,946 --> 00:34:22,496
We take developer requests
and feedback very seriously.


632
00:34:22,766 --> 00:34:25,456
In fact, there are
many features we add


633
00:34:25,456 --> 00:34:27,335
to accelerate framework based


634
00:34:27,335 --> 00:34:30,056
on developer requests,
your requests.


635
00:34:30,466 --> 00:34:33,516
So if you see something
missing that you want to use,


636
00:34:33,826 --> 00:34:35,206
please file a feature request.


637
00:34:35,706 --> 00:34:39,636
For more information, we
have online documentation


638
00:34:39,636 --> 00:34:41,485
for vDSP and compression.


639
00:34:41,775 --> 00:34:45,536
If you want to know more about
other parts of accelerate,


640
00:34:45,866 --> 00:34:49,196
you can check out the video
of our previous WWDC sessions.


641
00:34:49,735 --> 00:34:53,326
We also have simple code
for compression vDSP.


642
00:34:53,326 --> 00:34:56,246
You can join the
discussion in the forum


643
00:34:56,656 --> 00:35:01,126
or any general inquiries,
Paul Dembo is our evangelist.


644
00:35:02,546 --> 00:35:06,786
Here are the related sessions
we talk about model I/0, Metal,


645
00:35:06,786 --> 00:35:11,126
Swift, in our talk you can check
out the videos of these sessions


646
00:35:11,126 --> 00:35:12,016
if you want to learn more.


647
00:35:12,476 --> 00:35:13,016
That's it.


648
00:35:13,596 --> 00:35:14,476
Thank you for coming.


649
00:35:14,476 --> 00:35:15,996
We look forward to
seeing you in the Lab.


650
00:35:15,996 --> 00:35:16,876
Thank you very much!


651
00:35:18,516 --> 00:35:32,680
[ Applause ]

