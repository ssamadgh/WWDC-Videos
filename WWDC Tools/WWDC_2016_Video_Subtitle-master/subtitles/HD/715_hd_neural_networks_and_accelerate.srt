1
00:00:06,516 --> 00:00:18,500
[ Music ]


2
00:00:23,716 --> 00:00:26,626
>> Good afternoon, and welcome
to the Accelerate Session.


3
00:00:27,456 --> 00:00:31,226
My name is Eric Bainville,
and I'm with the Core OS,


4
00:00:31,226 --> 00:00:32,336
Vector and Numerics group.


5
00:00:34,246 --> 00:00:38,116
Our group provides
performance libraries for CPU.


6
00:00:38,986 --> 00:00:42,646
Performance libraries means
we usually are [inaudible] can


7
00:00:42,646 --> 00:00:46,366
provide very compute intense
functions like matrix products,


8
00:00:46,686 --> 00:00:48,196
[inaudible] transform,
these kind of things.


9
00:00:49,586 --> 00:00:51,616
Most, most of these
functions are inside the


10
00:00:51,616 --> 00:00:52,656
Accelerate framework.


11
00:00:52,896 --> 00:00:54,536
We find for example, image,


12
00:00:54,536 --> 00:00:57,006
which is an image
processing library.


13
00:00:57,766 --> 00:00:59,976
It will transform between types


14
00:00:59,976 --> 00:01:02,916
and also do geometric
transformations on images.


15
00:01:03,836 --> 00:01:06,706
Next to vImage you will
find vDSP, which is more


16
00:01:06,706 --> 00:01:08,856
for signal processing,
[inaudible] transforms,


17
00:01:09,526 --> 00:01:11,386
and other signal
processing functions.


18
00:01:13,426 --> 00:01:18,736
Then we have the BLAS, which
is linear algebra library.


19
00:01:18,736 --> 00:01:20,076
It's very old.


20
00:01:20,146 --> 00:01:22,286
It comes straight from the 70's.


21
00:01:22,286 --> 00:01:24,266
Few years ago we added
Sparse, SparseBLAS,


22
00:01:24,266 --> 00:01:27,026
SparseBLAS computation on
the vector and matrices.


23
00:01:27,616 --> 00:01:30,376
We also provided LAPACK
in the LinearAlgebra,


24
00:01:30,516 --> 00:01:32,796
which is higher level
linear algebra library.


25
00:01:34,196 --> 00:01:39,166
Outside of Accelerate, we also
have a few libraries, like simd,


26
00:01:39,166 --> 00:01:42,766
which is a set of headers
providing you direct access


27
00:01:42,796 --> 00:01:45,366
to vector instructions
and vector types,


28
00:01:45,366 --> 00:01:47,376
and they are not
directed to vector units


29
00:01:47,376 --> 00:01:50,346
in the CPU's [inaudible]
without writing [inaudible]


30
00:01:50,556 --> 00:01:53,276
or assembly code.


31
00:01:54,356 --> 00:01:56,726
We also have compression,
which we introduced last year


32
00:01:56,866 --> 00:01:57,946
for lossless compression,


33
00:01:58,876 --> 00:02:01,776
and everything we
provide is optimized


34
00:02:01,776 --> 00:02:03,226
for all the CPU's we support.


35
00:02:03,706 --> 00:02:06,296
So when you get a new phone,
we optimize a code for it


36
00:02:06,916 --> 00:02:08,636
so you don't have to do it.


37
00:02:09,045 --> 00:02:11,996
Alright. Today, first, we start


38
00:02:11,996 --> 00:02:13,826
with a quick refresh
on compression.


39
00:02:14,866 --> 00:02:18,476
Then we'll introduce two new
libraries inside Accelerate;


40
00:02:19,106 --> 00:02:22,266
BNNS, which is a set of
low level compute functions


41
00:02:22,266 --> 00:02:26,856
for neural networks;
and the Quadrature,


42
00:02:27,466 --> 00:02:29,686
which is a small
library dedicated


43
00:02:29,686 --> 00:02:31,736
to numerical integration
of functions;


44
00:02:32,146 --> 00:02:34,166
and then my colleague,
Steve, will come on stage


45
00:02:34,286 --> 00:02:36,356
and introduce new
additions to simd.


46
00:02:37,756 --> 00:02:39,366
Well, first, before we start,


47
00:02:39,566 --> 00:02:42,036
let me tell you quickly
how to use Accelerate.


48
00:02:42,946 --> 00:02:46,866
So depending on your language of
choice, you will have to import


49
00:02:46,866 --> 00:02:49,746
or include the, the
declarations of the functions,


50
00:02:50,546 --> 00:02:53,786
and then you need to link
with the Accelerate framework.


51
00:02:54,066 --> 00:02:57,656
So from Xcode in your project
settings you will navigate


52
00:02:57,826 --> 00:03:01,986
on your target, and then you
click on the, on build phases,


53
00:03:02,876 --> 00:03:04,056
and as a window appears,


54
00:03:04,326 --> 00:03:06,826
you click on link
binary with libraries.


55
00:03:06,826 --> 00:03:07,456
You open that.


56
00:03:08,646 --> 00:03:10,856
Alright. And those
are the ones appear


57
00:03:10,886 --> 00:03:14,566
with the little plus here,
and you will get a list


58
00:03:14,566 --> 00:03:17,016
of all the libraries and
frameworks you can link with,


59
00:03:17,016 --> 00:03:18,386
and Accelerate is the first one.


60
00:03:20,516 --> 00:03:24,546
[ Applause ]


61
00:03:25,046 --> 00:03:27,466
Yes. Alright.


62
00:03:27,496 --> 00:03:28,846
Well, if it's not the first one,


63
00:03:28,846 --> 00:03:30,596
there's a search
bar on top of it.


64
00:03:30,596 --> 00:03:32,856
So if you look for compression,
you type compression,


65
00:03:32,856 --> 00:03:33,836
I knew we'd get compression.


66
00:03:34,516 --> 00:03:36,516
Alright. So that's, now
you're using Accelerate.


67
00:03:36,866 --> 00:03:41,266
OK. Yes. Then compression,
remember last year


68
00:03:41,436 --> 00:03:45,726
when Sebastian introduced the
LZFSE's the platform state


69
00:03:45,726 --> 00:03:48,306
of the union where we still
don't know who these guys are,


70
00:03:48,826 --> 00:03:53,396
but today we're announcing
that LZFSE is open source.


71
00:03:53,936 --> 00:03:55,396
So you will find
it on the github,


72
00:03:55,836 --> 00:03:58,026
and we publish it
under BSD license.


73
00:03:59,206 --> 00:04:02,976
Just let me remind you why
you will want to use that.


74
00:04:03,726 --> 00:04:07,006
So these are the platform
comparison between LZFSE


75
00:04:07,006 --> 00:04:09,446
and zlib compared
to the same options.


76
00:04:10,026 --> 00:04:14,416
So we are 1.4x faster for encode
and 2.6x faster for decode.


77
00:04:14,866 --> 00:04:17,906
Alright. That was
our compression.


78
00:04:18,315 --> 00:04:19,805
Now let's jump to BNNS.


79
00:04:20,286 --> 00:04:21,896
Basic neural networks
subroutines.


80
00:04:22,886 --> 00:04:24,636
So the name looks
really like BLAS.


81
00:04:24,676 --> 00:04:26,906
BLAS means basic linear
algebra subroutines.


82
00:04:27,196 --> 00:04:29,506
As I said, it comes
straight from the 70's,


83
00:04:30,726 --> 00:04:33,866
and BNNS provides a, a set


84
00:04:33,866 --> 00:04:35,946
of very low-level
computation routines.


85
00:04:36,106 --> 00:04:37,636
So I will [inaudible] later.


86
00:04:37,726 --> 00:04:42,186
We do only low level
computations


87
00:04:42,186 --> 00:04:44,726
like matrix products but
dedicated to neural networks.


88
00:04:46,046 --> 00:04:48,626
Before I enter into
details of what it does


89
00:04:49,106 --> 00:04:50,346
and what the API is,


90
00:04:50,346 --> 00:04:54,606
let me remind you how
neural network works.


91
00:04:55,366 --> 00:04:58,746
Let's say we have this
network, which is trained


92
00:04:58,746 --> 00:05:00,756
to recognize animals, OK.


93
00:05:00,756 --> 00:05:02,546
So on input you will
have an image,


94
00:05:02,676 --> 00:05:04,436
and then you have
this big orange box.


95
00:05:04,896 --> 00:05:08,706
Inside the orange box, the red
box represents the, the weights.


96
00:05:09,216 --> 00:05:12,486
These are the parameters of,
of the network, and on output,


97
00:05:12,486 --> 00:05:14,386
this thing we output
four values,


98
00:05:14,716 --> 00:05:17,016
which are the probability
of having a cat, a dog,


99
00:05:17,116 --> 00:05:18,686
a giraffe, or a snake.


100
00:05:19,496 --> 00:05:21,146
OK. So first you
need to train it.


101
00:05:21,396 --> 00:05:22,816
Let's say you have a cat image.


102
00:05:23,746 --> 00:05:27,526
You process the cat image to the
network, and you get an answer.


103
00:05:28,486 --> 00:05:29,816
So that's the highest
probability.


104
00:05:29,816 --> 00:05:30,516
It says dog.


105
00:05:31,556 --> 00:05:33,256
Well, that's what you
need to train them.


106
00:05:33,886 --> 00:05:37,016
So it was a cat,
and what you do is


107
00:05:37,516 --> 00:05:39,326
that you [inaudible]
the good answer,


108
00:05:39,606 --> 00:05:41,706
and what if I slightly
modify the weights.


109
00:05:42,476 --> 00:05:47,186
So the networks now will go
slightly in the cat direction,


110
00:05:47,826 --> 00:05:53,586
and you need to do this millions
of times with a lot of images,


111
00:05:53,916 --> 00:05:58,276
and at some point, you will
have a trained network answering


112
00:05:58,276 --> 00:06:00,736
correctly for, for
on every request.


113
00:06:01,036 --> 00:06:05,886
So if it was a, a giraffe,
and it says giraffe


114
00:06:05,886 --> 00:06:07,036
because we trained it properly.


115
00:06:08,066 --> 00:06:10,936
Alright. So that's
what the network does,


116
00:06:11,046 --> 00:06:13,626
and notice the difference
between training and inference.


117
00:06:13,626 --> 00:06:15,276
During inference, we
didn't change the weight.


118
00:06:16,086 --> 00:06:17,986
So usually let's say
you have this as an app.


119
00:06:19,016 --> 00:06:21,586
What will happen is that you
will do the training offline


120
00:06:22,076 --> 00:06:23,876
when you build your app,
you will do your training,


121
00:06:23,876 --> 00:06:25,966
and then when you shift the
app, you will shift the weights


122
00:06:26,356 --> 00:06:29,156
and the, the network
apology, and what happens


123
00:06:29,156 --> 00:06:31,706
on the device is on
this inference part.


124
00:06:32,616 --> 00:06:35,646
OK. What's inside this
orange box, though?


125
00:06:35,646 --> 00:06:36,856
So let me show you an example.


126
00:06:36,856 --> 00:06:39,326
If you have been to the
What's New in Metal,


127
00:06:39,976 --> 00:06:42,936
Part II speech yesterday, you
have seen a bigger example of,


128
00:06:43,286 --> 00:06:45,356
that was a scene
recognition network,


129
00:06:45,746 --> 00:06:47,896
and lots of demo [inaudible]
recognition network,


130
00:06:48,186 --> 00:06:50,686
these are more advanced networks
with hundreds of layers.


131
00:06:51,236 --> 00:06:54,896
This is state of the art from
five years ago, and this,


132
00:06:54,936 --> 00:06:58,986
this describes a, a
[inaudible] recognition network.


133
00:06:59,616 --> 00:07:03,316
So on input you have the small
image, which is a capture


134
00:07:03,316 --> 00:07:06,776
of something written, and then
it goes to different layers.


135
00:07:07,526 --> 00:07:09,516
Here you have a 5
x 5 convolution,


136
00:07:10,136 --> 00:07:12,326
and the output will be
a stack of five images.


137
00:07:13,436 --> 00:07:15,386
That's an output of five
different convolutions


138
00:07:15,386 --> 00:07:19,806
with different weights, and
then you add another layer.


139
00:07:19,846 --> 00:07:23,376
You take these five images,
apply a lot of convolutions


140
00:07:23,376 --> 00:07:25,496
to them, and you will
get another bigger stack


141
00:07:26,206 --> 00:07:28,956
of fifty images with
5 x 5 pixels.


142
00:07:29,346 --> 00:07:33,866
So what's happening here is
you go from the image space


143
00:07:33,926 --> 00:07:36,876
into the feature space
one layer at a time.


144
00:07:37,516 --> 00:07:40,006
So the content of this
small images becomes more


145
00:07:40,006 --> 00:07:42,846
and more abstract, and at the
end what you have is the feature


146
00:07:43,336 --> 00:07:45,306
which will tell you that's
a zero, that's the one.


147
00:07:45,536 --> 00:07:46,866
This is what you want on output.


148
00:07:47,536 --> 00:07:50,816
OK. And so with a very
small number of layers,


149
00:07:51,356 --> 00:07:53,986
you can really do that,
and it works really well.


150
00:07:54,656 --> 00:07:59,086
So after these convolutions,
we take this 5 x 5 x 50 values,


151
00:07:59,976 --> 00:08:02,976
take them as a single big vector


152
00:08:03,036 --> 00:08:05,156
and apply a fully
connected layer,


153
00:08:05,156 --> 00:08:07,146
which is just a big
metrics product,


154
00:08:07,146 --> 00:08:09,736
and it will mix all
these values together


155
00:08:09,736 --> 00:08:11,666
and produce a set of 100 values.


156
00:08:12,446 --> 00:08:15,206
That's called the hidden
layer in this specific model,


157
00:08:16,446 --> 00:08:21,516
and then you need a last one,
which will make this 100 values,


158
00:08:22,006 --> 00:08:24,696
mix them together, and produce
the ten outputs you want.


159
00:08:24,916 --> 00:08:27,936
So at this point you are
computing future space each


160
00:08:27,936 --> 00:08:31,036
value here is a probability
of being one specific digit.


161
00:08:32,015 --> 00:08:34,645
Alright. So that's what
is inside the network,


162
00:08:34,645 --> 00:08:37,466
and as you have seen here,
we have two different kinds


163
00:08:37,466 --> 00:08:40,206
of layers, and this is
what we do inside of BNNS.


164
00:08:40,206 --> 00:08:42,606
We provide the compute
part of these layers.


165
00:08:43,216 --> 00:08:47,286
OK. Before we start about
what we really compute


166
00:08:47,286 --> 00:08:49,406
and what the API is, let
me show you some numbers.


167
00:08:49,476 --> 00:08:52,306
So this, this, here
[inaudible] , Caffe,


168
00:08:52,306 --> 00:08:54,316
which is a well-known package


169
00:08:54,316 --> 00:08:56,786
for [inaudible] network
computation,


170
00:08:57,416 --> 00:08:59,326
and this is a convolution
part of Caffe.


171
00:08:59,586 --> 00:09:02,026
So we have 14 different
convolution sizes.


172
00:09:02,576 --> 00:09:03,396
Here, you can read them.


173
00:09:04,586 --> 00:09:10,776
And this is the time Caffe takes
to, actually, that's the speed.


174
00:09:10,776 --> 00:09:11,676
So higher is better.


175
00:09:11,676 --> 00:09:13,706
This is Caffe on
this convolutions,


176
00:09:14,086 --> 00:09:15,676
and this is what
you get with BNNS.


177
00:09:16,776 --> 00:09:18,696
So on average, it's 2.1x faster,


178
00:09:18,696 --> 00:09:21,086
and if you have even
bigger convolutions,


179
00:09:21,086 --> 00:09:23,756
you can even reach
almost four times faster.


180
00:09:25,436 --> 00:09:27,976
Alright. So that
was all the numbers.


181
00:09:28,106 --> 00:09:30,726
Now let me tell you
what is inside BNNS.


182
00:09:32,106 --> 00:09:34,146
So that's a set of low
level compute functions.


183
00:09:34,146 --> 00:09:35,246
Very similar to BLAS.


184
00:09:35,246 --> 00:09:37,076
That's why we named it BNNS.


185
00:09:37,746 --> 00:09:41,036
It doesn't really know what's,
what is a neural network.


186
00:09:41,316 --> 00:09:43,646
That's, that's your
side of the equation.


187
00:09:43,786 --> 00:09:48,506
What it does is provide really
only the compute part of it.


188
00:09:48,746 --> 00:09:50,856
OK. And it only,
only does inference.


189
00:09:51,856 --> 00:09:54,356
Actually I'm not sure
it would make sense to,


190
00:09:54,356 --> 00:09:55,696
to run training on the device.


191
00:09:55,696 --> 00:09:56,786
That's, that's too expensive.


192
00:09:56,786 --> 00:09:58,976
You need millions of images
and million of computations.


193
00:09:59,686 --> 00:10:00,406
That wouldn't fit.


194
00:10:00,956 --> 00:10:04,106
So usually inference would be
done offline, and as I told,


195
00:10:04,456 --> 00:10:07,866
you will just do the inference
part on your, in your app.


196
00:10:08,156 --> 00:10:09,846
And we provide three
different types of layers.


197
00:10:10,446 --> 00:10:12,116
Convolution layers,
pooling layers,


198
00:10:12,326 --> 00:10:13,896
and the fully-connected layers.


199
00:10:14,386 --> 00:10:18,296
Why? Well, actually in
the, in the modern network,


200
00:10:18,296 --> 00:10:20,046
you spend more than 75 percent


201
00:10:20,046 --> 00:10:21,826
of the time computing
convolutions,


202
00:10:22,476 --> 00:10:24,786
and then the next one on
the list of pooling layers


203
00:10:24,786 --> 00:10:26,486
with something like 15 percent,


204
00:10:27,156 --> 00:10:29,566
and fully-connected layers
also take a lot of time


205
00:10:29,566 --> 00:10:32,726
but usually you find them only
at the end of the network, like,


206
00:10:33,296 --> 00:10:35,436
like we have seen in the
example where we only two,


207
00:10:35,816 --> 00:10:37,646
two fully-connected
layers at the end,


208
00:10:38,236 --> 00:10:40,706
but this did take a lot of time.


209
00:10:40,816 --> 00:10:44,826
OK. Now that we know what
is inside, I will enter


210
00:10:44,826 --> 00:10:47,296
into the details of the three
different kind of layers


211
00:10:47,536 --> 00:10:51,316
and what we compute and how to
create them through the API.


212
00:10:51,316 --> 00:10:54,856
Let's start with the
convolution layers.


213
00:10:55,316 --> 00:10:56,586
So this is a convolution.


214
00:10:56,586 --> 00:11:01,156
It takes an input image, a block
of weight that the orange matrix


215
00:11:01,156 --> 00:11:05,346
on the middle, and each pixel
in the output image is computed


216
00:11:05,346 --> 00:11:08,346
by taking a block of
input pixels, multiply,


217
00:11:08,576 --> 00:11:10,306
multiplying them by
the, the weights,


218
00:11:10,686 --> 00:11:11,846
and then you take
the sum of that,


219
00:11:11,846 --> 00:11:13,146
and you get your upper pixel.


220
00:11:13,776 --> 00:11:16,226
And you need to do that
for every output pixel.


221
00:11:16,866 --> 00:11:19,236
So if you count, that's
a four dimensional loop


222
00:11:19,236 --> 00:11:21,166
because you need
to loop on x, y,


223
00:11:21,446 --> 00:11:23,066
and then on the kernel
dimensions.


224
00:11:24,096 --> 00:11:26,586
Actually what they do is
slightly more complex than that


225
00:11:26,586 --> 00:11:29,386
because what we have on input
is just, is not one image.


226
00:11:29,386 --> 00:11:30,596
We have a stack of images.


227
00:11:31,186 --> 00:11:32,966
So we need to duplicate
the weights


228
00:11:33,866 --> 00:11:37,776
and know what we compute is
this convolution on each layer,


229
00:11:38,356 --> 00:11:40,786
and then we take the sum of
all these guys, and what we,


230
00:11:40,786 --> 00:11:42,346
and we get, to get
our output pixel.


231
00:11:42,706 --> 00:11:45,056
So now the, the loop
is five dimensional.


232
00:11:45,756 --> 00:11:48,076
I added the IC [inaudible]
of the equation,


233
00:11:48,996 --> 00:11:50,866
and really this is
not what we compute


234
00:11:51,376 --> 00:11:53,196
because we also have
a stack in output.


235
00:11:53,766 --> 00:11:56,356
So what we really compute
in this convolution is this.


236
00:11:56,806 --> 00:12:00,546
We do this many times, one
time for each output layer,


237
00:12:01,276 --> 00:12:03,896
and now we have a
six-dimensional loop.


238
00:12:04,126 --> 00:12:06,826
That means even if the
dimensions are very small,


239
00:12:06,936 --> 00:12:10,746
like in this example, we
are nothing bigger than 264.


240
00:12:11,196 --> 00:12:13,426
It's very small, but when
you multiply all these stuff


241
00:12:13,426 --> 00:12:16,116
together, what you get is
billions of operations.


242
00:12:16,116 --> 00:12:18,676
That means tens of
milliseconds of compute,


243
00:12:19,416 --> 00:12:21,976
and in an entire network,
which is much bigger,


244
00:12:22,586 --> 00:12:25,306
you can have trillions of,
of floating point operations.


245
00:12:25,306 --> 00:12:28,556
That means seconds of CPU time.


246
00:12:28,716 --> 00:12:30,786
OK. A compute in the
convolution layer,


247
00:12:30,786 --> 00:12:32,936
now how do you create
that with BNNS?


248
00:12:33,446 --> 00:12:34,786
Well, first thing
you need to do is


249
00:12:34,826 --> 00:12:36,716
to describe your input stacks.


250
00:12:37,596 --> 00:12:40,956
So you need to specify the
dimensions of the image,


251
00:12:41,366 --> 00:12:46,096
the number of channels, and
also all the layered in memory.


252
00:12:46,316 --> 00:12:49,796
So the increment between
two rows and the increment


253
00:12:49,796 --> 00:12:53,056
between two layers, and
also a very important thing,


254
00:12:53,406 --> 00:12:57,306
what type is used to store them.


255
00:12:57,436 --> 00:13:01,916
For example, we, we work
on float -- 64-bit float.


256
00:13:02,586 --> 00:13:04,766
On neural networks, we don't
need all this precision,


257
00:13:04,976 --> 00:13:08,766
and usually people will use
64-bit float, and that's good


258
00:13:08,766 --> 00:13:10,196
because the storage
is half of it.


259
00:13:10,266 --> 00:13:13,376
So instead of having 20
megabytes, we have only 10,


260
00:13:13,706 --> 00:13:16,196
and if you can go on
integer, eight-bit integer,


261
00:13:16,196 --> 00:13:17,676
you will have only
five megabytes.


262
00:13:18,366 --> 00:13:21,356
And you usually, usually don't
use any precision in the output.


263
00:13:22,836 --> 00:13:25,876
So you can specify the type
used to store the input.


264
00:13:26,226 --> 00:13:29,696
You need to do the same for the
output stack, and then you need


265
00:13:29,696 --> 00:13:31,326
to describe the convolution
itself.


266
00:13:31,816 --> 00:13:33,076
That's the kernel dimensions,


267
00:13:33,816 --> 00:13:35,816
the padding which is
a [inaudible] band of,


268
00:13:35,896 --> 00:13:39,816
of 0 added to the input, the
stride, which is the increment


269
00:13:39,816 --> 00:13:44,556
of the loop in x and y, and the,
again, you need to repeat the,


270
00:13:44,556 --> 00:13:47,806
the channel counts for input and
output and specify the weights.


271
00:13:48,086 --> 00:13:51,966
That's what the orange block in
the middle, specify the weights,


272
00:13:51,966 --> 00:13:54,666
and, again, you can have a,
a different storage type of,


273
00:13:54,666 --> 00:13:58,366
of the weights, and
usually you want 16- or,


274
00:13:58,366 --> 00:14:00,286
or 8-bit storage
for the weights.


275
00:14:00,836 --> 00:14:03,696
Again, because it will
lower the memory usage


276
00:14:03,696 --> 00:14:06,796
and the storage needed
to store them.


277
00:14:08,076 --> 00:14:09,636
Then once you have done that,


278
00:14:09,806 --> 00:14:12,276
you can create a convolution
filter with this function.


279
00:14:13,276 --> 00:14:15,056
You tell it, OK, this
is my input stack,


280
00:14:15,056 --> 00:14:18,056
that's my output stack, that's
my convolution create a filter,


281
00:14:18,056 --> 00:14:20,886
and you will get a filter
object which then you can use


282
00:14:20,996 --> 00:14:23,776
to apply the convolution
on your, on your data,


283
00:14:23,966 --> 00:14:25,226
and once you are done with it,


284
00:14:25,226 --> 00:14:28,006
you will call a [inaudible]
filter to get rid of it and,


285
00:14:28,006 --> 00:14:30,066
and really the resource is used.


286
00:14:30,696 --> 00:14:32,346
This was for the
convolution layers.


287
00:14:32,346 --> 00:14:35,076
Now, let's go and with
the pooling layers.


288
00:14:35,356 --> 00:14:37,546
Pooling is slightly
simpler than convolutions.


289
00:14:38,076 --> 00:14:40,966
So what to compute one output
pixel what you do is you take a


290
00:14:40,966 --> 00:14:45,046
block of input pixels and take
the max value of the average,


291
00:14:45,446 --> 00:14:47,336
and that's your result,
and you do this


292
00:14:47,336 --> 00:14:49,686
for all pixels in all channels.


293
00:14:50,726 --> 00:14:52,006
That's what the formula says.


294
00:14:52,996 --> 00:14:55,926
Well, again, to create a, a
filter for the pooling layer,


295
00:14:56,046 --> 00:15:00,386
you need to describe the input
and output stacks the same way


296
00:15:00,386 --> 00:15:03,446
as before, and you also
need to describe the,


297
00:15:03,446 --> 00:15:04,706
the pooling layer itself.


298
00:15:04,706 --> 00:15:07,546
Again, with the kernel
dimensions, padding, stride,


299
00:15:07,616 --> 00:15:10,336
and this time you don't have
weights, but which function


300
00:15:10,336 --> 00:15:12,226
to use to compute the output.


301
00:15:12,226 --> 00:15:14,276
That will be max average.


302
00:15:15,616 --> 00:15:17,686
And then once you have done
that, you can create the filter,


303
00:15:18,306 --> 00:15:19,806
and you get a filter
object similar


304
00:15:19,806 --> 00:15:20,836
to the one we had before.


305
00:15:21,726 --> 00:15:24,006
The last kind of layers
we support is the fully


306
00:15:24,006 --> 00:15:24,706
connected layers.


307
00:15:26,486 --> 00:15:28,076
Well, it's called
fully connected


308
00:15:28,076 --> 00:15:29,876
but that's a matrix
product hidden here.


309
00:15:30,126 --> 00:15:33,796
So on input you have a vector,
and then you will multiply it


310
00:15:33,866 --> 00:15:37,216
by the matrix, add some,
add a vector of bias,


311
00:15:37,216 --> 00:15:38,356
and then you get the output.


312
00:15:39,186 --> 00:15:40,576
So just a, a matrix product.


313
00:15:41,846 --> 00:15:44,326
So this time, you don't have
images in it for your vector.


314
00:15:44,326 --> 00:15:47,386
So you need to describe the
vector, that's its size.


315
00:15:47,916 --> 00:15:50,166
Point out to the data
and also the type you use


316
00:15:50,166 --> 00:15:55,136
to store these values, and,
again, you can use 32, 16 bits,


317
00:15:55,206 --> 00:15:56,386
floating point, and integer.


318
00:15:59,266 --> 00:16:02,596
And the you need to
describe the layer itself


319
00:16:02,646 --> 00:16:04,506
through the dimensions
of the matrix


320
00:16:04,916 --> 00:16:06,186
and the matrix co-efficients.


321
00:16:06,676 --> 00:16:10,556
And also the bias is not on
the slide, but you have a bias.


322
00:16:11,186 --> 00:16:16,026
And then you can create the
convolution filter, and you get,


323
00:16:16,026 --> 00:16:17,386
again, if [inaudible]
to the others.


324
00:16:17,386 --> 00:16:20,606
Now what we have on this
filters is how do we apply them.


325
00:16:21,306 --> 00:16:23,526
So you have your input
data as your output data


326
00:16:24,276 --> 00:16:25,756
and your filter,
and you will have,


327
00:16:25,796 --> 00:16:29,476
you have two functions
to apply them.


328
00:16:29,546 --> 00:16:32,436
So it's called filter apply if
you have only one pair of input


329
00:16:32,436 --> 00:16:35,006
and output, and you have,
if you have several of them,


330
00:16:35,336 --> 00:16:39,746
you will call filter apply
batch, and you, you tell,


331
00:16:39,966 --> 00:16:42,586
we tell you the number of
pairs you hvee and how to get


332
00:16:42,586 --> 00:16:43,806
from one pair to the next one.


333
00:16:43,806 --> 00:16:45,016
That's a stride in memory.


334
00:16:45,016 --> 00:16:48,416
Alright. And that's it for BNNS.


335
00:16:48,416 --> 00:16:50,396
Let me wrap up.


336
00:16:50,666 --> 00:16:54,306
So BNNS is a set of, of
low-level compute functions


337
00:16:54,306 --> 00:16:55,416
for our neural networks.


338
00:16:56,406 --> 00:16:57,726
Really low level.


339
00:16:57,726 --> 00:16:58,686
We do the compute.


340
00:16:58,686 --> 00:16:59,286
We do it well.


341
00:16:59,286 --> 00:17:03,286
We do it fast, but it doesn't
know what a neural network is.


342
00:17:03,896 --> 00:17:07,415
It just does the compute.


343
00:17:07,656 --> 00:17:10,136
So we optimize it to be
fast and energy efficient,


344
00:17:10,756 --> 00:17:17,776
and important thing, it will
under multiple data types.


345
00:17:18,425 --> 00:17:19,826
OK, that was it for BNNS.


346
00:17:20,006 --> 00:17:21,066
Now Quadrature.


347
00:17:21,556 --> 00:17:27,156
We have a request, will, people
who are asking us about library


348
00:17:27,156 --> 00:17:29,186
to do a numerical
integration of functions.


349
00:17:29,316 --> 00:17:30,046
So here it is.


350
00:17:30,876 --> 00:17:33,696
Yeah, remember your,
your school days.


351
00:17:34,166 --> 00:17:35,576
So this is computing
the integral


352
00:17:35,576 --> 00:17:37,306
of a function between a and b.


353
00:17:37,866 --> 00:17:42,586
So that's a green area
between the curve and the axis.


354
00:17:42,586 --> 00:17:45,506
Well, so to do that, you first
need to describe your function.


355
00:17:45,916 --> 00:17:47,246
So you need to provide
a callback.


356
00:17:47,566 --> 00:17:50,806
Once thing we did, and which
is different from your,


357
00:17:50,806 --> 00:17:52,616
the old library's
doing the same thing is


358
00:17:52,616 --> 00:17:55,946
that the callbacks takes a
number of points to evaluate.


359
00:17:57,006 --> 00:17:59,206
Because usually when you compute
the integral, you will have


360
00:17:59,256 --> 00:18:01,336
to evaluate the function
in, at many points,


361
00:18:01,336 --> 00:18:03,646
and if you have a callback,
a vectorized callback doing


362
00:18:03,646 --> 00:18:05,516
that faster, it's all good.


363
00:18:05,516 --> 00:18:10,326
You can do much, much faster
with this multiple x callback.


364
00:18:10,326 --> 00:18:13,766
So it will, it will pass
you a number of values in x,


365
00:18:13,766 --> 00:18:15,726
and you will have
to fill it with a f


366
00:18:15,726 --> 00:18:17,936
of xi inside each value of y.


367
00:18:19,516 --> 00:18:20,986
So that's your function,
and then you need


368
00:18:20,986 --> 00:18:22,826
to tell it how to integrate it.


369
00:18:22,876 --> 00:18:25,406
So we provide a set of three
different integration methods


370
00:18:26,166 --> 00:18:29,986
with different complexity
and, and time, and also some


371
00:18:29,986 --> 00:18:32,816
of them are able to integrate
to infinity and, note,


372
00:18:32,816 --> 00:18:34,986
you will find details in
the Quadrature header.


373
00:18:36,516 --> 00:18:39,166
And you also need to specify
what is the error you want


374
00:18:39,166 --> 00:18:41,666
on the output, and
the max number


375
00:18:41,666 --> 00:18:43,826
of intervals we can
subdivide maybe to,


376
00:18:43,936 --> 00:18:44,916
to complete the output.


377
00:18:45,546 --> 00:18:47,546
And then you pass that to
the integrate function,


378
00:18:49,426 --> 00:18:51,476
and you also tell it a, b,


379
00:18:51,586 --> 00:18:55,416
and you pass a point
to receive the error.


380
00:18:55,416 --> 00:18:59,226
It's called estimated
error is here,


381
00:18:59,886 --> 00:19:01,996
and it will return you the
estimated error on the output


382
00:19:01,996 --> 00:19:04,796
and also status, we receive
the status of the computation


383
00:19:04,796 --> 00:19:07,226
because if you ask a
very, very low error,


384
00:19:07,226 --> 00:19:08,676
sometimes it's not
able to convert,


385
00:19:08,676 --> 00:19:11,406
and we will get that
in a status.


386
00:19:11,786 --> 00:19:12,896
And that's it for Quadrature.


387
00:19:13,606 --> 00:19:16,316
And now let me call
Steve on stage.


388
00:19:16,636 --> 00:19:18,716
He will tell you about
new additions to simd.


389
00:19:19,956 --> 00:19:20,756
>> Thanks very much, Eric.


390
00:19:21,306 --> 00:19:22,246
My name's Steven Canon.


391
00:19:22,246 --> 00:19:24,066
I work in the Vector,
Numerics group with Eric.


392
00:19:24,586 --> 00:19:27,806
Eric took you back to
calculus just then.


393
00:19:27,806 --> 00:19:29,846
I'm going to take you
ahead again a little bit


394
00:19:29,846 --> 00:19:32,206
to linear algebra right now.


395
00:19:32,496 --> 00:19:34,926
We have this nice module, simd,


396
00:19:35,686 --> 00:19:39,506
which provides geometric
operations and vector operations


397
00:19:39,986 --> 00:19:43,506
for C, Objective-C,
C ++, and Swift.


398
00:19:44,596 --> 00:19:47,656
And it really closely mirrors
the Metal shading language.


399
00:19:48,486 --> 00:19:51,956
It ties in closely with
SceneKit and Model I/O


400
00:19:51,956 --> 00:19:53,466
and all those graphics
libraries.


401
00:19:53,876 --> 00:19:57,196
If you find yourself writing
vector code to do, you know,


402
00:19:57,196 --> 00:19:59,446
small, you know, 3 x 3,


403
00:19:59,446 --> 00:20:01,526
4 x 4 kind of linear
algebra operations,


404
00:20:01,806 --> 00:20:03,496
this is the library you probably
want to be using instead


405
00:20:03,496 --> 00:20:03,976
of writing that by hand.


406
00:20:04,596 --> 00:20:06,446
We have most of what
you want available.


407
00:20:06,696 --> 00:20:08,416
If something's not there,
ask us to provide it.


408
00:20:08,796 --> 00:20:10,486
It's all really fast
and there it is.


409
00:20:10,776 --> 00:20:11,646
So what's there now?


410
00:20:12,486 --> 00:20:13,996
We've have a bunch of types.


411
00:20:14,606 --> 00:20:18,916
There's vectors of floats and of
doubles, and we also have signed


412
00:20:18,916 --> 00:20:20,766
and unsigned integers
like 2, 3, and 4.


413
00:20:22,336 --> 00:20:25,236
And we have matrices of floats
and doubles of those same sizes.


414
00:20:26,686 --> 00:20:29,406
And this is just what's
available in every language,


415
00:20:30,236 --> 00:20:31,976
in C and C++ and Objective-C.


416
00:20:31,976 --> 00:20:34,156
There's a bunch of other types
as well which are really useful


417
00:20:34,156 --> 00:20:36,616
for writing your own generic
vector code, but I'm going


418
00:20:36,616 --> 00:20:39,376
to focus on sort of the common
subset of what's available


419
00:20:39,376 --> 00:20:42,226
on all the languages on all the
platforms in our talk today.


420
00:20:42,556 --> 00:20:45,186
We have operations on those
types, too, obviously.


421
00:20:45,186 --> 00:20:48,836
There's all the usual
arithmetic operations


422
00:20:48,866 --> 00:20:50,716
for vectors and for matrices.


423
00:20:51,656 --> 00:20:54,186
And we have all the familiar
geometry and shader functions


424
00:20:54,186 --> 00:20:55,826
if you've done any
shader programming before.


425
00:20:55,826 --> 00:20:57,186
Most of what you
would want to use is,


426
00:20:57,186 --> 00:20:58,376
is going to be available here.


427
00:20:58,916 --> 00:21:00,876
I'll show you a little
example now.


428
00:21:01,766 --> 00:21:04,766
So this is the same
function written three times


429
00:21:04,766 --> 00:21:05,736
in three different languages.


430
00:21:06,346 --> 00:21:09,386
We have it in Objective-C
up top, we have it in C ++


431
00:21:09,386 --> 00:21:11,306
in the middle, and we have
it in Swift on the bottom,


432
00:21:11,756 --> 00:21:14,506
and you can see that the
boilerplate is a little bit


433
00:21:14,506 --> 00:21:15,696
different between
the languages just


434
00:21:15,696 --> 00:21:17,316
because function
declarations work differently


435
00:21:17,316 --> 00:21:19,486
in these languages,
but if we focus


436
00:21:19,486 --> 00:21:23,126
in on the actual computation
part, it's essentially the same


437
00:21:23,126 --> 00:21:26,906
in all the languages, and it
also really closely mirrors the,


438
00:21:27,046 --> 00:21:28,896
the way you would just write
this naturally in math.


439
00:21:29,576 --> 00:21:31,046
So you don't have a lot
of weird function calls.


440
00:21:31,046 --> 00:21:33,166
You don't have to write
four loops, all that stuff.


441
00:21:33,596 --> 00:21:35,966
You just write your code in
this, this natural fluent style,


442
00:21:36,276 --> 00:21:38,716
and we translate
all that for you.


443
00:21:38,996 --> 00:21:41,776
So it's, it's nice and easy
to write, and this looks just


444
00:21:41,776 --> 00:21:43,406
like the Metal code you would
write to do this as well.


445
00:21:43,866 --> 00:21:46,636
Now it happens that the reflect
function is already available


446
00:21:46,636 --> 00:21:47,926
built into the library already.


447
00:21:47,926 --> 00:21:49,396
So you don't need to
write this yourself.


448
00:21:50,866 --> 00:21:52,756
Calling functions
between languages


449
00:21:52,756 --> 00:21:54,696
like there's a whole bunch
of stuff in model I/O


450
00:21:54,696 --> 00:21:58,506
that exposes Objective-C
API's that take these types.


451
00:21:58,506 --> 00:21:59,506
Situation is pretty good.


452
00:21:59,626 --> 00:22:02,506
The vector types are
compiler extensions in C,


453
00:22:02,506 --> 00:22:04,286
Objective-C, and in C++.


454
00:22:05,176 --> 00:22:07,576
And in Swift, they're
defined as structs,


455
00:22:07,626 --> 00:22:10,896
but the compiler knows how
to map between them for you.


456
00:22:11,606 --> 00:22:13,376
So you don't really
need to do anything.


457
00:22:13,846 --> 00:22:14,856
Here's a simple example.


458
00:22:14,966 --> 00:22:16,876
If I have an Objective-C
function,


459
00:22:16,876 --> 00:22:19,596
I call some function here
that operates on vector types,


460
00:22:20,356 --> 00:22:21,806
and I want to call that function


461
00:22:21,956 --> 00:22:23,976
from Swift using the
Swift vector types,


462
00:22:24,016 --> 00:22:25,796
I can just do that,
and it just works.


463
00:22:25,796 --> 00:22:27,086
I don't need to do
anything fancy.


464
00:22:27,626 --> 00:22:28,856
These types have
the same layout.


465
00:22:28,856 --> 00:22:31,206
So there's no cost to converting
or anything like that.


466
00:22:32,266 --> 00:22:33,976
Similarly, for matrices,


467
00:22:34,246 --> 00:22:37,456
the Swift matrix types are
layout compatible with the C,


468
00:22:37,456 --> 00:22:39,046
Objective-C, and C++ types.


469
00:22:39,666 --> 00:22:42,516
So if I need to work with
them, here I have a Swift,


470
00:22:42,906 --> 00:22:45,356
I'm going to create a
Swift type from the C type.


471
00:22:46,376 --> 00:22:47,906
All I need to do is
use the init function.


472
00:22:48,426 --> 00:22:49,056
Works fine.


473
00:22:49,206 --> 00:22:50,646
Doesn't have any
computational cost.


474
00:22:50,646 --> 00:22:53,416
It just sort of changes the
types silently for me, and,


475
00:22:53,416 --> 00:22:56,616
similarly, I can use the C
matrix property if I need


476
00:22:56,616 --> 00:23:00,856
to get a C type to call C or
Objective-C or C++ function.


477
00:23:01,576 --> 00:23:03,086
So we have a few things
that are new this year


478
00:23:03,286 --> 00:23:05,886
that I want to show to you.


479
00:23:05,886 --> 00:23:08,976
We have three new functions
- simd orient, simd incircle,


480
00:23:08,976 --> 00:23:12,066
simd insphere - and these are
overloaded to support a bunch


481
00:23:12,066 --> 00:23:14,236
of different types and different
lengths and things like that.


482
00:23:14,696 --> 00:23:16,856
Basically everything in the
simd library works that way.


483
00:23:16,856 --> 00:23:18,636
So even though we have
just three new functions,


484
00:23:18,636 --> 00:23:20,376
it's actually a lot
of new stuff.


485
00:23:21,096 --> 00:23:22,746
So I'm going to start
with orient.


486
00:23:23,766 --> 00:23:26,156
What orient does is it lets us
answer the question is a set


487
00:23:26,156 --> 00:23:27,406
of vectors positively oriented?


488
00:23:28,196 --> 00:23:29,356
And what that means,


489
00:23:29,356 --> 00:23:31,056
if you don't remember
your linear algebra,


490
00:23:31,446 --> 00:23:33,056
is do they obey the
right-hand rule.


491
00:23:33,436 --> 00:23:35,916
You might remember that from
physics, or, equivalently,


492
00:23:36,246 --> 00:23:37,466
is there determinate positive.


493
00:23:37,466 --> 00:23:39,846
If there's any math
majors in the audience,


494
00:23:39,846 --> 00:23:41,306
you're objecting
right now that a set


495
00:23:41,456 --> 00:23:43,656
of vectors does not
have a determinate.


496
00:23:43,756 --> 00:23:45,636
What I mean here is just
you, you kind of take them,


497
00:23:45,636 --> 00:23:47,736
and you slam them
together to make a matrix.


498
00:23:47,736 --> 00:23:49,516
Take the determinate
of that matrix.


499
00:23:49,806 --> 00:23:51,666
Is that positive or not?


500
00:23:51,856 --> 00:23:53,556
So why do we care about this?


501
00:23:53,656 --> 00:23:55,516
This is kind of simple
and stupid.


502
00:23:55,796 --> 00:23:58,556
You can use this to answer a lot


503
00:23:58,556 --> 00:24:01,206
of computational geometry
questions that are very useful.


504
00:24:01,576 --> 00:24:04,246
Like, is the triangle facing
towards me or away from me?


505
00:24:04,276 --> 00:24:07,506
If you think about a
tetrahedron, there are two faces


506
00:24:07,506 --> 00:24:09,496
that are pointing towards you,
and there are also two faces


507
00:24:09,496 --> 00:24:11,486
on the backside that
face away from you.


508
00:24:11,486 --> 00:24:14,016
And if you're doing graphics
operations, it's useful to know


509
00:24:14,016 --> 00:24:15,836
which ones are facing
towards you


510
00:24:15,836 --> 00:24:17,126
because those are the ones
you want to operate on.


511
00:24:18,206 --> 00:24:20,596
Similarly, if I have
a line and a point,


512
00:24:20,596 --> 00:24:22,316
and I want to answer the
question is the point


513
00:24:22,316 --> 00:24:25,036
on the line, or if it's not,
which side of the line is it on.


514
00:24:25,036 --> 00:24:27,856
I can use the orient predicate
to answer that question.


515
00:24:27,856 --> 00:24:31,646
Now this seems kind of
simple, and it is simple except


516
00:24:31,646 --> 00:24:33,996
that it can be very hard to
actually answer this question


517
00:24:33,996 --> 00:24:36,086
when the point is close to the
line, and I'm going to talk


518
00:24:36,086 --> 00:24:36,796
about that more later.


519
00:24:38,056 --> 00:24:39,966
So here's a simple
example of that.


520
00:24:39,966 --> 00:24:43,336
I'm going to have a plane over
on the right-hand side of the,


521
00:24:43,336 --> 00:24:44,866
the display here, and I have,


522
00:24:44,866 --> 00:24:46,296
going to put some
points on that plane.


523
00:24:46,296 --> 00:24:50,876
So I have three points,
a, b, and c, and I'm going


524
00:24:50,876 --> 00:24:53,706
to query the orientation
of those with simd orient.


525
00:24:54,256 --> 00:24:57,426
Now because we go
counterclockwise as we go from a


526
00:24:57,426 --> 00:25:01,826
to b to c, there, we say that
these are positively oriented.


527
00:25:01,826 --> 00:25:02,466
That's what it means


528
00:25:02,466 --> 00:25:03,996
to be positively
oriented in the plane.


529
00:25:04,076 --> 00:25:09,146
If we move one of the points so
that that order is clockwise,


530
00:25:09,416 --> 00:25:14,086
now it's negatively oriented,
and if I move the point c


531
00:25:14,086 --> 00:25:16,546
so it's exactly on the
line between a and b,


532
00:25:16,826 --> 00:25:19,706
then they're co-linear
and the orientation is 0,


533
00:25:19,706 --> 00:25:20,696
or you might say
it's degenerate.


534
00:25:21,606 --> 00:25:24,906
Now it's very hard to tell in
general if a point is exactly


535
00:25:24,906 --> 00:25:27,056
on the line especially with
floating point coordinates.


536
00:25:27,736 --> 00:25:30,936
And that's because orientation
is numerically unstable.


537
00:25:31,526 --> 00:25:34,496
So because of floating point
rounding, if the results


538
00:25:34,566 --> 00:25:36,066
of this determinate
is very nearly 0,


539
00:25:36,416 --> 00:25:39,356
you can easily get the wrong
sign, and for some algorithms,


540
00:25:39,356 --> 00:25:41,626
that doesn't matter, but
for other algorithms,


541
00:25:41,626 --> 00:25:43,216
you can actually
fail to converge,


542
00:25:43,216 --> 00:25:45,846
or you may get the wrong result
when you're working with this.


543
00:25:45,846 --> 00:25:47,436
For collision detection
and things like that,


544
00:25:47,436 --> 00:25:49,336
it can actually be quite
important to be able


545
00:25:49,336 --> 00:25:50,096
to answer this question.


546
00:25:50,096 --> 00:25:53,046
Also for things like triangular
mesh generation for models


547
00:25:53,046 --> 00:25:54,386
and things, this is
an important question


548
00:25:54,386 --> 00:25:55,896
to be able to answer exactly.


549
00:25:56,646 --> 00:25:59,086
So let's look at
an example where,


550
00:25:59,086 --> 00:26:01,026
where this is actually
hard to answer.


551
00:26:02,256 --> 00:26:04,946
I'm going to put two vectors
in the plane, u and v,


552
00:26:04,946 --> 00:26:07,566
and these are almost
identical vectors, right.


553
00:26:07,566 --> 00:26:08,456
They, they differ


554
00:26:08,456 --> 00:26:10,566
by the smallest amount
they could possibly differ.


555
00:26:11,826 --> 00:26:14,666
And I've magnified
them enormously


556
00:26:14,666 --> 00:26:15,426
on the right-hand side.


557
00:26:15,426 --> 00:26:16,866
So you can see that
they're actually different.


558
00:26:16,866 --> 00:26:18,066
If I drew these to
scale, they would,


559
00:26:18,066 --> 00:26:19,286
they would overlap entirely.


560
00:26:20,606 --> 00:26:23,716
And if we compute the
orientation in the way


561
00:26:23,716 --> 00:26:26,716
that you would normally compute
this, we get a result of 0


562
00:26:26,716 --> 00:26:27,736
because of floating
point rounding.


563
00:26:27,736 --> 00:26:29,806
Now this is a simple
example where we get 0.


564
00:26:30,356 --> 00:26:32,236
With dimensions bigger
than 2 x 2,


565
00:26:32,646 --> 00:26:34,866
we can actually get the
wrong sign entirely for this.


566
00:26:34,866 --> 00:26:36,726
So it's not just that it could
be 0 when it shouldn't be.


567
00:26:37,536 --> 00:26:39,036
But if we use the
simd orient function,


568
00:26:39,676 --> 00:26:41,906
we get a tiny positive
number, which is,


569
00:26:41,906 --> 00:26:42,626
which is the right result.


570
00:26:42,696 --> 00:26:43,856
These are positively oriented.


571
00:26:44,896 --> 00:26:46,636
And I should point out here


572
00:26:46,636 --> 00:26:49,656
that don't interpret
this tiny positive number


573
00:26:49,656 --> 00:26:50,616
as meaning anything.


574
00:26:50,616 --> 00:26:51,606
It is not the determinate.


575
00:26:52,496 --> 00:26:56,566
It is sometimes the determinate,
but it's sometimes just going


576
00:26:56,566 --> 00:26:57,316
to have the right sign.


577
00:26:57,316 --> 00:26:58,676
So what we're really
interested here is


578
00:26:58,676 --> 00:27:00,036
in the sign of this number.


579
00:27:00,376 --> 00:27:01,446
How are we able to do this?


580
00:27:01,726 --> 00:27:02,986
So the geometric [inaudible]


581
00:27:02,986 --> 00:27:04,766
that I'm showing you
today use something called


582
00:27:04,766 --> 00:27:05,576
adaptive precision.


583
00:27:06,216 --> 00:27:09,246
We go and compute as many
bits as we need to compute


584
00:27:09,356 --> 00:27:10,186
to get the right result,


585
00:27:10,576 --> 00:27:12,626
and this lets us return
the right result very,


586
00:27:12,626 --> 00:27:16,456
very fast in most cases,
but if we need to go off


587
00:27:16,456 --> 00:27:18,726
and do the exact computation
to give you the right answer,


588
00:27:18,976 --> 00:27:20,066
we will, we will do that.


589
00:27:20,066 --> 00:27:21,806
So you can just trust that that
this gives you the right answer


590
00:27:21,806 --> 00:27:24,006
in your code, and you don't need
to worry about that yourself.


591
00:27:25,526 --> 00:27:27,116
Incircle is very similar.


592
00:27:27,446 --> 00:27:28,916
We take three points
in the plane.


593
00:27:29,116 --> 00:27:30,036
That determines the circle.


594
00:27:30,276 --> 00:27:32,566
You can notice that
they're positively oriented


595
00:27:32,566 --> 00:27:33,326
around the circle here.


596
00:27:33,506 --> 00:27:34,096
That's important.


597
00:27:35,006 --> 00:27:37,266
And if I put a point
in the middle, x,


598
00:27:37,486 --> 00:27:39,816
then simd incircle can
tell me if it's inside.


599
00:27:40,056 --> 00:27:41,886
In that case, I get
a positive result.


600
00:27:42,476 --> 00:27:45,016
If it's on the circle, I get 0.


601
00:27:45,696 --> 00:27:48,036
And if it's outside the
circle, I get a negative result.


602
00:27:48,616 --> 00:27:51,926
And insphere is exactly
the same thing.


603
00:27:52,426 --> 00:27:55,376
Just in three dimensions now.


604
00:27:55,376 --> 00:27:56,806
I need four dimensions
to determine the sphere.


605
00:27:57,506 --> 00:27:59,246
I give it my point x,
and I get a result.


606
00:28:00,656 --> 00:28:02,856
I'll show you an example I
talked about earlier figuring


607
00:28:02,856 --> 00:28:05,506
out if a triangle faces
towards you or away from you.


608
00:28:06,166 --> 00:28:08,036
Here I've got a really
simple struct


609
00:28:08,036 --> 00:28:09,466
to represent a triangle
in Swift.


610
00:28:09,956 --> 00:28:11,766
Triangle is determined
by three vertices


611
00:28:11,766 --> 00:28:13,646
that I happen, a
collection here.


612
00:28:14,506 --> 00:28:18,296
And I have this predicate
is facing to tell me if the,


613
00:28:18,296 --> 00:28:20,486
the triangle is facing
towards the camera or not.


614
00:28:21,216 --> 00:28:23,526
So the usual way you
would compute this is


615
00:28:23,526 --> 00:28:26,556
to compute a normal to the
triangle with the cross product,


616
00:28:27,316 --> 00:28:30,216
and then take the dot product
of that with the vector


617
00:28:30,276 --> 00:28:31,746
to the camera, and
if that's positive,


618
00:28:31,796 --> 00:28:33,066
then the triangle
faces the camera.


619
00:28:33,576 --> 00:28:36,886
We can simplify this code
a lot and make it correct


620
00:28:38,326 --> 00:28:40,936
by just using the
simd orient predicate.


621
00:28:41,306 --> 00:28:43,556
So my code is simpler,
it's fast,


622
00:28:43,736 --> 00:28:44,766
and it gives me the
right answer.


623
00:28:44,836 --> 00:28:47,286
Is it, these are all things I
like, and that's what we try


624
00:28:47,286 --> 00:28:50,266
to do in Accelerate across the
board is give you simple things


625
00:28:50,266 --> 00:28:52,336
for complex mathematical
calculations.


626
00:28:53,976 --> 00:28:55,956
So we showed you a bunch
of new stuff today.


627
00:28:57,136 --> 00:29:00,486
We have some new
libraries entirely.


628
00:29:00,666 --> 00:29:02,746
We have BNNS for
neural networks.


629
00:29:02,956 --> 00:29:06,946
We have Quadrature, and we
also have some new features,


630
00:29:07,706 --> 00:29:09,496
orientation and incircle
in simd.


631
00:29:09,846 --> 00:29:13,926
Every one of these features and
libraries is added in response


632
00:29:13,926 --> 00:29:16,766
to a request that we
got from developers.


633
00:29:16,856 --> 00:29:19,676
So we really want to hear from
you guys about what you need,


634
00:29:20,116 --> 00:29:22,736
what we can add to make your
computational workloads easier.


635
00:29:22,736 --> 00:29:24,356
We want to give you
simple interfaces


636
00:29:24,796 --> 00:29:26,716
that let you do what you
need to do efficiently.


637
00:29:27,726 --> 00:29:29,386
We've also done a ton of
other stuff this year.


638
00:29:30,206 --> 00:29:33,206
In vImage, which Eric
mentioned briefly in passing,


639
00:29:34,006 --> 00:29:36,196
we have a whole set
of geometry operations


640
00:29:36,196 --> 00:29:37,316
for interleaved chroma planes.


641
00:29:37,346 --> 00:29:38,716
This is absolutely one


642
00:29:38,716 --> 00:29:40,886
of the most requested features
we've had in recent years.


643
00:29:41,306 --> 00:29:42,946
So we're, we're really
excited to have that.


644
00:29:42,946 --> 00:29:44,706
If you don't know what that
is, don't worry about it,


645
00:29:44,806 --> 00:29:46,906
but if you know what it is,
you understand why it's useful.


646
00:29:46,906 --> 00:29:50,836
And we also have expanded
supports for new formats


647
00:29:50,836 --> 00:29:51,976
in the vImage conversion
routines.


648
00:29:52,036 --> 00:29:53,996
This underlies a lot
of the deep color stuff


649
00:29:53,996 --> 00:29:54,846
that you may have heard about.


650
00:29:54,846 --> 00:29:58,246
So this is, this is
really important for that.


651
00:29:58,616 --> 00:30:00,516
We have improved the performance


652
00:30:00,516 --> 00:30:02,476
for interleaved complex
formats in vDSP.


653
00:30:02,646 --> 00:30:06,766
With the FFT's, we support both
interleaved and planer layouts


654
00:30:06,766 --> 00:30:09,046
where the complex and imaginary
parts are either separated


655
00:30:09,046 --> 00:30:09,976
or put together.


656
00:30:11,746 --> 00:30:14,336
Planer layouts are what we
really prefer to operate with,


657
00:30:14,336 --> 00:30:16,896
and we recommend you do
that, but if you happen


658
00:30:16,896 --> 00:30:19,776
to have interleaved data, you
can just use the FFT's now,


659
00:30:19,776 --> 00:30:20,766
and they'll be really fast.


660
00:30:22,126 --> 00:30:23,386
We've also improved
the performance


661
00:30:23,386 --> 00:30:24,926
of all the Level
II BLAS operations.


662
00:30:24,926 --> 00:30:27,926
Some of that's motivated by
the, the BNNS stuff you saw,


663
00:30:27,926 --> 00:30:30,186
and some of that's just
opportunity that we saw to go


664
00:30:30,186 --> 00:30:32,356
after that, and there's a
ton of other stuff that's new


665
00:30:32,356 --> 00:30:34,346
in Accelerate, lots
of improved stuff.


666
00:30:35,056 --> 00:30:36,926
Every time new processors
come out, we make sure


667
00:30:36,926 --> 00:30:37,946
that we're tuning for that.


668
00:30:38,306 --> 00:30:39,646
We want to take care


669
00:30:39,646 --> 00:30:41,556
of all those low-level
computational details


670
00:30:41,586 --> 00:30:44,826
so you can focus on writing
high-level algorithms that,


671
00:30:44,886 --> 00:30:46,226
that just sit on top of that,


672
00:30:46,226 --> 00:30:47,806
and you can do the
job you need to do.


673
00:30:49,256 --> 00:30:52,126
In summary, we want to
be your single-stop shop


674
00:30:52,126 --> 00:30:53,806
for computational algorithms


675
00:30:53,976 --> 00:30:55,506
where we give you
implementations


676
00:30:55,506 --> 00:30:58,946
that are correct, they're fast,
and they're energy efficient,


677
00:30:59,566 --> 00:31:01,716
and we're going to keep
tuning them for new hardware


678
00:31:01,716 --> 00:31:03,666
as it comes along so you don't
need to worry about that.


679
00:31:03,666 --> 00:31:06,626
If you want to [inaudible]
yourself and we ship a,


680
00:31:06,666 --> 00:31:09,736
a new processor, then you're
going to need to update


681
00:31:09,736 --> 00:31:12,646
for that, and if you let
us handle that for you,


682
00:31:12,876 --> 00:31:14,066
then you don't need to worry.


683
00:31:15,386 --> 00:31:16,756
Keep the future requests coming.


684
00:31:16,886 --> 00:31:17,886
We love to get them from you.


685
00:31:18,316 --> 00:31:20,316
We talked to a bunch of you
in the labs earlier today.


686
00:31:20,316 --> 00:31:22,116
We got a bunch of great
future requests already.


687
00:31:22,646 --> 00:31:23,596
Can file radars.


688
00:31:23,636 --> 00:31:24,786
We love for that to happen.


689
00:31:25,116 --> 00:31:30,236
If you want more information,
the link for this talk is here.


690
00:31:30,816 --> 00:31:33,546
I also encourage you to check
out previous years' talks


691
00:31:33,546 --> 00:31:35,946
where you've gone into more
detail about other aspects


692
00:31:35,946 --> 00:31:37,116
of the library that are useful.


693
00:31:38,316 --> 00:31:40,336
There were two great
Metal sessions


694
00:31:40,336 --> 00:31:42,546
that I really highly
recommend everyone check


695
00:31:42,546 --> 00:31:44,626
out from yesterday,
especially if you're interested


696
00:31:44,626 --> 00:31:45,956
in this kind of stuff.


697
00:31:46,046 --> 00:31:47,306
Thanks very much for
coming out everyone.


698
00:31:48,516 --> 00:31:58,380
[ Applause ]

