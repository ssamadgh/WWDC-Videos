1
00:00:07,016 --> 00:00:15,500
[ Music ]


2
00:00:20,516 --> 00:00:25,636
[ Applause ]


3
00:00:26,136 --> 00:00:26,966
>> Hello and good afternoon


4
00:00:26,966 --> 00:00:27,406
everyone.


5
00:00:27,806 --> 00:00:29,096
Welcome to our session on


6
00:00:29,096 --> 00:00:30,336
natural language processing.


7
00:00:30,486 --> 00:00:32,226
I'm delighted to see so many of


8
00:00:32,226 --> 00:00:33,836
you here today, and I'm really


9
00:00:33,836 --> 00:00:35,356
excited to tell you about some


10
00:00:35,356 --> 00:00:36,896
of the new and cool features


11
00:00:37,126 --> 00:00:38,206
we've been working in the NLP


12
00:00:38,206 --> 00:00:39,526
space for you.


13
00:00:40,206 --> 00:00:42,066
I'm Vivek, and I'll be jointly


14
00:00:42,066 --> 00:00:43,526
presenting this session with my


15
00:00:43,526 --> 00:00:44,676
colleague, Doug Davidson.


16
00:00:45,176 --> 00:00:47,756
Let's get started.


17
00:00:47,756 --> 00:00:49,646
Last year, we placed your app


18
00:00:49,646 --> 00:00:52,166
center stage and told you how


19
00:00:52,166 --> 00:00:53,336
you could harness the power of


20
00:00:53,416 --> 00:00:56,426
NLP to make your apps smarter


21
00:00:56,426 --> 00:00:57,186
and more intelligent.


22
00:00:57,586 --> 00:00:59,926
We did this by walking you


23
00:01:00,236 --> 00:01:02,206
through the NLP APIs available


24
00:01:02,506 --> 00:01:03,736
in NSLinguisticTagger.


25
00:01:04,976 --> 00:01:06,706
NSLinguisticTagger, as most of


26
00:01:06,706 --> 00:01:07,906
you are familiar with or have


27
00:01:07,906 --> 00:01:10,426
used at some point, is a class


28
00:01:10,426 --> 00:01:12,266
and foundation that provides the


29
00:01:12,336 --> 00:01:13,636
fundamental building blocks for


30
00:01:13,676 --> 00:01:13,916
NLP.


31
00:01:14,676 --> 00:01:15,646
Everything from language


32
00:01:15,646 --> 00:01:17,376
identification to organization,


33
00:01:17,376 --> 00:01:19,306
part of speech tagging, and so


34
00:01:19,906 --> 00:01:19,973
on.


35
00:01:20,406 --> 00:01:21,426
We achieve this in


36
00:01:21,476 --> 00:01:23,546
NSLinguisticTagger by seamlessly


37
00:01:23,546 --> 00:01:25,346
blending linguistics and machine


38
00:01:25,346 --> 00:01:26,386
learning behind the scenes.


39
00:01:26,736 --> 00:01:28,996
So you, as a developer, can just


40
00:01:28,996 --> 00:01:30,856
focus on using these APIs and


41
00:01:30,856 --> 00:01:33,206
focus on your task.


42
00:01:33,206 --> 00:01:34,066
All that's great.


43
00:01:34,586 --> 00:01:36,016
So what's new in NLP for this


44
00:01:36,016 --> 00:01:36,216
year?


45
00:01:36,916 --> 00:01:38,346
Well, we are delighted to


46
00:01:38,346 --> 00:01:39,356
announce that we have a


47
00:01:39,356 --> 00:01:40,816
brand-new framework for NLP


48
00:01:40,816 --> 00:01:42,446
called Natural Language.


49
00:01:43,216 --> 00:01:44,736
Natural Language is now going to


50
00:01:44,736 --> 00:01:47,116
be a one-stop shop for doing all


51
00:01:47,116 --> 00:01:49,536
things NLP on device across all


52
00:01:49,536 --> 00:01:50,486
Apple platforms.


53
00:01:51,176 --> 00:01:52,856
Natural Language has some really


54
00:01:52,856 --> 00:01:54,496
cool features, and let me talk


55
00:01:54,496 --> 00:01:55,396
about each of those.


56
00:01:55,846 --> 00:01:58,846
First, it has a completely


57
00:01:58,846 --> 00:02:01,476
redesigned API surface, so it


58
00:02:01,476 --> 00:02:02,936
supports all the functionalities


59
00:02:02,936 --> 00:02:04,496
that NSLinguisticTagger used to


60
00:02:04,496 --> 00:02:07,206
and still does but with really,


61
00:02:07,296 --> 00:02:08,376
really Swift APIs.


62
00:02:09,326 --> 00:02:10,446
But that's not it.


63
00:02:11,026 --> 00:02:12,746
We now have support for custom


64
00:02:12,866 --> 00:02:13,716
NLP models.


65
00:02:14,056 --> 00:02:15,136
These are models that you can


66
00:02:15,136 --> 00:02:17,226
create using Create ML and


67
00:02:17,226 --> 00:02:18,856
deploy the model either using


68
00:02:18,856 --> 00:02:20,576
Code ML API or through Natural


69
00:02:20,576 --> 00:02:20,976
Language.


70
00:02:22,396 --> 00:02:23,716
Everything that we support in


71
00:02:23,716 --> 00:02:24,886
Natural Language, all of the


72
00:02:24,886 --> 00:02:26,956
machine learning NLP is high


73
00:02:27,026 --> 00:02:27,566
performed.


74
00:02:27,776 --> 00:02:29,076
It is optimized for Apple


75
00:02:29,076 --> 00:02:30,586
hardware and also for model


76
00:02:30,586 --> 00:02:30,946
size.


77
00:02:32,366 --> 00:02:35,106
And finally, everything is


78
00:02:35,106 --> 00:02:35,966
completely private.


79
00:02:36,186 --> 00:02:37,566
All of the machine learning in


80
00:02:37,566 --> 00:02:39,326
NLP that is powered in Natural


81
00:02:39,326 --> 00:02:41,256
Language is done on device to


82
00:02:41,256 --> 00:02:42,486
protect user's privacy.


83
00:02:42,906 --> 00:02:44,626
This is the very same technology


84
00:02:44,626 --> 00:02:46,776
that we use at Apple to bring


85
00:02:46,776 --> 00:02:48,916
NLP on device for our own


86
00:02:48,916 --> 00:02:49,446
features.


87
00:02:50,306 --> 00:02:51,716
So let me talk about each of


88
00:02:51,716 --> 00:02:53,336
these features of Natural


89
00:02:53,336 --> 00:02:53,686
Language.


90
00:02:54,026 --> 00:02:55,756
Let's start with the Swift APIs.


91
00:02:56,286 --> 00:02:59,996
As I mentioned, Natural Language


92
00:02:59,996 --> 00:03:01,136
supports all the fundamental


93
00:03:01,136 --> 00:03:01,966
building blocks that


94
00:03:01,966 --> 00:03:03,866
NSLinguisticTagger does but with


95
00:03:04,176 --> 00:03:05,976
significantly better and easier


96
00:03:05,976 --> 00:03:07,516
to use APIs.


97
00:03:08,056 --> 00:03:09,726
In order to provide some of


98
00:03:09,726 --> 00:03:11,336
these APIs and go over them, I'm


99
00:03:11,336 --> 00:03:12,346
going to illustrate them with


100
00:03:12,346 --> 00:03:13,506
hypothetical apps.


101
00:03:14,066 --> 00:03:16,396
So the first app that we have


102
00:03:16,396 --> 00:03:18,356
here is an app that you wrote,


103
00:03:19,656 --> 00:03:20,976
and as part of the app, you


104
00:03:20,976 --> 00:03:22,656
enable a social messaging or


105
00:03:22,656 --> 00:03:23,936
peer-to-peer messaging feature.


106
00:03:24,426 --> 00:03:26,586
And an add-on feature in this


107
00:03:26,586 --> 00:03:28,586
app that you've created is the


108
00:03:28,586 --> 00:03:30,446
ability to show the right


109
00:03:30,446 --> 00:03:30,966
stickers.


110
00:03:31,526 --> 00:03:33,046
So based on the content of the


111
00:03:33,136 --> 00:03:34,936
message, which in this case is


112
00:03:34,936 --> 00:03:36,116
"It's getting late, I'm tired,


113
00:03:36,456 --> 00:03:37,216
we'll pick it up tomorrow


114
00:03:37,216 --> 00:03:38,076
morning, good night."


115
00:03:38,406 --> 00:03:39,866
Your app shows the appropriate


116
00:03:39,866 --> 00:03:40,116
sticker.


117
00:03:40,116 --> 00:03:41,706
You parsed this text, and you


118
00:03:41,706 --> 00:03:42,596
bring up the sticker.


119
00:03:42,796 --> 00:03:44,166
The user can attach it and send


120
00:03:44,166 --> 00:03:44,976
it as a response.


121
00:03:45,476 --> 00:03:46,286
So all of this is great.


122
00:03:46,286 --> 00:03:47,376
This app has been doing really


123
00:03:47,376 --> 00:03:47,626
well.


124
00:03:47,626 --> 00:03:48,426
You've been getting rave


125
00:03:48,426 --> 00:03:48,936
reviews.


126
00:03:49,926 --> 00:03:51,436
But you also get feedback that


127
00:03:51,436 --> 00:03:52,776
your app is not multilingual.


128
00:03:53,186 --> 00:03:55,266
So it so happens that users are


129
00:03:55,416 --> 00:03:56,426
bilingual these days.


130
00:03:56,426 --> 00:03:57,546
They tend to communicate in


131
00:03:57,546 --> 00:03:59,186
several different languages, and


132
00:03:59,186 --> 00:04:00,226
your app, when it gets the


133
00:04:00,226 --> 00:04:01,886
message in Chinese, simply


134
00:04:01,886 --> 00:04:03,886
doesn't know what to do with it.


135
00:04:04,086 --> 00:04:05,246
So how can we use natural


136
00:04:05,246 --> 00:04:06,346
language to overcome this


137
00:04:06,346 --> 00:04:06,756
problem?


138
00:04:06,926 --> 00:04:09,816
Well, we can do this with two


139
00:04:09,816 --> 00:04:11,266
simple calls to two different


140
00:04:11,266 --> 00:04:11,636
APIs.


141
00:04:12,216 --> 00:04:13,256
The first is language


142
00:04:13,256 --> 00:04:13,986
identification.


143
00:04:14,326 --> 00:04:16,106
With the new Natural Language


144
00:04:16,106 --> 00:04:17,396
framework, you start off by


145
00:04:17,456 --> 00:04:18,696
importing Natural Language.


146
00:04:19,836 --> 00:04:21,226
You create an instance of


147
00:04:21,226 --> 00:04:23,146
NLLanguageRecognizer class.


148
00:04:24,006 --> 00:04:25,456
You attach the string that you


149
00:04:25,456 --> 00:04:27,476
would like to process, and you


150
00:04:27,476 --> 00:04:28,676
simply call the dominant


151
00:04:28,676 --> 00:04:29,436
language API.


152
00:04:30,036 --> 00:04:32,256
Now this will return the single


153
00:04:32,256 --> 00:04:34,596
best hypothesis in terms of


154
00:04:34,596 --> 00:04:35,746
language for the string.


155
00:04:36,806 --> 00:04:38,036
So the output here is


156
00:04:38,036 --> 00:04:39,446
essentially simplified Chinese.


157
00:04:40,466 --> 00:04:41,856
Now in Natural Language, we also


158
00:04:41,856 --> 00:04:42,906
support a new API.


159
00:04:43,496 --> 00:04:44,766
There are instances where you


160
00:04:44,766 --> 00:04:46,046
would like to know the top-end


161
00:04:46,046 --> 00:04:47,506
hypothesis for a particular


162
00:04:47,506 --> 00:04:47,876
string.


163
00:04:48,146 --> 00:04:49,996
So you'd like to know what are


164
00:04:49,996 --> 00:04:51,396
the top languages along with


165
00:04:51,396 --> 00:04:52,796
their associated probabilities.


166
00:04:52,836 --> 00:04:54,776
So you can envision using this


167
00:04:54,776 --> 00:04:55,476
in several different


168
00:04:55,476 --> 00:04:56,906
applications where there's a lot


169
00:04:56,906 --> 00:04:58,636
of multilinguality, and you want


170
00:04:58,636 --> 00:05:00,066
that leeway in terms of what


171
00:05:00,066 --> 00:05:01,366
could be the top hypothesis.


172
00:05:02,126 --> 00:05:03,636
So you can do this with a new


173
00:05:03,636 --> 00:05:05,856
API called Language Hypotheses.


174
00:05:06,176 --> 00:05:07,336
You can specify the maximum


175
00:05:07,336 --> 00:05:08,306
number of languages that you


176
00:05:08,306 --> 00:05:09,866
want, and what you get back is


177
00:05:09,866 --> 00:05:11,126
an object with the top-end


178
00:05:11,126 --> 00:05:12,706
languages and their associated


179
00:05:12,706 --> 00:05:13,446
probabilities.


180
00:05:14,596 --> 00:05:15,836
Now in order to tokenize this


181
00:05:15,836 --> 00:05:17,816
Chinese text, you can again see


182
00:05:17,816 --> 00:05:19,186
the pattern is very similar.


183
00:05:20,066 --> 00:05:21,196
You again import Natural


184
00:05:21,196 --> 00:05:21,646
Language.


185
00:05:22,316 --> 00:05:23,616
You create an instance of the


186
00:05:23,616 --> 00:05:25,856
NLTokenizer, and in this


187
00:05:25,856 --> 00:05:27,576
particular instance, you specify


188
00:05:27,576 --> 00:05:28,856
the unit to be word because you


189
00:05:28,856 --> 00:05:30,126
want to tokenize the string into


190
00:05:30,126 --> 00:05:30,666
words.


191
00:05:31,176 --> 00:05:34,776
You attach the string, and you


192
00:05:34,776 --> 00:05:36,486
simply call the tokens method on


193
00:05:36,486 --> 00:05:38,006
the string, on the object.


194
00:05:39,006 --> 00:05:40,446
And what you get is an array of


195
00:05:40,516 --> 00:05:41,096
tokens here.


196
00:05:41,526 --> 00:05:43,296
Now with this array of tokens,


197
00:05:43,546 --> 00:05:44,896
you can look up the particular


198
00:05:45,136 --> 00:05:47,756
token here is goodnight, and lo


199
00:05:47,756 --> 00:05:48,676
and behold, you have


200
00:05:48,676 --> 00:05:50,256
multilingual support in your


201
00:05:50,716 --> 00:05:50,783
app.


202
00:05:51,026 --> 00:05:52,746
So your app can now support


203
00:05:52,746 --> 00:05:54,626
Chinese with simple calls to


204
00:05:54,626 --> 00:05:56,446
language identification and


205
00:05:56,636 --> 00:05:58,966
tokenization APIs.


206
00:05:59,026 --> 00:06:00,456
Now let's look at a different


207
00:06:00,456 --> 00:06:01,046
sort of an API.


208
00:06:01,046 --> 00:06:02,206
I mean language identification


209
00:06:02,206 --> 00:06:03,586
and tokenization are good, but


210
00:06:03,586 --> 00:06:05,406
we would also like to use auto


211
00:06:05,406 --> 00:06:06,636
speech tagging, named entity


212
00:06:06,636 --> 00:06:07,746
recognition, and so on.


213
00:06:08,066 --> 00:06:09,616
So let me illustrate how to use


214
00:06:09,616 --> 00:06:10,936
named entity recognition API


215
00:06:11,356 --> 00:06:14,106
again with the hypothetical app.


216
00:06:14,346 --> 00:06:15,276
So here's an app.


217
00:06:15,506 --> 00:06:17,026
It's a news recommendation app.


218
00:06:17,646 --> 00:06:18,906
So as part of this app, your


219
00:06:18,906 --> 00:06:20,576
user has been going and reading


220
00:06:20,576 --> 00:06:21,906
a lot of things about the royal


221
00:06:21,906 --> 00:06:23,206
wedding, so a really curious


222
00:06:23,206 --> 00:06:23,456
user.


223
00:06:23,456 --> 00:06:24,326
They want to find everything


224
00:06:24,326 --> 00:06:25,076
about the royal wedding.


225
00:06:25,466 --> 00:06:26,806
So they've perused a lot of


226
00:06:26,806 --> 00:06:28,886
pages in your app, and then they


227
00:06:28,886 --> 00:06:30,556
go on to the search bar, and


228
00:06:30,556 --> 00:06:31,556
they type Harry.


229
00:06:32,276 --> 00:06:34,406
And what they see is completely


230
00:06:34,686 --> 00:06:35,756
things that are not pertinent to


231
00:06:35,756 --> 00:06:37,026
what they've been looking for.


232
00:06:37,286 --> 00:06:38,686
You get Harry Potter and so on


233
00:06:38,686 --> 00:06:39,246
and so forth.


234
00:06:39,616 --> 00:06:42,376
What you'd like to see is Prince


235
00:06:42,376 --> 00:06:44,036
Harry, something related to the


236
00:06:44,116 --> 00:06:44,716
royal wedding.


237
00:06:45,296 --> 00:06:46,656
So now you can overcome this


238
00:06:46,656 --> 00:06:48,436
issue in your app by using the


239
00:06:48,436 --> 00:06:50,056
name entity recognition API.


240
00:06:51,376 --> 00:06:52,776
Again, as I mentioned, the


241
00:06:52,776 --> 00:06:54,346
syntax here is very familiar.


242
00:06:54,586 --> 00:06:56,006
Those of you who have been used


243
00:06:56,006 --> 00:06:57,506
to using NSLinguisticTagger,


244
00:06:57,776 --> 00:06:58,836
they should look familiar and


245
00:06:58,836 --> 00:07:00,276
feel familiar, but it's much


246
00:07:00,276 --> 00:07:01,656
easier to remember and to use.


247
00:07:02,126 --> 00:07:03,986
You import Natural Language.


248
00:07:04,866 --> 00:07:06,246
You now create an instance of


249
00:07:06,306 --> 00:07:08,516
NLTagger, and you specify the


250
00:07:08,516 --> 00:07:09,996
scheme type to be name type.


251
00:07:10,926 --> 00:07:11,896
If you want part of speech


252
00:07:11,896 --> 00:07:13,076
tagging, then you would specify


253
00:07:13,076 --> 00:07:14,426
the scheme type to be lexical


254
00:07:14,426 --> 00:07:14,906
class.


255
00:07:15,656 --> 00:07:17,556
You again specify the string


256
00:07:17,556 --> 00:07:18,546
that you want to process.


257
00:07:19,466 --> 00:07:21,336
In this particular instance, you


258
00:07:21,336 --> 00:07:23,446
specify the language, so you set


259
00:07:23,446 --> 00:07:24,636
the language to be English.


260
00:07:24,866 --> 00:07:27,016
So if you were not familiar or


261
00:07:27,016 --> 00:07:28,026
if you're not sure about what


262
00:07:28,026 --> 00:07:29,646
the language is, Natural


263
00:07:29,646 --> 00:07:30,956
Language will automatically


264
00:07:30,956 --> 00:07:32,726
recognize the language using the


265
00:07:32,726 --> 00:07:34,076
language identification API


266
00:07:34,076 --> 00:07:34,886
under the hood.


267
00:07:35,116 --> 00:07:37,816
And finally, you call the tags


268
00:07:38,296 --> 00:07:39,936
method on this object that you


269
00:07:39,936 --> 00:07:40,556
just created.


270
00:07:40,836 --> 00:07:42,456
You specify the unit to be word


271
00:07:42,456 --> 00:07:44,026
and the scheme to be name type.


272
00:07:44,536 --> 00:07:46,826
And what you get as an output is


273
00:07:47,166 --> 00:07:49,006
the person names here, Prince


274
00:07:49,006 --> 00:07:50,426
Harry and Meghan Markle, and the


275
00:07:50,426 --> 00:07:51,526
location to be Windsor.


276
00:07:51,776 --> 00:07:53,376
Now if the user were to go back


277
00:07:53,376 --> 00:07:55,246
to the search bar, based on the


278
00:07:55,246 --> 00:07:56,596
contextual information of what


279
00:07:56,596 --> 00:07:58,006
the user has been browsing, you


280
00:07:58,006 --> 00:07:59,326
can significantly enhance the


281
00:07:59,326 --> 00:08:01,386
search experience in your app.


282
00:08:03,526 --> 00:08:04,976
So there's a lot more


283
00:08:04,976 --> 00:08:06,256
information about how to use


284
00:08:06,256 --> 00:08:06,666
these APIs.


285
00:08:06,666 --> 00:08:07,906
You can go to the developer


286
00:08:07,906 --> 00:08:09,326
documentation and find more


287
00:08:09,326 --> 00:08:09,936
information.


288
00:08:10,376 --> 00:08:11,926
What I'd like to emphasize here


289
00:08:11,976 --> 00:08:15,106
is NSLinguisticTagger is still


290
00:08:15,106 --> 00:08:17,006
supported, but the future of NLP


291
00:08:17,006 --> 00:08:18,596
is in Natural Language.


292
00:08:18,596 --> 00:08:20,296
So we recommend that and


293
00:08:20,296 --> 00:08:22,026
encourage you to move to Natural


294
00:08:22,026 --> 00:08:23,566
Language so that you can get all


295
00:08:23,566 --> 00:08:24,896
the latest functionalities of


296
00:08:24,946 --> 00:08:26,226
NLP in this framework.


297
00:08:26,996 --> 00:08:29,616
Now let's shift gears and look


298
00:08:29,616 --> 00:08:31,996
at a situation where you have an


299
00:08:31,996 --> 00:08:34,525
idea for an app, or you need a


300
00:08:34,525 --> 00:08:36,376
functionality within your app


301
00:08:36,376 --> 00:08:37,666
that Natural Language does not


302
00:08:37,666 --> 00:08:38,076
support.


303
00:08:38,506 --> 00:08:40,106
What do you do?


304
00:08:40,106 --> 00:08:42,876
So you can certainly create


305
00:08:42,876 --> 00:08:44,936
something, which will be great,


306
00:08:45,306 --> 00:08:46,356
but what if we gave you the


307
00:08:46,356 --> 00:08:48,176
tools to make that much easier?


308
00:08:50,756 --> 00:08:53,376
To talk about custom NLP models


309
00:08:53,376 --> 00:08:54,656
and how to build custom NLP


310
00:08:54,656 --> 00:08:56,766
models using Create ML and use


311
00:08:56,766 --> 00:08:58,706
the subsequent models, which are


312
00:08:58,706 --> 00:09:00,126
essentially Code ML models in


313
00:09:00,126 --> 00:09:01,496
Natural Language, I'm going to


314
00:09:01,576 --> 00:09:02,816
hand it over to Doug Davidson.


315
00:09:03,516 --> 00:09:09,206
[ Applause ]


316
00:09:09,706 --> 00:09:10,366
>> Thanks Vivek.


317
00:09:11,146 --> 00:09:13,816
So I'm really excited about the


318
00:09:13,816 --> 00:09:15,386
new Natural Language framework,


319
00:09:15,736 --> 00:09:17,226
but the part I'm most excited


320
00:09:17,226 --> 00:09:19,586
about is support for portraying


321
00:09:19,586 --> 00:09:21,226
and using custom models.


322
00:09:21,536 --> 00:09:22,526
And why is that?


323
00:09:23,426 --> 00:09:24,576
I'd just like you to think for a


324
00:09:24,576 --> 00:09:27,636
second about your apps, maybe


325
00:09:27,636 --> 00:09:29,366
the apps you've written or the


326
00:09:29,366 --> 00:09:31,606
apps that you want to write, and


327
00:09:31,606 --> 00:09:32,856
think about how they could


328
00:09:33,336 --> 00:09:35,166
improve the user experience if


329
00:09:35,166 --> 00:09:36,756
they just knew a little more


330
00:09:37,536 --> 00:09:39,116
about the text that they deal


331
00:09:39,116 --> 00:09:39,286
with.


332
00:09:40,356 --> 00:09:42,206
And then think for a second


333
00:09:42,646 --> 00:09:44,756
about how you analyze text.


334
00:09:44,996 --> 00:09:46,756
So maybe you look at some


335
00:09:46,756 --> 00:09:49,446
examples of text, and you learn


336
00:09:49,446 --> 00:09:51,816
from them, and then you


337
00:09:52,266 --> 00:09:53,876
understand what's going on, and


338
00:09:53,876 --> 00:09:54,836
then you can look at a piece of


339
00:09:54,836 --> 00:09:56,346
text, and at a glance, you can


340
00:09:56,346 --> 00:09:57,476
figure out something about


341
00:09:57,476 --> 00:09:58,386
what's going on with it.


342
00:09:59,366 --> 00:10:01,306
Well, if that's the case, then


343
00:10:01,356 --> 00:10:02,566
there's at least a reasonable


344
00:10:02,566 --> 00:10:05,746
chance that you can train a


345
00:10:05,966 --> 00:10:08,216
machine learning model to do


346
00:10:08,216 --> 00:10:09,686
that sort of analysis in your


347
00:10:09,686 --> 00:10:11,206
app automatically for you,


348
00:10:11,646 --> 00:10:14,166
giving it examples that it can


349
00:10:14,166 --> 00:10:16,576
train and learn from and produce


350
00:10:16,576 --> 00:10:18,396
a model that can do that


351
00:10:18,396 --> 00:10:19,156
analysis.


352
00:10:19,966 --> 00:10:23,316
Now, there are many, many types


353
00:10:23,346 --> 00:10:24,776
of machine learning models for


354
00:10:24,826 --> 00:10:26,176
NLP, and there are many


355
00:10:26,176 --> 00:10:27,206
different ways of training it.


356
00:10:27,206 --> 00:10:28,606
Probably many of you are already


357
00:10:28,606 --> 00:10:29,546
training machine learning


358
00:10:29,546 --> 00:10:32,516
models, but our task here has


359
00:10:32,516 --> 00:10:35,286
been to produce ways to make


360
00:10:35,286 --> 00:10:36,806
this sort of training really,


361
00:10:36,806 --> 00:10:39,056
really easy and to make it


362
00:10:39,056 --> 00:10:41,036
integrate really well with the


363
00:10:41,036 --> 00:10:42,696
Natural Language framework and


364
00:10:42,696 --> 00:10:43,146
APIs.


365
00:10:43,556 --> 00:10:45,626
So with that in mind, we are


366
00:10:45,626 --> 00:10:47,826
supporting two types of models


367
00:10:48,366 --> 00:10:50,776
that we think support a broad


368
00:10:50,776 --> 00:10:53,016
range of functionality and that


369
00:10:53,016 --> 00:10:55,216
work well with our paradigm in


370
00:10:55,216 --> 00:10:57,386
NLTagger of applying labels to


371
00:10:57,386 --> 00:10:58,476
pieces of text.


372
00:10:58,796 --> 00:11:00,016
So the first of model we're


373
00:11:00,016 --> 00:11:02,366
supporting is a text classifier.


374
00:11:02,896 --> 00:11:05,376
A text classifier takes a chunk


375
00:11:05,376 --> 00:11:07,146
of text, maybe it's a sentence


376
00:11:07,146 --> 00:11:09,126
or a paragraph or an entire


377
00:11:09,126 --> 00:11:11,226
document, and applies a label to


378
00:11:11,226 --> 00:11:11,366
it.


379
00:11:11,786 --> 00:11:13,376
Examples of this in our existing


380
00:11:13,376 --> 00:11:15,286
APIs are things like language


381
00:11:15,286 --> 00:11:16,466
identification, script


382
00:11:16,466 --> 00:11:17,216
identification.


383
00:11:18,276 --> 00:11:19,436
The second type of model we


384
00:11:19,436 --> 00:11:21,846
support is a word tagger, and a


385
00:11:21,846 --> 00:11:23,836
word tagger takes a sentence


386
00:11:23,886 --> 00:11:25,856
considered as a sequence of


387
00:11:25,856 --> 00:11:28,196
words, and then applies a label


388
00:11:28,196 --> 00:11:30,386
to each word in a sentence in


389
00:11:30,386 --> 00:11:32,366
context, and examples of


390
00:11:32,366 --> 00:11:34,916
existing APIs are things like


391
00:11:34,916 --> 00:11:36,576
speech tagging and named entity


392
00:11:36,576 --> 00:11:37,116
recognition.


393
00:11:37,636 --> 00:11:39,316
But those are sort of general


394
00:11:39,396 --> 00:11:42,656
purpose examples of these kinds


395
00:11:42,656 --> 00:11:43,336
of models.


396
00:11:44,006 --> 00:11:45,616
You can do a lot more with them


397
00:11:46,026 --> 00:11:47,236
if you have a special purpose


398
00:11:47,236 --> 00:11:48,446
model for your specific


399
00:11:48,446 --> 00:11:49,006
application.


400
00:11:49,006 --> 00:11:50,036
Let me give you some


401
00:11:50,036 --> 00:11:51,366
hypothetical examples.


402
00:11:51,956 --> 00:11:53,576
So, for text classification,


403
00:11:53,946 --> 00:11:55,556
suppose you're dealing with user


404
00:11:55,556 --> 00:11:57,066
reviews, and you want to know


405
00:11:57,066 --> 00:11:59,406
automatically whether a given


406
00:11:59,406 --> 00:12:02,016
review is a positive review or a


407
00:12:02,016 --> 00:12:03,356
negative review or somewhere in


408
00:12:03,356 --> 00:12:03,816
between.


409
00:12:04,296 --> 00:12:05,216
Well, this is the sort of thing


410
00:12:05,216 --> 00:12:06,256
you could train a text


411
00:12:06,286 --> 00:12:07,066
classifier to do.


412
00:12:07,066 --> 00:12:07,746
This is sentiment


413
00:12:07,816 --> 00:12:08,536
classification.


414
00:12:10,516 --> 00:12:12,556
Or suppose you have articles or


415
00:12:12,556 --> 00:12:14,206
just article summaries or maybe


416
00:12:14,206 --> 00:12:15,596
even just article headlines, and


417
00:12:15,596 --> 00:12:16,376
you want to determine


418
00:12:16,376 --> 00:12:18,486
automatically what topic they


419
00:12:18,486 --> 00:12:19,726
belong to according to your


420
00:12:19,726 --> 00:12:21,866
favorite topic classification


421
00:12:21,866 --> 00:12:22,266
scheme.


422
00:12:22,746 --> 00:12:23,836
This again is the sort of thing


423
00:12:23,836 --> 00:12:26,596
that you can train a text


424
00:12:26,596 --> 00:12:28,096
classifier to do for you.


425
00:12:28,656 --> 00:12:31,246
Or going a little further,


426
00:12:31,246 --> 00:12:32,486
suppose you're writing an


427
00:12:32,486 --> 00:12:35,286
automated travel agent, and when


428
00:12:35,286 --> 00:12:36,976
you get a request from a client,


429
00:12:37,246 --> 00:12:38,426
the first thing you want to know


430
00:12:38,426 --> 00:12:40,216
probably is what are they asking


431
00:12:40,216 --> 00:12:40,526
about?


432
00:12:40,796 --> 00:12:42,506
Is it hotels or restaurants or


433
00:12:42,506 --> 00:12:43,746
flights or whatever else that


434
00:12:43,746 --> 00:12:44,266
you handle.


435
00:12:44,886 --> 00:12:46,056
This is the sort of thing that


436
00:12:46,056 --> 00:12:47,576
you could train a text


437
00:12:47,636 --> 00:12:49,316
classifier to answer for you.


438
00:12:50,946 --> 00:12:52,566
Going on to word tagging.


439
00:12:53,106 --> 00:12:56,586
So we provide word taggers that


440
00:12:56,586 --> 00:12:58,166
do part of speech tagging for a


441
00:12:58,166 --> 00:12:59,326
number of different languages,


442
00:12:59,696 --> 00:13:01,646
but suppose you happen to need


443
00:13:01,646 --> 00:13:03,066
to do part of speech tagging for


444
00:13:03,066 --> 00:13:04,296
some language that we don't


445
00:13:04,296 --> 00:13:05,686
happen to support quite yet.


446
00:13:06,426 --> 00:13:07,956
Well, with custom model support,


447
00:13:07,956 --> 00:13:10,006
you could train a word tagger to


448
00:13:10,006 --> 00:13:10,906
do that for you.


449
00:13:12,266 --> 00:13:14,206
Or named entity recognition.


450
00:13:14,746 --> 00:13:16,056
So we provide built-in named


451
00:13:16,056 --> 00:13:16,976
entity recognition that


452
00:13:16,976 --> 00:13:19,106
recognizes names of people and


453
00:13:19,106 --> 00:13:21,516
places and organizations, but


454
00:13:21,516 --> 00:13:22,766
suppose you had some other kind


455
00:13:22,766 --> 00:13:23,356
of name that you were


456
00:13:23,356 --> 00:13:24,686
particularly interested in that


457
00:13:24,686 --> 00:13:25,906
we don't happen to support right


458
00:13:25,906 --> 00:13:26,166
now.


459
00:13:26,506 --> 00:13:28,136
So, like for example, product


460
00:13:28,136 --> 00:13:28,456
names.


461
00:13:28,456 --> 00:13:30,146
So you could train your own


462
00:13:30,146 --> 00:13:31,876
custom named entity recognizer


463
00:13:32,036 --> 00:13:33,656
as a word tagger that would


464
00:13:33,656 --> 00:13:36,456
recognize names or other terms


465
00:13:36,496 --> 00:13:37,526
of whatever sort you are


466
00:13:37,526 --> 00:13:39,426
particularly interested in.


467
00:13:40,616 --> 00:13:42,956
Even further, for your automated


468
00:13:42,956 --> 00:13:44,356
travel agent, once you know what


469
00:13:44,356 --> 00:13:45,646
the user is asking about,


470
00:13:46,166 --> 00:13:47,326
probably the next thing you want


471
00:13:47,326 --> 00:13:49,106
to know is what are the relevant


472
00:13:49,106 --> 00:13:50,566
terms in their request.


473
00:13:50,626 --> 00:13:51,816
For example, if it's a flight


474
00:13:51,816 --> 00:13:52,416
request.


475
00:13:52,676 --> 00:13:53,856
Where do they want to go from


476
00:13:53,856 --> 00:13:54,386
and to?


477
00:13:55,546 --> 00:13:57,566
So a word tagger can identify


478
00:13:58,216 --> 00:14:01,386
various kinds of terms in a


479
00:14:01,996 --> 00:14:02,236
sentence.


480
00:14:02,326 --> 00:14:04,766
Or another application, if you


481
00:14:04,766 --> 00:14:06,086
need to take a sentence and


482
00:14:06,086 --> 00:14:07,966
divide it up into phrases, noun


483
00:14:07,966 --> 00:14:09,356
phrases, verb phrases,


484
00:14:09,426 --> 00:14:10,596
propositional phrases.


485
00:14:10,996 --> 00:14:11,906
With the appropriate sort of


486
00:14:11,906 --> 00:14:14,186
labeling, you could train a word


487
00:14:14,186 --> 00:14:16,206
tagger to do this, and many,


488
00:14:16,206 --> 00:14:19,556
many other kinds of tasks can be


489
00:14:19,556 --> 00:14:21,796
phrased in terms of labeling,


490
00:14:22,136 --> 00:14:23,946
applying labels to portions of


491
00:14:23,946 --> 00:14:26,346
text, either words in sequence


492
00:14:26,696 --> 00:14:29,176
or chunks of text in the text


493
00:14:29,176 --> 00:14:29,756
classifier.


494
00:14:33,676 --> 00:14:36,246
So these are supervised machine


495
00:14:36,246 --> 00:14:37,526
learning models, so there are


496
00:14:37,526 --> 00:14:39,946
two phases always involved.


497
00:14:40,266 --> 00:14:41,806
The first phase is training, and


498
00:14:41,996 --> 00:14:43,506
the second phase is inference.


499
00:14:43,786 --> 00:14:46,016
So training is what you do in


500
00:14:46,016 --> 00:14:46,996
part of your development


501
00:14:46,996 --> 00:14:47,726
process.


502
00:14:48,256 --> 00:14:50,926
You take labeled training data,


503
00:14:51,936 --> 00:14:54,356
and you feed it into Create ML


504
00:14:54,606 --> 00:14:55,806
and produce a model.


505
00:14:57,266 --> 00:14:59,266
Inference is then what happens


506
00:14:59,336 --> 00:15:01,246
in your app when you incorporate


507
00:15:01,486 --> 00:15:03,036
that model into your application


508
00:15:03,486 --> 00:15:05,406
at run time when it encounters


509
00:15:05,406 --> 00:15:06,956
some piece of data from the


510
00:15:06,956 --> 00:15:08,986
user, and then it analyzes that


511
00:15:08,986 --> 00:15:09,976
data and predicts the


512
00:15:09,976 --> 00:15:11,346
appropriate labels for it.


513
00:15:11,806 --> 00:15:13,396
So let's see how these phases


514
00:15:13,396 --> 00:15:13,676
work.


515
00:15:14,936 --> 00:15:16,246
So let's start with training,


516
00:15:17,076 --> 00:15:18,366
and training always starts with


517
00:15:18,426 --> 00:15:18,786
data.


518
00:15:19,406 --> 00:15:22,056
You take your training data, and


519
00:15:22,056 --> 00:15:23,856
then you feed it in to in this


520
00:15:23,856 --> 00:15:26,956
case Create ML in a playground


521
00:15:26,956 --> 00:15:29,236
let us say or a script


522
00:15:30,286 --> 00:15:30,986
[inaudible] as you may have seen


523
00:15:30,986 --> 00:15:32,066
in the Create ML session.


524
00:15:32,616 --> 00:15:34,606
Create ML calls the Natural


525
00:15:34,606 --> 00:15:35,446
Language framework under the


526
00:15:35,446 --> 00:15:38,196
hood to do the training, and


527
00:15:38,196 --> 00:15:40,806
what comes out is a core ML


528
00:15:40,806 --> 00:15:43,726
model that's optimized for use


529
00:15:43,766 --> 00:15:44,586
on device.


530
00:15:45,956 --> 00:15:47,616
So let's look at what this data


531
00:15:47,616 --> 00:15:49,416
might look like.


532
00:15:49,786 --> 00:15:51,396
So Create ML supports a number


533
00:15:51,396 --> 00:15:52,856
of different data formats.


534
00:15:53,276 --> 00:15:55,376
Right here we're showing our


535
00:15:55,376 --> 00:15:57,666
data in JSON because JSON makes


536
00:15:57,666 --> 00:16:00,946
things perfectly clear, and this


537
00:16:00,946 --> 00:16:04,436
is a piece of training data for


538
00:16:04,816 --> 00:16:06,516
a text classifier that's a


539
00:16:06,516 --> 00:16:07,666
sediment classifier.


540
00:16:07,916 --> 00:16:11,396
So each training example, like


541
00:16:11,396 --> 00:16:13,546
this one, consists of two parts,


542
00:16:13,856 --> 00:16:16,686
a chunk of text, and the


543
00:16:16,686 --> 00:16:17,986
appropriate label for it.


544
00:16:18,676 --> 00:16:20,696
And so this for example is a


545
00:16:20,696 --> 00:16:22,056
positive sentence, so the label


546
00:16:22,056 --> 00:16:23,206
is positive, but you can pick


547
00:16:23,246 --> 00:16:27,636
whatever label set you want.


548
00:16:27,816 --> 00:16:29,896
Now, then when you start using


549
00:16:29,896 --> 00:16:31,806
Create ML, the Create ML


550
00:16:32,476 --> 00:16:34,066
provides a very, very simple way


551
00:16:34,156 --> 00:16:36,296
to train models in just a few


552
00:16:36,296 --> 00:16:36,986
lines of code.


553
00:16:36,986 --> 00:16:40,046
First line, we just load our


554
00:16:40,146 --> 00:16:42,446
training data from our JSON


555
00:16:42,446 --> 00:16:42,846
file.


556
00:16:42,886 --> 00:16:44,676
So we give it a URL to the JSON


557
00:16:44,676 --> 00:16:48,116
file, create a Create ML data


558
00:16:48,116 --> 00:16:48,836
table from it.


559
00:16:49,996 --> 00:16:52,216
Then in one line of code, create


560
00:16:52,216 --> 00:16:55,046
and train a text classifier from


561
00:16:55,046 --> 00:16:55,586
this data.


562
00:16:55,806 --> 00:16:57,086
All you have to tell it is what


563
00:16:57,086 --> 00:16:58,456
the names of the fields are,


564
00:16:58,866 --> 00:17:02,046
text and label, and then once


565
00:17:02,046 --> 00:17:03,536
you have it, one line of code


566
00:17:03,536 --> 00:17:07,116
writes that model out to disk.


567
00:17:07,286 --> 00:17:08,675
Now for training a Word Tagger,


568
00:17:08,776 --> 00:17:09,556
it's very similar.


569
00:17:09,976 --> 00:17:11,286
The data is just a little more


570
00:17:11,286 --> 00:17:13,526
complicated because each example


571
00:17:13,526 --> 00:17:15,076
is not a single piece of text.


572
00:17:15,306 --> 00:17:18,226
It's a sequence of tokens, and


573
00:17:18,786 --> 00:17:20,316
the labels are, again, a


574
00:17:20,316 --> 00:17:21,726
sequence of labels, the same


575
00:17:21,726 --> 00:17:23,955
number of labels, one label for


576
00:17:23,955 --> 00:17:24,546
each token.


577
00:17:24,935 --> 00:17:26,776
So this, for example, is


578
00:17:26,826 --> 00:17:29,976
training data for a Word Tagger


579
00:17:29,976 --> 00:17:31,236
that does name identity


580
00:17:31,236 --> 00:17:34,526
recognition, and each word, each


581
00:17:34,526 --> 00:17:36,836
token, has a label, either none,


582
00:17:36,836 --> 00:17:39,696
it's not a name, or org, it's an


583
00:17:39,696 --> 00:17:42,376
organization name or prod, it's


584
00:17:42,376 --> 00:17:44,296
product name, or a number of


585
00:17:44,326 --> 00:17:45,856
different other labels for


586
00:17:45,856 --> 00:17:47,146
whatever kinds of names you're


587
00:17:47,146 --> 00:17:47,806
recognizing.


588
00:17:48,236 --> 00:17:51,016
So each token has a label, and


589
00:17:51,016 --> 00:17:54,306
each sample consists of one


590
00:17:54,456 --> 00:17:55,906
sequence of tokens and their


591
00:17:55,906 --> 00:17:56,986
corresponding labels.


592
00:17:58,606 --> 00:18:01,066
And then the Create ML to train


593
00:18:01,066 --> 00:18:02,886
this is almost identical.


594
00:18:03,996 --> 00:18:06,276
You load the training data into


595
00:18:06,276 --> 00:18:08,346
a data table from the JSON.


596
00:18:09,896 --> 00:18:12,996
Then you create and train a Word


597
00:18:12,996 --> 00:18:14,336
Tagger, in this case instead of


598
00:18:14,336 --> 00:18:16,426
a text classifier, and then you


599
00:18:16,426 --> 00:18:17,806
write it out to disk.


600
00:18:18,306 --> 00:18:20,576
Now, there are a number of other


601
00:18:20,576 --> 00:18:23,026
options and APIs available in


602
00:18:23,026 --> 00:18:23,626
Create ML.


603
00:18:23,626 --> 00:18:24,906
I encourage you to, if you


604
00:18:24,906 --> 00:18:26,026
haven't already, take a look at


605
00:18:26,026 --> 00:18:27,826
the Create ML session, which


606
00:18:27,826 --> 00:18:29,696
happened yesterday, and Create


607
00:18:29,696 --> 00:18:31,306
ML documentation for more


608
00:18:31,306 --> 00:18:32,236
information on that.


609
00:18:33,066 --> 00:18:35,086
Now, once you have your model,


610
00:18:35,276 --> 00:18:36,236
we then go to the inference


611
00:18:36,236 --> 00:18:36,506
part.


612
00:18:36,696 --> 00:18:38,406
So you take your model, you drag


613
00:18:38,406 --> 00:18:39,796
it into your Xcode project.


614
00:18:40,186 --> 00:18:41,846
Xcode compiles it and includes


615
00:18:41,846 --> 00:18:43,126
it in your applications


616
00:18:43,226 --> 00:18:45,156
resources, and then what do you


617
00:18:45,156 --> 00:18:45,996
do at run time?


618
00:18:46,686 --> 00:18:49,056
Well, it's a Core ML model.


619
00:18:49,056 --> 00:18:50,136
You could use it like any other


620
00:18:50,136 --> 00:18:51,736
Core ML model, but the


621
00:18:51,736 --> 00:18:53,626
interesting thing is that these


622
00:18:53,626 --> 00:18:56,216
models are able to work well


623
00:18:56,596 --> 00:18:58,706
with the Natural Language APIs


624
00:18:59,356 --> 00:19:01,566
just like our built-in models


625
00:19:01,566 --> 00:19:02,826
that provide the existing


626
00:19:02,826 --> 00:19:04,956
functionality for NLP.


627
00:19:06,666 --> 00:19:09,696
So what will happen is data


628
00:19:09,696 --> 00:19:10,306
comes in.


629
00:19:11,296 --> 00:19:12,976
You pass it to Natural Language,


630
00:19:14,126 --> 00:19:16,226
which will use that model and do


631
00:19:16,226 --> 00:19:18,936
everything necessary to get all


632
00:19:18,936 --> 00:19:20,806
of the labels out and then pass


633
00:19:20,856 --> 00:19:22,706
back either a label, single


634
00:19:22,706 --> 00:19:24,196
label for a classifier or a


635
00:19:24,196 --> 00:19:27,106
sequence of labels for a tagger.


636
00:19:27,656 --> 00:19:32,316
And so how do you do this in


637
00:19:32,316 --> 00:19:33,296
Natural Language API?


638
00:19:34,326 --> 00:19:36,176
First thing you have to do is


639
00:19:36,176 --> 00:19:38,366
just locate that model in your


640
00:19:38,366 --> 00:19:40,296
application's resources, and


641
00:19:40,296 --> 00:19:41,636
then you create an instance of a


642
00:19:41,636 --> 00:19:43,406
class in Natural Language called


643
00:19:43,456 --> 00:19:45,626
ML Model from it.


644
00:19:46,026 --> 00:19:48,026
And then, well, the simplest


645
00:19:48,026 --> 00:19:49,036
thing you can do with it, at


646
00:19:49,076 --> 00:19:51,396
least for a classifier, is just


647
00:19:51,396 --> 00:19:52,976
pass it in a chunk of text and


648
00:19:52,976 --> 00:19:53,756
get a label out.


649
00:19:54,696 --> 00:19:55,886
But the more interesting thing


650
00:19:55,936 --> 00:19:59,116
is that you can use these models


651
00:19:59,116 --> 00:20:02,226
with NLTagger in exactly the


652
00:20:02,226 --> 00:20:03,866
same way that you use our


653
00:20:03,866 --> 00:20:06,976
built-in models for an existing


654
00:20:06,976 --> 00:20:07,716
functionality.


655
00:20:08,176 --> 00:20:09,036
So let me show you how that


656
00:20:09,036 --> 00:20:09,466
works.


657
00:20:10,366 --> 00:20:12,326
In addition to the existing tag


658
00:20:12,326 --> 00:20:13,776
schemes that we have for things


659
00:20:13,776 --> 00:20:15,336
like named identity recognition


660
00:20:15,336 --> 00:20:17,136
part of speech tagging, you can


661
00:20:17,136 --> 00:20:18,676
create your own custom tag


662
00:20:18,676 --> 00:20:22,446
scheme, give it a name, and then


663
00:20:22,516 --> 00:20:24,636
you can create a tagger that


664
00:20:24,636 --> 00:20:26,066
includes any number of different


665
00:20:26,066 --> 00:20:26,776
tag schemes.


666
00:20:27,146 --> 00:20:28,826
Your custom tag scheme or any of


667
00:20:28,826 --> 00:20:30,426
our built-in tag schemes or all


668
00:20:31,086 --> 00:20:33,086
of them, and then all you have


669
00:20:33,086 --> 00:20:35,496
to do is to tell the tagger to


670
00:20:35,496 --> 00:20:37,136
use your custom model for your


671
00:20:37,136 --> 00:20:38,146
custom tag scheme.


672
00:20:38,686 --> 00:20:40,886
And then you just use it


673
00:20:42,186 --> 00:20:42,706
normally.


674
00:20:43,246 --> 00:20:44,776
You attach a string to the


675
00:20:44,776 --> 00:20:46,756
tagger, and you can go through


676
00:20:46,756 --> 00:20:49,256
and look at the tags for


677
00:20:49,706 --> 00:20:51,736
whatever unit of text is


678
00:20:51,736 --> 00:20:52,996
appropriate for your particular


679
00:20:52,996 --> 00:20:56,296
model, and the tagger will


680
00:20:56,296 --> 00:20:58,246
automatically call the model as


681
00:20:58,246 --> 00:20:59,886
necessary to get the tags and


682
00:20:59,886 --> 00:21:01,436
return the tags to you and will


683
00:21:01,436 --> 00:21:03,616
do all the other things that


684
00:21:03,616 --> 00:21:05,116
NLTagger does automatically,


685
00:21:05,476 --> 00:21:06,886
like language identification,


686
00:21:06,886 --> 00:21:08,246
tokenization and so on and so


687
00:21:08,246 --> 00:21:08,526
forth.


688
00:21:08,526 --> 00:21:12,386
So I want to show this to you in


689
00:21:12,386 --> 00:21:14,016
a simple hypothetical example.


690
00:21:14,606 --> 00:21:16,686
And this hypothetical example is


691
00:21:17,726 --> 00:21:21,266
an app that users will use to


692
00:21:21,266 --> 00:21:23,076
store bookmarks to articles they


693
00:21:23,076 --> 00:21:24,526
may have run across and then


694
00:21:24,596 --> 00:21:26,056
might be intending to read later


695
00:21:26,056 --> 00:21:26,296
on.


696
00:21:27,236 --> 00:21:28,836
But the problem with this


697
00:21:28,836 --> 00:21:29,866
application as it currently


698
00:21:29,866 --> 00:21:32,066
stands is that the list of


699
00:21:32,066 --> 00:21:33,626
bookmarks is just one long list


700
00:21:33,626 --> 00:21:34,996
with no organization to it.


701
00:21:35,566 --> 00:21:37,176
Wouldn't it be nice if we could


702
00:21:37,336 --> 00:21:39,056
automatically classify these


703
00:21:39,056 --> 00:21:41,046
articles and put them into some


704
00:21:41,046 --> 00:21:42,726
organization according to topic?


705
00:21:43,316 --> 00:21:45,376
Well, we can train a classifier


706
00:21:45,376 --> 00:21:46,276
to do that for us.


707
00:21:47,066 --> 00:21:48,346
And the other thing is that the


708
00:21:48,346 --> 00:21:49,526
articles, when we look at them,


709
00:21:49,856 --> 00:21:51,346
it's a long stream of text.


710
00:21:52,386 --> 00:21:53,666
Maybe we'd like to highlight


711
00:21:53,666 --> 00:21:55,966
some interesting things in those


712
00:21:55,966 --> 00:21:58,286
articles like for example names.


713
00:21:58,796 --> 00:22:01,596
Well, we have provided built-in


714
00:22:01,596 --> 00:22:03,156
name identity recognition for


715
00:22:03,156 --> 00:22:04,666
names of people, places, and


716
00:22:04,666 --> 00:22:07,406
organizations, but maybe we also


717
00:22:07,406 --> 00:22:08,426
want to highlight names of


718
00:22:08,426 --> 00:22:10,016
products, so we could train a


719
00:22:10,076 --> 00:22:12,366
custom word tagger to identify


720
00:22:12,366 --> 00:22:14,086
those names for us.


721
00:22:15,036 --> 00:22:16,726
So let me go over to the demo


722
00:22:16,726 --> 00:22:17,086
machine.


723
00:22:20,736 --> 00:22:25,746
And so here's our application as


724
00:22:25,746 --> 00:22:27,586
it stands before we apply any


725
00:22:27,856 --> 00:22:29,116
natural language processing.


726
00:22:29,406 --> 00:22:30,706
As you can see, even just one


727
00:22:30,706 --> 00:22:32,736
long list of articles on the


728
00:22:32,736 --> 00:22:35,456
side and a big chunk of text for


729
00:22:35,456 --> 00:22:36,606
our article on the right.


730
00:22:36,606 --> 00:22:37,486
Well let's fix that.


731
00:22:38,716 --> 00:22:42,506
So, let's go into -- so the


732
00:22:42,506 --> 00:22:44,996
first part of training a model


733
00:22:45,516 --> 00:22:48,776
is data, and fortunately, I have


734
00:22:48,776 --> 00:22:49,576
some very hard-working


735
00:22:49,576 --> 00:22:50,976
colleagues at Apple who have


736
00:22:50,976 --> 00:22:52,696
collected for me some training


737
00:22:52,696 --> 00:22:54,446
data to train two models.


738
00:22:54,776 --> 00:22:57,066
The first model is a text


739
00:22:57,066 --> 00:22:59,026
classifier that will classify


740
00:22:59,026 --> 00:23:00,446
articles according to topic.


741
00:23:00,856 --> 00:23:01,896
So this is some of what the


742
00:23:01,896 --> 00:23:03,136
training data looks like.


743
00:23:03,716 --> 00:23:05,466
Each training example is a chunk


744
00:23:05,466 --> 00:23:07,106
of text and the appropriate


745
00:23:07,106 --> 00:23:10,386
label by topic, entertainment,


746
00:23:10,386 --> 00:23:12,016
politics, sports, and so on and


747
00:23:12,016 --> 00:23:13,026
so forth.


748
00:23:15,196 --> 00:23:18,436
And I also have some training


749
00:23:18,436 --> 00:23:21,486
data to train a word tagger that


750
00:23:21,486 --> 00:23:25,136
will recognize product names in


751
00:23:25,136 --> 00:23:25,836
sentences.


752
00:23:26,256 --> 00:23:27,766
So this training data is pretty


753
00:23:27,766 --> 00:23:28,106
simple.


754
00:23:28,666 --> 00:23:31,486
Each example consists of a


755
00:23:31,486 --> 00:23:32,736
sentence considered as a


756
00:23:32,736 --> 00:23:34,326
sequence of tokens and then a


757
00:23:34,326 --> 00:23:35,936
sequence of labels, and each


758
00:23:35,936 --> 00:23:38,456
label is either none, it's not a


759
00:23:38,456 --> 00:23:40,716
product name, or prod, it is a


760
00:23:40,716 --> 00:23:43,436
product name.


761
00:23:43,696 --> 00:23:46,066
So, let's try to train with


762
00:23:46,106 --> 00:23:46,436
these.


763
00:23:47,186 --> 00:23:50,406
So first thing I want to do is


764
00:23:50,926 --> 00:23:52,726
bring up a playground that I


765
00:23:52,726 --> 00:23:55,786
have using Create ML, and this


766
00:23:55,826 --> 00:23:58,576
playground will just load.


767
00:23:58,646 --> 00:24:00,826
In this case, this is my product


768
00:24:00,826 --> 00:24:01,496
word tagger.


769
00:24:01,716 --> 00:24:02,886
It'll load the training data,


770
00:24:03,796 --> 00:24:05,206
create a word tagger from it,


771
00:24:05,886 --> 00:24:07,036
and write it out to disk.


772
00:24:07,036 --> 00:24:08,356
So let me just fire that off,


773
00:24:09,476 --> 00:24:10,626
and let it start running.


774
00:24:11,206 --> 00:24:14,326
It's loaded the data, and so


775
00:24:14,326 --> 00:24:16,176
under the hood, we automatically


776
00:24:16,176 --> 00:24:17,966
handle all of the tokenization,


777
00:24:17,966 --> 00:24:19,086
the feature extraction.


778
00:24:19,466 --> 00:24:20,316
We do the training.


779
00:24:20,316 --> 00:24:22,596
This is a fairly small model, so


780
00:24:22,596 --> 00:24:24,116
it doesn't take all that long to


781
00:24:24,116 --> 00:24:27,796
train, and I have it set to


782
00:24:27,796 --> 00:24:29,366
automatically write my model out


783
00:24:29,366 --> 00:24:35,956
to my desktop, and there it is.


784
00:24:36,196 --> 00:24:36,486
All right.


785
00:24:36,486 --> 00:24:37,436
So that's one model.


786
00:24:38,636 --> 00:24:40,636
Now I have another playground


787
00:24:40,636 --> 00:24:41,876
here that's set up to train my


788
00:24:41,876 --> 00:24:42,786
text classifier.


789
00:24:43,136 --> 00:24:44,076
As you can see, it looks very


790
00:24:44,076 --> 00:24:44,516
similar.


791
00:24:45,046 --> 00:24:47,836
Load the training data, create a


792
00:24:47,836 --> 00:24:49,396
text classifier from it, and


793
00:24:49,396 --> 00:24:50,266
write it out to disk.


794
00:24:51,716 --> 00:24:52,786
So I start that off.


795
00:24:53,426 --> 00:24:56,876
And again, automatically natural


796
00:24:56,876 --> 00:24:58,006
language is loading all the


797
00:24:58,006 --> 00:25:00,126
data, tokenizing it, extracting


798
00:25:00,126 --> 00:25:00,856
features from it.


799
00:25:01,266 --> 00:25:03,226
This one is a bit larger model.


800
00:25:03,226 --> 00:25:04,226
It takes a couple minutes to


801
00:25:04,226 --> 00:25:04,556
train.


802
00:25:04,556 --> 00:25:06,746
So let's just let that go, and


803
00:25:06,746 --> 00:25:08,696
in the meantime, take a look at


804
00:25:08,696 --> 00:25:10,966
some of the code we have to use


805
00:25:10,966 --> 00:25:12,026
these at run time.


806
00:25:12,546 --> 00:25:14,166
So I've written two very small


807
00:25:14,166 --> 00:25:16,106
classes to do what I need to do


808
00:25:16,106 --> 00:25:16,576
at run time.


809
00:25:17,106 --> 00:25:18,546
The first one uses the text


810
00:25:18,546 --> 00:25:22,136
classifier by finding that model


811
00:25:22,386 --> 00:25:24,386
in my apps resources and


812
00:25:24,386 --> 00:25:25,826
creating an NL model for it.


813
00:25:26,856 --> 00:25:28,656
And then when I run across an


814
00:25:28,656 --> 00:25:32,016
article, I just ask the model


815
00:25:32,016 --> 00:25:33,616
for a predicted label for that


816
00:25:33,616 --> 00:25:35,856
article, and that's really all


817
00:25:35,856 --> 00:25:38,246
there is to it.


818
00:25:38,506 --> 00:25:42,726
Slightly more code for use of my


819
00:25:42,726 --> 00:25:43,486
word tagger.


820
00:25:44,176 --> 00:25:46,006
So as you saw before, I have a


821
00:25:46,006 --> 00:25:49,026
custom tag scheme for my product


822
00:25:49,196 --> 00:25:51,616
name recognition, and the only


823
00:25:51,616 --> 00:25:53,566
tag I'm really interested in is


824
00:25:53,566 --> 00:25:54,546
the product tag.


825
00:25:54,716 --> 00:25:57,416
So I create a custom tag for


826
00:25:58,036 --> 00:25:58,146
that.


827
00:25:58,366 --> 00:26:00,396
Again, I have to find the model


828
00:26:00,546 --> 00:26:03,366
in my bundles resources, create


829
00:26:03,366 --> 00:26:05,536
an NL model for it, and then


830
00:26:05,536 --> 00:26:07,586
create an NLTagger, and this


831
00:26:07,736 --> 00:26:09,826
NLTagger I'm specifying two


832
00:26:09,826 --> 00:26:10,376
schemes.


833
00:26:10,416 --> 00:26:11,986
The first is the built-in name


834
00:26:11,986 --> 00:26:14,876
type scheme to do name identity


835
00:26:14,876 --> 00:26:16,426
recognition, and the second one


836
00:26:16,426 --> 00:26:18,176
is my custom product tag scheme,


837
00:26:18,516 --> 00:26:19,486
and they'll both function in


838
00:26:19,486 --> 00:26:20,576
exactly the same way.


839
00:26:21,546 --> 00:26:23,006
And then I just have to tell


840
00:26:23,096 --> 00:26:24,766
that tagger to use my custom


841
00:26:24,766 --> 00:26:26,866
model for my custom scheme.


842
00:26:27,716 --> 00:26:29,176
Now if I supported multiple


843
00:26:29,176 --> 00:26:30,286
languages, I might have more


844
00:26:30,286 --> 00:26:31,956
than one model in here for this


845
00:26:31,956 --> 00:26:32,216
scheme.


846
00:26:32,806 --> 00:26:38,426
And then what I'm going to do is


847
00:26:38,426 --> 00:26:40,306
highlight text in this article


848
00:26:40,716 --> 00:26:42,466
that is located, determined to


849
00:26:42,466 --> 00:26:43,896
be a name of one sort or


850
00:26:43,896 --> 00:26:44,266
another.


851
00:26:44,566 --> 00:26:46,586
So I'm going to get a mutable


852
00:26:46,586 --> 00:26:47,716
attributed string, and I'm going


853
00:26:47,716 --> 00:26:49,026
to add some attributes to it.


854
00:26:50,106 --> 00:26:51,996
So I'll take this string of that


855
00:26:52,206 --> 00:26:53,246
mutable attributed string,


856
00:26:53,246 --> 00:26:56,196
attach that to my tagger, and


857
00:26:56,196 --> 00:26:57,496
then I'm going to do a couple of


858
00:26:57,496 --> 00:26:59,216
enumerations over tags.


859
00:26:59,676 --> 00:27:01,916
The first one uses the built-in


860
00:27:01,916 --> 00:27:03,926
name-type scheme for name


861
00:27:03,926 --> 00:27:05,576
identity recognition of people,


862
00:27:05,576 --> 00:27:07,576
places, and organizations, and


863
00:27:07,576 --> 00:27:08,686
if I find something that's


864
00:27:08,686 --> 00:27:11,436
tagged as a person or place or


865
00:27:11,436 --> 00:27:13,226
organization, then I'm going to


866
00:27:13,226 --> 00:27:14,946
add an attribute to the


867
00:27:14,946 --> 00:27:17,326
attributed string that will give


868
00:27:17,326 --> 00:27:17,976
it some color.


869
00:27:20,256 --> 00:27:21,576
And then we can do exactly the


870
00:27:21,576 --> 00:27:23,696
same thing with our custom


871
00:27:23,696 --> 00:27:24,046
model.


872
00:27:24,846 --> 00:27:26,336
We're going to enumerate using


873
00:27:26,336 --> 00:27:28,176
our custom product tag scheme,


874
00:27:29,586 --> 00:27:31,386
and in that case, if we find


875
00:27:31,386 --> 00:27:32,516
something that's labeled with


876
00:27:32,516 --> 00:27:34,586
our custom product tag, then I


877
00:27:34,586 --> 00:27:37,106
can add color to it in exactly


878
00:27:37,106 --> 00:27:37,786
the same way.


879
00:27:39,106 --> 00:27:41,026
So you can use custom models


880
00:27:41,626 --> 00:27:44,326
with Natural Language API just


881
00:27:44,516 --> 00:27:45,916
in the same way that you use


882
00:27:45,916 --> 00:27:46,916
built-in models.


883
00:27:47,566 --> 00:27:48,736
Now, let's go back to our


884
00:27:48,736 --> 00:27:50,936
playground, and we see that the


885
00:27:50,936 --> 00:27:52,986
model training has finished, and


886
00:27:52,986 --> 00:27:54,496
in fact there are now two models


887
00:27:54,496 --> 00:27:55,996
showing up on my desktop.


888
00:27:57,776 --> 00:27:59,936
So all I need to do is drag


889
00:27:59,936 --> 00:28:01,176
those into my application.


890
00:28:02,116 --> 00:28:05,286
Let's take this one and drag it


891
00:28:05,286 --> 00:28:06,146
right in.


892
00:28:08,026 --> 00:28:14,676
Okay. And let's take this one


893
00:28:15,226 --> 00:28:19,786
and drag it in, and Xcode will


894
00:28:19,786 --> 00:28:21,116
automatically compile these and


895
00:28:21,116 --> 00:28:22,546
include them in my application.


896
00:28:22,846 --> 00:28:25,656
So all I have to do is build and


897
00:28:25,656 --> 00:28:25,946
run it.


898
00:28:31,386 --> 00:28:34,336
And let's hide that.


899
00:28:34,926 --> 00:28:36,796
Here's my new application, and


900
00:28:36,796 --> 00:28:38,096
you'll notice that my list of


901
00:28:38,096 --> 00:28:40,386
articles is all neatly sorted


902
00:28:40,506 --> 00:28:47,166
automatically by topic, and if I


903
00:28:47,166 --> 00:28:49,596
go in and take a look at one of


904
00:28:49,596 --> 00:28:51,396
these articles, you'll notice


905
00:28:51,396 --> 00:28:53,556
that names are highlighted in


906
00:28:53,556 --> 00:28:55,296
it, and you can see, using our


907
00:28:55,296 --> 00:28:56,566
built-in name identity


908
00:28:56,566 --> 00:28:57,976
recognition, we highlight names


909
00:28:57,976 --> 00:28:59,026
of people, places, and


910
00:28:59,026 --> 00:29:00,806
organizations, but if you look a


911
00:29:00,806 --> 00:29:03,456
little further, you can see that


912
00:29:03,496 --> 00:29:05,696
it has used our custom product


913
00:29:05,696 --> 00:29:07,546
tagger to highlight the names of


914
00:29:07,546 --> 00:29:10,056
products like iPad, MacBook,


915
00:29:10,356 --> 00:29:12,386
iPad mini, and so forth.


916
00:29:13,256 --> 00:29:16,606
So this shows how easy it is to


917
00:29:16,606 --> 00:29:18,966
train your own custom models and


918
00:29:18,966 --> 00:29:21,346
to use them with the natural


919
00:29:21,346 --> 00:29:22,866
language APIs.


920
00:29:24,516 --> 00:29:30,546
[ Applause ]


921
00:29:31,046 --> 00:29:32,836
So now I'm going to turn things


922
00:29:32,886 --> 00:29:34,486
back over to Vivek to talk about


923
00:29:34,486 --> 00:29:36,236
some important considerations


924
00:29:36,286 --> 00:29:37,646
for training models.


925
00:29:40,516 --> 00:29:43,506
[ Applause ]


926
00:29:44,006 --> 00:29:45,316
Thank you, Doug, for telling us


927
00:29:45,316 --> 00:29:46,536
how to use these custom NLP


928
00:29:46,536 --> 00:29:46,966
models.


929
00:29:46,966 --> 00:29:48,646
We are really excited to sort of


930
00:29:48,646 --> 00:29:50,646
have a very tight integration of


931
00:29:50,646 --> 00:29:52,686
natural language with Create ML


932
00:29:52,686 --> 00:29:54,146
and the Core ML [inaudible], and


933
00:29:54,146 --> 00:29:55,246
we hope that you do some really


934
00:29:55,246 --> 00:29:56,416
unbelievable things with this


935
00:29:56,416 --> 00:29:57,316
new API.


936
00:29:57,316 --> 00:30:00,156
So I'd like to shift attention


937
00:30:00,156 --> 00:30:01,406
now again and talk about


938
00:30:01,406 --> 00:30:01,996
performance.


939
00:30:02,446 --> 00:30:04,106
So as I mentioned before,


940
00:30:04,106 --> 00:30:05,656
Natural Language is available


941
00:30:05,936 --> 00:30:08,166
across all Apple platforms, and


942
00:30:08,166 --> 00:30:10,226
it also offers you what we call


943
00:30:10,226 --> 00:30:12,296
as standardized text processing.


944
00:30:12,646 --> 00:30:14,296
So let's take a moment again to


945
00:30:14,296 --> 00:30:15,636
understand what we mean by this.


946
00:30:16,426 --> 00:30:17,686
Now if you were to look at a


947
00:30:17,686 --> 00:30:18,876
conventional machine learning


948
00:30:18,876 --> 00:30:20,786
pipeline that didn't use Create


949
00:30:20,786 --> 00:30:22,236
ML, where would you start?


950
00:30:22,346 --> 00:30:23,526
You would start with some amount


951
00:30:23,526 --> 00:30:24,336
of training data.


952
00:30:25,346 --> 00:30:26,136
You would take this training


953
00:30:26,136 --> 00:30:26,426
data.


954
00:30:26,426 --> 00:30:27,436
You would tokenize this data.


955
00:30:27,436 --> 00:30:28,716
You'd probably extract some


956
00:30:28,716 --> 00:30:29,246
features.


957
00:30:29,506 --> 00:30:30,896
This is really important for


958
00:30:30,896 --> 00:30:32,076
languages like Chinese and


959
00:30:32,076 --> 00:30:33,676
Japanese where tokenization is


960
00:30:33,676 --> 00:30:34,286
very important.


961
00:30:34,926 --> 00:30:36,386
You would throw that into your


962
00:30:36,386 --> 00:30:37,236
favorite machine learning


963
00:30:37,236 --> 00:30:40,236
toolkit, and you'd get a machine


964
00:30:40,236 --> 00:30:41,516
learning model out of it.


965
00:30:41,966 --> 00:30:43,226
Now in order to use that machine


966
00:30:43,226 --> 00:30:44,196
learning model on an Apple


967
00:30:44,196 --> 00:30:45,756
device, you'd have to convert


968
00:30:45,756 --> 00:30:47,146
that into a Core ML model.


969
00:30:47,606 --> 00:30:48,736
So what would you do?


970
00:30:48,906 --> 00:30:49,716
You would use a Core ML


971
00:30:49,716 --> 00:30:50,746
converter to do this.


972
00:30:51,376 --> 00:30:53,176
This is sort of the training


973
00:30:53,176 --> 00:30:55,426
procedure in order to get from


974
00:30:55,426 --> 00:30:57,386
data to a model and deploy it on


975
00:30:57,386 --> 00:30:59,896
an Apple device.


976
00:30:59,896 --> 00:31:02,486
Now, at inference time, what you


977
00:31:02,486 --> 00:31:04,876
do is you drop the model in your


978
00:31:05,186 --> 00:31:06,266
app, but that's not it.


979
00:31:07,216 --> 00:31:09,406
You also have to make sure that


980
00:31:09,406 --> 00:31:10,386
you write the code for


981
00:31:10,386 --> 00:31:11,586
tokenization and feature


982
00:31:11,586 --> 00:31:13,336
extraction that is consistent


983
00:31:13,336 --> 00:31:14,626
with what happened at training


984
00:31:15,996 --> 00:31:16,136
time.


985
00:31:16,596 --> 00:31:17,956
It's a lot of effort because you


986
00:31:17,956 --> 00:31:19,686
have to think about maximizing


987
00:31:19,686 --> 00:31:20,836
the fidelity of your model.


988
00:31:20,836 --> 00:31:22,776
It's absolutely important that


989
00:31:22,776 --> 00:31:24,086
the tokenization featured


990
00:31:24,086 --> 00:31:26,066
extraction is identical at both


991
00:31:26,066 --> 00:31:26,996
training and inference time.


992
00:31:27,916 --> 00:31:29,486
But now with the use of Natural


993
00:31:29,486 --> 00:31:30,506
Language, you can completely


994
00:31:30,506 --> 00:31:31,246
obviate this.


995
00:31:31,516 --> 00:31:34,096
So if you look at the sequence


996
00:31:34,096 --> 00:31:35,216
at training time, you have


997
00:31:35,216 --> 00:31:35,886
training data.


998
00:31:37,276 --> 00:31:38,526
You just pass it to Create ML


999
00:31:38,596 --> 00:31:39,506
through the APIs that we've


1000
00:31:39,506 --> 00:31:40,276
discussed so far.


1001
00:31:41,176 --> 00:31:42,916
Create ML calls Natural Language


1002
00:31:42,956 --> 00:31:44,136
under the hood, which does the


1003
00:31:44,136 --> 00:31:45,986
tokenization feature extraction,


1004
00:31:46,356 --> 00:31:47,316
chooses the machine learning


1005
00:31:47,316 --> 00:31:49,456
library, does all the work, and


1006
00:31:49,456 --> 00:31:51,456
returns a model which is a Core


1007
00:31:51,456 --> 00:31:52,866
ML model.


1008
00:31:53,076 --> 00:31:54,366
Now at inference time, what you


1009
00:31:54,366 --> 00:31:56,236
do is you still drop this model


1010
00:31:56,236 --> 00:31:58,876
in your app, but you don't have


1011
00:31:58,876 --> 00:32:00,146
to worry about tokenization


1012
00:32:00,146 --> 00:32:01,286
feature extraction or anything


1013
00:32:01,286 --> 00:32:01,496
else.


1014
00:32:01,706 --> 00:32:03,236
In fact, you don't have to write


1015
00:32:03,236 --> 00:32:04,376
a single line of code because


1016
00:32:04,636 --> 00:32:06,446
Natural Language does all of


1017
00:32:06,446 --> 00:32:07,036
that for you.


1018
00:32:07,356 --> 00:32:08,916
You just focus on your app and


1019
00:32:08,916 --> 00:32:10,296
your task and simply drag and


1020
00:32:10,296 --> 00:32:11,176
drop the model in.


1021
00:32:11,636 --> 00:32:15,026
The other aspect of Natural


1022
00:32:15,026 --> 00:32:16,226
Language as I mentioned before


1023
00:32:16,296 --> 00:32:17,856
is it's optimized for Apple


1024
00:32:17,856 --> 00:32:19,916
hardware and for model sizes.


1025
00:32:20,036 --> 00:32:21,626
So let's look at this through a


1026
00:32:21,626 --> 00:32:22,586
couple of examples.


1027
00:32:23,276 --> 00:32:25,066
So Doug talked about named


1028
00:32:25,066 --> 00:32:26,556
entity recognition and chunking,


1029
00:32:27,096 --> 00:32:28,146
and here are two different


1030
00:32:28,146 --> 00:32:28,746
benchmarks.


1031
00:32:28,746 --> 00:32:30,876
So these are models that we


1032
00:32:30,876 --> 00:32:32,726
built using an open source tool


1033
00:32:32,726 --> 00:32:34,396
kit called CRF Suite, and


1034
00:32:34,396 --> 00:32:35,336
through Natural Language.


1035
00:32:35,716 --> 00:32:36,706
The models were built from


1036
00:32:36,776 --> 00:32:38,856
identical training data and


1037
00:32:38,856 --> 00:32:40,346
tested on identical test data.


1038
00:32:40,856 --> 00:32:41,906
The same sort of features were


1039
00:32:41,906 --> 00:32:42,306
used.


1040
00:32:42,496 --> 00:32:43,836
The accuracy obtained by both


1041
00:32:43,836 --> 00:32:45,066
these models is the same.


1042
00:32:45,546 --> 00:32:47,316
But you look at the model sizes


1043
00:32:47,316 --> 00:32:48,496
that Natural Language is able to


1044
00:32:48,496 --> 00:32:48,886
generate.


1045
00:32:49,446 --> 00:32:50,926
It's simply just about 1.4


1046
00:32:50,926 --> 00:32:53,076
megabytes of data size, model


1047
00:32:53,076 --> 00:32:53,936
size for named entity


1048
00:32:53,936 --> 00:32:55,576
recognition and 1.8 megabytes


1049
00:32:55,576 --> 00:32:56,076
for chunking.


1050
00:32:56,576 --> 00:32:57,916
That saves you an enormous


1051
00:32:57,916 --> 00:32:59,296
amount of space within your app


1052
00:32:59,296 --> 00:33:00,056
to do other things.


1053
00:33:00,596 --> 00:33:03,976
In terms of machine learning


1054
00:33:03,976 --> 00:33:06,586
algorithms, we support two


1055
00:33:06,586 --> 00:33:07,516
different options.


1056
00:33:07,966 --> 00:33:09,636
We can specify this for text


1057
00:33:09,636 --> 00:33:10,216
classification.


1058
00:33:10,216 --> 00:33:11,416
So for text classification, we


1059
00:33:11,416 --> 00:33:12,486
have two different choices.


1060
00:33:12,966 --> 00:33:14,326
One is maxEnt, which is an


1061
00:33:14,326 --> 00:33:15,506
abbreviation for Maximum


1062
00:33:15,506 --> 00:33:15,956
Entropy.


1063
00:33:16,586 --> 00:33:18,596
In NLP, we call maxEnt is


1064
00:33:18,596 --> 00:33:19,576
essentially a multinomial


1065
00:33:19,576 --> 00:33:20,666
logistic regression model.


1066
00:33:20,906 --> 00:33:22,326
We just call it Maximum Entropy


1067
00:33:22,326 --> 00:33:22,986
in NLP feed.


1068
00:33:23,836 --> 00:33:25,666
The other one is CRF, which is


1069
00:33:25,666 --> 00:33:27,046
an abbreviation for Conditional


1070
00:33:27,046 --> 00:33:27,546
Random Feed.


1071
00:33:28,336 --> 00:33:29,606
The choice of these two


1072
00:33:29,606 --> 00:33:30,916
algorithms really depends upon


1073
00:33:30,916 --> 00:33:31,496
your task.


1074
00:33:31,946 --> 00:33:34,026
So we encourage you to try both


1075
00:33:34,026 --> 00:33:35,426
these options, build the models.


1076
00:33:35,426 --> 00:33:38,556
Now in terms of word tagging,


1077
00:33:38,966 --> 00:33:40,316
that is one default option,


1078
00:33:40,346 --> 00:33:41,386
which is a conditional random


1079
00:33:41,386 --> 00:33:41,626
feed.


1080
00:33:41,996 --> 00:33:43,496
When you instantiate an ML word


1081
00:33:43,566 --> 00:33:46,146
tagger, specify data to it, the


1082
00:33:46,146 --> 00:33:48,176
default model that you get is a


1083
00:33:48,176 --> 00:33:49,096
conditional random feed.


1084
00:33:49,956 --> 00:33:51,686
Now as I mentioned, the choice


1085
00:33:51,686 --> 00:33:52,736
of these algorithms really


1086
00:33:52,736 --> 00:33:54,626
depends on your task, but what


1087
00:33:54,626 --> 00:33:56,626
I'd like to emphasize is sort of


1088
00:33:56,626 --> 00:33:58,196
draw [inaudible] between your


1089
00:33:58,196 --> 00:33:59,536
conventional development


1090
00:33:59,536 --> 00:33:59,996
process.


1091
00:34:00,316 --> 00:34:01,496
So when you have an idea for an


1092
00:34:01,496 --> 00:34:02,836
app, you go through a


1093
00:34:02,836 --> 00:34:04,136
development cycle, right.


1094
00:34:04,226 --> 00:34:05,676
So you can think of machine


1095
00:34:05,676 --> 00:34:07,196
learning to be a very similar


1096
00:34:07,246 --> 00:34:08,136
sort of a work flow.


1097
00:34:08,235 --> 00:34:10,166
Where do you start, you start


1098
00:34:10,166 --> 00:34:12,416
with data, and then you have


1099
00:34:12,416 --> 00:34:14,196
data, you have to ask a couple


1100
00:34:14,196 --> 00:34:14,876
of questions.


1101
00:34:14,876 --> 00:34:16,565
You have to validate your


1102
00:34:16,565 --> 00:34:17,186
training data.


1103
00:34:17,315 --> 00:34:18,735
You have to make sure that there


1104
00:34:18,735 --> 00:34:20,166
are no spurious examples in your


1105
00:34:20,166 --> 00:34:21,315
data, and it's not tainted.


1106
00:34:22,275 --> 00:34:23,666
Once you do that, you can


1107
00:34:23,666 --> 00:34:24,896
inspect the number of training


1108
00:34:24,966 --> 00:34:26,315
instances per class.


1109
00:34:26,626 --> 00:34:27,735
Let's say that your training a


1110
00:34:27,735 --> 00:34:29,206
sentiment classification model,


1111
00:34:29,416 --> 00:34:30,646
and you have a thousand examples


1112
00:34:30,646 --> 00:34:31,775
for positive sentiment, you have


1113
00:34:31,856 --> 00:34:32,926
five examples for negative


1114
00:34:32,926 --> 00:34:33,406
sentiment.


1115
00:34:33,916 --> 00:34:36,606
You can't train a robust model


1116
00:34:36,606 --> 00:34:37,556
that can determine or


1117
00:34:37,556 --> 00:34:38,616
distinguish between those two


1118
00:34:38,616 --> 00:34:39,196
classes.


1119
00:34:39,565 --> 00:34:40,896
You have to make sure that the


1120
00:34:40,896 --> 00:34:42,146
training samples for each of


1121
00:34:42,146 --> 00:34:43,226
those classes are reasonably


1122
00:34:43,226 --> 00:34:43,775
balanced.


1123
00:34:44,585 --> 00:34:46,436
So once you do that with data,


1124
00:34:46,706 --> 00:34:48,226
the next step is training.


1125
00:34:48,536 --> 00:34:50,025
As I mentioned before, our


1126
00:34:50,025 --> 00:34:51,446
recommendation is that you run


1127
00:34:51,446 --> 00:34:52,396
the different options that are


1128
00:34:52,396 --> 00:34:54,525
available and figure out what is


1129
00:34:54,525 --> 00:34:56,866
good, but how do you define what


1130
00:34:56,866 --> 00:34:57,246
is good?


1131
00:34:57,686 --> 00:34:59,566
You have to evaluate the model


1132
00:34:59,766 --> 00:35:00,776
in order to figure out what


1133
00:35:00,776 --> 00:35:01,856
suits your application.


1134
00:35:01,926 --> 00:35:03,376
So the next step here in the


1135
00:35:03,376 --> 00:35:05,236
work flow is evaluation.


1136
00:35:06,956 --> 00:35:09,466
Evaluation in convention


1137
00:35:09,466 --> 00:35:10,356
[inaudible] for machine learning


1138
00:35:10,356 --> 00:35:11,366
is that when you procure your


1139
00:35:11,366 --> 00:35:13,326
training data, you split your


1140
00:35:13,326 --> 00:35:15,366
data into training set, into a


1141
00:35:15,366 --> 00:35:17,056
validation set, and into a test


1142
00:35:17,056 --> 00:35:18,836
set, and you typically tune the


1143
00:35:18,836 --> 00:35:20,106
parameters of the algorithm


1144
00:35:20,106 --> 00:35:21,566
using the validation set, and


1145
00:35:21,566 --> 00:35:22,686
you test it on the test set.


1146
00:35:22,956 --> 00:35:24,126
So we encourage you to do the


1147
00:35:24,126 --> 00:35:25,606
same thing, apply the same sort


1148
00:35:25,606 --> 00:35:27,166
of guidelines that have stood


1149
00:35:27,166 --> 00:35:28,286
machine learning in good stead


1150
00:35:28,286 --> 00:35:29,016
for a long time.


1151
00:35:29,556 --> 00:35:31,026
The other thing that we also


1152
00:35:31,026 --> 00:35:32,866
encourage you to do is test on


1153
00:35:32,866 --> 00:35:33,796
out-of-domain data.


1154
00:35:34,126 --> 00:35:35,066
What do I mean by this?


1155
00:35:35,496 --> 00:35:37,156
So when you have an idea for an


1156
00:35:37,156 --> 00:35:38,586
app, you think of a certain type


1157
00:35:38,586 --> 00:35:39,756
of data that is going to be


1158
00:35:39,826 --> 00:35:41,246
ingested by your machine


1159
00:35:41,246 --> 00:35:42,496
learning model.


1160
00:35:42,496 --> 00:35:43,836
Now let's say you're building an


1161
00:35:43,836 --> 00:35:46,056
app for hotel reviews, and you


1162
00:35:46,056 --> 00:35:48,806
want to classify hotel reviews


1163
00:35:48,806 --> 00:35:50,466
into different sorts of ratings.


1164
00:35:50,706 --> 00:35:53,186
And the user throws a data that


1165
00:35:53,186 --> 00:35:54,926
is completely out of domain.


1166
00:35:54,926 --> 00:35:56,106
Perhaps it's something to do


1167
00:35:56,106 --> 00:35:57,856
with a restaurant review or a


1168
00:35:57,856 --> 00:35:59,446
movie review, is your model


1169
00:35:59,446 --> 00:36:01,306
robust enough to handle it.


1170
00:36:01,646 --> 00:36:02,646
That's a question that you ought


1171
00:36:02,646 --> 00:36:03,366
to ask yourself.


1172
00:36:04,006 --> 00:36:06,066
And the final step is well in a


1173
00:36:06,166 --> 00:36:07,156
conventional development


1174
00:36:07,156 --> 00:36:09,086
workflow you write patches, you


1175
00:36:09,086 --> 00:36:11,516
fix bugs, and you update your


1176
00:36:11,516 --> 00:36:11,656
app.


1177
00:36:12,406 --> 00:36:13,576
How do you do that with machine


1178
00:36:13,576 --> 00:36:13,846
learning?


1179
00:36:14,096 --> 00:36:17,686
Well, the way to do that or fix


1180
00:36:17,956 --> 00:36:19,416
issues with machine learning is


1181
00:36:19,416 --> 00:36:21,276
to find out where your models do


1182
00:36:21,276 --> 00:36:22,876
not perform well, and you have


1183
00:36:22,876 --> 00:36:24,076
to supplement it with the right


1184
00:36:24,076 --> 00:36:24,706
sort of data.


1185
00:36:25,206 --> 00:36:26,776
By adding data and retraining


1186
00:36:26,776 --> 00:36:28,346
your model, you can essentially


1187
00:36:28,346 --> 00:36:29,716
get a new model out.


1188
00:36:29,716 --> 00:36:31,046
So it's, as I mentioned, it's


1189
00:36:31,046 --> 00:36:32,266
very similar to sort of the


1190
00:36:32,266 --> 00:36:34,196
development workflow, and they


1191
00:36:34,266 --> 00:36:34,676
are very [inaudible].


1192
00:36:34,676 --> 00:36:36,306
So you can think of it as part


1193
00:36:36,306 --> 00:36:37,786
of your fabric if you're


1194
00:36:38,176 --> 00:36:39,466
employing machine learning


1195
00:36:39,466 --> 00:36:40,906
models as part of your app, you


1196
00:36:40,906 --> 00:36:42,596
can just combine it with the


1197
00:36:42,596 --> 00:36:43,506
word process itself.


1198
00:36:44,006 --> 00:36:46,876
The last thing I'd like to


1199
00:36:46,876 --> 00:36:48,736
emphasize here is privacy.


1200
00:36:49,526 --> 00:36:50,706
So everything that you saw in


1201
00:36:50,706 --> 00:36:53,086
this session, all of the machine


1202
00:36:53,086 --> 00:36:54,576
learning and Natural Language


1203
00:36:54,576 --> 00:36:56,726
processing happens completely on


1204
00:36:56,726 --> 00:36:57,146
device.


1205
00:36:57,946 --> 00:36:59,246
So we at Apple take privacy


1206
00:36:59,246 --> 00:37:01,516
really seriously, and this is a


1207
00:37:01,516 --> 00:37:03,216
remarkable opportunity to use


1208
00:37:03,216 --> 00:37:04,456
machine learning completely on


1209
00:37:04,456 --> 00:37:05,546
device to protect user's


1210
00:37:05,546 --> 00:37:05,996
privacy.


1211
00:37:06,226 --> 00:37:09,126
So in that vein, Natural


1212
00:37:09,126 --> 00:37:10,716
Language is another step towards


1213
00:37:10,716 --> 00:37:11,976
privacy preserving machine


1214
00:37:11,976 --> 00:37:13,556
learning but in this case apply


1215
00:37:13,556 --> 00:37:14,846
to NLP.


1216
00:37:15,916 --> 00:37:19,266
So in summary, we talked about a


1217
00:37:19,266 --> 00:37:20,986
new framework called Natural


1218
00:37:20,986 --> 00:37:21,746
Language framework.


1219
00:37:22,126 --> 00:37:23,726
It's tightly integrated with the


1220
00:37:23,726 --> 00:37:24,936
Apple machine learning


1221
00:37:24,936 --> 00:37:25,626
[inaudible].


1222
00:37:25,626 --> 00:37:27,196
You can now train models using


1223
00:37:27,196 --> 00:37:29,536
Create ML and then use those


1224
00:37:29,536 --> 00:37:30,906
models either with the Core ML


1225
00:37:30,906 --> 00:37:32,816
APIs or with Natural Language.


1226
00:37:33,826 --> 00:37:35,446
The models that we generate


1227
00:37:35,446 --> 00:37:36,996
using Natural Language and the


1228
00:37:36,996 --> 00:37:38,386
APIs are highly performed and


1229
00:37:38,386 --> 00:37:40,226
optimized on Apple hardware


1230
00:37:40,326 --> 00:37:41,516
across all the platforms.


1231
00:37:41,976 --> 00:37:44,866
And finally, it supports privacy


1232
00:37:45,026 --> 00:37:46,066
because all of the machine


1233
00:37:46,066 --> 00:37:47,706
learning in NLP happens on


1234
00:37:47,706 --> 00:37:48,296
user's device.


1235
00:37:48,916 --> 00:37:52,076
So there's more information


1236
00:37:52,076 --> 00:37:52,376
here.


1237
00:37:52,376 --> 00:37:53,836
We have a Natural Language lab


1238
00:37:53,836 --> 00:37:55,396
tomorrow, so we encourage you to


1239
00:37:55,396 --> 00:37:57,016
try out these APIs and come talk


1240
00:37:57,016 --> 00:37:58,726
to us and ask us questions about


1241
00:37:58,726 --> 00:38:00,096
where you'd like enhancements or


1242
00:38:00,096 --> 00:38:01,136
perhaps some sort of


1243
00:38:01,136 --> 00:38:02,256
consultation with respect to


1244
00:38:02,256 --> 00:38:03,156
your app.


1245
00:38:03,666 --> 00:38:04,766
We also have a machine learning


1246
00:38:04,766 --> 00:38:06,596
get together, and there's a


1247
00:38:06,936 --> 00:38:08,406
subsequent [inaudible] Create ML


1248
00:38:08,406 --> 00:38:09,666
lab that's happening right now.


1249
00:38:10,046 --> 00:38:11,476
So you can continue coming and


1250
00:38:11,476 --> 00:38:12,856
talking to us as part of that


1251
00:38:12,856 --> 00:38:13,126
lab.


1252
00:38:13,766 --> 00:38:15,466
Thank you for your attention.


1253
00:38:15,566 --> 00:38:15,746
Thanks.


1254
00:38:16,016 --> 00:38:18,000
[ Applause ]

