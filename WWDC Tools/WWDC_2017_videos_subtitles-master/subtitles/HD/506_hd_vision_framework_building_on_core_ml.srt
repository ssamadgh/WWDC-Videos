1
00:00:21,126 --> 00:00:21,806
>> Hello everyone.


2
00:00:22,516 --> 00:00:26,546
[ Applause ]


3
00:00:27,046 --> 00:00:28,206
I hope you're have a great time


4
00:00:28,206 --> 00:00:29,006
at WWDC so far.


5
00:00:29,776 --> 00:00:31,216
Allow me to introduce myself.


6
00:00:31,906 --> 00:00:33,646
My name is Brett Keating and I'm


7
00:00:33,646 --> 00:00:34,896
here with my colleague Frank


8
00:00:34,896 --> 00:00:36,386
Doepke and we're here to tell


9
00:00:36,386 --> 00:00:38,256
you about Apple's new Vision


10
00:00:38,256 --> 00:00:39,836
framework, so let's get started.


11
00:00:39,836 --> 00:00:42,506
We're going to begin by showing


12
00:00:42,506 --> 00:00:43,716
you what Vision can do for your


13
00:00:43,716 --> 00:00:44,266
apps.


14
00:00:44,746 --> 00:00:45,686
We're going to go through a few


15
00:00:45,686 --> 00:00:47,126
visual examples of the


16
00:00:47,126 --> 00:00:48,106
algorithms that are going to be


17
00:00:48,106 --> 00:00:49,016
made available in the Vision


18
00:00:49,016 --> 00:00:49,766
framework this year.


19
00:00:50,936 --> 00:00:51,976
At which point I'll hand it off


20
00:00:51,976 --> 00:00:54,636
to Frank to talk about the


21
00:00:54,636 --> 00:00:56,116
concepts behind the Vision


22
00:00:56,116 --> 00:00:57,566
framework, why we designed


23
00:00:57,566 --> 00:00:58,836
things the way we did, what the


24
00:00:58,836 --> 00:01:00,696
mental model behind our API is.


25
00:01:00,796 --> 00:01:02,516
And then we'll go a little


26
00:01:02,516 --> 00:01:04,906
deeper and go through a code


27
00:01:04,906 --> 00:01:05,446
example.


28
00:01:06,006 --> 00:01:08,796
This code example brings


29
00:01:08,796 --> 00:01:09,716
together a few different


30
00:01:09,716 --> 00:01:11,386
technologies in our SDK,


31
00:01:11,496 --> 00:01:14,566
including Core Image, as well as


32
00:01:14,566 --> 00:01:16,406
the brand-new Core ML framework


33
00:01:16,656 --> 00:01:17,516
that we're offering this year


34
00:01:18,386 --> 00:01:19,926
which enables you to put in your


35
00:01:19,926 --> 00:01:21,226
own custom models and have them


36
00:01:21,276 --> 00:01:23,746
be accelerated using our


37
00:01:24,896 --> 00:01:26,106
hardware.


38
00:01:26,896 --> 00:01:28,716
So, let's begin with what you


39
00:01:28,716 --> 00:01:29,326
can do with Vision.


40
00:01:30,596 --> 00:01:32,986
Let's start off with face


41
00:01:32,986 --> 00:01:33,466
detection.


42
00:01:33,466 --> 00:01:35,676
Now face detection is something


43
00:01:35,676 --> 00:01:37,136
we already have in our SDK, but


44
00:01:37,866 --> 00:01:39,126
we're offering in the Vision


45
00:01:39,126 --> 00:01:40,536
framework new this year a face


46
00:01:40,536 --> 00:01:41,646
detection that's based on deep


47
00:01:41,646 --> 00:01:42,036
learning.


48
00:01:43,246 --> 00:01:44,776
And you may already know that


49
00:01:44,986 --> 00:01:46,006
deep learning has made


50
00:01:46,496 --> 00:01:47,786
groundbreaking changes in the


51
00:01:47,786 --> 00:01:49,916
accuracy in what we can do with


52
00:01:49,986 --> 00:01:51,296
Vision technologies and face


53
00:01:51,296 --> 00:01:53,386
detection is no exception.


54
00:01:54,136 --> 00:01:54,886
We're going to have higher


55
00:01:54,886 --> 00:01:56,086
precision which means fewer


56
00:01:56,086 --> 00:01:57,686
false positives, but we are also


57
00:01:57,686 --> 00:01:58,906
going to have dramatically


58
00:01:59,326 --> 00:02:01,806
higher recall which means we'll


59
00:02:01,806 --> 00:02:02,796
miss less faces.


60
00:02:03,286 --> 00:02:04,156
So, let's look at some of the


61
00:02:04,156 --> 00:02:05,986
examples of faces that we will


62
00:02:05,986 --> 00:02:07,446
now be able to detect with the


63
00:02:07,446 --> 00:02:08,106
Vision framework.


64
00:02:08,936 --> 00:02:10,515
For one thing, we'll be able to


65
00:02:10,515 --> 00:02:11,756
detect smaller faces.


66
00:02:13,936 --> 00:02:15,536
We'll also be doing a better job


67
00:02:15,536 --> 00:02:17,066
of detecting strong profiles.


68
00:02:19,516 --> 00:02:21,386
We'll also do a better job


69
00:02:21,846 --> 00:02:23,036
detecting more partially


70
00:02:23,036 --> 00:02:25,616
occluded faces and that includes


71
00:02:25,826 --> 00:02:27,276
things like hats and glasses.


72
00:02:27,276 --> 00:02:30,506
Sticking with the faces theme


73
00:02:30,506 --> 00:02:33,496
for a little longer, we now are


74
00:02:33,496 --> 00:02:34,786
offering in the Vision framework


75
00:02:34,786 --> 00:02:37,006
new this year face landmarks,


76
00:02:37,656 --> 00:02:38,596
what are face landmarks?


77
00:02:39,496 --> 00:02:41,026
This is a constellation of


78
00:02:41,026 --> 00:02:42,006
points that we detect on the


79
00:02:42,006 --> 00:02:43,696
facer, things like the corners


80
00:02:43,696 --> 00:02:45,036
of the eyes, the outline of the


81
00:02:45,036 --> 00:02:47,036
mouth, the contour of the chin.


82
00:02:48,316 --> 00:02:50,876
Here's an example, here's


83
00:02:50,876 --> 00:02:53,766
another example, and one more


84
00:02:53,766 --> 00:02:54,266
example.


85
00:02:55,396 --> 00:02:56,946
We're really excited about this


86
00:02:56,946 --> 00:02:57,956
I think there's going to be some


87
00:02:57,956 --> 00:02:59,056
great apps created with this


88
00:02:59,056 --> 00:02:59,596
technology.


89
00:03:01,996 --> 00:03:03,696
Next, also new this year in the


90
00:03:03,696 --> 00:03:04,776
Vision framework is image


91
00:03:04,776 --> 00:03:05,426
registration.


92
00:03:06,026 --> 00:03:07,176
If you don't know what image


93
00:03:07,176 --> 00:03:08,546
registration is it's basically


94
00:03:08,876 --> 00:03:11,126
aligning two images based on the


95
00:03:11,126 --> 00:03:12,226
features that are present in


96
00:03:12,226 --> 00:03:12,876
those images.


97
00:03:13,496 --> 00:03:15,826
You can use this for stitching


98
00:03:15,826 --> 00:03:17,166
together used for panorama kind


99
00:03:17,286 --> 00:03:18,686
of like this example or image


100
00:03:18,686 --> 00:03:19,726
stacking applications.


101
00:03:20,536 --> 00:03:22,406
We have two different kinds, one


102
00:03:22,406 --> 00:03:24,126
that's translation only and one


103
00:03:24,126 --> 00:03:24,756
that gives you for full


104
00:03:24,756 --> 00:03:27,176
homography for greater accuracy.


105
00:03:28,656 --> 00:03:31,136
We're also offering a few


106
00:03:31,136 --> 00:03:32,276
technologies that are already in


107
00:03:32,276 --> 00:03:33,566
our SDK through CIDetector


108
00:03:33,566 --> 00:03:34,406
interface.


109
00:03:34,796 --> 00:03:35,706
We're making them available in


110
00:03:35,706 --> 00:03:36,776
the Vision API as well.


111
00:03:36,896 --> 00:03:38,766
That includes rectangle


112
00:03:38,766 --> 00:03:40,516
detection as you can see, we


113
00:03:40,516 --> 00:03:42,296
detect the sign in the picture.


114
00:03:43,816 --> 00:03:45,566
We're also doing barcode


115
00:03:45,566 --> 00:03:46,776
detection and recognition in the


116
00:03:46,776 --> 00:03:50,726
Vision API and text detection as


117
00:03:52,256 --> 00:03:52,376
well.


118
00:03:53,766 --> 00:03:54,866
Another new technology,


119
00:03:55,116 --> 00:03:55,976
brand-new in the Vision


120
00:03:55,976 --> 00:03:56,986
framework this year is object


121
00:03:56,986 --> 00:03:57,366
tracking.


122
00:03:58,186 --> 00:04:00,576
You can use this to track a face


123
00:04:00,576 --> 00:04:01,726
if you've detected a face.


124
00:04:01,726 --> 00:04:03,246
You can use that face rectangle


125
00:04:03,246 --> 00:04:05,186
as an initial condition to the


126
00:04:05,186 --> 00:04:06,436
tracking and then the Vision


127
00:04:06,436 --> 00:04:07,756
framework will track that square


128
00:04:08,196 --> 00:04:09,056
throughout the rest of your


129
00:04:09,056 --> 00:04:09,446
video.


130
00:04:10,246 --> 00:04:12,486
Will also track rectangles and


131
00:04:12,486 --> 00:04:14,036
you can also define the initial


132
00:04:14,036 --> 00:04:15,036
condition yourself.


133
00:04:15,846 --> 00:04:17,426
So that's what I mean by general


134
00:04:17,426 --> 00:04:19,456
templates, if you decide to for


135
00:04:19,456 --> 00:04:21,305
example, put a square around


136
00:04:21,305 --> 00:04:24,656
this wakeboarder as I have, you


137
00:04:24,656 --> 00:04:27,116
can then go ahead and track


138
00:04:27,116 --> 00:04:27,376
that.


139
00:04:29,336 --> 00:04:30,556
You can see that we handle


140
00:04:30,556 --> 00:04:32,666
pretty large changes in scale,


141
00:04:32,756 --> 00:04:34,396
pretty large deformations fairly


142
00:04:34,396 --> 00:04:35,746
robustly with this technology.


143
00:04:39,096 --> 00:04:40,266
Another really exciting


144
00:04:41,296 --> 00:04:42,956
technology that's new in Apple's


145
00:04:42,956 --> 00:04:45,086
SDK this year Core ML and you


146
00:04:45,086 --> 00:04:46,126
can integrate your Core ML


147
00:04:46,126 --> 00:04:47,576
models directly into Vision.


148
00:04:48,236 --> 00:04:50,916
As I've mentioned, machine


149
00:04:50,916 --> 00:04:52,496
learning does great things for


150
00:04:52,496 --> 00:04:55,336
Computer Vision and you can use


151
00:04:55,336 --> 00:04:57,066
Core ML if you want to create


152
00:04:57,066 --> 00:04:58,166
your own models, do your own


153
00:04:58,166 --> 00:04:58,716
solution.


154
00:04:59,416 --> 00:05:01,246
Perhaps for example, you want to


155
00:05:01,246 --> 00:05:02,476
create a wedding application


156
00:05:02,986 --> 00:05:06,316
where you're able to detect this


157
00:05:06,316 --> 00:05:08,006
part of the wedding is the


158
00:05:08,006 --> 00:05:08,956
reception, this part of the


159
00:05:08,956 --> 00:05:09,816
wedding is where the bride is


160
00:05:09,816 --> 00:05:10,686
walking down the aisle.


161
00:05:11,416 --> 00:05:12,456
If you want to train your own


162
00:05:12,456 --> 00:05:14,706
model and you have the data to


163
00:05:14,706 --> 00:05:17,196
train your own model you can do


164
00:05:17,196 --> 00:05:17,476
that.


165
00:05:18,556 --> 00:05:20,766
Core ML as I mentioned, provides


166
00:05:20,766 --> 00:05:22,116
native acceleration for custom


167
00:05:22,116 --> 00:05:23,206
models so they'll run really


168
00:05:23,206 --> 00:05:25,706
fast and Vision provides the


169
00:05:25,706 --> 00:05:26,876
imaging pipeline to support


170
00:05:26,876 --> 00:05:28,776
these models, so you won't have


171
00:05:28,776 --> 00:05:30,276
to do any rescaling or anything


172
00:05:30,276 --> 00:05:31,156
like that we'll take care of all


173
00:05:31,156 --> 00:05:31,586
that for you.


174
00:05:31,586 --> 00:05:33,106
We know what your model is


175
00:05:33,106 --> 00:05:34,476
expecting and we'll put the


176
00:05:34,476 --> 00:05:35,626
image in the right format.


177
00:05:37,596 --> 00:05:38,826
If you're interested in Core ML


178
00:05:38,826 --> 00:05:40,396
there's some sessions that you


179
00:05:40,396 --> 00:05:42,576
can go to, we've listed the labs


180
00:05:42,576 --> 00:05:43,306
down here for you.


181
00:05:43,866 --> 00:05:45,346
One of them will be tomorrow


182
00:05:45,346 --> 00:05:47,356
morning and then another one on


183
00:05:47,356 --> 00:05:47,986
Friday afternoon.


184
00:05:49,446 --> 00:05:51,386
So that's basically the features


185
00:05:51,386 --> 00:05:52,346
that are in the Vision


186
00:05:52,346 --> 00:05:52,796
framework.


187
00:05:54,236 --> 00:05:56,526
Overall, what Apple's new Vision


188
00:05:56,526 --> 00:05:58,306
framework provides are


189
00:05:58,766 --> 00:06:00,776
high-level on-device solutions


190
00:06:01,306 --> 00:06:02,586
to Computer Vision problems


191
00:06:02,706 --> 00:06:03,756
through one simple API.


192
00:06:03,756 --> 00:06:06,576
Now let me break this statement


193
00:06:06,576 --> 00:06:08,266
down just a little bit.


194
00:06:09,276 --> 00:06:10,586
What do I mean by high-level


195
00:06:10,586 --> 00:06:11,186
solutions?


196
00:06:11,896 --> 00:06:14,336
Well we don't want you to have


197
00:06:14,336 --> 00:06:15,556
to be a Computer Vision expert


198
00:06:15,556 --> 00:06:16,876
to put the magic of Computer


199
00:06:16,876 --> 00:06:18,066
Vision into your applications.


200
00:06:18,656 --> 00:06:20,706
You don't want to necessarily


201
00:06:20,706 --> 00:06:22,216
have to know which feature


202
00:06:22,216 --> 00:06:23,636
detector you want to use in


203
00:06:23,636 --> 00:06:25,086
combination with what classifier


204
00:06:25,086 --> 00:06:26,966
or set of classifiers, we're


205
00:06:26,966 --> 00:06:28,366
going to handle that for you or


206
00:06:28,366 --> 00:06:29,196
whether or not you want to use


207
00:06:29,196 --> 00:06:30,216
machine learning for example.


208
00:06:30,976 --> 00:06:31,946
If you're a developer you're


209
00:06:32,326 --> 00:06:33,426
probably thinking I just want to


210
00:06:33,426 --> 00:06:34,246
know where the faces are.


211
00:06:35,626 --> 00:06:36,506
And so, we're going to handle


212
00:06:36,506 --> 00:06:37,666
all that complexity for you.


213
00:06:38,926 --> 00:06:42,166
Depending on your use case we'll


214
00:06:42,166 --> 00:06:43,896
be doing either traditional


215
00:06:43,896 --> 00:06:45,206
approach if that's what's needed


216
00:06:45,206 --> 00:06:46,846
for maybe real-time applications


217
00:06:46,846 --> 00:06:49,166
or deep learning algorithms for


218
00:06:49,416 --> 00:06:50,176
higher accuracy.


219
00:06:50,846 --> 00:06:54,186
Now I also mentioned that we're


220
00:06:54,186 --> 00:06:55,666
doing all these algorithms on


221
00:06:55,666 --> 00:06:58,116
the device, let's talk a little


222
00:06:58,116 --> 00:07:00,436
bit about why we'd want to do


223
00:07:00,436 --> 00:07:01,916
things on device versus provided


224
00:07:01,916 --> 00:07:02,906
a cloud-based solution.


225
00:07:03,396 --> 00:07:05,766
First of all, it's privacy.


226
00:07:06,866 --> 00:07:08,836
As you know, Apple cares a lot


227
00:07:08,836 --> 00:07:11,006
about privacy, I care a lot


228
00:07:11,006 --> 00:07:12,476
about privacy working at Apple,


229
00:07:12,796 --> 00:07:14,136
sometimes it makes my job a


230
00:07:14,136 --> 00:07:16,446
little harder, but nonetheless


231
00:07:17,026 --> 00:07:18,056
keeping all your data on the


232
00:07:18,056 --> 00:07:20,156
device is the best way to


233
00:07:20,156 --> 00:07:21,736
protect your user's data


234
00:07:21,736 --> 00:07:22,176
privacy.


235
00:07:24,456 --> 00:07:26,276
Furthermore, with certain


236
00:07:26,276 --> 00:07:28,036
cloud-based solutions there's a


237
00:07:28,036 --> 00:07:29,106
cost associated with it.


238
00:07:29,306 --> 00:07:31,616
If you're a developer maybe


239
00:07:31,616 --> 00:07:32,796
you're paying usage fees to use


240
00:07:32,796 --> 00:07:33,976
a cloud-based solution.


241
00:07:35,316 --> 00:07:37,146
Your users will have to transfer


242
00:07:37,146 --> 00:07:38,066
the data to that cloud.


243
00:07:39,516 --> 00:07:41,306
All these costs they can add up


244
00:07:41,306 --> 00:07:42,296
for both the developers and the


245
00:07:42,296 --> 00:07:42,726
users.


246
00:07:42,956 --> 00:07:44,506
So, when everything's on the


247
00:07:44,506 --> 00:07:45,586
device it's free.


248
00:07:45,816 --> 00:07:50,566
And you can support real-time


249
00:07:50,566 --> 00:07:51,966
use cases like the tracking


250
00:07:51,966 --> 00:07:52,756
example I showed you.


251
00:07:53,566 --> 00:07:54,616
Imagine trying to track


252
00:07:54,616 --> 00:07:55,536
something through a video by


253
00:07:55,536 --> 00:07:56,416
sending every frame to the


254
00:07:56,416 --> 00:07:57,926
cloud, I don't think that's


255
00:07:57,926 --> 00:07:58,556
going to work too well.


256
00:07:59,226 --> 00:08:01,456
So, no latency, fast execution


257
00:08:01,926 --> 00:08:02,946
that's what we're offering with


258
00:08:03,226 --> 00:08:03,896
the Vision framework.


259
00:08:04,786 --> 00:08:07,696
So, I hope you enjoyed that


260
00:08:08,076 --> 00:08:09,186
introduction, now we're going to


261
00:08:09,186 --> 00:08:12,226
go a little deeper and talk


262
00:08:12,226 --> 00:08:13,446
about the Vision concepts.


263
00:08:13,446 --> 00:08:13,996
For this part of the


264
00:08:13,996 --> 00:08:15,356
presentation I'm going to hand


265
00:08:15,356 --> 00:08:15,976
it off to Frank.


266
00:08:16,516 --> 00:08:19,566
[ Applause ]


267
00:08:20,066 --> 00:08:20,556
>> Thank you Brett.


268
00:08:22,956 --> 00:08:24,406
Hi, good afternoon, my name is


269
00:08:24,406 --> 00:08:25,466
Frank Doepke and I'm going to


270
00:08:25,466 --> 00:08:26,996
talk about more of the technical


271
00:08:26,996 --> 00:08:28,876
details what is part of our


272
00:08:28,876 --> 00:08:29,496
Vision framework.


273
00:08:29,796 --> 00:08:33,916
So, what do we want to do, when


274
00:08:33,916 --> 00:08:35,726
we want to analyze an image we


275
00:08:35,916 --> 00:08:37,836
have three major tasks that we


276
00:08:37,836 --> 00:08:39,816
actually want to perform.


277
00:08:40,275 --> 00:08:41,416
So, we [inaudible] finding out


278
00:08:41,416 --> 00:08:42,576
what is in the image and what do


279
00:08:42,576 --> 00:08:43,456
I want to know about it.


280
00:08:44,316 --> 00:08:45,356
There's the machinery,


281
00:08:46,076 --> 00:08:47,166
somebody's got to do the work


282
00:08:47,676 --> 00:08:48,956
and we get some results out of


283
00:08:48,956 --> 00:08:50,316
it, at least we hope that's


284
00:08:50,316 --> 00:08:51,206
what's going to happen.


285
00:08:52,106 --> 00:08:54,036
So, in terminology for Vision


286
00:08:54,036 --> 00:08:56,046
that means the asks these are


287
00:08:56,046 --> 00:08:56,836
requests.


288
00:08:57,476 --> 00:08:59,166
And I just did a few examples


289
00:08:59,166 --> 00:09:01,016
here like the barcode detection


290
00:09:01,016 --> 00:09:03,936
or face detection and we feed


291
00:09:03,936 --> 00:09:07,016
them into our request handler.


292
00:09:07,926 --> 00:09:08,996
That's the one in this case to


293
00:09:08,996 --> 00:09:10,106
be an image request and


294
00:09:10,176 --> 00:09:11,136
[inaudible] hold on to the image


295
00:09:11,446 --> 00:09:12,576
and it's going to do all the


296
00:09:12,576 --> 00:09:13,416
work for us.


297
00:09:14,066 --> 00:09:16,486
And as a result, we get back


298
00:09:16,526 --> 00:09:18,086
what we call observations, what


299
00:09:18,086 --> 00:09:19,446
did we observe in this image.


300
00:09:20,046 --> 00:09:21,666
And these observations depend on


301
00:09:21,666 --> 00:09:22,726
what you asked us to do.


302
00:09:23,006 --> 00:09:24,516
So, we have classification


303
00:09:24,516 --> 00:09:26,956
observation or detected objects.


304
00:09:27,786 --> 00:09:28,986
Now when you want to track


305
00:09:28,986 --> 00:09:30,076
something in the sequence like


306
00:09:30,076 --> 00:09:32,656
the wakeboarder it's basically


307
00:09:32,656 --> 00:09:33,466
the same concept.


308
00:09:33,566 --> 00:09:34,796
We have some asks, we have the


309
00:09:34,796 --> 00:09:36,376
machinery, and we get some


310
00:09:36,376 --> 00:09:39,096
results out of it in the end.


311
00:09:39,096 --> 00:09:40,826
Again, the asks are requests.


312
00:09:41,426 --> 00:09:42,936
Now since this changes with


313
00:09:42,936 --> 00:09:44,686
every frame the image actually


314
00:09:44,686 --> 00:09:45,926
travels with the request.


315
00:09:47,736 --> 00:09:48,936
Our machinery is again


316
00:09:48,936 --> 00:09:49,826
[inaudible] request handler it's


317
00:09:49,826 --> 00:09:52,506
the sequence request handler and


318
00:09:52,506 --> 00:09:54,046
we get results which are


319
00:09:54,046 --> 00:09:55,906
observations that go with our


320
00:09:55,976 --> 00:09:56,546
requests.


321
00:09:58,086 --> 00:10:00,866
So, let me talk a little bit


322
00:10:00,866 --> 00:10:02,066
more about these two image


323
00:10:02,066 --> 00:10:02,966
request handlers that I


324
00:10:02,966 --> 00:10:03,806
mentioned so far.


325
00:10:04,476 --> 00:10:05,606
So, we have the image request


326
00:10:05,606 --> 00:10:07,666
handler that is mostly if you


327
00:10:07,666 --> 00:10:09,026
want to do something interactive


328
00:10:09,026 --> 00:10:09,736
with the image.


329
00:10:10,166 --> 00:10:11,386
You want to do multiple Vision


330
00:10:11,386 --> 00:10:13,466
tasks on an image, sometimes you


331
00:10:13,466 --> 00:10:15,116
actually do one and then based


332
00:10:15,116 --> 00:10:16,566
on the results you then kick off


333
00:10:16,566 --> 00:10:17,926
the next one and that's what you


334
00:10:17,926 --> 00:10:19,016
want to use the image request


335
00:10:19,016 --> 00:10:19,486
handler for.


336
00:10:19,966 --> 00:10:21,296
It'll hold on to the image that


337
00:10:21,296 --> 00:10:22,446
it's set up with for its


338
00:10:22,446 --> 00:10:25,366
lifecycle and that allows us


339
00:10:25,396 --> 00:10:26,856
under the cover to do


340
00:10:26,856 --> 00:10:28,856
performance optimizations by


341
00:10:28,856 --> 00:10:30,996
holding on to intermediates to


342
00:10:31,146 --> 00:10:32,866
make these requests perform


343
00:10:32,866 --> 00:10:33,276
faster.


344
00:10:34,016 --> 00:10:36,466
On the flipside, if I want to


345
00:10:36,466 --> 00:10:37,726
track something we use the


346
00:10:37,726 --> 00:10:38,816
sequence request handler.


347
00:10:39,276 --> 00:10:40,516
The sequence request handler


348
00:10:40,516 --> 00:10:42,086
allows us to keep tracking


349
00:10:42,186 --> 00:10:42,996
[inaudible] in the sequence


350
00:10:42,996 --> 00:10:43,666
request handler.


351
00:10:44,316 --> 00:10:46,136
And it will not hold on to all


352
00:10:46,136 --> 00:10:47,466
the images that gets fed into it


353
00:10:47,466 --> 00:10:48,626
over its lifecycle so they get


354
00:10:48,626 --> 00:10:49,326
released earlier.


355
00:10:50,056 --> 00:10:51,536
But that means on the flipside,


356
00:10:51,776 --> 00:10:53,486
you cannot do the same


357
00:10:53,486 --> 00:10:54,866
optimizations if you want to do


358
00:10:54,866 --> 00:10:56,146
multiple requests on the same


359
00:10:56,146 --> 00:10:56,486
image.


360
00:10:57,716 --> 00:10:59,836
So how does this look in the


361
00:10:59,836 --> 00:11:01,596
code, we are developers that's


362
00:11:01,596 --> 00:11:02,526
what we want to see.


363
00:11:04,116 --> 00:11:05,606
So, we start as a blank slate


364
00:11:05,606 --> 00:11:07,316
that's always good and then we


365
00:11:07,316 --> 00:11:08,446
create a request.


366
00:11:08,886 --> 00:11:10,466
In this case, it's a face


367
00:11:10,466 --> 00:11:12,746
detection request.


368
00:11:13,346 --> 00:11:14,496
Now we create the request


369
00:11:14,496 --> 00:11:16,546
handler, what I'm choosing here


370
00:11:16,546 --> 00:11:17,816
is a request handler based on


371
00:11:17,816 --> 00:11:19,216
the files so I have a file on


372
00:11:19,216 --> 00:11:22,556
disk that I want to use.


373
00:11:22,556 --> 00:11:24,026
Now I ask myRequestHandler to


374
00:11:24,026 --> 00:11:25,646
perform my request and this in


375
00:11:25,646 --> 00:11:27,566
this case [inaudible] it's just


376
00:11:27,566 --> 00:11:29,716
one request I have my array, but


377
00:11:29,716 --> 00:11:32,746
it could be many and I get my


378
00:11:32,906 --> 00:11:33,836
observations back.


379
00:11:34,996 --> 00:11:36,606
And this can be many faces that


380
00:11:36,606 --> 00:11:37,266
I detected.


381
00:11:37,836 --> 00:11:39,216
Now the one thing I would like


382
00:11:39,216 --> 00:11:42,746
to highlight here is the results


383
00:11:43,006 --> 00:11:45,036
come back as part of the request


384
00:11:45,156 --> 00:11:46,076
that we actually set up


385
00:11:46,076 --> 00:11:46,526
initially.


386
00:11:46,526 --> 00:11:49,916
How does it look when we want to


387
00:11:51,076 --> 00:11:52,506
track something?


388
00:11:52,616 --> 00:11:53,946
We create a sequence request


389
00:11:54,966 --> 00:11:55,776
handler [inaudible] of course


390
00:11:55,776 --> 00:11:57,026
not set up as an image because


391
00:11:57,216 --> 00:11:58,236
we have to [inaudible] all the


392
00:11:58,236 --> 00:11:59,656
frames of the sequence.


393
00:12:01,476 --> 00:12:02,326
So, I started with an


394
00:12:02,326 --> 00:12:03,906
observation that I got from the


395
00:12:03,906 --> 00:12:05,906
previous detection or I mark


396
00:12:05,956 --> 00:12:07,596
something up and I create my


397
00:12:07,596 --> 00:12:09,276
tracking request.


398
00:12:09,826 --> 00:12:12,026
And I simply have to run the


399
00:12:12,026 --> 00:12:12,596
request.


400
00:12:13,246 --> 00:12:15,036
And I feed in in this case as a


401
00:12:15,036 --> 00:12:17,026
pixel buffer the frame that is


402
00:12:17,026 --> 00:12:17,996
currently being dragged.


403
00:12:19,406 --> 00:12:21,176
And out of it again I get some


404
00:12:21,176 --> 00:12:21,616
results.


405
00:12:22,246 --> 00:12:25,246
So, now that we have talked


406
00:12:25,246 --> 00:12:26,816
about how this API is kind of


407
00:12:26,816 --> 00:12:28,426
structured I would like to guide


408
00:12:28,426 --> 00:12:30,066
you through some best practices


409
00:12:30,066 --> 00:12:31,616
so that you get, you know, the


410
00:12:31,616 --> 00:12:33,006
best experience out of Vision.


411
00:12:33,666 --> 00:12:37,456
So, when we want to put together


412
00:12:37,456 --> 00:12:38,936
a Computer Vision task you have


413
00:12:38,936 --> 00:12:39,886
to think about a few things.


414
00:12:41,476 --> 00:12:43,216
Number one, what is the right


415
00:12:43,216 --> 00:12:44,586
image type that I want to use.


416
00:12:45,826 --> 00:12:47,876
Number two, what am I going to


417
00:12:47,876 --> 00:12:48,746
do with the image.


418
00:12:50,396 --> 00:12:52,196
And number three, what


419
00:12:52,226 --> 00:12:53,496
performance do I need or want.


420
00:12:53,496 --> 00:12:54,346
Of course, you always want


421
00:12:54,346 --> 00:12:55,346
fastest, but there are some


422
00:12:55,346 --> 00:12:56,356
tradeoffs that you have to think


423
00:12:56,356 --> 00:12:56,636
about.


424
00:12:57,026 --> 00:12:59,136
So, let's talk about the image


425
00:13:00,526 --> 00:13:00,636
type.


426
00:13:00,846 --> 00:13:02,346
Vision supports a number of


427
00:13:02,346 --> 00:13:04,196
image types and they range from


428
00:13:04,196 --> 00:13:06,836
CVPixelBuffer, CGIImage or even


429
00:13:06,836 --> 00:13:09,276
as we saw in the previous


430
00:13:09,276 --> 00:13:11,446
example just from data that I


431
00:13:11,446 --> 00:13:12,606
use in NSURL.


432
00:13:13,216 --> 00:13:15,776
And we go over all these types


433
00:13:15,886 --> 00:13:17,076
in the following slides so that


434
00:13:17,076 --> 00:13:18,396
you know what to choose when.


435
00:13:20,556 --> 00:13:22,756
Which to choose depends a lot of


436
00:13:22,756 --> 00:13:23,916
like what you want to do.


437
00:13:23,916 --> 00:13:25,836
If you run from a camera stream


438
00:13:25,836 --> 00:13:27,596
or if you run from files on disk


439
00:13:28,346 --> 00:13:29,996
you have to look at that


440
00:13:30,096 --> 00:13:31,396
[inaudible] kind of which type


441
00:13:31,396 --> 00:13:32,856
of image you want to use.


442
00:13:33,246 --> 00:13:34,536
Now two important things to


443
00:13:34,536 --> 00:13:37,576
remember is we already have an


444
00:13:37,666 --> 00:13:39,206
imaging pipeline in the Vision


445
00:13:39,206 --> 00:13:41,076
framework you don't need to


446
00:13:41,076 --> 00:13:42,006
scale the images.


447
00:13:42,406 --> 00:13:43,616
So, unless you already have a


448
00:13:43,616 --> 00:13:44,926
very small representation that


449
00:13:44,926 --> 00:13:45,806
you absolutely want to use,


450
00:13:45,856 --> 00:13:47,136
please don't pre-scale because


451
00:13:47,246 --> 00:13:50,476
we'll just do the work twice.


452
00:13:50,666 --> 00:13:52,356
And mind the orientation.


453
00:13:52,526 --> 00:13:54,086
Computer Vision algorithms are


454
00:13:54,086 --> 00:13:57,056
mostly not, you know, sensitive


455
00:13:57,056 --> 00:13:59,226
to orientation or sorry, they


456
00:13:59,356 --> 00:14:01,176
are sensitive to orientation so


457
00:14:01,236 --> 00:14:02,576
you have to pass that in.


458
00:14:03,166 --> 00:14:04,586
And that is an important part


459
00:14:04,586 --> 00:14:05,676
because if you pass in a


460
00:14:05,816 --> 00:14:07,206
portrait image that's actually


461
00:14:07,206 --> 00:14:08,166
lying on its side we will not


462
00:14:08,166 --> 00:14:09,616
find the faces and that's one of


463
00:14:09,616 --> 00:14:10,996
the common mistakes that usually


464
00:14:10,996 --> 00:14:11,386
happens.


465
00:14:12,906 --> 00:14:14,396
So, I promised to go over the


466
00:14:14,396 --> 00:14:14,846
types.


467
00:14:15,796 --> 00:14:16,696
When you want to do something


468
00:14:16,696 --> 00:14:17,866
streaming we want to use the


469
00:14:17,866 --> 00:14:18,746
CVPixelBuffer.


470
00:14:19,906 --> 00:14:21,956
When you create a VideoDataOut


471
00:14:21,956 --> 00:14:23,206
[inaudible] capture you will get


472
00:14:23,286 --> 00:14:24,776
CMSampleBuffers and through


473
00:14:24,916 --> 00:14:25,806
those we get your


474
00:14:25,866 --> 00:14:26,686
CVPixelBuffers.


475
00:14:27,856 --> 00:14:29,456
It's also a pretty good format


476
00:14:29,456 --> 00:14:30,886
if you already have something


477
00:14:30,886 --> 00:14:32,806
where you keep your image data


478
00:14:32,806 --> 00:14:34,376
raw in memory like it's LGB


479
00:14:34,376 --> 00:14:35,936
pixels and wrap them into a


480
00:14:35,936 --> 00:14:37,276
CVPixelBuffer this is a great


481
00:14:37,276 --> 00:14:38,496
format to pass into Vision.


482
00:14:40,666 --> 00:14:41,976
When you get files from disk


483
00:14:42,146 --> 00:14:44,046
please use the URL or if it


484
00:14:44,046 --> 00:14:44,996
comes from the web use the


485
00:14:44,996 --> 00:14:46,926
NSData path.


486
00:14:47,126 --> 00:14:48,886
The great thing about that is it


487
00:14:48,886 --> 00:14:50,796
really allows us to reduce the


488
00:14:50,796 --> 00:14:51,936
memory for print in your


489
00:14:51,936 --> 00:14:52,616
application.


490
00:14:53,086 --> 00:14:54,566
Vision will only read what it


491
00:14:54,566 --> 00:14:56,236
needs to perform the task.


492
00:14:57,156 --> 00:14:58,256
If you think about you want to


493
00:14:58,256 --> 00:14:59,226
do face detection on a


494
00:14:59,226 --> 00:15:02,376
64-megapixel panorama Vision


495
00:15:02,376 --> 00:15:03,526
will actually reduce your memory


496
00:15:03,526 --> 00:15:04,936
for it, but not reading the full


497
00:15:04,936 --> 00:15:06,436
file actually into the memory


498
00:15:06,436 --> 00:15:07,466
and that is an important thing


499
00:15:07,466 --> 00:15:08,136
to keep in mind.


500
00:15:10,396 --> 00:15:12,006
We will read in this case the


501
00:15:12,006 --> 00:15:13,336
EXIF Orientation out of the


502
00:15:13,336 --> 00:15:15,336
file, but you can override it if


503
00:15:15,336 --> 00:15:17,266
you have to for those formats


504
00:15:17,266 --> 00:15:18,706
that don't support it.


505
00:15:20,666 --> 00:15:22,126
If you're already using Core


506
00:15:22,166 --> 00:15:24,076
Image in your application by all


507
00:15:24,076 --> 00:15:25,286
means process the CI image.


508
00:15:25,286 --> 00:15:27,366
This is also important when you


509
00:15:27,366 --> 00:15:28,096
want to actually do some


510
00:15:28,096 --> 00:15:28,806
preprocessing.


511
00:15:28,806 --> 00:15:29,786
If you have some domain


512
00:15:29,786 --> 00:15:30,946
knowledge of what you want to do


513
00:15:30,946 --> 00:15:33,286
in your Computer Vision task you


514
00:15:33,286 --> 00:15:34,506
can do some preprocessing and


515
00:15:34,506 --> 00:15:35,766
try and enhance the image and,


516
00:15:35,806 --> 00:15:37,186
therefore, enhance the Vision


517
00:15:37,186 --> 00:15:37,716
results.


518
00:15:39,346 --> 00:15:40,316
If you want to learn a bit more


519
00:15:40,316 --> 00:15:42,706
about Core Image, there's a


520
00:15:42,706 --> 00:15:45,006
session on Thursday at 1:50 and


521
00:15:45,396 --> 00:15:46,146
they will also show the


522
00:15:46,146 --> 00:15:47,836
integration with our Vision


523
00:15:47,836 --> 00:15:48,286
framework.


524
00:15:48,756 --> 00:15:52,276
Last but not least, if you have


525
00:15:52,276 --> 00:15:54,246
all the images in your UI you


526
00:15:54,426 --> 00:15:56,346
can use the CG image [inaudible]


527
00:15:56,716 --> 00:15:59,116
out of the NS image or the UI


528
00:15:59,226 --> 00:16:01,196
images let's say it comes from


529
00:16:01,196 --> 00:16:02,906
the UI image picker and pass


530
00:16:02,976 --> 00:16:03,686
those into Vision.


531
00:16:03,806 --> 00:16:07,006
Now what am I going to do with


532
00:16:07,096 --> 00:16:08,496
the image and that's where we


533
00:16:08,496 --> 00:16:10,236
have to decide if I want to do


534
00:16:10,236 --> 00:16:11,346
something interactive with the


535
00:16:11,426 --> 00:16:13,206
image in that case I use my


536
00:16:13,206 --> 00:16:14,336
ImageRequestHandler.


537
00:16:14,606 --> 00:16:16,226
It will hold on to the image for


538
00:16:16,226 --> 00:16:17,926
the time and I can do multiple


539
00:16:17,926 --> 00:16:19,656
passes on that image and get the


540
00:16:19,656 --> 00:16:20,866
best results out of that.


541
00:16:22,126 --> 00:16:23,426
Now the CVPixelBuffer


542
00:16:23,426 --> 00:16:24,926
technically will allow you that


543
00:16:24,926 --> 00:16:26,826
you could change the pixels


544
00:16:27,226 --> 00:16:28,216
[inaudible], but we see them as


545
00:16:28,216 --> 00:16:29,756
immutable so don't do that


546
00:16:29,756 --> 00:16:30,776
because we'll get some strange


547
00:16:30,776 --> 00:16:31,246
results.


548
00:16:31,546 --> 00:16:35,406
Next, if you want to track


549
00:16:35,606 --> 00:16:36,816
something we use the


550
00:16:36,816 --> 00:16:38,156
SequenceRequestHandler.


551
00:16:39,636 --> 00:16:40,506
It allows us to keep the


552
00:16:40,506 --> 00:16:42,946
tracking state and lifecycle of


553
00:16:42,946 --> 00:16:44,676
my image is not tied to those


554
00:16:44,736 --> 00:16:46,256
requests handler anymore, but


555
00:16:46,296 --> 00:16:47,576
just how long it needs it for


556
00:16:47,576 --> 00:16:47,976
the tracking.


557
00:16:52,176 --> 00:16:53,976
Performance, so these Vision


558
00:16:53,976 --> 00:16:55,796
tasks are computationally


559
00:16:55,796 --> 00:16:57,726
intensive very often and they do


560
00:16:57,726 --> 00:16:59,056
take time, so you have to think


561
00:16:59,056 --> 00:17:01,486
about that you want to actually


562
00:17:01,486 --> 00:17:03,376
run your task on a different


563
00:17:03,856 --> 00:17:05,465
queue not your main queue.


564
00:17:06,616 --> 00:17:08,296
And think about if you want to


565
00:17:08,296 --> 00:17:09,376
do it in the background, which


566
00:17:09,376 --> 00:17:11,086
is a bit slower or if you need


567
00:17:11,086 --> 00:17:12,356
it very quickly use a more


568
00:17:12,356 --> 00:17:13,915
interactive quality of service


569
00:17:14,366 --> 00:17:15,516
to get the performance.


570
00:17:16,596 --> 00:17:19,276
A good practice is to use the


571
00:17:19,276 --> 00:17:22,026
completion handler to get the


572
00:17:22,056 --> 00:17:23,606
results back, this is part of


573
00:17:23,606 --> 00:17:24,026
our API.


574
00:17:24,026 --> 00:17:26,165
But keep in mind that this


575
00:17:26,215 --> 00:17:27,866
completion handler gets called


576
00:17:27,915 --> 00:17:29,236
on that queue in which you


577
00:17:29,236 --> 00:17:30,686
actually set it off.


578
00:17:30,686 --> 00:17:32,576
So, if you need to update your


579
00:17:32,576 --> 00:17:33,936
UI you have to dispatch that


580
00:17:33,936 --> 00:17:34,746
back to the main queue.


581
00:17:35,426 --> 00:17:38,796
So as Brett already highlighted,


582
00:17:38,796 --> 00:17:40,036
we have a new face detection and


583
00:17:40,036 --> 00:17:41,206
you might say oh God, yet


584
00:17:41,206 --> 00:17:41,706
another one.


585
00:17:43,946 --> 00:17:45,156
But we have good reasons for


586
00:17:45,156 --> 00:17:45,446
this.


587
00:17:45,706 --> 00:17:47,546
Vision uses deep learning and


588
00:17:47,546 --> 00:17:48,976
this gives us really a lot


589
00:17:49,186 --> 00:17:50,516
better precision and recall,


590
00:17:50,926 --> 00:17:52,366
therefore, much better results.


591
00:17:52,916 --> 00:17:55,826
The downside of it, on older


592
00:17:55,826 --> 00:17:56,976
hardware it will run a bit


593
00:17:56,976 --> 00:17:57,356
slower.


594
00:17:57,436 --> 00:17:58,946
So, let's look a little bit at


595
00:17:58,946 --> 00:18:01,006
our overall landscape of face


596
00:18:01,006 --> 00:18:01,926
detectors that we have


597
00:18:01,926 --> 00:18:02,476
available.


598
00:18:03,236 --> 00:18:04,876
So, we have Vision which really


599
00:18:04,876 --> 00:18:06,346
gives us the best results and


600
00:18:06,346 --> 00:18:08,736
it's pretty fast and also pretty


601
00:18:08,736 --> 00:18:09,966
good in its power use as it is


602
00:18:09,966 --> 00:18:10,926
optimized for that.


603
00:18:11,246 --> 00:18:12,766
And we have it available on all


604
00:18:12,766 --> 00:18:14,376
platforms except the watchOS.


605
00:18:15,446 --> 00:18:16,886
And this is the same in terms of


606
00:18:16,886 --> 00:18:18,566
availability for Core Image and


607
00:18:18,566 --> 00:18:19,686
it's a bit faster, but the


608
00:18:19,686 --> 00:18:21,106
results are not quite as good.


609
00:18:21,736 --> 00:18:24,266
In the AV capture session which


610
00:18:24,266 --> 00:18:25,536
is only happening during the


611
00:18:25,536 --> 00:18:27,016
capture side we can actually use


612
00:18:27,016 --> 00:18:28,436
hardware so it's really fast in


613
00:18:28,436 --> 00:18:30,176
performance, but the results


614
00:18:30,176 --> 00:18:31,666
again are not as good as we get


615
00:18:31,666 --> 00:18:31,976
out of Vision.


616
00:18:32,466 --> 00:18:34,006
So, you have to choose depending


617
00:18:34,006 --> 00:18:35,286
on your application what you


618
00:18:35,286 --> 00:18:36,996
want to do choose the right


619
00:18:36,996 --> 00:18:38,146
technology for the face


620
00:18:38,146 --> 00:18:38,566
detection.


621
00:18:40,406 --> 00:18:41,446
Now I did mention that our


622
00:18:41,446 --> 00:18:43,056
quality is better, so let me try


623
00:18:43,056 --> 00:18:44,096
to prove that a little bit.


624
00:18:44,576 --> 00:18:46,796
So, I have here an image and I


625
00:18:46,796 --> 00:18:47,896
ran the face detection through


626
00:18:47,896 --> 00:18:48,506
Core Image.


627
00:18:48,976 --> 00:18:50,956
And we find two faces and we see


628
00:18:50,956 --> 00:18:53,056
roughly where the eyes and where


629
00:18:53,656 --> 00:18:55,126
the mouth are.


630
00:18:55,396 --> 00:18:56,986
Now in Vision we find all four


631
00:18:56,986 --> 00:18:58,496
faces even the occluded ones and


632
00:18:58,496 --> 00:18:59,986
we get a whole lot more details


633
00:19:00,036 --> 00:19:03,196
with the visual landmarks.


634
00:19:04,356 --> 00:19:05,486
Speaking of Core Image, I would


635
00:19:05,486 --> 00:19:06,426
like to highlight a little bit


636
00:19:06,426 --> 00:19:07,306
what's happening with the


637
00:19:07,386 --> 00:19:08,046
CIDetectors.


638
00:19:08,076 --> 00:19:11,456
So, whoever uses it already can


639
00:19:11,456 --> 00:19:12,536
keep on using them they are


640
00:19:12,536 --> 00:19:15,596
still in Core Image, but all new


641
00:19:15,596 --> 00:19:17,136
parts and all the improvements


642
00:19:17,136 --> 00:19:18,296
in terms of algorithms for


643
00:19:18,296 --> 00:19:20,216
computer moving forward will be


644
00:19:20,216 --> 00:19:22,046
in Vision that's the new home


645
00:19:22,046 --> 00:19:22,946
for Computer Vision.


646
00:19:23,546 --> 00:19:28,126
So, an awful lot of sides, how


647
00:19:28,126 --> 00:19:28,656
about a demo.


648
00:19:29,606 --> 00:19:30,826
So, what I'm going to show you


649
00:19:30,826 --> 00:19:33,756
is an application that runs an


650
00:19:33,756 --> 00:19:35,726
AV capture session on the device


651
00:19:35,996 --> 00:19:37,256
if the demo Gods are with us.


652
00:19:37,896 --> 00:19:39,936
And we will do a very simple


653
00:19:39,936 --> 00:19:41,296
rectangle detection request.


654
00:19:42,376 --> 00:19:43,916
So, what do I have to do to set


655
00:19:43,916 --> 00:19:44,296
this up?


656
00:19:46,156 --> 00:19:48,386
What you see here is I create my


657
00:19:48,386 --> 00:19:52,136
request, that's my simple


658
00:19:52,136 --> 00:19:54,516
rectangle detection request in


659
00:19:55,886 --> 00:19:56,816
this case.


660
00:19:56,996 --> 00:19:58,376
I'm actually in the wrong sample


661
00:19:58,376 --> 00:19:59,416
that's why I'm getting confused


662
00:19:59,416 --> 00:19:59,826
here, my apologies.


663
00:20:00,356 --> 00:20:02,516
Here we go.


664
00:20:02,546 --> 00:20:03,826
Okay we have our rectangle


665
00:20:03,826 --> 00:20:05,956
detection request and I'm


666
00:20:05,956 --> 00:20:07,406
setting some parameters just as


667
00:20:07,406 --> 00:20:08,856
an example here, I only want


668
00:20:08,856 --> 00:20:10,626
them this minimum size in our


669
00:20:10,626 --> 00:20:11,946
coordinates are normalized so I


670
00:20:11,946 --> 00:20:13,796
only want a 10% minimize size of


671
00:20:13,796 --> 00:20:16,666
the image and I just want 20


672
00:20:16,666 --> 00:20:17,206
rectangles.


673
00:20:17,436 --> 00:20:19,036
I could get more, but I want 20,


674
00:20:19,036 --> 00:20:20,156
I just picked a number.


675
00:20:21,386 --> 00:20:23,086
I set up my area of the request


676
00:20:23,116 --> 00:20:25,986
that I want to perform and the


677
00:20:26,236 --> 00:20:28,506
right here this is our


678
00:20:28,676 --> 00:20:30,616
completion handler and all that


679
00:20:30,616 --> 00:20:32,326
I'm going to do is I'm going to


680
00:20:32,326 --> 00:20:34,406
draw my rectangles, but as you


681
00:20:34,406 --> 00:20:35,816
notice I'm just patching it to


682
00:20:35,816 --> 00:20:37,826
the main queue to update our UI.


683
00:20:39,176 --> 00:20:40,216
Where do our images come from?


684
00:20:40,216 --> 00:20:41,736
So, we look at the capture


685
00:20:41,736 --> 00:20:45,526
output here and as I promised,


686
00:20:45,666 --> 00:20:47,006
in the capture output we get our


687
00:20:47,006 --> 00:20:48,566
pixelBuffer from the


688
00:20:48,566 --> 00:20:49,606
CMSampleBuffer.


689
00:20:50,376 --> 00:20:54,276
Right here I'm getting the


690
00:20:54,276 --> 00:20:55,166
cameraIntrinsics.


691
00:20:55,166 --> 00:20:56,496
Now this is something that is


692
00:20:56,596 --> 00:20:57,606
important in some of these


693
00:20:57,656 --> 00:20:58,816
Computer Vision paths where we


694
00:20:58,816 --> 00:21:00,126
actually know what the camera is


695
00:21:00,126 --> 00:21:00,866
kind of looking at.


696
00:21:02,406 --> 00:21:04,206
As I mentioned, we don't forget


697
00:21:04,206 --> 00:21:06,446
the acts of orientation and I


698
00:21:06,446 --> 00:21:07,876
create an image request handler


699
00:21:08,026 --> 00:21:09,696
and perform our tasks.


700
00:21:09,696 --> 00:21:10,636
So, how does this look when we


701
00:21:10,636 --> 00:21:11,886
actually run it?


702
00:21:12,156 --> 00:21:13,006
All right, so what we're going


703
00:21:13,006 --> 00:21:14,296
to see here is that now we


704
00:21:14,586 --> 00:21:16,516
tracked this rectangle and


705
00:21:16,516 --> 00:21:17,906
that's as simple as it is, we


706
00:21:17,906 --> 00:21:19,386
can find other rectangles.


707
00:21:20,896 --> 00:21:22,036
If the cable is long enough we


708
00:21:22,036 --> 00:21:23,076
can actually look oh, there we


709
00:21:23,076 --> 00:21:24,406
find a computer with various


710
00:21:24,406 --> 00:21:24,906
rectangles.


711
00:21:25,516 --> 00:21:27,976
Now I chose the yellow kind of


712
00:21:27,976 --> 00:21:29,666
on purpose because it's the same


713
00:21:29,666 --> 00:21:31,456
color as you saw in the demo


714
00:21:31,726 --> 00:21:33,246
during the keynote for the new


715
00:21:33,246 --> 00:21:34,516
document camera on notes.


716
00:21:34,576 --> 00:21:36,536
And I borrowed their color


717
00:21:36,536 --> 00:21:37,706
because they borrowed our code


718
00:21:37,706 --> 00:21:38,686
to do actually the rectangle


719
00:21:38,686 --> 00:21:38,976
detection.


720
00:21:39,516 --> 00:21:44,636
[ Applause ]


721
00:21:45,136 --> 00:21:45,876
Thank you.


722
00:21:46,016 --> 00:21:48,000
[ Applause ]


723
00:21:51,046 --> 00:21:52,516
Now that was simple, let's do a


724
00:21:52,516 --> 00:21:52,916
bit more.


725
00:21:56,396 --> 00:21:58,746
So, how about we throw some


726
00:21:58,746 --> 00:22:00,096
machine learning at this as well


727
00:22:00,096 --> 00:22:01,286
just for the fun of it.


728
00:22:01,956 --> 00:22:03,776
So, what I have to do is I have


729
00:22:03,776 --> 00:22:06,136
a little model that I just


730
00:22:06,196 --> 00:22:07,566
dragged into my project here.


731
00:22:14,156 --> 00:22:16,456
And that is a classifier that


732
00:22:16,456 --> 00:22:17,576
will tell us a bit something


733
00:22:17,576 --> 00:22:18,276
about the image.


734
00:22:19,456 --> 00:22:21,726
And we see when we look at this


735
00:22:21,726 --> 00:22:25,536
part here that we need to feed


736
00:22:25,536 --> 00:22:27,116
it an image of a very strange


737
00:22:27,116 --> 00:22:30,556
size and get out of it some


738
00:22:30,556 --> 00:22:31,416
classification.


739
00:22:32,496 --> 00:22:33,596
Now you don't need to worry


740
00:22:33,596 --> 00:22:35,886
about that size because Vision


741
00:22:35,886 --> 00:22:37,006
will do the work for you.


742
00:22:37,006 --> 00:22:43,026
So, what do I need to do?


743
00:22:43,626 --> 00:22:46,076
I first need to create a Vision


744
00:22:46,076 --> 00:22:48,226
model and my request with that.


745
00:22:48,286 --> 00:22:50,326
And that is the part that we


746
00:22:50,326 --> 00:22:52,736
have here, so I'm simply loading


747
00:22:52,736 --> 00:22:56,166
the inception model and I create


748
00:22:56,166 --> 00:22:57,456
my classification request.


749
00:22:58,726 --> 00:22:59,476
Now it tells me there's


750
00:22:59,476 --> 00:23:00,696
something missing and I will get


751
00:23:00,696 --> 00:23:01,706
to that in just a moment.


752
00:23:01,996 --> 00:23:03,256
The last thing I want to


753
00:23:03,256 --> 00:23:05,006
highlight here is it says that


754
00:23:05,366 --> 00:23:06,956
it was okay square image, but


755
00:23:06,956 --> 00:23:09,446
our cameras don't see squares so


756
00:23:09,446 --> 00:23:10,786
I need to tell it actually how


757
00:23:10,786 --> 00:23:12,586
to handle just, you know, the


758
00:23:12,586 --> 00:23:14,136
aspect ratio that I want to use.


759
00:23:14,136 --> 00:23:15,636
And I say okay I want to just


760
00:23:15,636 --> 00:23:15,976
send a crop.


761
00:23:20,276 --> 00:23:22,636
So, I need a completion handler


762
00:23:22,636 --> 00:23:26,246
for my task and I have that


763
00:23:26,246 --> 00:23:28,336
already pre canned here as well.


764
00:23:30,496 --> 00:23:31,806
So, in this completion handler I


765
00:23:31,806 --> 00:23:33,296
simply look at my observation


766
00:23:33,296 --> 00:23:34,656
and I will get so this


767
00:23:34,696 --> 00:23:36,216
classifier can see a thousand


768
00:23:36,216 --> 00:23:37,876
different things and I don't


769
00:23:37,876 --> 00:23:39,206
want to show all of them I only


770
00:23:39,376 --> 00:23:40,816
show the ones that I care about.


771
00:23:40,816 --> 00:23:42,576
So, what I'm doing is a little


772
00:23:42,576 --> 00:23:43,516
bit of filtering, I only take


773
00:23:43,516 --> 00:23:46,216
the top four and I only look at


774
00:23:46,216 --> 00:23:47,706
the ones that have a confidence


775
00:23:47,706 --> 00:23:49,676
of at least 30%, it just works


776
00:23:49,676 --> 00:23:50,946
well for my demo here, but you


777
00:23:50,946 --> 00:23:52,126
know you will figure out what


778
00:23:52,126 --> 00:23:53,356
kind of works well for your


779
00:23:53,356 --> 00:23:53,696
model.


780
00:23:54,036 --> 00:23:57,286
And all I have to do next is add


781
00:23:58,016 --> 00:24:01,136
my classification request into


782
00:24:01,136 --> 00:24:02,516
my area of request and now I


783
00:24:02,516 --> 00:24:05,346
will actually run two requests.


784
00:24:06,026 --> 00:24:07,416
So, I have this already loaded


785
00:24:07,416 --> 00:24:09,886
on my device, let's see how this


786
00:24:09,886 --> 00:24:11,196
actually looks.


787
00:24:12,156 --> 00:24:13,526
Of course, you will see it when


788
00:24:13,526 --> 00:24:14,826
I switch to the correct machine,


789
00:24:15,386 --> 00:24:15,686
there we go.


790
00:24:22,046 --> 00:24:25,056
Okay, so we have a coffee mug


791
00:24:25,456 --> 00:24:27,016
which is empty, somebody better


792
00:24:27,016 --> 00:24:27,966
fill that for me.


793
00:24:27,966 --> 00:24:33,066
We have a ballpoint pen, we have


794
00:24:33,066 --> 00:24:37,616
a padlock and look an iPod.


795
00:24:38,396 --> 00:24:44,076
Who has stolen those empty cards


796
00:24:44,076 --> 00:24:45,436
away and didn't realize it was


797
00:24:45,436 --> 00:24:45,976
an iPod?


798
00:24:46,516 --> 00:24:49,500
[ Applause ]


799
00:24:53,056 --> 00:24:54,676
All right, let's go back to the


800
00:24:54,676 --> 00:24:58,096
slides before we get to the next


801
00:24:58,096 --> 00:24:59,406
show-and-tell.


802
00:25:00,456 --> 00:25:01,846
For my next demo, I want to do


803
00:25:01,846 --> 00:25:02,976
something a little bit more


804
00:25:02,976 --> 00:25:06,516
elaborate and with that I chose


805
00:25:06,516 --> 00:25:07,246
something that's called


806
00:25:07,246 --> 00:25:08,006
MNISTVision.


807
00:25:09,486 --> 00:25:10,876
People in the machine learning


808
00:25:10,876 --> 00:25:12,296
community have already looked at


809
00:25:12,296 --> 00:25:13,376
that a little bit more.


810
00:25:13,566 --> 00:25:15,766
MNIST is a dataset where a bunch


811
00:25:15,766 --> 00:25:17,076
of government employees and high


812
00:25:17,076 --> 00:25:18,536
school students wrote numbers


813
00:25:18,596 --> 00:25:21,086
down and this was marked up and


814
00:25:21,146 --> 00:25:22,106
people were trained in our


815
00:25:22,106 --> 00:25:23,166
classifier on that.


816
00:25:23,796 --> 00:25:25,156
Note this is basically like


817
00:25:25,156 --> 00:25:26,326
white numbers on black


818
00:25:26,396 --> 00:25:28,336
background, so I guess they've


819
00:25:28,336 --> 00:25:30,626
written it with chalk on an old


820
00:25:30,666 --> 00:25:31,136
blackboard.


821
00:25:32,516 --> 00:25:34,346
So, in this sample code I'm


822
00:25:34,346 --> 00:25:35,846
going to show you I want to show


823
00:25:35,846 --> 00:25:37,356
a few concepts that are kind of


824
00:25:37,356 --> 00:25:38,646
important like making something


825
00:25:38,646 --> 00:25:40,606
a bit more elaborate with


826
00:25:41,276 --> 00:25:41,456
Vision.


827
00:25:41,606 --> 00:25:43,646
First, we'll spin off model


828
00:25:43,646 --> 00:25:45,006
requests based on top of each


829
00:25:45,006 --> 00:25:47,516
other then we use Core Image in


830
00:25:47,516 --> 00:25:49,566
between to do some image process


831
00:25:49,566 --> 00:25:51,666
and last but not least, we use


832
00:25:51,666 --> 00:25:53,246
Core ML again for the machine


833
00:25:53,246 --> 00:25:53,716
learning part.


834
00:25:56,666 --> 00:25:57,686
So, how is this going to work?


835
00:25:58,926 --> 00:26:00,596
We have here an image on which


836
00:26:00,596 --> 00:26:01,506
we find a sticky note.


837
00:26:02,106 --> 00:26:04,846
Well we find it by using the


838
00:26:04,846 --> 00:26:06,796
rectangle detector, there's our


839
00:26:06,796 --> 00:26:07,326
sticky note.


840
00:26:07,896 --> 00:26:09,226
Now that is prospectively


841
00:26:09,226 --> 00:26:10,866
distorted and it's clearly not


842
00:26:10,926 --> 00:26:12,306
white text on black background.


843
00:26:13,456 --> 00:26:14,846
So, we use Core Image in the


844
00:26:14,846 --> 00:26:16,686
next step and we'll actually do


845
00:26:16,686 --> 00:26:18,046
the perspective correction of it


846
00:26:18,686 --> 00:26:20,666
and invert the color and enhance


847
00:26:20,666 --> 00:26:22,086
also the contrast so that we get


848
00:26:22,086 --> 00:26:23,356
rid this black-and-white image.


849
00:26:24,896 --> 00:26:26,076
And last but not least, I need


850
00:26:26,076 --> 00:26:28,826
to run my MNIST classifier on it


851
00:26:29,016 --> 00:26:30,476
and it should tell me that this


852
00:26:30,476 --> 00:26:32,256
is the number four and this has


853
00:26:32,256 --> 00:26:35,096
80% confidence that this is the


854
00:26:35,096 --> 00:26:35,636
number four.


855
00:26:35,636 --> 00:26:38,666
Again, let's see how this looks


856
00:26:38,666 --> 00:26:39,066
in the app.


857
00:26:41,496 --> 00:26:43,426
So again, I start off as a


858
00:26:43,426 --> 00:26:45,236
rectangle detector request, it's


859
00:26:45,236 --> 00:26:46,316
my favorite I know.


860
00:26:47,886 --> 00:26:48,946
But it's more interesting what


861
00:26:48,946 --> 00:26:50,246
I'm going to do in the


862
00:26:50,246 --> 00:26:51,206
completion handler.


863
00:26:52,426 --> 00:26:53,746
So, I do some validation just to


864
00:26:53,746 --> 00:26:54,876
make sure that the rectangles


865
00:26:54,876 --> 00:26:55,746
I'm getting out of it are


866
00:26:55,746 --> 00:26:56,456
actually okay.


867
00:26:56,456 --> 00:26:58,656
But the interesting part happens


868
00:26:58,736 --> 00:26:58,926
here.


869
00:26:59,596 --> 00:27:01,256
I get the coordinates of the


870
00:27:01,256 --> 00:27:04,126
corners and feed them into CI to


871
00:27:04,126 --> 00:27:05,786
use the CIPerspectiveCorrection.


872
00:27:06,476 --> 00:27:07,636
That allows me to take this


873
00:27:07,636 --> 00:27:09,226
prospectively distorted image


874
00:27:09,226 --> 00:27:10,916
and actually bring it upright as


875
00:27:10,916 --> 00:27:11,966
if I would have the camera


876
00:27:11,966 --> 00:27:12,506
straight on.


877
00:27:14,076 --> 00:27:15,936
I use the CIColorControls to


878
00:27:16,186 --> 00:27:17,456
really bring out the contrast of


879
00:27:17,506 --> 00:27:19,066
the image to make it kind of


880
00:27:19,066 --> 00:27:19,546
binarized.


881
00:27:20,896 --> 00:27:22,246
And as I said, I have to color


882
00:27:22,246 --> 00:27:23,306
invert it.


883
00:27:24,976 --> 00:27:26,776
Now the resulting image of that


884
00:27:26,776 --> 00:27:28,116
I feed into a new request in


885
00:27:28,116 --> 00:27:28,936
there because we have a new


886
00:27:28,936 --> 00:27:31,146
image on which I'll run the


887
00:27:31,146 --> 00:27:32,086
classification.


888
00:27:32,296 --> 00:27:34,066
So, how does the classification


889
00:27:34,066 --> 00:27:34,416
look like?


890
00:27:35,906 --> 00:27:37,446
The classification for that I


891
00:27:37,446 --> 00:27:39,446
use my endless model which I


892
00:27:39,446 --> 00:27:41,776
actually have ready and this is


893
00:27:41,776 --> 00:27:43,046
a small model that I've really


894
00:27:43,046 --> 00:27:44,866
trained actually on this laptop


895
00:27:44,866 --> 00:27:45,996
very easily give us a few lines


896
00:27:45,996 --> 00:27:47,576
of code script and then thanks


897
00:27:47,576 --> 00:27:49,386
to Core ML I can just drag that


898
00:27:49,386 --> 00:27:50,706
in and use this very easily.


899
00:27:51,106 --> 00:27:52,316
So, I have my model here.


900
00:27:53,766 --> 00:27:55,046
Now again, this one part I would


901
00:27:55,046 --> 00:27:57,636
like to highlight this, so that


902
00:27:57,636 --> 00:27:59,436
takes in this case a very small


903
00:27:59,436 --> 00:28:00,166
grayscale image.


904
00:28:00,166 --> 00:28:02,636
So, an image 28 by 28 pixels it


905
00:28:02,636 --> 00:28:03,636
should be able to read these


906
00:28:03,636 --> 00:28:03,976
numbers.


907
00:28:12,056 --> 00:28:12,526
So that's where my


908
00:28:12,526 --> 00:28:14,716
classification is coming from


909
00:28:15,326 --> 00:28:16,906
and now I need to feed in the


910
00:28:16,906 --> 00:28:17,216
image.


911
00:28:17,216 --> 00:28:19,056
So, this sample code has been


912
00:28:19,056 --> 00:28:20,046
made available also for the


913
00:28:20,046 --> 00:28:21,506
session to make it easy also to


914
00:28:21,506 --> 00:28:22,716
run a simulator not running it


915
00:28:22,716 --> 00:28:24,266
live off of the camera I'm just


916
00:28:24,266 --> 00:28:25,196
going to use actually the


917
00:28:25,196 --> 00:28:29,046
UIImagePicker and feed it into


918
00:28:29,086 --> 00:28:32,306
my VMImageRequestHandler and let


919
00:28:32,306 --> 00:28:33,526
it just perform the rectangle


920
00:28:33,526 --> 00:28:33,986
request.


921
00:28:33,986 --> 00:28:37,136
Now notice I buried the request


922
00:28:37,136 --> 00:28:38,906
for the classification into the


923
00:28:38,906 --> 00:28:40,396
completion handler of my


924
00:28:40,396 --> 00:28:42,146
rectangle detection and that


925
00:28:42,146 --> 00:28:43,496
allows us to basically cascade


926
00:28:43,496 --> 00:28:44,966
multiple requests on top of each


927
00:28:44,966 --> 00:28:45,146
other.


928
00:28:46,096 --> 00:28:47,546
So, let's try the demo for this.


929
00:28:50,896 --> 00:28:55,016
Okay, so I have my app here and


930
00:28:55,196 --> 00:28:56,656
well [inaudible] giveaway.


931
00:28:59,076 --> 00:29:00,696
Okay, so what I see here is


932
00:29:00,696 --> 00:29:01,976
again I have my image on the


933
00:29:01,976 --> 00:29:03,436
top, this was actually the photo


934
00:29:03,436 --> 00:29:04,346
that I took earlier.


935
00:29:04,926 --> 00:29:07,476
We see its correctly classifying


936
00:29:07,476 --> 00:29:10,366
as a number one, it was a really


937
00:29:10,366 --> 00:29:11,556
high confidence in this case.


938
00:29:11,556 --> 00:29:12,646
And what you see on the bottom


939
00:29:12,646 --> 00:29:14,816
is just basically just to


940
00:29:14,816 --> 00:29:16,126
visualize that I took this


941
00:29:16,126 --> 00:29:17,296
intermediate image that we


942
00:29:17,296 --> 00:29:19,676
created in CI and show this as


943
00:29:19,676 --> 00:29:19,946
well.


944
00:29:19,946 --> 00:29:21,706
Let's choose another number.


945
00:29:23,066 --> 00:29:24,316
Yes, this is the number three.


946
00:29:26,466 --> 00:29:27,756
Can we guess what this number


947
00:29:27,846 --> 00:29:28,826
is, it's the number four?


948
00:29:29,626 --> 00:29:30,706
It works correctly.


949
00:29:32,576 --> 00:29:33,806
All right, thank you.


950
00:29:34,516 --> 00:29:37,556
[ Applause ]


951
00:29:38,056 --> 00:29:39,176
Let me go back to our slides.


952
00:29:41,486 --> 00:29:42,936
So that is our Vision framework.


953
00:29:43,986 --> 00:29:45,316
Let's capitalize a little bit on


954
00:29:45,316 --> 00:29:46,466
what we really have seen here.


955
00:29:47,486 --> 00:29:49,426
So, Vision is a high-level


956
00:29:49,426 --> 00:29:50,696
framework for Computer Vision


957
00:29:50,756 --> 00:29:51,816
and it should really make it


958
00:29:51,816 --> 00:29:53,756
easy for you to use this in your


959
00:29:53,756 --> 00:29:55,356
applications even if you're not


960
00:29:55,356 --> 00:29:56,576
a Computer Vision expert.


961
00:29:57,276 --> 00:29:58,756
We have various detectors and


962
00:29:58,756 --> 00:30:00,166
there's a whole variety of that


963
00:30:00,166 --> 00:30:01,946
and they all run through one


964
00:30:01,946 --> 00:30:03,296
consistent interface which would


965
00:30:03,336 --> 00:30:04,896
make it very easy to learn that


966
00:30:04,896 --> 00:30:05,446
set of APIs.


967
00:30:06,826 --> 00:30:08,596
And last but not least, the


968
00:30:08,596 --> 00:30:09,886
integration with Core ML.


969
00:30:10,376 --> 00:30:11,736
By bringing your own custom


970
00:30:11,736 --> 00:30:13,696
models you can do a lot in your


971
00:30:13,696 --> 00:30:15,586
application, you can find


972
00:30:15,586 --> 00:30:17,426
hotdogs and see if they are


973
00:30:17,426 --> 00:30:18,076
really hotdogs.


974
00:30:19,416 --> 00:30:20,336
I had to make that joke.


975
00:30:23,576 --> 00:30:24,606
So, if you want to learn more


976
00:30:24,606 --> 00:30:26,816
about our session, please go to


977
00:30:26,816 --> 00:30:28,626
our website and I would


978
00:30:28,626 --> 00:30:29,746
definitely highlight there are


979
00:30:29,796 --> 00:30:31,576
some related sessions that you


980
00:30:31,576 --> 00:30:33,266
should have watched perhaps in


981
00:30:33,266 --> 00:30:33,706
the past.


982
00:30:33,706 --> 00:30:34,786
I'll read you the Core ML one,


983
00:30:34,786 --> 00:30:35,876
but you can find it on our


984
00:30:35,876 --> 00:30:36,106
website.


985
00:30:36,106 --> 00:30:39,616
Please come for our get-together


986
00:30:39,616 --> 00:30:42,146
that we have at 6:30 today, chat


987
00:30:42,146 --> 00:30:43,416
about what we can do.


988
00:30:43,896 --> 00:30:45,536
And for the little bit more


989
00:30:45,536 --> 00:30:46,776
advanced part of Core ML there's


990
00:30:46,826 --> 00:30:48,886
a session on Thursday, as well


991
00:30:48,886 --> 00:30:50,256
as we have a session with Core


992
00:30:50,256 --> 00:30:51,486
Image where they will also do


993
00:30:51,486 --> 00:30:53,556
some very fancy stuff with Core


994
00:30:53,556 --> 00:30:54,246
Image and Vision.


995
00:30:55,826 --> 00:30:57,126
And with that I'd like to thank


996
00:30:57,126 --> 00:30:58,646
you for coming today and enjoy


997
00:30:58,646 --> 00:30:59,276
the rest of WWDC.


998
00:30:59,276 --> 00:30:59,456
Thank you.


999
00:31:00,516 --> 00:31:06,770
[ Applause ]

