1
00:00:06,516 --> 00:00:15,500
[ Music ]


2
00:00:21,516 --> 00:00:24,396
[ Applause ]


3
00:00:24,896 --> 00:00:25,196
>> Hello!


4
00:00:26,016 --> 00:00:28,000
[ Applause ]


5
00:00:31,236 --> 00:00:33,126
So welcome to the second session


6
00:00:33,286 --> 00:00:34,696
of Core ML.


7
00:00:34,696 --> 00:00:36,246
My name is Aseem, and I'm an


8
00:00:36,246 --> 00:00:39,806
engineer in the Core ML team.


9
00:00:40,656 --> 00:00:42,526
As you all know, Core ML is


10
00:00:42,526 --> 00:00:44,486
Apple's machine learning


11
00:00:44,486 --> 00:00:45,976
framework for on-device


12
00:00:45,976 --> 00:00:46,406
inference.


13
00:00:48,406 --> 00:00:49,906
And the one thing I really like


14
00:00:49,906 --> 00:00:51,416
about Core ML is that it's


15
00:00:51,546 --> 00:00:53,846
optimized on all Apple hardware.


16
00:00:55,176 --> 00:00:58,116
Over the last year, we have seen


17
00:00:58,406 --> 00:01:01,516
lots of amazing apps across all


18
00:01:01,516 --> 00:01:02,426
Apple platforms.


19
00:01:03,096 --> 00:01:04,366
So that's really exciting.


20
00:01:05,286 --> 00:01:07,026
And we are even more excited


21
00:01:07,586 --> 00:01:08,986
with the new features that we


22
00:01:08,986 --> 00:01:09,736
have this year.


23
00:01:10,656 --> 00:01:13,716
Now you can reduce the size of


24
00:01:13,796 --> 00:01:14,936
your app by a lot.


25
00:01:16,606 --> 00:01:18,376
You can make your app much


26
00:01:18,376 --> 00:01:20,246
faster by using the new


27
00:01:20,246 --> 00:01:21,286
batch-predict API.


28
00:01:23,066 --> 00:01:25,356
And you can really easily


29
00:01:25,356 --> 00:01:27,596
include cutting-edge research


30
00:01:27,796 --> 00:01:29,246
right in your app using


31
00:01:29,246 --> 00:01:29,966
customization.


32
00:01:31,376 --> 00:01:33,546
So that was a recap of the first


33
00:01:33,546 --> 00:01:33,926
session.


34
00:01:34,426 --> 00:01:36,146
And in case you missed it, I


35
00:01:36,146 --> 00:01:37,466
would highly encourage you to go


36
00:01:37,466 --> 00:01:38,936
back and check the slides.


37
00:01:39,716 --> 00:01:43,436
In this session, we are going to


38
00:01:43,436 --> 00:01:46,296
see how to actually make use of


39
00:01:46,296 --> 00:01:47,046
these features.


40
00:01:47,996 --> 00:01:50,046
More specifically, we'll walk


41
00:01:50,046 --> 00:01:52,166
through a few examples and show


42
00:01:52,166 --> 00:01:54,606
you that how in a few simple


43
00:01:54,606 --> 00:01:57,466
steps using Core ML Tools.


44
00:01:58,046 --> 00:01:59,486
You can reduce the size of the


45
00:01:59,486 --> 00:02:01,426
model, and you can include a


46
00:02:01,426 --> 00:02:02,846
custom feature in your model.


47
00:02:04,586 --> 00:02:05,566
Here's the agenda of the


48
00:02:05,566 --> 00:02:05,966
session.


49
00:02:07,076 --> 00:02:08,556
We'll start by a really quick


50
00:02:08,556 --> 00:02:10,166
update on the Core ML Tools


51
00:02:10,226 --> 00:02:10,816
ecosystem.


52
00:02:11,736 --> 00:02:13,586
And then we'll dive into a demo


53
00:02:13,876 --> 00:02:16,866
of our quantization and custom


54
00:02:16,866 --> 00:02:17,346
conversion.


55
00:02:18,026 --> 00:02:18,846
So let me start with the


56
00:02:18,846 --> 00:02:19,356
ecosystem.


57
00:02:23,276 --> 00:02:25,306
So how do you get an ML model?


58
00:02:25,746 --> 00:02:27,846
Well, the best thing is that if


59
00:02:27,846 --> 00:02:32,986
you, if you can, if you find it


60
00:02:32,986 --> 00:02:35,036
online, you just download it,


61
00:02:36,086 --> 00:02:36,406
right?


62
00:02:36,406 --> 00:02:38,616
Very good place to download your


63
00:02:38,676 --> 00:02:41,176
ML models is the Apple Machine


64
00:02:41,176 --> 00:02:42,366
Learning landing page.


65
00:02:42,726 --> 00:02:44,206
We have a few models there.


66
00:02:45,126 --> 00:02:46,836
Now let's say you want to train


67
00:02:46,836 --> 00:02:48,456
a model on your data set.


68
00:02:49,046 --> 00:02:51,746
In that case, you can use Create


69
00:02:52,476 --> 00:02:52,546
ML.


70
00:02:52,806 --> 00:02:55,336
This is a new framework that we


71
00:02:55,336 --> 00:02:56,976
have just launched this year,


72
00:02:57,436 --> 00:02:59,206
and you do not have to be a


73
00:02:59,206 --> 00:03:00,646
machine learning expert to use


74
00:03:00,646 --> 00:03:00,816
it.


75
00:03:01,326 --> 00:03:02,496
It's really easy to use.


76
00:03:02,556 --> 00:03:03,786
It's right there in Xcode.


77
00:03:04,446 --> 00:03:06,936
So go and give it a try.


78
00:03:08,476 --> 00:03:10,316
Now some of you are already


79
00:03:10,316 --> 00:03:12,326
familiar with the amazing


80
00:03:12,326 --> 00:03:13,416
machine learning tools that we


81
00:03:13,416 --> 00:03:15,516
have outside in the community.


82
00:03:16,026 --> 00:03:19,036
And for that, last year we had


83
00:03:19,106 --> 00:03:21,406
released Core ML Tools, a Python


84
00:03:21,406 --> 00:03:21,956
package.


85
00:03:22,546 --> 00:03:24,306
And along with that, we had


86
00:03:24,306 --> 00:03:26,786
released a few converters.


87
00:03:28,366 --> 00:03:30,036
Now there has been a lot of


88
00:03:30,036 --> 00:03:31,606
activity in this area over the


89
00:03:31,606 --> 00:03:32,236
last year.


90
00:03:32,956 --> 00:03:34,436
And this is how the picture


91
00:03:34,436 --> 00:03:34,976
looks now.


92
00:03:35,686 --> 00:03:38,936
So as you can see, there are


93
00:03:38,936 --> 00:03:42,416
many more converters out there.


94
00:03:42,906 --> 00:03:44,526
And you really do have a lot of


95
00:03:44,706 --> 00:03:46,306
choice to choose your training


96
00:03:46,306 --> 00:03:47,016
framework now.


97
00:03:48,336 --> 00:03:50,346
And all of these converters are


98
00:03:50,346 --> 00:03:52,546
built on top of Core ML Tools.


99
00:03:52,856 --> 00:03:57,326
Now, I do want to highlight a


100
00:03:57,326 --> 00:03:58,816
couple of different converters


101
00:03:58,816 --> 00:03:59,056
here.


102
00:04:01,186 --> 00:04:03,566
Last year, we collaborated with


103
00:04:03,566 --> 00:04:05,286
Google and released the


104
00:04:05,286 --> 00:04:06,096
TensorFlow converter.


105
00:04:06,096 --> 00:04:08,686
So that was exciting.


106
00:04:10,396 --> 00:04:12,416
As you know, TensorFlow is quite


107
00:04:12,416 --> 00:04:14,216
popular with researchers who try


108
00:04:14,216 --> 00:04:16,586
out new layers so we recently


109
00:04:16,586 --> 00:04:18,336
added support for custom layers


110
00:04:18,555 --> 00:04:19,396
into the converter.


111
00:04:20,776 --> 00:04:22,976
And TensorFlow recently released


112
00:04:22,976 --> 00:04:25,236
support for quantization during


113
00:04:25,376 --> 00:04:27,506
training and that's Core ML 2


114
00:04:27,506 --> 00:04:28,676
supports quantization.


115
00:04:29,086 --> 00:04:30,586
This feature will be added soon


116
00:04:30,586 --> 00:04:31,196
to the converter.


117
00:04:33,186 --> 00:04:34,616
Another exciting partnership we


118
00:04:34,616 --> 00:04:37,286
had was with Facebook and


119
00:04:37,326 --> 00:04:37,836
Prisma.


120
00:04:38,336 --> 00:04:40,406
And this resulted in the ONNX


121
00:04:40,406 --> 00:04:40,866
converter.


122
00:04:42,216 --> 00:04:43,646
The nice thing about ONNX is


123
00:04:43,646 --> 00:04:48,056
that now you have access to a


124
00:04:48,056 --> 00:04:49,376
bunch of different training


125
00:04:49,376 --> 00:04:50,946
libraries that can all be


126
00:04:50,946 --> 00:04:53,106
converted to Core ML using the


127
00:04:53,106 --> 00:04:54,136
new ONNX converter.


128
00:04:54,136 --> 00:04:58,956
So that was a quick wrap-up of


129
00:04:58,956 --> 00:05:00,446
Core ML Tools ecosystem.


130
00:05:00,866 --> 00:05:02,496
Now to talk about quantization,


131
00:05:02,656 --> 00:05:04,446
I would like to invite my friend


132
00:05:04,446 --> 00:05:05,476
Sohaib on stage.


133
00:05:06,516 --> 00:05:13,596
[ Applause ]


134
00:05:14,096 --> 00:05:14,876
>> Good morning, everyone.


135
00:05:15,186 --> 00:05:15,976
My name is Sohaib.


136
00:05:16,206 --> 00:05:17,426
I'm an engineer in the Core ML


137
00:05:17,426 --> 00:05:17,716
team.


138
00:05:18,106 --> 00:05:19,156
And today we're going to be


139
00:05:19,156 --> 00:05:20,256
taking a look at new


140
00:05:20,256 --> 00:05:21,766
quantization utilities in Core


141
00:05:21,766 --> 00:05:23,026
ML Tools 2.0.


142
00:05:28,136 --> 00:05:29,826
Core ML Tools 2.0 has support


143
00:05:29,826 --> 00:05:31,206
for the latest Core ML model


144
00:05:31,206 --> 00:05:32,366
format specification.


145
00:05:32,796 --> 00:05:34,306
It also has utilities which make


146
00:05:34,306 --> 00:05:36,166
it really easy for you to add


147
00:05:36,226 --> 00:05:38,016
flexible shapes and quantize in


148
00:05:38,016 --> 00:05:38,876
your own network machine


149
00:05:38,876 --> 00:05:39,486
learning models.


150
00:05:40,296 --> 00:05:41,416
Using these great new features


151
00:05:41,416 --> 00:05:43,046
in Core ML, you can not only


152
00:05:43,046 --> 00:05:44,266
reduce the size of your models.


153
00:05:44,576 --> 00:05:46,366
But also reduce the number of


154
00:05:46,366 --> 00:05:48,126
models in your app, reducing the


155
00:05:48,126 --> 00:05:49,536
footprint of your app.


156
00:05:50,356 --> 00:05:51,896
Now let's start off by taking a


157
00:05:51,896 --> 00:05:53,316
look at quantization.


158
00:05:54,846 --> 00:05:55,876
Core ML Tools supports


159
00:05:55,906 --> 00:05:57,166
post-training quantization.


160
00:05:57,666 --> 00:05:59,186
We start off with a Core ML


161
00:05:59,286 --> 00:06:00,976
neural network model which has


162
00:06:01,106 --> 00:06:03,336
32-bit float weight parameters.


163
00:06:03,766 --> 00:06:05,276
And we use Core ML Tools to


164
00:06:05,276 --> 00:06:07,136
quantize the weights for this


165
00:06:07,136 --> 00:06:07,466
model.


166
00:06:08,126 --> 00:06:09,556
The resulting model is smaller


167
00:06:09,556 --> 00:06:10,026
in size.


168
00:06:10,836 --> 00:06:12,346
Now size reduction of the model


169
00:06:12,346 --> 00:06:13,606
is directly dependent on the


170
00:06:13,606 --> 00:06:15,296
number of bits we quantize our


171
00:06:15,296 --> 00:06:15,886
model to.


172
00:06:17,836 --> 00:06:19,316
Now, many of us may be wondering


173
00:06:19,466 --> 00:06:21,046
what exactly is quantization?


174
00:06:21,336 --> 00:06:22,616
And how can it reduce the size


175
00:06:22,616 --> 00:06:23,116
of my models?


176
00:06:24,036 --> 00:06:25,786
Let's step back and take a peek


177
00:06:25,786 --> 00:06:26,246
under the hood.


178
00:06:30,386 --> 00:06:31,986
Neural networks are composed of


179
00:06:31,986 --> 00:06:32,336
layers.


180
00:06:32,786 --> 00:06:33,906
And these layers can be thought


181
00:06:33,906 --> 00:06:35,336
of as mathematical functions.


182
00:06:35,776 --> 00:06:37,356
And these mathematical functions


183
00:06:37,466 --> 00:06:38,876
have parameters called weights.


184
00:06:39,076 --> 00:06:41,116
And these weights are usually


185
00:06:41,116 --> 00:06:42,716
stored as 32-bit floats.


186
00:06:43,426 --> 00:06:45,886
Now in our previous session, we


187
00:06:45,886 --> 00:06:47,206
took a look at ResNet50.


188
00:06:47,636 --> 00:06:49,036
A popular machine-learning model


189
00:06:49,036 --> 00:06:50,116
which is used for image


190
00:06:50,116 --> 00:06:51,536
classification amongst other


191
00:06:51,536 --> 00:06:51,816
things.


192
00:06:52,476 --> 00:06:53,576
Now this particular model has


193
00:06:53,576 --> 00:06:55,696
over 25 million weight


194
00:06:55,696 --> 00:06:56,286
parameters.


195
00:06:56,636 --> 00:06:58,286
So you can imagine, if you could


196
00:06:58,286 --> 00:07:00,206
somehow represent these param --


197
00:07:00,516 --> 00:07:02,026
these parameters using a fewer


198
00:07:02,026 --> 00:07:03,456
number of bits, we can


199
00:07:03,456 --> 00:07:04,676
drastically reduce the size of


200
00:07:04,676 --> 00:07:05,096
this model.


201
00:07:05,776 --> 00:07:08,626
In fact, this process is called


202
00:07:08,906 --> 00:07:09,676
quantization.


203
00:07:10,476 --> 00:07:12,116
In quantization, we take the


204
00:07:12,116 --> 00:07:13,846
weights for our layers which


205
00:07:13,846 --> 00:07:15,086
[inaudible] to minimum and to


206
00:07:15,086 --> 00:07:18,816
maximum value and we map them to


207
00:07:18,816 --> 00:07:19,736
unsigned integers.


208
00:07:20,936 --> 00:07:22,706
Now for APIC quantization, we


209
00:07:22,706 --> 00:07:24,526
map these values from a range of


210
00:07:24,586 --> 00:07:26,686
0 to 55.


211
00:07:27,106 --> 00:07:28,876
For 7-bit quantization, we map


212
00:07:28,926 --> 00:07:31,826
them from 0 to 127, all the way


213
00:07:31,826 --> 00:07:32,556
down to 1 bit.


214
00:07:32,786 --> 00:07:34,076
Where we map these weights as


215
00:07:34,076 --> 00:07:35,486
either zeros or ones.


216
00:07:36,106 --> 00:07:37,606
Since we're using fewer bits to


217
00:07:37,606 --> 00:07:38,856
represent the same information,


218
00:07:39,176 --> 00:07:40,476
we reduce the size of our model.


219
00:07:42,406 --> 00:07:44,786
Great. Now many of you may have


220
00:07:44,786 --> 00:07:46,276
noticed that we're mapping


221
00:07:46,276 --> 00:07:47,246
floats to integers.


222
00:07:47,646 --> 00:07:48,926
And you may have come to the


223
00:07:48,926 --> 00:07:50,046
conclusion that maybe there's


224
00:07:50,046 --> 00:07:52,366
some accuracy loss in this


225
00:07:52,366 --> 00:07:52,786
mapping.


226
00:07:53,326 --> 00:07:53,836
That's true.


227
00:07:54,736 --> 00:07:56,316
The rule of thumb is the lower


228
00:07:56,316 --> 00:07:57,416
the number of bits you quantize


229
00:07:57,416 --> 00:07:59,226
your model to, the more of a hit


230
00:07:59,226 --> 00:08:00,346
our model takes in terms of


231
00:08:00,346 --> 00:08:00,786
accuracy.


232
00:08:00,786 --> 00:08:02,206
And we'll get back to that in a


233
00:08:02,206 --> 00:08:02,436
bit.


234
00:08:03,556 --> 00:08:04,716
So that's an overview of


235
00:08:04,716 --> 00:08:05,326
quantization.


236
00:08:06,016 --> 00:08:06,936
But the question remains.


237
00:08:07,246 --> 00:08:08,716
How do we obtain this mapping?


238
00:08:09,506 --> 00:08:10,796
Well, there are many popular


239
00:08:10,796 --> 00:08:12,386
algorithms and techniques out


240
00:08:12,386 --> 00:08:13,346
there which help you to do this.


241
00:08:13,716 --> 00:08:15,536
And Core ML supports two of the


242
00:08:15,536 --> 00:08:17,456
most popular ones: linear


243
00:08:17,456 --> 00:08:19,026
quantization and lookup table


244
00:08:19,026 --> 00:08:19,706
quantization.


245
00:08:20,626 --> 00:08:21,986
Let's have a brief overview.


246
00:08:26,276 --> 00:08:27,566
Linear quantization is an


247
00:08:27,566 --> 00:08:29,116
algorithm in which you map these


248
00:08:29,156 --> 00:08:31,596
full parameters equally.


249
00:08:32,946 --> 00:08:34,736
The quantization is parametrized


250
00:08:34,736 --> 00:08:37,106
by a scale and by values.


251
00:08:37,106 --> 00:08:38,775
And these values are calculated


252
00:08:38,885 --> 00:08:40,416
based on the parameters of the


253
00:08:40,416 --> 00:08:42,405
layers that we're quantizing.


254
00:08:43,086 --> 00:08:44,866
Now and a really intuitive way


255
00:08:44,926 --> 00:08:46,946
to see how this mapping works is


256
00:08:46,946 --> 00:08:47,896
if we take a step back.


257
00:08:47,896 --> 00:08:49,356
And see how we would go back


258
00:08:49,386 --> 00:08:50,756
from our quantized weights which


259
00:08:50,756 --> 00:08:52,236
are at the bottom back to our


260
00:08:52,236 --> 00:08:53,106
original float weights.


261
00:08:53,876 --> 00:08:55,416
In linear quantization, we would


262
00:08:55,416 --> 00:08:57,276
simply multiply our quantized


263
00:08:57,276 --> 00:08:58,846
weights with the scale parameter


264
00:08:59,196 --> 00:08:59,886
and add the bias.


265
00:09:02,186 --> 00:09:03,346
The second quantization


266
00:09:03,346 --> 00:09:04,726
technique that Core ML supports


267
00:09:05,046 --> 00:09:06,356
is lookup table quantization.


268
00:09:07,226 --> 00:09:08,456
And this technique is exactly


269
00:09:08,456 --> 00:09:09,166
what it sounds like.


270
00:09:09,716 --> 00:09:10,816
We construct a lookup table.


271
00:09:11,986 --> 00:09:13,396
Now again it's helpful if we


272
00:09:13,396 --> 00:09:14,596
imagine how we would go back


273
00:09:14,596 --> 00:09:15,946
from our quantized weights back


274
00:09:15,946 --> 00:09:16,676
to our original weights.


275
00:09:17,016 --> 00:09:19,056
And in this case, the quantized


276
00:09:19,056 --> 00:09:21,006
weights are simply indices back


277
00:09:21,006 --> 00:09:22,746
into our lookup table.


278
00:09:24,036 --> 00:09:25,326
Now, if you notice, unlike


279
00:09:25,326 --> 00:09:26,546
linear quantization, we have the


280
00:09:26,546 --> 00:09:28,986
ability to move our quantized


281
00:09:28,986 --> 00:09:29,536
weights around.


282
00:09:29,696 --> 00:09:30,956
They don't have to be spaced out


283
00:09:30,956 --> 00:09:31,866
in a linear fashion.


284
00:09:33,906 --> 00:09:35,806
So to recap, Core ML Tools


285
00:09:35,806 --> 00:09:38,746
supports linear quantization and


286
00:09:38,746 --> 00:09:40,126
lookup table quantization where


287
00:09:40,126 --> 00:09:41,266
we start off with a full


288
00:09:41,266 --> 00:09:42,576
precision neural network model.


289
00:09:42,936 --> 00:09:43,906
And quantize the weights for


290
00:09:43,906 --> 00:09:45,396
that model using the utilities.


291
00:09:46,226 --> 00:09:48,276
Now you may be wondering well


292
00:09:48,276 --> 00:09:49,646
great, I can reduce the size of


293
00:09:49,646 --> 00:09:50,106
my model.


294
00:09:50,836 --> 00:09:52,326
But how do I figure out the


295
00:09:52,466 --> 00:09:53,956
parameters for my quantization?


296
00:09:54,436 --> 00:09:55,296
If I'm doing linear


297
00:09:55,296 --> 00:09:56,706
quantization, how do I figure


298
00:09:56,706 --> 00:09:57,676
out my scale and bias?


299
00:09:58,366 --> 00:09:59,336
If I'm doing lookup table


300
00:09:59,336 --> 00:10:01,326
quantization, how do I construct


301
00:10:01,326 --> 00:10:02,036
my lookup table?


302
00:10:03,076 --> 00:10:04,426
I'm here to tell you that you


303
00:10:04,426 --> 00:10:05,946
don't have to worry about any of


304
00:10:05,986 --> 00:10:06,236
that.


305
00:10:06,986 --> 00:10:08,536
All you do is decide on the


306
00:10:08,536 --> 00:10:09,546
number of bits you want to


307
00:10:09,546 --> 00:10:10,596
quantize your model to.


308
00:10:10,876 --> 00:10:11,926
And decide on the algorithm you


309
00:10:11,926 --> 00:10:13,706
want to use, and let Core ML


310
00:10:13,706 --> 00:10:15,166
Tools do the rest.


311
00:10:16,106 --> 00:10:16,576
In fact --


312
00:10:17,516 --> 00:10:22,216
[ Applause ]


313
00:10:22,716 --> 00:10:24,756
In fact, it's so simple to take


314
00:10:24,756 --> 00:10:26,186
a Core ML neural network model.


315
00:10:26,286 --> 00:10:27,036
And quantize it.


316
00:10:27,446 --> 00:10:29,116
Then we can do it in a few lines


317
00:10:29,116 --> 00:10:29,806
of Python code.


318
00:10:29,996 --> 00:10:31,286
But why stand here and talk


319
00:10:31,286 --> 00:10:32,396
about it when we can show you a


320
00:10:32,396 --> 00:10:32,676
demo?


321
00:10:40,406 --> 00:10:42,296
So for the purposes of this


322
00:10:42,296 --> 00:10:44,346
demo, I'm going to need a neural


323
00:10:44,346 --> 00:10:45,596
network in the Core ML model


324
00:10:45,596 --> 00:10:46,026
format.


325
00:10:46,706 --> 00:10:47,846
Now, as my colleague Aseem


326
00:10:47,846 --> 00:10:49,376
mentioned, a great place to find


327
00:10:49,376 --> 00:10:50,686
these models is on the Core ML


328
00:10:50,686 --> 00:10:51,716
machine learning home page.


329
00:10:52,056 --> 00:10:52,896
And I've gone ahead and


330
00:10:52,896 --> 00:10:54,016
downloaded one of the models


331
00:10:54,106 --> 00:10:54,846
from that page.


332
00:10:55,306 --> 00:10:56,706
So this model's called


333
00:10:56,706 --> 00:10:57,176
SqueezeNet.


334
00:10:57,176 --> 00:10:58,146
And let's go ahead and open it


335
00:10:58,146 --> 00:10:58,296
up.


336
00:10:59,736 --> 00:11:02,356
As we can see, this model is 5


337
00:11:02,356 --> 00:11:03,226
megabytes in size.


338
00:11:03,616 --> 00:11:05,986
It has a input which is an image


339
00:11:06,036 --> 00:11:08,456
of 227 by 227 pixels.


340
00:11:08,676 --> 00:11:09,796
And it has two outputs.


341
00:11:10,476 --> 00:11:11,776
One of the outputs is the class


342
00:11:11,906 --> 00:11:13,696
label which is a string, and


343
00:11:13,696 --> 00:11:15,996
this is the most likely label


344
00:11:15,996 --> 00:11:17,416
for the, for the input image.


345
00:11:17,736 --> 00:11:19,386
And the second output is a


346
00:11:19,386 --> 00:11:20,526
mapping of strings to


347
00:11:20,526 --> 00:11:23,386
probabilities given that if we


348
00:11:23,386 --> 00:11:24,756
pass an image, it's going to be


349
00:11:24,936 --> 00:11:25,976
a list of probabilities of what


350
00:11:25,976 --> 00:11:26,666
that image may be.


351
00:11:28,826 --> 00:11:30,126
Now let's start quantizing this


352
00:11:30,126 --> 00:11:30,486
model.


353
00:11:30,736 --> 00:11:32,696
So the first thing I want to do


354
00:11:32,696 --> 00:11:34,066
is I want to get into a Python


355
00:11:34,066 --> 00:11:34,556
environment.


356
00:11:34,986 --> 00:11:36,406
Now a Jupyter Notebook is one


357
00:11:36,406 --> 00:11:37,206
such environment that I'm


358
00:11:37,206 --> 00:11:37,776
comfortable with.


359
00:11:38,156 --> 00:11:39,086
So I'm going to go ahead and


360
00:11:39,086 --> 00:11:39,526
open that up.


361
00:11:46,696 --> 00:11:49,346
Let's open up a new notebook and


362
00:11:49,346 --> 00:11:50,496
zoom in on that.


363
00:11:51,346 --> 00:11:53,476
Alright. So let's start off by


364
00:11:53,476 --> 00:11:54,926
importing Core ML Tools.


365
00:11:58,476 --> 00:11:59,536
Let's run that.


366
00:11:59,536 --> 00:12:01,186
Now the second thing I want to


367
00:12:01,186 --> 00:12:02,576
do is I want to import all the


368
00:12:02,576 --> 00:12:03,906
new quantization utilities that


369
00:12:03,906 --> 00:12:04,966
we have in Core ML Tools.


370
00:12:05,026 --> 00:12:06,976
And we do that by running this.


371
00:12:16,206 --> 00:12:17,516
And now we need to load up the


372
00:12:17,516 --> 00:12:18,866
model which we want to quantize.


373
00:12:19,186 --> 00:12:20,376
And we just saw the SqueezeNet


374
00:12:20,376 --> 00:12:21,126
model a minute okay.


375
00:12:21,126 --> 00:12:22,106
We're going to go ahead and get


376
00:12:22,106 --> 00:12:22,976
an instance of that model.


377
00:12:32,056 --> 00:12:37,716
Send this to my desktop.


378
00:12:38,086 --> 00:12:40,416
Great. Now to quantize this


379
00:12:40,416 --> 00:12:41,996
model, we just need to make one


380
00:12:41,996 --> 00:12:43,056
simple API call.


381
00:12:43,516 --> 00:12:44,666
And let's try a linear,


382
00:12:44,666 --> 00:12:46,166
quantizing this model using


383
00:12:46,166 --> 00:12:47,256
linear quantization.


384
00:12:51,176 --> 00:12:53,166
And its API is simply called


385
00:12:53,456 --> 00:12:54,346
quantize weights.


386
00:12:54,816 --> 00:12:56,216
And the first parameter we pass


387
00:12:56,216 --> 00:12:58,236
in is the original model which


388
00:12:58,236 --> 00:12:59,396
you just loaded up.


389
00:12:59,576 --> 00:13:00,996
The number of bits we want to


390
00:13:00,996 --> 00:13:02,066
quantize our model to.


391
00:13:02,226 --> 00:13:03,536
In this case, it's 8 bits.


392
00:13:04,716 --> 00:13:05,996
And the quantization algorithm


393
00:13:05,996 --> 00:13:06,666
we want to use.


394
00:13:07,066 --> 00:13:08,366
Let's try linear quantization.


395
00:13:09,866 --> 00:13:11,436
Now what's happening is that the


396
00:13:11,436 --> 00:13:12,996
utility is iterating over all of


397
00:13:12,996 --> 00:13:14,586
the layers of the linear


398
00:13:14,586 --> 00:13:15,126
networks.


399
00:13:15,126 --> 00:13:16,876
And is quantizing all the


400
00:13:16,876 --> 00:13:17,726
weights in those layers.


401
00:13:18,186 --> 00:13:18,756
And we're finished.


402
00:13:20,496 --> 00:13:23,246
Now, if you recall a few moments


403
00:13:23,246 --> 00:13:24,806
ago I mentioned that quantizing


404
00:13:24,806 --> 00:13:26,676
our model had an associated loss


405
00:13:26,676 --> 00:13:27,276
in accuracy.


406
00:13:27,586 --> 00:13:28,966
So we want to know how our


407
00:13:28,966 --> 00:13:30,746
quantized model stacks up to the


408
00:13:30,746 --> 00:13:31,286
original model.


409
00:13:31,476 --> 00:13:33,236
And the easiest way of doing


410
00:13:33,236 --> 00:13:34,706
this is taking some data,


411
00:13:35,236 --> 00:13:37,936
passing and getting inference on


412
00:13:37,936 --> 00:13:39,556
that data using our original


413
00:13:39,556 --> 00:13:39,916
model.


414
00:13:40,596 --> 00:13:42,296
And doing the same inference on


415
00:13:42,296 --> 00:13:43,406
the same data using our


416
00:13:43,406 --> 00:13:45,206
quantized model and comparing


417
00:13:45,206 --> 00:13:46,416
the predictions from that model.


418
00:13:47,146 --> 00:13:48,436
And seeing how well they agree.


419
00:13:49,066 --> 00:13:50,296
Core ML Tools has utilities


420
00:13:50,296 --> 00:13:51,226
which help you to do that.


421
00:13:51,486 --> 00:13:53,096
And we can do that by making


422
00:13:53,096 --> 00:13:54,176
this call which is called


423
00:13:54,176 --> 00:13:55,596
compare models.


424
00:13:56,236 --> 00:13:57,616
We pass in our full precision


425
00:13:57,616 --> 00:13:59,796
model, and we pass in our model


426
00:13:59,796 --> 00:14:01,136
which we had just quantized.


427
00:14:01,956 --> 00:14:03,126
And because this model is a


428
00:14:03,126 --> 00:14:05,976
simple image classifier which it


429
00:14:05,976 --> 00:14:07,386
only has one image inputs.


430
00:14:08,136 --> 00:14:09,026
We, we have a convenience


431
00:14:09,026 --> 00:14:09,426
utility.


432
00:14:09,426 --> 00:14:11,296
So we can just pass in a folder


433
00:14:11,296 --> 00:14:12,766
containing sample data images.


434
00:14:13,146 --> 00:14:14,746
Now on my desktop here, I have a


435
00:14:14,746 --> 00:14:16,216
folder with a set of images


436
00:14:16,216 --> 00:14:17,156
which are relevant for my


437
00:14:17,156 --> 00:14:17,816
application.


438
00:14:18,146 --> 00:14:18,966
So I'm going to go ahead and


439
00:14:18,966 --> 00:14:22,186
pass a path to this folder as my


440
00:14:22,186 --> 00:14:22,816
[inaudible] parameter.


441
00:14:28,066 --> 00:14:29,776
Great. So now we see we're


442
00:14:29,776 --> 00:14:31,306
analyzing all the images in that


443
00:14:31,306 --> 00:14:31,736
folder.


444
00:14:31,766 --> 00:14:33,796
We're running inference on the,


445
00:14:33,796 --> 00:14:35,546
we're using full prediction or


446
00:14:35,546 --> 00:14:36,486
full precision model.


447
00:14:36,576 --> 00:14:37,676
And we're running inference on


448
00:14:37,676 --> 00:14:38,606
our quantized model.


449
00:14:38,636 --> 00:14:39,616
And we're comparing our two


450
00:14:39,616 --> 00:14:40,176
predictions.


451
00:14:41,436 --> 00:14:42,376
So we seem to have finished


452
00:14:42,376 --> 00:14:42,616
that.


453
00:14:42,616 --> 00:14:45,066
And you can see our Top 1


454
00:14:45,066 --> 00:14:46,946
Agreement is 94.8%.


455
00:14:47,766 --> 00:14:50,186
Not bad. Now what does this Top


456
00:14:50,186 --> 00:14:50,996
1 Agreement mean?


457
00:14:51,586 --> 00:14:52,996
This means that when I pass in


458
00:14:52,996 --> 00:14:55,896
my original model, that image of


459
00:14:55,896 --> 00:14:57,546
a dog for example, and it


460
00:14:57,546 --> 00:14:58,576
predicted that this image was a


461
00:14:58,576 --> 00:14:58,916
dog.


462
00:14:59,226 --> 00:15:00,766
My quantized model did the same.


463
00:15:00,766 --> 00:15:03,476
And that happened over 98, 94.8%


464
00:15:03,476 --> 00:15:04,126
of the data set.


465
00:15:05,846 --> 00:15:07,576
So I can go ahead and use this


466
00:15:07,576 --> 00:15:08,276
model in my app.


467
00:15:08,746 --> 00:15:10,416
But I want to see if other


468
00:15:10,416 --> 00:15:11,576
quantization techniques work


469
00:15:11,576 --> 00:15:12,546
better on this model.


470
00:15:13,526 --> 00:15:15,146
As I mentioned, Core ML supports


471
00:15:15,176 --> 00:15:16,496
two types of quantization


472
00:15:16,496 --> 00:15:16,936
techniques.


473
00:15:17,096 --> 00:15:18,686
Linear quantization and lookup


474
00:15:18,686 --> 00:15:19,546
table quantization.


475
00:15:19,956 --> 00:15:21,636
So let's go ahead and try and


476
00:15:21,636 --> 00:15:23,326
quantize this model using lookup


477
00:15:23,326 --> 00:15:24,136
table quantization.


478
00:15:30,936 --> 00:15:31,996
Again, we pass in an original


479
00:15:31,996 --> 00:15:33,496
model, the number of bits we


480
00:15:33,496 --> 00:15:34,726
want to quantize our model to.


481
00:15:35,666 --> 00:15:37,306
And our quantization techniques.


482
00:15:37,896 --> 00:15:39,976
Oops, made a typo there.


483
00:15:48,156 --> 00:15:49,816
Let's go ahead and run this.


484
00:15:50,656 --> 00:15:52,746
Now, k-means is a simple


485
00:15:52,746 --> 00:15:54,136
clustering algorithm which


486
00:15:54,196 --> 00:15:55,856
approximates the distribution of


487
00:15:55,856 --> 00:15:56,346
our weights.


488
00:15:56,606 --> 00:15:58,566
And using this distribution, we


489
00:15:58,566 --> 00:16:00,606
can construct the lookup table


490
00:16:00,806 --> 00:16:02,046
for our weights.


491
00:16:02,546 --> 00:16:04,116
And what we're doing over here


492
00:16:04,116 --> 00:16:05,866
is that we're iterating over all


493
00:16:05,866 --> 00:16:06,556
the layers in the neural


494
00:16:06,556 --> 00:16:06,916
network.


495
00:16:07,306 --> 00:16:08,496
And we're quantizing and we're


496
00:16:08,496 --> 00:16:09,586
figuring out the lookup table


497
00:16:09,936 --> 00:16:11,426
for that particular layer.


498
00:16:12,226 --> 00:16:14,626
Now, if you're an expert and you


499
00:16:14,626 --> 00:16:15,836
know that your model, you know


500
00:16:15,836 --> 00:16:17,586
your model architecture and you


501
00:16:17,586 --> 00:16:18,826
know that k-means is not the


502
00:16:18,826 --> 00:16:20,426
algorithm for you, you have the


503
00:16:20,426 --> 00:16:22,456
flexibility of passing in your


504
00:16:22,456 --> 00:16:24,996
own custom function instead of


505
00:16:24,996 --> 00:16:26,456
this algorithm and the utility


506
00:16:26,456 --> 00:16:28,076
will use your custom function to


507
00:16:28,076 --> 00:16:29,126
actually construct the lookup


508
00:16:29,126 --> 00:16:29,436
table.


509
00:16:30,866 --> 00:16:32,686
So we finished quantizing this


510
00:16:32,686 --> 00:16:34,036
model again using the lookup


511
00:16:34,036 --> 00:16:34,646
table approach.


512
00:16:35,066 --> 00:16:36,596
And now let's see how well this


513
00:16:36,596 --> 00:16:38,006
model compares with our original


514
00:16:38,006 --> 00:16:38,306
model.


515
00:16:38,626 --> 00:16:40,016
So once again we call our


516
00:16:40,016 --> 00:16:41,286
compare model's API.


517
00:16:42,276 --> 00:16:43,766
We pass in our original model


518
00:16:43,766 --> 00:16:46,036
and we pass in our lookup table


519
00:16:46,036 --> 00:16:46,366
model.


520
00:16:47,036 --> 00:16:51,036
And again we pass in our sample


521
00:16:51,036 --> 00:16:51,656
data folder.


522
00:16:52,266 --> 00:16:56,406
Again, we run inference over all


523
00:16:56,406 --> 00:16:58,616
the images using both the


524
00:16:59,596 --> 00:17:00,866
original model and the quantized


525
00:17:00,866 --> 00:17:01,166
model.


526
00:17:01,686 --> 00:17:02,936
And we see this time we're


527
00:17:02,936 --> 00:17:04,286
getting a much better, little


528
00:17:04,286 --> 00:17:05,726
bit better Top 1 Agreement.


529
00:17:06,256 --> 00:17:08,425
Now for this model, we see that


530
00:17:08,425 --> 00:17:09,526
lookup table was the right way


531
00:17:09,526 --> 00:17:09,856
to go.


532
00:17:10,386 --> 00:17:11,016
But again, this is


533
00:17:11,016 --> 00:17:12,226
model-dependent and for other


534
00:17:12,226 --> 00:17:13,886
models, linear may be the way.


535
00:17:14,136 --> 00:17:15,705
So now that we're happy with


536
00:17:15,705 --> 00:17:17,536
this and we see that this is


537
00:17:17,685 --> 00:17:18,566
good enough for at least my


538
00:17:18,566 --> 00:17:19,836
application, let's go ahead and


539
00:17:19,836 --> 00:17:21,406
save this model out.


540
00:17:22,695 --> 00:17:24,925
We do that by causing or calling


541
00:17:24,925 --> 00:17:25,276
save.


542
00:17:31,076 --> 00:17:31,616
I'm going to give it the


543
00:17:31,616 --> 00:17:33,576
creative name of Quantized


544
00:17:33,656 --> 00:17:34,186
SqueezeNet.


545
00:17:41,076 --> 00:17:41,746
And there we go.


546
00:17:42,236 --> 00:17:43,256
We have a quantized model.


547
00:17:43,816 --> 00:17:45,616
So this was an original model.


548
00:17:45,616 --> 00:17:46,806
And we saw that it was 5


549
00:17:46,806 --> 00:17:47,626
megabytes in size.


550
00:17:48,226 --> 00:17:49,726
Let's open up our quantized


551
00:17:49,726 --> 00:17:50,036
model.


552
00:17:50,036 --> 00:17:53,646
And the first thing we notice


553
00:17:53,646 --> 00:17:54,826
right off the bat is that this


554
00:17:54,826 --> 00:17:57,206
model is only 1.3 megabytes in


555
00:17:57,206 --> 00:17:57,556
size.


556
00:17:58,516 --> 00:18:03,636
[ Applause ]


557
00:18:04,136 --> 00:18:06,306
So if you notice, all the


558
00:18:06,306 --> 00:18:07,696
details about, about our


559
00:18:07,696 --> 00:18:10,196
quantized model are the same as


560
00:18:10,196 --> 00:18:10,866
the original model.


561
00:18:11,226 --> 00:18:12,306
It still takes in an image


562
00:18:12,306 --> 00:18:14,066
input, and it still has two


563
00:18:14,066 --> 00:18:14,566
outputs.


564
00:18:15,216 --> 00:18:17,046
Now, if I had an app using this


565
00:18:17,046 --> 00:18:18,616
model, what I could do as we saw


566
00:18:18,616 --> 00:18:19,476
in the previous demo.


567
00:18:20,056 --> 00:18:21,256
Is we could just drag this


568
00:18:21,256 --> 00:18:22,766
quantized model into our app and


569
00:18:22,766 --> 00:18:23,876
start using that instead.


570
00:18:23,876 --> 00:18:25,456
And just like that, we reduce


571
00:18:25,456 --> 00:18:25,956
the size of our app.


572
00:18:32,386 --> 00:18:34,036
So that was quantization using


573
00:18:34,036 --> 00:18:34,906
Core ML Tools.


574
00:18:38,696 --> 00:18:40,996
To recap, we saw how easy it was


575
00:18:41,096 --> 00:18:43,246
to use Core ML Tools to quantize


576
00:18:43,246 --> 00:18:43,676
our model.


577
00:18:44,356 --> 00:18:46,466
Using a simple API, we provided


578
00:18:47,406 --> 00:18:49,176
our original model, the number


579
00:18:49,176 --> 00:18:50,446
of bits we wanted to quantize


580
00:18:50,446 --> 00:18:51,746
our model to, and the


581
00:18:51,746 --> 00:18:53,016
quantization algorithm we wanted


582
00:18:53,016 --> 00:18:53,396
to use.


583
00:18:54,236 --> 00:18:55,736
We also saw that Core ML Tools


584
00:18:55,736 --> 00:18:57,736
has utilities which help us to


585
00:18:57,736 --> 00:18:59,476
compare our quantized model to


586
00:18:59,476 --> 00:19:01,126
see how it performs against our


587
00:19:01,126 --> 00:19:01,696
original model.


588
00:19:03,516 --> 00:19:05,986
Now as we saw in the demo, there


589
00:19:05,986 --> 00:19:07,766
is a loss of accuracy associated


590
00:19:07,766 --> 00:19:08,876
with quantizing our model.


591
00:19:09,846 --> 00:19:12,056
And this loss of accuracy is


592
00:19:12,056 --> 00:19:13,756
highly model and data dependent.


593
00:19:14,636 --> 00:19:17,236
Some models work well or perform


594
00:19:17,236 --> 00:19:18,076
better than others after


595
00:19:18,076 --> 00:19:18,746
quantization.


596
00:19:19,006 --> 00:19:20,446
As a general rule of thumb


597
00:19:20,566 --> 00:19:22,006
again, the lower the number of


598
00:19:22,006 --> 00:19:23,696
bits we quantize our model to


599
00:19:24,376 --> 00:19:25,426
the more of a precision hit we


600
00:19:25,426 --> 00:19:25,676
take.


601
00:19:26,596 --> 00:19:28,226
Now in the demo we saw that we


602
00:19:28,436 --> 00:19:30,256
were able to use Core ML Tools


603
00:19:30,446 --> 00:19:31,806
to compare our quantized model


604
00:19:31,806 --> 00:19:33,326
and the original model using our


605
00:19:33,326 --> 00:19:34,586
Top 1 Agreement metric.


606
00:19:35,466 --> 00:19:37,596
But you have to figure out what


607
00:19:37,596 --> 00:19:38,756
the relevant metric for your


608
00:19:38,756 --> 00:19:40,386
model and your use case is and


609
00:19:40,386 --> 00:19:41,796
validate that your quantize


610
00:19:41,796 --> 00:19:42,866
model is acceptable.


611
00:19:44,026 --> 00:19:45,796
Now in a previous session, we


612
00:19:45,796 --> 00:19:47,136
took a look at a style transfer


613
00:19:47,136 --> 00:19:47,426
demo.


614
00:19:48,226 --> 00:19:50,096
And this network took in an


615
00:19:50,126 --> 00:19:52,566
input image, and the output for


616
00:19:52,566 --> 00:19:54,106
this network was a stylized


617
00:19:54,106 --> 00:19:54,496
image.


618
00:19:55,506 --> 00:19:56,566
Let's take a look at how this


619
00:19:56,736 --> 00:19:57,826
model performs at different


620
00:19:57,826 --> 00:19:58,886
levels of quantization.


621
00:20:00,126 --> 00:20:03,686
So on the top, top left here,


622
00:20:03,836 --> 00:20:04,276
your left.


623
00:20:04,766 --> 00:20:06,536
We see that original model is 30


624
00:20:06,686 --> 00:20:09,306
-- is 32 bits and it's 6.7


625
00:20:09,306 --> 00:20:10,186
megabytes in size.


626
00:20:10,326 --> 00:20:12,106
And our 8-bit linearly quantized


627
00:20:12,106 --> 00:20:14,146
model is only 1.7 megabits in


628
00:20:14,146 --> 00:20:14,446
size.


629
00:20:14,776 --> 00:20:16,016
And we see that the performance


630
00:20:16,486 --> 00:20:18,256
by visual inspection it's good


631
00:20:18,256 --> 00:20:19,406
enough for my style transfer


632
00:20:19,406 --> 00:20:19,746
demo.


633
00:20:20,756 --> 00:20:23,176
Now we can see that even down to


634
00:20:23,176 --> 00:20:25,016
4 bits, we don't lose out much


635
00:20:25,016 --> 00:20:25,896
in the way of performance.


636
00:20:26,636 --> 00:20:27,736
I would even argue that for my


637
00:20:27,736 --> 00:20:29,256
app at least, the 3 bit will


638
00:20:29,256 --> 00:20:30,066
work fine as well.


639
00:20:30,066 --> 00:20:32,946
And we see at 2 bit, we start to


640
00:20:32,946 --> 00:20:34,886
see a lot of artifacts and this


641
00:20:34,886 --> 00:20:36,406
may not be the right model for


642
00:20:37,236 --> 00:20:37,303
us.


643
00:20:38,906 --> 00:20:40,406
And that was quantization using


644
00:20:40,406 --> 00:20:41,186
Core ML Tools.


645
00:20:42,146 --> 00:20:43,056
Now I'm going to hand it back to


646
00:20:43,056 --> 00:20:44,836
Aseem who's going to talk about


647
00:20:44,836 --> 00:20:45,596
custom conversion.


648
00:20:45,596 --> 00:20:45,936
Thank you.


649
00:20:46,516 --> 00:20:51,846
[ Applause ]


650
00:20:52,346 --> 00:20:52,976
>> Thank you, Sohaib.


651
00:20:53,646 --> 00:20:56,136
So I want to talk about a


652
00:20:56,266 --> 00:20:58,396
feature that is essential to


653
00:20:58,396 --> 00:20:59,706
keep pace with the machine


654
00:20:59,706 --> 00:21:00,546
learning research that's


655
00:21:00,646 --> 00:21:02,616
happening around us.


656
00:21:02,616 --> 00:21:04,746
As you all know, the field of


657
00:21:04,746 --> 00:21:06,336
machine learning is expanding


658
00:21:06,336 --> 00:21:07,146
very rapidly.


659
00:21:07,766 --> 00:21:09,576
So it's very critical for us at


660
00:21:09,696 --> 00:21:11,836
Core ML to provide you with the


661
00:21:11,836 --> 00:21:14,046
necessary software tools to help


662
00:21:14,046 --> 00:21:15,616
with that.


663
00:21:15,886 --> 00:21:16,866
Now let's take an example.


664
00:21:17,836 --> 00:21:19,986
Let's say you are experimenting


665
00:21:19,986 --> 00:21:21,036
with a new model that that is


666
00:21:21,036 --> 00:21:22,096
not supported on Core ML.


667
00:21:22,426 --> 00:21:24,536
Or let's say you have a neural


668
00:21:24,536 --> 00:21:27,636
network that runs on Core ML but


669
00:21:27,636 --> 00:21:29,056
maybe there's a layer or two


670
00:21:29,436 --> 00:21:31,606
that Core ML does not have yet.


671
00:21:32,886 --> 00:21:34,606
In that case, you should still


672
00:21:34,606 --> 00:21:37,466
be able to use the power of Core


673
00:21:37,466 --> 00:21:38,446
ML, right?


674
00:21:38,736 --> 00:21:40,386
And the answer to that question


675
00:21:40,386 --> 00:21:40,766
is yes.


676
00:21:41,666 --> 00:21:43,726
And the feature of customization


677
00:21:43,986 --> 00:21:44,786
will help you there.


678
00:21:45,756 --> 00:21:47,906
In the next few minutes, I want


679
00:21:47,906 --> 00:21:50,346
to really focus on the specific


680
00:21:50,346 --> 00:21:52,866
use case of having a new neural


681
00:21:52,866 --> 00:21:53,506
network layer.


682
00:21:53,916 --> 00:21:55,546
And show you how you would


683
00:21:55,616 --> 00:21:57,496
convert it to Core ML and then


684
00:21:57,666 --> 00:21:58,796
how you would implement it in


685
00:21:58,826 --> 00:22:00,596
your app.


686
00:22:00,866 --> 00:22:02,496
So let's take a look at model


687
00:22:02,496 --> 00:22:02,956
conversion.


688
00:22:03,756 --> 00:22:05,396
So if you have used one of our


689
00:22:05,396 --> 00:22:06,916
converters, or even if you have


690
00:22:06,916 --> 00:22:08,996
not, it's a really simple API.


691
00:22:08,996 --> 00:22:10,276
It's just a call to one


692
00:22:10,276 --> 00:22:10,706
function.


693
00:22:11,146 --> 00:22:14,096
This is how it looks for the


694
00:22:14,166 --> 00:22:15,076
Keras converter.


695
00:22:15,466 --> 00:22:17,536
And it's very similar for say


696
00:22:17,656 --> 00:22:19,396
the ONNX converter or the


697
00:22:19,396 --> 00:22:21,416
TensorFlow converter.


698
00:22:21,776 --> 00:22:22,976
Now when you call this function,


699
00:22:23,406 --> 00:22:25,626
mostly everything goes right.


700
00:22:26,006 --> 00:22:27,656
But sometimes you might get an


701
00:22:27,776 --> 00:22:29,156
error message like this.


702
00:22:30,496 --> 00:22:32,826
It might say, "Hey, unsupported


703
00:22:32,826 --> 00:22:34,146
operation of such-and-such


704
00:22:34,146 --> 00:22:34,366
kind."


705
00:22:35,086 --> 00:22:37,546
Now if that happens to you, you


706
00:22:37,586 --> 00:22:39,286
only need to do a little bit


707
00:22:39,286 --> 00:22:40,886
more to get past this error.


708
00:22:41,666 --> 00:22:43,576
More specifically, such an error


709
00:22:43,576 --> 00:22:44,896
message is an indication that


710
00:22:44,896 --> 00:22:47,166
you should be using a custom


711
00:22:47,166 --> 00:22:47,346
layer.


712
00:22:48,976 --> 00:22:50,556
And before I show you what is


713
00:22:50,556 --> 00:22:51,946
the little bit of extra effort


714
00:22:51,946 --> 00:22:53,016
that you need to do to convert,


715
00:22:53,016 --> 00:22:55,816
let's look at a few examples


716
00:22:55,816 --> 00:22:57,156
where you would need to use a


717
00:22:57,156 --> 00:22:57,656
custom layer.


718
00:23:01,426 --> 00:23:02,776
So let's say you have an image


719
00:23:02,816 --> 00:23:03,426
classifier.


720
00:23:03,786 --> 00:23:05,846
This is how it looks in Xcode.


721
00:23:06,246 --> 00:23:07,096
So it will be high-level


722
00:23:07,096 --> 00:23:08,316
description of the model.


723
00:23:08,886 --> 00:23:11,136
If you look inside, it's very


724
00:23:11,136 --> 00:23:12,166
likely that it's a neural


725
00:23:12,166 --> 00:23:12,606
network.


726
00:23:13,046 --> 00:23:14,456
And it's very likely that it's a


727
00:23:14,456 --> 00:23:16,116
convolutional neural network.


728
00:23:16,116 --> 00:23:17,416
So it has a lot of layers,


729
00:23:17,656 --> 00:23:19,396
convolution, activation.


730
00:23:20,346 --> 00:23:22,386
Now it might happen that there's


731
00:23:22,386 --> 00:23:23,526
a new activation layer that


732
00:23:23,556 --> 00:23:24,856
comes up that Core ML does not


733
00:23:24,856 --> 00:23:25,206
support.


734
00:23:25,846 --> 00:23:29,066
And it's like at every machine


735
00:23:29,066 --> 00:23:30,916
learning conference, researchers


736
00:23:30,916 --> 00:23:32,046
are coming up with new layers


737
00:23:32,046 --> 00:23:32,516
all the time.


738
00:23:32,596 --> 00:23:33,476
So this is a very common


739
00:23:33,476 --> 00:23:33,906
scenario.


740
00:23:33,906 --> 00:23:37,356
Now if this happens, you only


741
00:23:37,356 --> 00:23:39,306
need to use a custom


742
00:23:39,486 --> 00:23:40,566
implementation of this new


743
00:23:40,566 --> 00:23:40,886
layer.


744
00:23:41,276 --> 00:23:42,256
And then you are good to go.


745
00:23:42,716 --> 00:23:43,796
So this is how the model will


746
00:23:43,796 --> 00:23:44,246
look like.


747
00:23:44,246 --> 00:23:46,096
The only difference is this


748
00:23:46,096 --> 00:23:48,016
dependency section at the


749
00:23:48,636 --> 00:23:49,446
bottom.


750
00:23:49,446 --> 00:23:52,416
Which would say that this model


751
00:23:52,416 --> 00:23:53,976
contains a description of this


752
00:23:54,026 --> 00:23:54,476
custom layer.


753
00:23:54,856 --> 00:23:56,246
Let's take a look at another


754
00:23:56,246 --> 00:23:56,746
example.


755
00:23:57,056 --> 00:23:58,576
Let's say we have a very simple


756
00:23:58,906 --> 00:23:59,946
digit classifier.


757
00:24:01,196 --> 00:24:03,526
Now I came across this research


758
00:24:03,526 --> 00:24:04,946
paper recently.


759
00:24:05,056 --> 00:24:06,486
It's called Spatial Transformer


760
00:24:06,486 --> 00:24:06,976
Network.


761
00:24:07,576 --> 00:24:09,236
And what it does is this.


762
00:24:09,886 --> 00:24:12,746
So it inserts a neural network


763
00:24:12,946 --> 00:24:15,036
after the digit that tries to


764
00:24:15,036 --> 00:24:16,516
localize the digit.


765
00:24:17,326 --> 00:24:18,856
And then it feeds it through a


766
00:24:18,856 --> 00:24:21,156
grid sampler layer which renders


767
00:24:21,156 --> 00:24:22,786
the digit again, but this time


768
00:24:23,066 --> 00:24:25,026
it has already focused on the


769
00:24:25,026 --> 00:24:25,276
digit.


770
00:24:25,596 --> 00:24:26,686
And then you pass it through


771
00:24:26,686 --> 00:24:29,516
your old classify method.


772
00:24:29,516 --> 00:24:31,626
Now we don't need to worry about


773
00:24:31,626 --> 00:24:32,416
the details here.


774
00:24:32,416 --> 00:24:33,716
But the point to note is that


775
00:24:33,756 --> 00:24:35,946
the portion in green is what


776
00:24:35,946 --> 00:24:36,986
Core ML supports.


777
00:24:37,136 --> 00:24:38,906
And the portion in red, which is


778
00:24:38,906 --> 00:24:41,456
this new grid sampler layer, is


779
00:24:41,456 --> 00:24:42,846
this new experimental layer that


780
00:24:42,846 --> 00:24:43,746
Core ML does not support.


781
00:24:44,136 --> 00:24:45,596
So I want to take an example of


782
00:24:45,716 --> 00:24:47,826
this particular model and show


783
00:24:47,826 --> 00:24:49,896
you how you would convert it


784
00:24:50,116 --> 00:24:51,316
using Core ML Tools.


785
00:24:51,566 --> 00:24:53,336
So let's go to demo.


786
00:25:00,076 --> 00:25:01,566
I hope it works on the first


787
00:25:01,566 --> 00:25:01,826
try.


788
00:25:03,236 --> 00:25:04,836
Back, oh yes.


789
00:25:05,996 --> 00:25:09,476
Okay. So let me close off these


790
00:25:09,476 --> 00:25:09,966
windows.


791
00:25:14,636 --> 00:25:16,766
Let me get, clear this.


792
00:25:16,926 --> 00:25:17,596
Clear the ML.


793
00:25:18,106 --> 00:25:20,246
Okay, so I'm also going to use


794
00:25:20,446 --> 00:25:22,526
Jupyter Notebook to show the


795
00:25:22,526 --> 00:25:22,846
demo.


796
00:25:30,046 --> 00:25:31,696
So I just navigate to the folder


797
00:25:31,696 --> 00:25:32,956
where I have my pre-trained


798
00:25:33,026 --> 00:25:33,356
network.


799
00:25:34,346 --> 00:25:35,976
So what you see here is that I


800
00:25:35,976 --> 00:25:38,386
have this spatial transformer


801
00:25:38,386 --> 00:25:39,056
dot [inaudible] file.


802
00:25:39,156 --> 00:25:41,136
This is a pre-trained Keras


803
00:25:41,136 --> 00:25:41,436
model.


804
00:25:42,346 --> 00:25:44,056
And if you are wondering if I


805
00:25:44,056 --> 00:25:45,526
did something special to get


806
00:25:45,526 --> 00:25:46,756
this model.


807
00:25:47,956 --> 00:25:50,536
Basically what I did was I could


808
00:25:50,646 --> 00:25:52,076
easily find an open source


809
00:25:52,076 --> 00:25:53,126
implementation of spatial


810
00:25:53,126 --> 00:25:53,676
transformer.


811
00:25:54,086 --> 00:25:55,616
I just exhibited that script in


812
00:25:55,616 --> 00:25:57,046
Keras, and I got this model.


813
00:25:57,646 --> 00:25:59,196
And along with this model, I


814
00:25:59,196 --> 00:26:01,476
also got this grid sampler layer


815
00:26:01,646 --> 00:26:02,436
Python script.


816
00:26:03,206 --> 00:26:04,936
Now this grid sampler layer that


817
00:26:04,936 --> 00:26:07,086
I'm talking about, it's also not


818
00:26:07,086 --> 00:26:08,676
supported on Keras natively.


819
00:26:09,326 --> 00:26:11,376
So the implementation that I got


820
00:26:11,376 --> 00:26:13,666
online used that Keras custom


821
00:26:13,666 --> 00:26:15,666
layer to implement the layer.


822
00:26:16,376 --> 00:26:18,496
So as you can see, the concept


823
00:26:18,496 --> 00:26:20,046
of customization is not unique


824
00:26:20,046 --> 00:26:20,606
to Core ML.


825
00:26:20,806 --> 00:26:22,986
In fact, it's very common in


826
00:26:22,986 --> 00:26:23,816
most machine learning


827
00:26:23,816 --> 00:26:24,416
frameworks.


828
00:26:24,756 --> 00:26:26,556
This is how people experiment in


829
00:26:26,556 --> 00:26:27,086
new layers.


830
00:26:27,966 --> 00:26:29,956
Okay, so so far, I just have a


831
00:26:30,006 --> 00:26:30,636
Keras model.


832
00:26:30,826 --> 00:26:32,306
And now I want to focus on how


833
00:26:32,306 --> 00:26:33,666
can I get a Core ML model?


834
00:26:34,406 --> 00:26:40,716
So I'll open -- there, let me


835
00:26:40,716 --> 00:26:42,506
launch a new Python notebook.


836
00:26:43,766 --> 00:26:46,176
So I'll start by importing this


837
00:26:46,226 --> 00:26:47,526
Keras model into my Python


838
00:26:47,526 --> 00:26:48,086
environment.


839
00:26:49,396 --> 00:26:53,576
Okay? So I import Keras, I


840
00:26:53,576 --> 00:26:56,676
import the, the custom layer


841
00:26:56,676 --> 00:26:57,976
that we have in Keras.


842
00:26:58,316 --> 00:27:00,366
And now I will load the model in


843
00:27:00,406 --> 00:27:00,926
Keras.


844
00:27:02,676 --> 00:27:04,656
Okay? So this is how you load


845
00:27:05,086 --> 00:27:06,196
model, Keras models.


846
00:27:06,196 --> 00:27:07,506
You give the part to the model


847
00:27:07,506 --> 00:27:08,536
and if there's a custom layer,


848
00:27:08,536 --> 00:27:09,796
you give a part to that.


849
00:27:11,016 --> 00:27:12,886
Okay. So we have the model now.


850
00:27:13,066 --> 00:27:14,276
Now let's convert this to Core


851
00:27:14,276 --> 00:27:14,466
ML.


852
00:27:15,656 --> 00:27:16,986
So I'm going to import Core ML


853
00:27:16,986 --> 00:27:17,626
Tools.


854
00:27:18,356 --> 00:27:19,076
Execute that.


855
00:27:19,896 --> 00:27:22,106
And now as I, as I showed you


856
00:27:22,106 --> 00:27:24,156
before that this is just a call


857
00:27:24,156 --> 00:27:25,626
to one function to convert it.


858
00:27:26,086 --> 00:27:26,966
So let me do that.


859
00:27:28,476 --> 00:27:29,196
That's my call.


860
00:27:29,776 --> 00:27:32,086
And I get an error as expected.


861
00:27:32,626 --> 00:27:35,096
Python likes to throw these huge


862
00:27:35,096 --> 00:27:35,696
error messages.


863
00:27:36,136 --> 00:27:37,946
But really what we're focused on


864
00:27:38,666 --> 00:27:40,076
is this last line.


865
00:27:40,336 --> 00:27:40,896
Let me --


866
00:27:46,626 --> 00:27:47,906
So as we can see in this last


867
00:27:47,906 --> 00:27:49,676
line it says that hey, the layer


868
00:27:49,676 --> 00:27:51,326
or sampler is not supported.


869
00:27:51,836 --> 00:27:53,146
So now let's see what we need to


870
00:27:53,146 --> 00:27:55,126
do to get rid of that.


871
00:27:55,756 --> 00:27:57,066
Maybe I clear this all so that


872
00:27:57,066 --> 00:27:57,466
you can see.


873
00:27:57,786 --> 00:27:59,616
Okay. So now I change my


874
00:27:59,616 --> 00:28:00,956
converter call just a little bit


875
00:28:01,176 --> 00:28:03,046
so I have my Core ML model.


876
00:28:03,646 --> 00:28:08,096
And now I'm going to pass one


877
00:28:08,096 --> 00:28:08,946
additional argument.


878
00:28:09,176 --> 00:28:14,176
It's called custom conversion


879
00:28:14,576 --> 00:28:14,976
functions.


880
00:28:19,396 --> 00:28:20,676
And this will be a dictionary


881
00:28:21,396 --> 00:28:23,796
from the name of the layer to a


882
00:28:23,796 --> 00:28:25,496
function that I will define in a


883
00:28:25,496 --> 00:28:25,826
minute.


884
00:28:26,186 --> 00:28:27,066
And that I'm calling a good


885
00:28:27,066 --> 00:28:27,386
sampler.


886
00:28:27,926 --> 00:28:29,006
So let me take a step back and


887
00:28:29,006 --> 00:28:30,026
explain what is happening here.


888
00:28:30,576 --> 00:28:31,796
So as we know the way converter


889
00:28:31,796 --> 00:28:34,126
works is that it goes through


890
00:28:34,346 --> 00:28:35,666
each and every Keras layer.


891
00:28:36,016 --> 00:28:38,386
It will, if you look at the


892
00:28:38,476 --> 00:28:38,986
first layer.


893
00:28:38,986 --> 00:28:40,046
Then [inaudible] its parameters


894
00:28:40,046 --> 00:28:40,666
to Core ML.


895
00:28:40,666 --> 00:28:41,816
If you go to the second layer,


896
00:28:41,816 --> 00:28:43,146
then translate its parameters


897
00:28:43,146 --> 00:28:44,306
and so on.


898
00:28:44,676 --> 00:28:45,936
Now when it hits this custom


899
00:28:45,936 --> 00:28:46,886
layer, it doesn't know what to


900
00:28:46,886 --> 00:28:47,146
do.


901
00:28:47,756 --> 00:28:49,446
So this function that I'm


902
00:28:49,446 --> 00:28:50,886
passing here that convert this


903
00:28:50,936 --> 00:28:53,246
sampler is going to help my


904
00:28:53,246 --> 00:28:54,256
converter in doing that.


905
00:28:54,876 --> 00:28:56,316
And let me show you what this


906
00:28:56,316 --> 00:28:56,976
function looks like.


907
00:29:01,046 --> 00:29:01,806
So this is a function.


908
00:29:01,806 --> 00:29:03,976
There are a few lines of code,


909
00:29:03,976 --> 00:29:05,866
but all that it's doing is three


910
00:29:05,866 --> 00:29:06,276
things.


911
00:29:07,226 --> 00:29:10,966
First, it's giving a name of a


912
00:29:11,026 --> 00:29:11,306
class.


913
00:29:11,806 --> 00:29:13,566
So as we might have noticed, the


914
00:29:13,616 --> 00:29:15,126
implementation of the layer is


915
00:29:15,126 --> 00:29:15,546
not here.


916
00:29:15,906 --> 00:29:17,096
The implementation will come


917
00:29:17,096 --> 00:29:19,506
later in the app and it will be


918
00:29:19,506 --> 00:29:20,866
encapsulated in a class.


919
00:29:21,286 --> 00:29:22,136
And this is the name of the


920
00:29:22,136 --> 00:29:23,836
class that we'll later


921
00:29:23,836 --> 00:29:24,286
implement.


922
00:29:24,666 --> 00:29:26,436
So during conversion, we just


923
00:29:26,436 --> 00:29:27,756
need to specify this class name.


924
00:29:28,156 --> 00:29:28,456
That's it.


925
00:29:29,156 --> 00:29:30,436
And then there's the description


926
00:29:30,816 --> 00:29:32,206
which is a, which you should


927
00:29:32,206 --> 00:29:34,536
provide so that if anybody is,


928
00:29:34,746 --> 00:29:37,086
if somebody is looking at your


929
00:29:37,086 --> 00:29:38,266
model, they know what it has.


930
00:29:39,276 --> 00:29:40,926
And the third thing is basically


931
00:29:40,996 --> 00:29:42,766
translating any parameters that


932
00:29:42,766 --> 00:29:44,466
the Keras layer had to Core ML.


933
00:29:45,046 --> 00:29:46,206
For this particular layer, it


934
00:29:46,206 --> 00:29:47,146
has two parameters.


935
00:29:47,276 --> 00:29:48,756
The output height, and output


936
00:29:48,816 --> 00:29:48,946
weight.


937
00:29:49,336 --> 00:29:50,716
And I'm just translating it to


938
00:29:50,716 --> 00:29:51,066
Core ML.


939
00:29:51,866 --> 00:29:52,956
If your custom layer that does


940
00:29:52,956 --> 00:29:54,356
not have any parameters, then


941
00:29:54,356 --> 00:29:56,626
you load, then you do not need


942
00:29:56,626 --> 00:29:57,616
to do, do this.


943
00:29:58,356 --> 00:29:59,516
If your layer has lots of


944
00:29:59,516 --> 00:30:01,116
parameters, they can all go


945
00:30:01,116 --> 00:30:02,466
here, and they will all be


946
00:30:02,466 --> 00:30:04,736
encapsulated inside the Core ML


947
00:30:04,736 --> 00:30:05,066
model.


948
00:30:05,996 --> 00:30:07,486
So as you might have noticed


949
00:30:07,486 --> 00:30:09,096
that all I did here was very


950
00:30:09,096 --> 00:30:10,576
similar to how you would define


951
00:30:10,576 --> 00:30:11,466
a class, right?


952
00:30:11,786 --> 00:30:12,656
You give a class name.


953
00:30:12,656 --> 00:30:14,106
Maybe a description, maybe some


954
00:30:14,106 --> 00:30:14,746
parameters.


955
00:30:15,546 --> 00:30:16,646
So now let me execute this.


956
00:30:20,066 --> 00:30:20,916
And now we see that the


957
00:30:20,916 --> 00:30:23,606
converter went, conversion went


958
00:30:25,196 --> 00:30:25,336
fine.


959
00:30:25,596 --> 00:30:30,216
So let me this is behaving very


960
00:30:30,216 --> 00:30:31,636
weirdly for some reason.


961
00:30:35,296 --> 00:30:36,836
If you don't mind, I'm just


962
00:30:36,886 --> 00:30:38,656
going to delete this all.


963
00:30:40,556 --> 00:30:42,396
So let me visualize this model,


964
00:30:42,676 --> 00:30:44,186
and you can do that very simply


965
00:30:44,356 --> 00:30:47,136
using function in Core ML Tools.


966
00:30:47,636 --> 00:30:48,596
That's called visualize spec.


967
00:30:50,766 --> 00:30:53,256
And here you can see a


968
00:30:53,256 --> 00:30:54,256
visualization of the model.


969
00:30:54,586 --> 00:30:56,166
So as we can see, we have the


970
00:30:56,216 --> 00:30:56,996
[inaudible] and some layers


971
00:30:56,996 --> 00:30:57,416
there.


972
00:30:57,876 --> 00:31:00,336
And this is our custom layer.


973
00:31:00,336 --> 00:31:01,936
And if I click on this, I see


974
00:31:01,936 --> 00:31:03,176
the parameters that it has.


975
00:31:03,176 --> 00:31:04,806
So this is the name of the class


976
00:31:05,086 --> 00:31:05,706
that I gave.


977
00:31:06,256 --> 00:31:07,816
And this, and these are the


978
00:31:07,816 --> 00:31:08,746
parameters that I set.


979
00:31:10,026 --> 00:31:10,886
It's always a good idea to


980
00:31:10,886 --> 00:31:13,146
visualize your Core ML model


981
00:31:13,146 --> 00:31:14,176
before you drag and drop just to


982
00:31:14,176 --> 00:31:15,906
see if everything looks fine.


983
00:31:17,346 --> 00:31:20,546
Okay. This is the wrong


984
00:31:20,546 --> 00:31:20,946
notebook.


985
00:31:21,256 --> 00:31:23,036
Okay. And now I'll save out this


986
00:31:23,036 --> 00:31:23,406
model.


987
00:31:30,256 --> 00:31:32,106
And now let's take a look at


988
00:31:32,106 --> 00:31:32,606
this model.


989
00:31:33,846 --> 00:31:35,816
So let me close this.


990
00:31:37,756 --> 00:31:37,926
Okay.


991
00:31:42,086 --> 00:31:44,696
Let me actually let me navigate


992
00:31:44,816 --> 00:31:50,476
to the directory that I have.


993
00:31:51,976 --> 00:31:52,736
And here's my model.


994
00:31:52,736 --> 00:31:53,946
So if I click on it and see it


995
00:31:53,946 --> 00:31:55,336
in Xcode just to see how it


996
00:31:55,336 --> 00:31:55,666
looks.


997
00:31:55,666 --> 00:31:58,756
We can see that it has the


998
00:31:58,756 --> 00:32:00,266
custom description here.


999
00:32:01,546 --> 00:32:03,186
Okay. Let me go back to slides.


1000
00:32:04,516 --> 00:32:09,896
[ Applause ]


1001
00:32:10,396 --> 00:32:12,466
So what we just saw was with a


1002
00:32:12,466 --> 00:32:15,126
few simple lines, we could


1003
00:32:15,156 --> 00:32:16,386
exhibit a convert a function to


1004
00:32:16,546 --> 00:32:17,246
Core ML.


1005
00:32:17,246 --> 00:32:19,496
And the process is pretty much


1006
00:32:19,526 --> 00:32:21,266
the same if you are using the


1007
00:32:21,266 --> 00:32:23,566
TensorFlow converter or the ONNX


1008
00:32:23,566 --> 00:32:23,956
converter.


1009
00:32:25,266 --> 00:32:26,596
So we have our model here on the


1010
00:32:26,596 --> 00:32:27,176
left-hand side.


1011
00:32:27,616 --> 00:32:29,286
The custom layer model with the


1012
00:32:29,286 --> 00:32:29,856
parameters.


1013
00:32:30,496 --> 00:32:31,646
Now when you drag and drop this


1014
00:32:31,646 --> 00:32:33,966
model into Xcode, you will need


1015
00:32:33,966 --> 00:32:35,336
to provide the implementation of


1016
00:32:35,426 --> 00:32:36,456
the class.


1017
00:32:36,456 --> 00:32:38,186
In a file say, for example,


1018
00:32:38,186 --> 00:32:38,476
[inaudible].


1019
00:32:38,476 --> 00:32:39,536
And this is how it would look


1020
00:32:39,536 --> 00:32:39,726
like.


1021
00:32:40,206 --> 00:32:41,646
So you have your class, so


1022
00:32:42,006 --> 00:32:44,986
you'll have the initializer


1023
00:32:45,246 --> 00:32:45,706
function.


1024
00:32:45,996 --> 00:32:47,456
So this would be just


1025
00:32:47,456 --> 00:32:49,106
initializing any parameters that


1026
00:32:49,106 --> 00:32:50,116
we had in the model.


1027
00:32:50,486 --> 00:32:52,636
And then the main function in


1028
00:32:52,636 --> 00:32:55,906
this class would be evaluate.


1029
00:32:56,846 --> 00:32:58,996
This is where the actual


1030
00:32:59,266 --> 00:33:00,706
implementation of whatever


1031
00:33:00,706 --> 00:33:01,876
mathematical function the layer


1032
00:33:01,876 --> 00:33:03,396
is supposed to perform will go


1033
00:33:03,596 --> 00:33:04,376
here, in here.


1034
00:33:04,806 --> 00:33:05,976
And then there's one more


1035
00:33:05,976 --> 00:33:07,266
function called output shape or


1036
00:33:07,266 --> 00:33:07,946
input shapes.


1037
00:33:08,136 --> 00:33:10,036
This just specifies the size of


1038
00:33:10,036 --> 00:33:11,496
the output area that the layer


1039
00:33:11,496 --> 00:33:12,106
produces.


1040
00:33:12,556 --> 00:33:14,906
This helps Core ML in allocating


1041
00:33:14,906 --> 00:33:17,116
the buffer size at load time so


1042
00:33:17,116 --> 00:33:18,726
that your app is more efficient


1043
00:33:19,076 --> 00:33:19,646
at runtime.


1044
00:33:21,966 --> 00:33:24,256
So we just saw how you would


1045
00:33:24,326 --> 00:33:26,136
tackle a new layer in a neural


1046
00:33:26,136 --> 00:33:26,516
network.


1047
00:33:26,826 --> 00:33:29,446
There's a very similar concept


1048
00:33:29,506 --> 00:33:30,556
to a custom layer, and it's


1049
00:33:30,556 --> 00:33:32,336
called custom model.


1050
00:33:32,336 --> 00:33:35,426
It has the same idea, but it's


1051
00:33:35,426 --> 00:33:36,766
sort of more generic.


1052
00:33:37,226 --> 00:33:39,966
So with a custom model, you can


1053
00:33:39,966 --> 00:33:41,546
deal with any sort of network.


1054
00:33:41,806 --> 00:33:43,696
It need not be a neural -- it


1055
00:33:43,726 --> 00:33:44,856
need not be a neural network.


1056
00:33:45,306 --> 00:33:46,876
And basically gives you just


1057
00:33:46,876 --> 00:33:48,646
more flexibility overall.


1058
00:33:50,156 --> 00:33:51,796
So let me summarize the session.


1059
00:33:52,356 --> 00:33:57,026
We saw how much more rich is


1060
00:33:57,076 --> 00:33:58,656
this ecosystem around Core ML


1061
00:33:58,656 --> 00:34:00,116
Tools and that's great for you


1062
00:34:00,116 --> 00:34:00,396
guys.


1063
00:34:00,396 --> 00:34:01,346
Because now you have lot of


1064
00:34:01,436 --> 00:34:03,416
choice to get Core ML models


1065
00:34:03,416 --> 00:34:03,726
from.


1066
00:34:04,476 --> 00:34:06,526
We saw how easy it was to


1067
00:34:06,846 --> 00:34:09,666
quantize this, quantize Core ML


1068
00:34:09,666 --> 00:34:10,235
model.


1069
00:34:10,716 --> 00:34:12,485
And we saw that with a few lines


1070
00:34:13,156 --> 00:34:16,565
of code, we could easily


1071
00:34:16,565 --> 00:34:18,295
integrate a new custom layer in


1072
00:34:19,356 --> 00:34:20,456
the model.


1073
00:34:20,806 --> 00:34:22,136
You can find more information at


1074
00:34:22,136 --> 00:34:23,206
our documentation page.


1075
00:34:23,596 --> 00:34:25,476
And come to the labs and talk to


1076
00:34:25,476 --> 00:34:25,626
us.


1077
00:34:26,176 --> 00:34:26,966
Okay, thank you.


1078
00:34:27,507 --> 00:34:29,507
[ Applause ]

