1
00:00:07,016 --> 00:00:15,500
[ Music ]


2
00:00:20,516 --> 00:00:26,786
[ Applause ]


3
00:00:27,286 --> 00:00:27,936
>> Good morning.


4
00:00:28,826 --> 00:00:31,276
Welcome. I'm Michael, and this


5
00:00:31,276 --> 00:00:33,316
session is about what's new in


6
00:00:33,316 --> 00:00:33,816
Core ML.


7
00:00:35,626 --> 00:00:37,786
So introduced a year ago, Core


8
00:00:37,846 --> 00:00:39,286
ML is all about making it


9
00:00:39,286 --> 00:00:41,346
unbelievably simple for you to


10
00:00:41,346 --> 00:00:42,476
integrate machine learning


11
00:00:42,476 --> 00:00:43,846
models into your app.


12
00:00:45,436 --> 00:00:46,696
It's been wonderful to see the


13
00:00:46,696 --> 00:00:48,286
adoption over the past year.


14
00:00:49,946 --> 00:00:51,276
We hope it has all of you


15
00:00:51,276 --> 00:00:53,316
thinking about what great new


16
00:00:53,316 --> 00:00:55,776
experiences you can enable if


17
00:00:55,776 --> 00:00:57,496
your app had the ability to do


18
00:00:57,496 --> 00:00:59,756
things like understand the


19
00:00:59,756 --> 00:01:03,826
content of images, or perhaps,


20
00:01:04,726 --> 00:01:05,876
analyze some text.


21
00:01:08,696 --> 00:01:11,266
What could you do if your app


22
00:01:11,266 --> 00:01:13,026
could reason about audio or


23
00:01:13,026 --> 00:01:16,536
music, or interpret your users'


24
00:01:16,536 --> 00:01:17,956
actions based on their motion


25
00:01:17,956 --> 00:01:21,506
activity, or even transform or


26
00:01:21,506 --> 00:01:22,976
generate new content for them?


27
00:01:24,416 --> 00:01:26,106
All of this, and much, much


28
00:01:26,106 --> 00:01:28,336
more, is easily within reach.


29
00:01:29,046 --> 00:01:30,356
And that's because this type of


30
00:01:30,356 --> 00:01:32,496
functionality can be encoded in


31
00:01:32,496 --> 00:01:33,356
a Core ML model.


32
00:01:35,146 --> 00:01:36,786
Now if we take a peek inside one


33
00:01:36,786 --> 00:01:39,126
of these, we may find a neural


34
00:01:39,126 --> 00:01:41,396
network, tree ensemble, or some


35
00:01:41,396 --> 00:01:42,426
other model architecture.


36
00:01:43,686 --> 00:01:44,876
They may have millions of


37
00:01:44,926 --> 00:01:46,876
parameters, the values of which


38
00:01:46,876 --> 00:01:47,976
have been learned from large


39
00:01:47,976 --> 00:01:48,666
amounts of data.


40
00:01:50,596 --> 00:01:52,686
But for you, you could focus on


41
00:01:52,686 --> 00:01:53,446
a single file.


42
00:01:54,166 --> 00:01:55,716
You can focus on the


43
00:01:55,716 --> 00:01:57,566
functionality it provides and


44
00:01:57,566 --> 00:01:59,816
the experience it enables rather


45
00:01:59,816 --> 00:02:00,816
than those implementation


46
00:02:00,816 --> 00:02:01,216
details.


47
00:02:04,806 --> 00:02:06,556
Adding a Core ML model to your


48
00:02:06,926 --> 00:02:08,106
app is simple as adding that


49
00:02:08,196 --> 00:02:09,666
file to your Xcode project.


50
00:02:11,226 --> 00:02:12,456
Xcode will give you a simple


51
00:02:12,456 --> 00:02:15,206
view, describe what it does in


52
00:02:15,236 --> 00:02:17,106
terms of the inputs it requires


53
00:02:17,416 --> 00:02:18,616
and the outputs it provides.


54
00:02:20,136 --> 00:02:21,476
Xcode will take this one step


55
00:02:21,476 --> 00:02:22,576
further and generate an


56
00:02:22,576 --> 00:02:25,186
interface for you, so that


57
00:02:25,186 --> 00:02:26,336
interacting with this model is


58
00:02:26,336 --> 00:02:29,206
just a few lines of code, one to


59
00:02:29,206 --> 00:02:31,916
load the model, one to make a


60
00:02:31,916 --> 00:02:34,526
prediction, and sometimes one to


61
00:02:34,526 --> 00:02:35,876
pull out the specific output you


62
00:02:35,876 --> 00:02:36,516
are interested in.


63
00:02:38,146 --> 00:02:39,206
Note that in some cases you


64
00:02:39,206 --> 00:02:40,116
don't even have to write back


65
00:02:40,146 --> 00:02:41,806
code because Core ML integrates


66
00:02:41,806 --> 00:02:43,296
with some of our higher-level


67
00:02:43,296 --> 00:02:44,966
APIs and allows you to customize


68
00:02:44,966 --> 00:02:46,416
their behavior if you give them


69
00:02:46,416 --> 00:02:47,286
a Core ML model.


70
00:02:47,736 --> 00:02:49,576
So with Vision, this is done


71
00:02:49,576 --> 00:02:51,106
through the VNCoreML Request


72
00:02:51,106 --> 00:02:51,496
object.


73
00:02:51,946 --> 00:02:53,106
And in the new Natural Language


74
00:02:53,106 --> 00:02:54,586
framework, you can instantiate


75
00:02:54,686 --> 00:02:56,536
an MLModel from a CoreML model.


76
00:02:57,246 --> 00:03:00,786
So that's Core ML in a nutshell.


77
00:03:01,606 --> 00:03:02,576
But we are here to talk about


78
00:03:02,576 --> 00:03:03,096
what's new.


79
00:03:04,326 --> 00:03:05,826
We took all the great feedback


80
00:03:06,026 --> 00:03:06,886
we received from you over the


81
00:03:06,886 --> 00:03:08,796
past year and focused on some


82
00:03:09,116 --> 00:03:11,086
key enhancements to CoreML 2.


83
00:03:12,586 --> 00:03:13,666
And we are going to talk about


84
00:03:13,666 --> 00:03:14,736
these in two sessions.


85
00:03:15,656 --> 00:03:16,836
In the first session, the one


86
00:03:16,836 --> 00:03:17,626
you are all sitting in right


87
00:03:17,626 --> 00:03:18,786
now, we are going to talk about


88
00:03:18,786 --> 00:03:20,176
what's new from the perspective


89
00:03:20,176 --> 00:03:21,646
of your app.


90
00:03:21,846 --> 00:03:23,146
In the second session, which


91
00:03:23,146 --> 00:03:24,716
starts immediately after this at


92
00:03:24,716 --> 00:03:26,736
10 a.m. after a short break, we


93
00:03:26,736 --> 00:03:29,046
are going to talk about tools


94
00:03:29,366 --> 00:03:30,786
and how you can update and


95
00:03:30,786 --> 00:03:32,186
convert models to take advantage


96
00:03:32,186 --> 00:03:33,456
of the new features in Core ML


97
00:03:33,496 --> 00:03:33,676
2.


98
00:03:36,936 --> 00:03:37,836
When it comes to your app, we


99
00:03:37,836 --> 00:03:39,426
are going to focus on three key


100
00:03:39,426 --> 00:03:39,896
areas.


101
00:03:40,736 --> 00:03:42,746
The first is how you can reduce


102
00:03:42,796 --> 00:03:44,656
the size and number of models


103
00:03:45,076 --> 00:03:46,246
using your app while still


104
00:03:46,246 --> 00:03:47,616
getting the same functionality.


105
00:03:48,886 --> 00:03:50,206
Then we'll look about how you


106
00:03:50,206 --> 00:03:51,846
can get more performance out of


107
00:03:51,846 --> 00:03:52,496
a single model.


108
00:03:52,996 --> 00:03:55,286
And then we'll conclude about


109
00:03:55,336 --> 00:03:57,086
how using Core ML will allow you


110
00:03:57,086 --> 00:03:58,196
to keep pace with the


111
00:03:58,196 --> 00:03:59,686
state-of-the-art and rapidly


112
00:03:59,686 --> 00:04:00,946
moving field of machine


113
00:04:00,946 --> 00:04:01,236
learning.


114
00:04:02,286 --> 00:04:03,466
So to kick it off, let's talk


115
00:04:03,466 --> 00:04:04,346
about model size.


116
00:04:04,486 --> 00:04:05,366
I'm going to hand it off to


117
00:04:05,366 --> 00:04:05,966
Francesco.


118
00:04:06,516 --> 00:04:09,996
[ Applause ]


119
00:04:10,496 --> 00:04:11,166
>> Thank you Michael.


120
00:04:12,876 --> 00:04:15,936
Hello. Every ways to reduce the


121
00:04:15,936 --> 00:04:17,476
size of your Core ML app is very


122
00:04:17,476 --> 00:04:18,005
important.


123
00:04:18,636 --> 00:04:19,935
My name is Francesco, and I am


124
00:04:19,935 --> 00:04:21,286
going to introduce quantization


125
00:04:21,375 --> 00:04:23,266
and flexible shapes, two new


126
00:04:23,266 --> 00:04:25,086
features in Core ML 2 that can


127
00:04:25,086 --> 00:04:28,306
help reduce your app size.


128
00:04:28,516 --> 00:04:30,326
So Core ML, [inaudible] why


129
00:04:30,326 --> 00:04:32,506
should you learn in models and


130
00:04:33,316 --> 00:04:33,486
device.


131
00:04:33,636 --> 00:04:34,786
This gives your app four key


132
00:04:34,786 --> 00:04:36,316
advantages compared to running


133
00:04:36,316 --> 00:04:36,966
them in the cloud.


134
00:04:37,766 --> 00:04:39,746
First of all, user privacy is


135
00:04:39,746 --> 00:04:40,476
fully respected.


136
00:04:40,476 --> 00:04:42,066
There are many machine-learning


137
00:04:42,066 --> 00:04:42,976
models on device.


138
00:04:43,876 --> 00:04:45,406
We guarantee that the data never


139
00:04:45,406 --> 00:04:46,836
leaves the device of the user.


140
00:04:48,016 --> 00:04:49,636
Second, it can help you achieve


141
00:04:49,636 --> 00:04:50,626
real-time performance.


142
00:04:52,046 --> 00:04:52,956
And for silicon [phonetic] and


143
00:04:52,956 --> 00:04:54,956
devices are super- efficient for


144
00:04:54,956 --> 00:04:56,136
machine-learning workloads.


145
00:04:57,106 --> 00:04:58,226
Furthermore, you don't have to


146
00:04:58,226 --> 00:04:59,796
maintain and pay for Internet


147
00:04:59,796 --> 00:05:00,256
servers.


148
00:05:01,346 --> 00:05:02,486
And Core ML inference is


149
00:05:02,486 --> 00:05:04,976
available anywhere at any time


150
00:05:05,546 --> 00:05:06,806
despite natural connectivity


151
00:05:06,806 --> 00:05:07,226
issues.


152
00:05:07,916 --> 00:05:09,256
All these great benefits come


153
00:05:09,256 --> 00:05:10,706
with the fact that you now need


154
00:05:10,706 --> 00:05:12,106
to store your machine-learning


155
00:05:12,106 --> 00:05:13,076
models on device.


156
00:05:14,046 --> 00:05:15,246
And if the machine-learning


157
00:05:15,246 --> 00:05:17,326
models are big, then you might


158
00:05:17,326 --> 00:05:18,506
be concerned about the size of


159
00:05:18,506 --> 00:05:18,866
your app.


160
00:05:19,796 --> 00:05:21,416
For example, you are -- you have


161
00:05:21,416 --> 00:05:22,846
your [inaudible] map, and it's


162
00:05:22,846 --> 00:05:23,856
full of cool features.


163
00:05:24,446 --> 00:05:25,846
And your users are very happy


164
00:05:25,846 --> 00:05:26,246
about it.


165
00:05:26,836 --> 00:05:28,156
And now you want to take


166
00:05:28,156 --> 00:05:28,996
advantage of the new


167
00:05:28,996 --> 00:05:31,096
opportunities offered by machine


168
00:05:31,096 --> 00:05:32,526
learning on device, and you want


169
00:05:32,526 --> 00:05:33,936
to add new amazing capabilities


170
00:05:33,936 --> 00:05:34,946
to your app.


171
00:05:34,946 --> 00:05:36,166
So what you do, you train some


172
00:05:36,166 --> 00:05:38,126
Core ML models and you add them


173
00:05:38,126 --> 00:05:39,446
to your app.


174
00:05:39,806 --> 00:05:41,046
What this means is that your app


175
00:05:41,046 --> 00:05:42,716
has become more awesome and your


176
00:05:42,716 --> 00:05:43,916
users are even happier.


177
00:05:45,176 --> 00:05:46,546
But some of them might notice


178
00:05:47,006 --> 00:05:48,456
that your app has increased in


179
00:05:48,456 --> 00:05:49,276
size a little bit.


180
00:05:49,966 --> 00:05:51,376
It's not uncommon to see that


181
00:05:51,376 --> 00:05:53,066
apps grow up, either to tens or


182
00:05:53,106 --> 00:05:54,366
hundreds of megabytes after


183
00:05:54,366 --> 00:05:55,216
adding machine-learning


184
00:05:55,216 --> 00:05:56,106
capabilities to them.


185
00:05:57,546 --> 00:05:58,926
And as you keep adding more and


186
00:05:58,926 --> 00:06:01,216
more features to your app, your


187
00:06:01,216 --> 00:06:02,806
app size might simply become out


188
00:06:02,806 --> 00:06:03,286
of control.


189
00:06:04,696 --> 00:06:05,756
So that's the first thing that


190
00:06:05,756 --> 00:06:06,666
you can do about it.


191
00:06:06,976 --> 00:06:08,736
And if these machine-learning


192
00:06:08,736 --> 00:06:10,076
models are supporting other


193
00:06:10,116 --> 00:06:11,946
features to your app, you can


194
00:06:11,946 --> 00:06:13,506
keep them outside your initial


195
00:06:13,506 --> 00:06:13,956
bundle.


196
00:06:13,956 --> 00:06:17,196
And then as the user uses the


197
00:06:17,196 --> 00:06:18,666
other features, you can download


198
00:06:18,666 --> 00:06:20,546
them on demand, compile them on


199
00:06:20,546 --> 00:06:21,036
device.


200
00:06:21,796 --> 00:06:23,036
So this -- in this case, this


201
00:06:23,036 --> 00:06:24,356
user is happy in the beginning


202
00:06:24,356 --> 00:06:26,166
because the installation size is


203
00:06:26,166 --> 00:06:26,746
unchanged.


204
00:06:27,246 --> 00:06:28,956
But since the user downloads and


205
00:06:28,956 --> 00:06:30,106
uses all the current Core ML


206
00:06:30,106 --> 00:06:32,156
functionality in your app, at


207
00:06:32,156 --> 00:06:33,406
the end of the day the size of


208
00:06:33,406 --> 00:06:34,786
the -- of your app is still


209
00:06:34,786 --> 00:06:35,046
large.


210
00:06:36,216 --> 00:06:38,096
So wouldn't it be better if,


211
00:06:38,096 --> 00:06:40,766
instead, we could tackle this


212
00:06:40,806 --> 00:06:43,646
problem by reducing the size of


213
00:06:43,646 --> 00:06:44,616
the models itself?


214
00:06:46,016 --> 00:06:47,496
This would give us a smaller


215
00:06:47,496 --> 00:06:48,886
bundle in case we ship the


216
00:06:48,886 --> 00:06:51,716
models inside the app, faster


217
00:06:51,716 --> 00:06:54,026
and smaller downloads if instead


218
00:06:54,026 --> 00:06:55,346
of shipping the models in the


219
00:06:55,346 --> 00:06:56,206
app we download them.


220
00:06:56,206 --> 00:06:58,936
And in any case, your app will


221
00:06:58,936 --> 00:07:00,406
enjoy a lower memory footprint.


222
00:07:00,906 --> 00:07:02,456
Using less memory is better for


223
00:07:02,456 --> 00:07:04,176
the performance of your app and


224
00:07:04,176 --> 00:07:05,446
great for the system in general.


225
00:07:06,286 --> 00:07:08,046
So let's see how we can


226
00:07:08,046 --> 00:07:09,616
decompose the size of a Core ML


227
00:07:09,616 --> 00:07:11,026
app into factors to better


228
00:07:11,026 --> 00:07:11,886
tackle this problem.


229
00:07:13,386 --> 00:07:14,276
First, there is the number of


230
00:07:14,276 --> 00:07:14,666
models.


231
00:07:14,906 --> 00:07:16,046
This depends on how many


232
00:07:16,716 --> 00:07:18,006
machine-learning functionalities


233
00:07:18,006 --> 00:07:18,536
your app has.


234
00:07:19,286 --> 00:07:20,186
Then there is the number of


235
00:07:20,266 --> 00:07:20,576
weights.


236
00:07:21,466 --> 00:07:23,156
The number of weights depends on


237
00:07:23,156 --> 00:07:24,166
the architecture that you have


238
00:07:24,166 --> 00:07:24,926
chosen to solve your


239
00:07:24,926 --> 00:07:25,816
machine-learning problem.


240
00:07:26,446 --> 00:07:29,026
As Michael was mentioning, the


241
00:07:29,026 --> 00:07:30,216
number of weight -- the weights


242
00:07:30,416 --> 00:07:31,336
are the place in which the


243
00:07:31,336 --> 00:07:33,236
machine-learning model stores


244
00:07:33,236 --> 00:07:34,796
the information that it has been


245
00:07:34,796 --> 00:07:35,666
learning during training.


246
00:07:36,336 --> 00:07:38,036
So it is -- if it has been


247
00:07:38,036 --> 00:07:39,826
trained to do a complex task,


248
00:07:39,966 --> 00:07:41,556
it's not uncommon to see a model


249
00:07:41,556 --> 00:07:43,366
requiring tens of millions of


250
00:07:43,416 --> 00:07:43,696
weights.


251
00:07:45,216 --> 00:07:46,736
Finally, there is the size of


252
00:07:46,776 --> 00:07:47,196
the weight.


253
00:07:47,386 --> 00:07:48,456
How are we storing these


254
00:07:48,886 --> 00:07:50,086
parameters that we are learning


255
00:07:50,086 --> 00:07:50,666
during training?


256
00:07:51,796 --> 00:07:52,846
Let's focus on this factor


257
00:07:52,846 --> 00:07:53,166
first.


258
00:07:54,466 --> 00:07:55,776
For neural networks, we have


259
00:07:55,776 --> 00:07:57,346
several options to represent and


260
00:07:57,346 --> 00:07:57,976
store the weights.


261
00:07:59,476 --> 00:08:00,966
And the first, really, is of


262
00:08:00,966 --> 00:08:02,606
Core ML in iOS 11.


263
00:08:03,486 --> 00:08:04,816
Neural networks were stored


264
00:08:04,816 --> 00:08:07,296
using floating-point 32-bit


265
00:08:07,506 --> 00:08:07,856
weights.


266
00:08:08,496 --> 00:08:12,466
In iOS 11.2, we heard your


267
00:08:12,466 --> 00:08:13,856
feedback and we introduced half


268
00:08:13,856 --> 00:08:16,006
precision floating-point 16


269
00:08:16,006 --> 00:08:16,246
weight.


270
00:08:16,676 --> 00:08:18,986
This gives your app half the


271
00:08:18,986 --> 00:08:21,126
storage required for the same


272
00:08:21,126 --> 00:08:21,646
accuracy.


273
00:08:22,236 --> 00:08:23,756
But this year we wanted to take


274
00:08:23,756 --> 00:08:25,616
several steps further, and we


275
00:08:25,616 --> 00:08:27,116
are introducing quantized


276
00:08:27,116 --> 00:08:27,336
weights.


277
00:08:28,576 --> 00:08:29,836
With quantized weights we are no


278
00:08:29,836 --> 00:08:31,526
longer restricted to use either


279
00:08:31,526 --> 00:08:33,756
Float 32 or Float 16 values.


280
00:08:34,155 --> 00:08:35,236
But neural networks can be


281
00:08:35,236 --> 00:08:37,956
encoded using 8 bits, 4 bits,


282
00:08:38,476 --> 00:08:40,515
any bits all the way down to 1


283
00:08:41,876 --> 00:08:41,976
bit.


284
00:08:42,186 --> 00:08:43,876
So let's now see what


285
00:08:43,876 --> 00:08:45,166
quantization here is.


286
00:08:46,336 --> 00:08:47,746
Here we are representing a


287
00:08:47,746 --> 00:08:49,176
subset of the weights of our


288
00:08:49,176 --> 00:08:49,986
neural networks.


289
00:08:50,546 --> 00:08:52,186
As we can see, these weights can


290
00:08:52,186 --> 00:08:54,166
take any value in a continuous


291
00:08:54,166 --> 00:08:54,486
range.


292
00:08:55,536 --> 00:08:57,426
This means that in theory, a


293
00:08:57,426 --> 00:08:58,726
single weight can take an


294
00:08:58,726 --> 00:09:00,086
infinite number of possible


295
00:09:00,086 --> 00:09:00,576
values.


296
00:09:01,096 --> 00:09:02,506
So in practice, in neural


297
00:09:02,506 --> 00:09:04,476
networks we store weights using


298
00:09:04,476 --> 00:09:07,186
floater 32 point -- Float 32


299
00:09:07,186 --> 00:09:07,586
numbers.


300
00:09:08,096 --> 00:09:09,256
This means that this weight can


301
00:09:09,256 --> 00:09:10,946
take billions of values to


302
00:09:10,946 --> 00:09:13,206
better represent the -- their


303
00:09:13,206 --> 00:09:14,036
continuous nature.


304
00:09:14,576 --> 00:09:16,146
But it turns out the neural


305
00:09:16,146 --> 00:09:18,006
networks also work with lower


306
00:09:18,006 --> 00:09:18,716
precision weights.


307
00:09:19,946 --> 00:09:21,776
Quantization is the process it


308
00:09:21,826 --> 00:09:23,536
takes to discontinue strings of


309
00:09:23,536 --> 00:09:25,566
values and constrains them to


310
00:09:25,566 --> 00:09:27,926
take a very small and discrete


311
00:09:28,016 --> 00:09:30,606
subset of possible values.


312
00:09:31,176 --> 00:09:32,956
For example, here quantization


313
00:09:32,956 --> 00:09:34,156
has turned this continuous


314
00:09:34,156 --> 00:09:37,156
spectrum of weights into only


315
00:09:37,156 --> 00:09:39,196
256 possible values.


316
00:09:39,536 --> 00:09:41,156
So before quantization, the


317
00:09:41,156 --> 00:09:42,556
weights would take any possible


318
00:09:42,556 --> 00:09:42,986
values.


319
00:09:43,316 --> 00:09:44,906
After quantization, they only


320
00:09:44,906 --> 00:09:46,926
have 256 options.


321
00:09:47,976 --> 00:09:50,326
Now since its weight can be


322
00:09:50,366 --> 00:09:52,796
taken from this small set, Core


323
00:09:52,796 --> 00:09:54,366
ML now needs only 8 bits of


324
00:09:54,366 --> 00:09:55,606
stored information of a weight.


325
00:09:56,876 --> 00:09:58,856
But nothing can stop us here.


326
00:09:59,296 --> 00:09:59,976
We can go further.


327
00:10:00,066 --> 00:10:01,966
And for example, we can


328
00:10:02,436 --> 00:10:04,276
constrain the network to take,


329
00:10:04,276 --> 00:10:06,156
instead of one of 56 different


330
00:10:06,156 --> 00:10:08,056
values, for example, just 8.


331
00:10:08,536 --> 00:10:11,246
And since now we now have only 8


332
00:10:11,246 --> 00:10:13,926
options, Core ML will need 3-bit


333
00:10:13,966 --> 00:10:15,966
values per weight to store your


334
00:10:15,966 --> 00:10:16,306
model.


335
00:10:17,556 --> 00:10:19,726
There are now some details about


336
00:10:19,726 --> 00:10:20,806
how we are going to choose these


337
00:10:20,806 --> 00:10:22,316
values to represent the weights.


338
00:10:22,786 --> 00:10:24,486
They can be uniformly


339
00:10:24,516 --> 00:10:26,256
distributed in this range, and


340
00:10:26,256 --> 00:10:27,576
in this case we have linear


341
00:10:27,576 --> 00:10:30,276
quantization instead in lookup


342
00:10:30,356 --> 00:10:33,856
table quantization, we can have


343
00:10:33,906 --> 00:10:35,356
these values scattered in this


344
00:10:35,356 --> 00:10:37,146
range in an arbitrary manner.


345
00:10:37,766 --> 00:10:39,246
So let's see practically how


346
00:10:39,246 --> 00:10:40,906
quantization can help us reduce


347
00:10:41,066 --> 00:10:41,826
the size of our model.


348
00:10:41,826 --> 00:10:43,256
In this example, you are


349
00:10:43,256 --> 00:10:45,506
focusing on Resnet50, which is a


350
00:10:45,506 --> 00:10:47,186
common architecture used by many


351
00:10:47,186 --> 00:10:48,526
applications for many different


352
00:10:48,526 --> 00:10:48,846
tasks.


353
00:10:50,056 --> 00:10:52,106
It includes 25 million trained


354
00:10:52,106 --> 00:10:54,426
parameters and this means that


355
00:10:54,426 --> 00:10:56,186
you have to use 32-bit floats to


356
00:10:56,186 --> 00:10:56,916
represent it.


357
00:10:57,946 --> 00:10:59,386
Then the total model size is


358
00:10:59,386 --> 00:11:00,616
more than 100 megabytes.


359
00:11:01,196 --> 00:11:03,856
If we quantize it to 8-bits,


360
00:11:04,396 --> 00:11:05,616
then the architecture hasn't


361
00:11:05,616 --> 00:11:07,476
changed; we still have 25


362
00:11:07,476 --> 00:11:08,886
million parameters.


363
00:11:09,616 --> 00:11:11,606
But we are now using only 1 byte


364
00:11:12,686 --> 00:11:14,236
to store a single weight, and


365
00:11:14,236 --> 00:11:15,396
this means that the model size


366
00:11:15,396 --> 00:11:16,856
is reduced by a factor of 4x.


367
00:11:17,326 --> 00:11:18,686
It's only -- it now only takes


368
00:11:18,686 --> 00:11:20,066
26 megabytes to store this


369
00:11:20,066 --> 00:11:20,366
model.


370
00:11:20,906 --> 00:11:22,046
And we can go further.


371
00:11:22,426 --> 00:11:23,386
We can use that quantized


372
00:11:23,386 --> 00:11:24,926
representation that only uses 4


373
00:11:24,926 --> 00:11:26,716
bits per weight in this model


374
00:11:27,286 --> 00:11:28,586
and end up with a model that is


375
00:11:28,586 --> 00:11:28,976
even smaller.


376
00:11:29,516 --> 00:11:36,016
[ Applause ]


377
00:11:36,516 --> 00:11:38,986
And again, Core ML supports all


378
00:11:38,986 --> 00:11:40,626
the quantization modes all the


379
00:11:40,626 --> 00:11:43,876
way down to 8 bits.


380
00:11:44,296 --> 00:11:47,096
Now quantization is a powerful


381
00:11:47,096 --> 00:11:48,886
technique to take an existing


382
00:11:48,886 --> 00:11:50,286
architecture and of a smaller


383
00:11:50,286 --> 00:11:50,896
version of it.


384
00:11:51,396 --> 00:11:52,616
But how can you obtain quantized


385
00:11:52,616 --> 00:11:52,896
model?


386
00:11:54,516 --> 00:11:56,486
If you have any neural


387
00:11:56,486 --> 00:11:58,306
networking in Core ML format,


388
00:11:58,306 --> 00:11:59,626
you can use Core ML Tools to


389
00:11:59,626 --> 00:12:00,586
obtain a quantized


390
00:12:00,586 --> 00:12:01,476
representation of it.


391
00:12:01,716 --> 00:12:03,496
So Core ML 2 should quantize for


392
00:12:03,496 --> 00:12:04,216
you automatically.


393
00:12:05,516 --> 00:12:07,386
Or you can train quantized


394
00:12:07,386 --> 00:12:07,756
models.


395
00:12:09,316 --> 00:12:10,596
You can either train quantized


396
00:12:10,596 --> 00:12:11,416
-- with a quantization


397
00:12:11,416 --> 00:12:12,866
constraint from scratch, or


398
00:12:12,866 --> 00:12:14,256
retrain existing models with


399
00:12:14,256 --> 00:12:15,406
quantization constraints.


400
00:12:16,376 --> 00:12:17,426
After you have obtained your


401
00:12:17,426 --> 00:12:18,336
quantized model with your


402
00:12:18,336 --> 00:12:19,556
training tools, you can then


403
00:12:19,556 --> 00:12:21,616
convert it to Core ML as usual.


404
00:12:22,156 --> 00:12:23,466
And nothing will change in the


405
00:12:23,466 --> 00:12:24,596
app in the way you use the


406
00:12:24,596 --> 00:12:24,976
model.


407
00:12:25,626 --> 00:12:27,466
Inside the model, the numbers


408
00:12:27,466 --> 00:12:28,716
are going to be stored in


409
00:12:28,716 --> 00:12:29,826
different precision, but the


410
00:12:29,826 --> 00:12:31,816
interface for using the model


411
00:12:32,176 --> 00:12:32,976
will not change at all.


412
00:12:35,216 --> 00:12:36,856
However, we always have to


413
00:12:36,856 --> 00:12:38,426
consider that quantized models


414
00:12:38,986 --> 00:12:40,016
have lower-precision


415
00:12:40,016 --> 00:12:41,586
approximations of the original


416
00:12:41,586 --> 00:12:43,446
reference floating-point models.


417
00:12:44,256 --> 00:12:45,686
And this means that quantized


418
00:12:45,686 --> 00:12:47,166
models come with an accuracy


419
00:12:47,166 --> 00:12:48,316
versus size-of-the-model


420
00:12:48,316 --> 00:12:48,836
tradeoff.


421
00:12:49,736 --> 00:12:51,436
This tradeoff is model dependent


422
00:12:51,546 --> 00:12:53,096
and use case dependent.


423
00:12:53,096 --> 00:12:55,166
And it's also a very active area


424
00:12:55,166 --> 00:12:55,706
of research.


425
00:12:56,456 --> 00:12:57,846
So it's always recommended to


426
00:12:57,936 --> 00:12:58,846
check the accuracy of the


427
00:12:58,846 --> 00:13:00,786
quantized model and compare it


428
00:13:00,786 --> 00:13:02,396
with the referenced


429
00:13:02,396 --> 00:13:04,186
floating-point version for


430
00:13:04,186 --> 00:13:05,586
relevant this data and form


431
00:13:05,586 --> 00:13:07,076
metrics that are valid for your


432
00:13:07,076 --> 00:13:07,876
app and use case.


433
00:13:08,966 --> 00:13:10,786
Now let's see a demo of how we


434
00:13:10,786 --> 00:13:13,586
can use -- adopt quantized


435
00:13:13,586 --> 00:13:14,796
models to reduce the size of an


436
00:13:14,796 --> 00:13:14,936
app.


437
00:13:15,516 --> 00:13:18,500
[ Applause ]


438
00:13:25,236 --> 00:13:26,556
I would like to show you a style


439
00:13:26,556 --> 00:13:27,566
transfer app.


440
00:13:28,116 --> 00:13:29,416
In style transfer, a neural


441
00:13:29,416 --> 00:13:31,356
network has been trained to


442
00:13:31,356 --> 00:13:33,456
render user images using styles


443
00:13:33,456 --> 00:13:34,326
that have been learned by


444
00:13:34,326 --> 00:13:35,646
watching paintings or other


445
00:13:35,646 --> 00:13:36,086
images.


446
00:13:36,696 --> 00:13:37,616
So let me load my app.


447
00:13:37,616 --> 00:13:41,216
As we can see, I am shipping


448
00:13:41,216 --> 00:13:43,226
this app with four styles; City,


449
00:13:43,226 --> 00:13:45,066
Glass, Oils and Waves.


450
00:13:45,656 --> 00:13:47,606
And then I can pick images from


451
00:13:47,606 --> 00:13:49,256
the photo library of the users


452
00:13:49,256 --> 00:13:51,096
and then process them blending


453
00:13:51,096 --> 00:13:52,736
them in different styles right


454
00:13:52,736 --> 00:13:53,226
on device.


455
00:13:53,976 --> 00:13:55,406
So this is the original image,


456
00:13:56,136 --> 00:13:57,076
and I am going to render the


457
00:13:57,076 --> 00:13:57,716
City style,


458
00:13:59,636 --> 00:13:59,976
Glass,


459
00:14:02,336 --> 00:14:02,706
Oils,


460
00:14:04,506 --> 00:14:04,956
and Waves.


461
00:14:07,076 --> 00:14:08,746
Let's see how this app has been


462
00:14:08,746 --> 00:14:09,516
built in Xcode.


463
00:14:10,966 --> 00:14:13,296
This app uses Core ML and Vision


464
00:14:13,396 --> 00:14:15,406
API to perform this stylization.


465
00:14:16,226 --> 00:14:17,786
And as we can see, we have four


466
00:14:17,786 --> 00:14:19,586
Core ML models here bundled in


467
00:14:19,586 --> 00:14:21,436
Xcode; City, Glass, Oils, and


468
00:14:21,436 --> 00:14:22,706
Waves, the same ones we are


469
00:14:22,706 --> 00:14:23,406
seeing in the app.


470
00:14:23,406 --> 00:14:25,796
And we can see -- we can inspect


471
00:14:25,796 --> 00:14:26,336
this model.


472
00:14:26,766 --> 00:14:27,976
These are seen as quantized


473
00:14:27,976 --> 00:14:29,166
model, so each one of these


474
00:14:29,166 --> 00:14:31,246
models is 6.7 megabytes of this


475
00:14:31,246 --> 00:14:31,876
space on disk.


476
00:14:33,096 --> 00:14:34,266
We see that the models take an


477
00:14:34,266 --> 00:14:35,226
input image of a certain


478
00:14:35,226 --> 00:14:37,706
resolution and produce an image


479
00:14:37,706 --> 00:14:39,176
called Stylized of the same


480
00:14:39,176 --> 00:14:39,716
resolution.


481
00:14:41,016 --> 00:14:43,106
Now we want to investigate how


482
00:14:43,106 --> 00:14:44,736
much storage space and memory


483
00:14:44,736 --> 00:14:46,366
space we can use by -- we can


484
00:14:46,366 --> 00:14:47,796
save by switching to quantized,


485
00:14:47,796 --> 00:14:48,096
models.


486
00:14:48,096 --> 00:14:49,716
So I have been playing with Core


487
00:14:49,716 --> 00:14:52,656
ML Tools and obtained quantizer


488
00:14:52,656 --> 00:14:55,236
presentation for these models.


489
00:14:56,216 --> 00:14:57,486
And for a tutorial about how to


490
00:14:57,486 --> 00:14:59,446
obtain these models, stay for


491
00:14:59,446 --> 00:15:01,056
Part 2 that is going to cover


492
00:15:01,056 --> 00:15:02,766
quantization with Core ML Tools


493
00:15:02,766 --> 00:15:02,976
in detail.


494
00:15:04,126 --> 00:15:05,396
So I want to focus first on the


495
00:15:05,396 --> 00:15:07,856
Glass style and see how the


496
00:15:07,856 --> 00:15:09,416
different quantization versions


497
00:15:09,696 --> 00:15:10,746
work for these styles.


498
00:15:11,836 --> 00:15:13,646
So all I have to do is drag


499
00:15:13,646 --> 00:15:15,076
these new models inside the


500
00:15:15,076 --> 00:15:17,756
Xcode project, and rerun the


501
00:15:17,876 --> 00:15:17,943
app.


502
00:15:18,046 --> 00:15:19,976
And then we are going to see how


503
00:15:19,976 --> 00:15:20,806
these models behave.


504
00:15:21,296 --> 00:15:23,986
First we can see that the size


505
00:15:23,986 --> 00:15:25,856
has been greatly reduced.


506
00:15:25,856 --> 00:15:27,906
For example, the 8-bit version


507
00:15:27,906 --> 00:15:29,906
already from 6 or 7 megabytes


508
00:15:29,906 --> 00:15:30,976
went down to just 1.7.


509
00:15:31,516 --> 00:15:36,576
[ Applause ]


510
00:15:37,076 --> 00:15:39,226
In 4-bit, we can save even more,


511
00:15:39,226 --> 00:15:40,876
and now the model is less than 1


512
00:15:40,876 --> 00:15:41,336
megabyte.


513
00:15:41,936 --> 00:15:43,286
In 3-bit, it is -- that's even


514
00:15:43,286 --> 00:15:44,956
smaller, at 49 kilobytes.


515
00:15:44,956 --> 00:15:45,946
And so on.


516
00:15:47,106 --> 00:15:49,326
Now let's go back to the app.


517
00:15:50,826 --> 00:15:52,616
Let's make this same image for


518
00:15:52,616 --> 00:15:54,486
reference and apply the Glass


519
00:15:54,536 --> 00:15:56,276
style in the original version.


520
00:15:57,136 --> 00:15:58,266
Still looks as before.


521
00:15:59,016 --> 00:16:00,676
Now we can compare it with the


522
00:16:00,676 --> 00:16:01,716
8-bit version.


523
00:16:02,036 --> 00:16:05,796
And you can see nothing has


524
00:16:05,866 --> 00:16:06,186
changed.


525
00:16:06,796 --> 00:16:07,806
This is because 8-bit


526
00:16:07,806 --> 00:16:09,156
quantization methods are very


527
00:16:09,156 --> 00:16:09,536
solid.


528
00:16:10,806 --> 00:16:13,176
We can also venture further and


529
00:16:13,176 --> 00:16:14,956
try the 4-bit version of this


530
00:16:14,956 --> 00:16:15,316
model.


531
00:16:15,866 --> 00:16:18,256
Wow. The results are still


532
00:16:18,256 --> 00:16:18,626
great.


533
00:16:19,896 --> 00:16:21,366
And now let's try the 3-bit


534
00:16:21,366 --> 00:16:21,836
version.


535
00:16:24,556 --> 00:16:26,386
We see that there are -- we see


536
00:16:26,386 --> 00:16:27,506
the first color shift.


537
00:16:27,506 --> 00:16:29,116
So it probably is good if we go


538
00:16:29,116 --> 00:16:30,426
and check with the designers if


539
00:16:30,426 --> 00:16:32,446
this effect is still acceptable.


540
00:16:33,196 --> 00:16:34,596
And now, as we see the 2-bit


541
00:16:34,596 --> 00:16:37,256
version, this is not really what


542
00:16:37,256 --> 00:16:37,926
we were looking for.


543
00:16:37,926 --> 00:16:39,056
Maybe we will save it for a


544
00:16:39,056 --> 00:16:40,376
horror app, but I am not going


545
00:16:40,376 --> 00:16:40,976
to show this to the designer.


546
00:16:41,516 --> 00:16:46,026
[ Applause ]


547
00:16:46,526 --> 00:16:47,526
Let's go back to the 4-bit


548
00:16:47,566 --> 00:16:48,626
version and hide this one.


549
00:16:49,196 --> 00:16:51,056
This was just a reminder that


550
00:16:51,056 --> 00:16:52,686
quantized models are


551
00:16:52,686 --> 00:16:54,156
approximation of the original


552
00:16:54,156 --> 00:16:54,356
models.


553
00:16:55,216 --> 00:16:56,366
So it's always recommended to


554
00:16:56,416 --> 00:16:57,866
check them and compare with the


555
00:16:57,866 --> 00:16:58,696
original versions.


556
00:16:59,106 --> 00:17:00,716
Now for every model and


557
00:17:00,716 --> 00:17:02,156
quantization technique, there is


558
00:17:02,156 --> 00:17:03,356
always a point in which things


559
00:17:03,356 --> 00:17:06,526
start to mismatch.


560
00:17:06,526 --> 00:17:08,376
Now we -- after some discussion


561
00:17:08,376 --> 00:17:09,526
with the designer, extensive


562
00:17:09,526 --> 00:17:10,846
evaluation of many images, we


563
00:17:10,846 --> 00:17:12,156
decided to ship the 4-bit


564
00:17:12,156 --> 00:17:13,415
version of this model, which is


565
00:17:13,715 --> 00:17:15,086
the smallest size for the best


566
00:17:15,086 --> 00:17:15,506
quality.


567
00:17:16,955 --> 00:17:17,886
So let's remove all the


568
00:17:17,886 --> 00:17:19,566
floating-point version of the


569
00:17:19,566 --> 00:17:21,445
models that were taking a lot of


570
00:17:21,445 --> 00:17:23,996
space in our app and replace


571
00:17:23,996 --> 00:17:25,976
them with the 4-bit version.


572
00:17:30,736 --> 00:17:32,516
And now let's run the app one


573
00:17:32,516 --> 00:17:32,956
last time.


574
00:17:40,226 --> 00:17:41,846
OK. Let's pick the same image


575
00:17:41,846 --> 00:17:48,886
again and show all the styles.


576
00:17:48,886 --> 00:17:54,186
This was the City, Glass, Oils,


577
00:17:54,926 --> 00:17:56,996
and big Wave.


578
00:17:58,306 --> 00:18:02,076
So in this demo we saw how we


579
00:18:02,076 --> 00:18:04,336
started with four models and


580
00:18:04,336 --> 00:18:06,606
they were huge, in 32-bit -- or


581
00:18:06,606 --> 00:18:08,716
total app size was 27 megabytes.


582
00:18:09,426 --> 00:18:10,966
Then we evaluated the quality


583
00:18:10,966 --> 00:18:12,466
and switched to 4-bit models,


584
00:18:13,026 --> 00:18:14,406
and the total size of our app


585
00:18:14,406 --> 00:18:16,526
went down to just 3.4 megabytes.


586
00:18:16,806 --> 00:18:16,873
Now --


587
00:18:17,516 --> 00:18:22,646
[ Applause ]


588
00:18:23,146 --> 00:18:24,646
This doesn't cost us anything in


589
00:18:24,646 --> 00:18:26,756
terms of quality because all


590
00:18:26,756 --> 00:18:27,646
these versions -- these


591
00:18:27,646 --> 00:18:29,456
quantized versions look the


592
00:18:29,456 --> 00:18:31,626
same, and the quality is still


593
00:18:31,626 --> 00:18:32,076
amazing.


594
00:18:32,616 --> 00:18:36,006
We showed how quantization can


595
00:18:36,006 --> 00:18:37,446
help us reduce the size of an


596
00:18:37,446 --> 00:18:39,166
app by reducing the size of the


597
00:18:39,166 --> 00:18:40,796
weight at the very microscopic


598
00:18:40,796 --> 00:18:41,046
level.


599
00:18:41,046 --> 00:18:44,356
Now let's see how we can reduce


600
00:18:44,356 --> 00:18:45,546
the number of models that your


601
00:18:45,546 --> 00:18:45,976
app needs.


602
00:18:46,666 --> 00:18:48,866
In the most straightforward


603
00:18:48,866 --> 00:18:51,446
case, if your app has three


604
00:18:51,446 --> 00:18:52,936
machine-learning functionalities


605
00:18:53,306 --> 00:18:54,496
then you need three different


606
00:18:54,496 --> 00:18:55,536
machine-learning models.


607
00:18:56,156 --> 00:18:58,206
But in some cases, it is


608
00:18:58,206 --> 00:19:01,206
possible to have the same model


609
00:19:01,426 --> 00:19:03,116
to support two different


610
00:19:03,116 --> 00:19:03,676
functions.


611
00:19:04,406 --> 00:19:06,006
For example, you can train a


612
00:19:06,006 --> 00:19:06,906
multi-task model.


613
00:19:06,906 --> 00:19:09,316
And multi-task models has been


614
00:19:09,316 --> 00:19:10,936
trained to perform multiple


615
00:19:10,936 --> 00:19:11,626
things at once.


616
00:19:12,446 --> 00:19:13,656
There is an example about style


617
00:19:13,656 --> 00:19:14,776
transferring, the Turi Create


618
00:19:14,836 --> 00:19:16,276
session about multi-task models.


619
00:19:16,936 --> 00:19:19,366
Or in some cases, you can use a


620
00:19:19,366 --> 00:19:20,856
yet new feature in Core ML


621
00:19:21,146 --> 00:19:22,596
called Flexible Shapes and


622
00:19:22,596 --> 00:19:23,076
Sizes.


623
00:19:24,386 --> 00:19:27,006
Let's go back to our Style


624
00:19:27,006 --> 00:19:27,726
Transfer demo.


625
00:19:28,166 --> 00:19:30,516
In Xcode we saw that the size of


626
00:19:30,516 --> 00:19:31,916
the input image and the output


627
00:19:31,916 --> 00:19:34,826
image was encoded in part of the


628
00:19:34,826 --> 00:19:35,966
definition of the model.


629
00:19:36,816 --> 00:19:38,066
But what if we want to run the


630
00:19:38,066 --> 00:19:39,666
same style on different image


631
00:19:39,666 --> 00:19:40,346
resolution?


632
00:19:41,056 --> 00:19:42,576
What if we want to run the same


633
00:19:42,576 --> 00:19:44,746
network on different image


634
00:19:44,746 --> 00:19:45,226
sizes?


635
00:19:46,796 --> 00:19:49,156
For example, the user might want


636
00:19:49,156 --> 00:19:50,726
to see a high-definition style


637
00:19:50,726 --> 00:19:51,186
transfer.


638
00:19:51,826 --> 00:19:53,906
So they use -- they give us a


639
00:19:53,906 --> 00:19:54,906
high-definition image.


640
00:19:55,646 --> 00:19:57,316
Now if I were Core ML model all


641
00:19:57,316 --> 00:19:58,546
it takes is a lower resolution


642
00:19:58,546 --> 00:20:00,766
as an input, all we can do as


643
00:20:00,766 --> 00:20:03,436
developers is size -- or resize


644
00:20:03,436 --> 00:20:06,236
the image down, process it, and


645
00:20:06,236 --> 00:20:06,976
then scale it back up.


646
00:20:07,786 --> 00:20:09,346
This is not really going to


647
00:20:09,346 --> 00:20:10,206
amaze the user.


648
00:20:10,206 --> 00:20:14,336
Even in the past, we could


649
00:20:14,336 --> 00:20:16,146
reship this model with Corel ML


650
00:20:16,146 --> 00:20:18,206
Tools and make it accept any


651
00:20:18,206 --> 00:20:20,396
resolution, in particular, a


652
00:20:20,396 --> 00:20:21,826
higher-resolution image.


653
00:20:23,246 --> 00:20:24,756
So even in the past we could do


654
00:20:24,756 --> 00:20:27,096
this feature and feed directly


655
00:20:27,096 --> 00:20:28,716
the high-resolution image to a


656
00:20:28,716 --> 00:20:30,446
Corel ML model, producing a


657
00:20:30,586 --> 00:20:31,576
high-definition result.


658
00:20:31,576 --> 00:20:34,366
This is because we wanted to


659
00:20:34,366 --> 00:20:36,436
introduce a finer detail in the


660
00:20:36,436 --> 00:20:38,706
stylization and the way finer


661
00:20:38,706 --> 00:20:40,966
strokes that are amazing when


662
00:20:40,966 --> 00:20:42,406
you zoom in, because they have


663
00:20:42,706 --> 00:20:43,826
-- they add a lot of work into


664
00:20:43,826 --> 00:20:44,516
the final image.


665
00:20:44,516 --> 00:20:47,646
So in the past we could do it,


666
00:20:47,646 --> 00:20:49,656
but we could do it by


667
00:20:49,656 --> 00:20:51,116
duplicating the model and


668
00:20:51,116 --> 00:20:52,606
creating two different versions:


669
00:20:53,096 --> 00:20:54,576
one for the standard definition


670
00:20:54,576 --> 00:20:55,376
and one for the high


671
00:20:55,376 --> 00:20:56,046
definitions.


672
00:20:56,516 --> 00:20:57,796
And this, of course, means that


673
00:20:57,796 --> 00:20:59,726
our app is twice as much the


674
00:20:59,726 --> 00:21:01,356
size -- besides the fact that


675
00:21:01,356 --> 00:21:02,516
the network has been trained to


676
00:21:02,516 --> 00:21:03,736
support any resolution.


677
00:21:04,526 --> 00:21:05,186
Not anymore.


678
00:21:05,586 --> 00:21:06,866
We are introducing flexible


679
00:21:06,866 --> 00:21:07,246
shapes.


680
00:21:07,816 --> 00:21:09,036
And with flexible shapes, you


681
00:21:09,036 --> 00:21:10,526
have -- if you have -- you can


682
00:21:10,526 --> 00:21:12,286
have the single model to process


683
00:21:12,286 --> 00:21:13,776
more resolutions and many more


684
00:21:13,776 --> 00:21:14,426
resolutions.


685
00:21:14,806 --> 00:21:15,926
So now in Xcode --


686
00:21:16,516 --> 00:21:20,886
[ Applause ]


687
00:21:21,386 --> 00:21:23,416
-- in Xcode you are going to see


688
00:21:24,366 --> 00:21:26,016
that this -- the input is still


689
00:21:26,016 --> 00:21:27,986
an image, but the size of the


690
00:21:27,986 --> 00:21:29,746
full resolution, the model also


691
00:21:29,746 --> 00:21:31,896
accepts flexible resolutions.


692
00:21:32,066 --> 00:21:33,806
In this simple example, SD and


693
00:21:34,106 --> 00:21:34,173
HD.


694
00:21:34,173 --> 00:21:37,566
This means that now you have to


695
00:21:37,566 --> 00:21:38,786
ship a single model.


696
00:21:38,786 --> 00:21:41,426
You don't have to have any


697
00:21:41,426 --> 00:21:42,266
redundant code.


698
00:21:43,316 --> 00:21:44,316
And if you need to switch


699
00:21:44,316 --> 00:21:45,726
between standard definition and


700
00:21:45,726 --> 00:21:46,906
high definition, you can do it


701
00:21:46,906 --> 00:21:48,386
much faster because we don't


702
00:21:48,386 --> 00:21:49,456
need to reload the model from


703
00:21:49,456 --> 00:21:50,876
scratch; we just need to resize


704
00:21:51,496 --> 00:21:51,566
it.


705
00:21:52,236 --> 00:21:54,186
You have two options to specify


706
00:21:54,186 --> 00:21:55,716
the flexibility of the model.


707
00:21:57,026 --> 00:21:58,556
You can define a range for its


708
00:21:58,556 --> 00:22:00,416
dimension, so you can define a


709
00:22:00,416 --> 00:22:02,036
minimal width and height and the


710
00:22:02,036 --> 00:22:03,096
maximum width and height.


711
00:22:03,646 --> 00:22:05,356
And then at inference pick any


712
00:22:05,356 --> 00:22:06,136
value in between.


713
00:22:06,766 --> 00:22:08,276
But there is also another way.


714
00:22:08,576 --> 00:22:10,486
You can enumerate all the shapes


715
00:22:10,486 --> 00:22:11,326
that you are going to use.


716
00:22:11,736 --> 00:22:12,936
For example, all different


717
00:22:12,936 --> 00:22:14,606
aspect ratios, all different


718
00:22:14,606 --> 00:22:16,536
resolutions, and this is better


719
00:22:16,536 --> 00:22:17,306
for performance.


720
00:22:17,586 --> 00:22:19,086
Core ML knows more about your


721
00:22:19,086 --> 00:22:21,006
use case earlier, so it can --


722
00:22:21,006 --> 00:22:22,236
it has the opportunities of


723
00:22:22,236 --> 00:22:24,106
performing more optimizations.


724
00:22:24,926 --> 00:22:26,496
And it also gives your app a


725
00:22:26,496 --> 00:22:27,776
smaller tested surface.


726
00:22:28,286 --> 00:22:30,956
Now which models are flexible?


727
00:22:30,956 --> 00:22:32,996
Which models can be trained to


728
00:22:32,996 --> 00:22:34,526
support multiple resolutions?


729
00:22:35,206 --> 00:22:37,426
Fully convolutional neural


730
00:22:37,426 --> 00:22:39,746
networks, commonly used for MS


731
00:22:39,746 --> 00:22:41,636
processing tasks such as style


732
00:22:41,636 --> 00:22:43,746
transfer, image enhancement,


733
00:22:43,916 --> 00:22:45,786
super resolution, and so on --


734
00:22:46,016 --> 00:22:47,716
and some of the architecture.


735
00:22:48,446 --> 00:22:50,956
Core ML Tools can check if a


736
00:22:50,956 --> 00:22:52,636
model has this capability for


737
00:22:52,636 --> 00:22:52,786
you.


738
00:22:54,236 --> 00:22:55,516
So we still have the number of


739
00:22:55,516 --> 00:22:57,126
models Core ML uses in flexible


740
00:22:57,126 --> 00:22:58,606
sizes, and the size of the


741
00:22:58,606 --> 00:22:59,516
weights can be reduced by


742
00:22:59,516 --> 00:23:00,246
quantization.


743
00:23:00,646 --> 00:23:01,606
But what about the number of


744
00:23:01,686 --> 00:23:01,956
weights?


745
00:23:03,176 --> 00:23:05,426
Core ML, given the fact that it


746
00:23:05,426 --> 00:23:07,006
supports many, many different


747
00:23:07,006 --> 00:23:08,356
architecture at any framework,


748
00:23:08,956 --> 00:23:10,886
has always helped you choose the


749
00:23:10,886 --> 00:23:12,556
right -- the model of the right


750
00:23:12,606 --> 00:23:13,706
size for your machine-learning


751
00:23:13,706 --> 00:23:14,086
problem.


752
00:23:14,636 --> 00:23:17,206
So Core ML can help you tackle


753
00:23:17,206 --> 00:23:19,376
the size of your app using this


754
00:23:20,036 --> 00:23:21,106
-- all these three factors.


755
00:23:21,476 --> 00:23:23,046
In any case, the inference is


756
00:23:23,046 --> 00:23:24,246
going to be super performant.


757
00:23:24,246 --> 00:23:26,706
And to introduce new features in


758
00:23:26,706 --> 00:23:28,066
performance and customization,


759
00:23:28,126 --> 00:23:29,196
let's welcome Bill March.


760
00:23:29,746 --> 00:23:29,976
Thank you.


761
00:23:30,516 --> 00:23:36,776
[ Applause ]


762
00:23:37,276 --> 00:23:38,346
>> Thank you.


763
00:23:39,826 --> 00:23:41,056
One of the fundamental design


764
00:23:41,056 --> 00:23:42,416
principles of Core ML from the


765
00:23:42,416 --> 00:23:43,886
very beginning has been that it


766
00:23:43,886 --> 00:23:45,166
should give your app the best


767
00:23:45,166 --> 00:23:46,206
possible performance.


768
00:23:46,806 --> 00:23:48,106
And in keeping with that goal,


769
00:23:48,176 --> 00:23:49,256
I'd like to highlight a new


770
00:23:49,256 --> 00:23:50,526
feature of Core ML to help


771
00:23:50,526 --> 00:23:52,176
ensure that your app will shine


772
00:23:52,176 --> 00:23:53,376
on any Apple device.


773
00:23:54,286 --> 00:23:55,986
Let's take a look at the style


774
00:23:55,986 --> 00:23:57,426
transfer example that Francesco


775
00:23:57,426 --> 00:23:57,966
showed us.


776
00:23:58,356 --> 00:23:59,316
From the perspective of your


777
00:23:59,316 --> 00:24:01,186
app, it takes an image of an


778
00:24:01,186 --> 00:24:02,776
input and simply returns the


779
00:24:02,776 --> 00:24:03,806
stylized image.


780
00:24:04,346 --> 00:24:05,736
And there are two key components


781
00:24:05,736 --> 00:24:06,976
that go into making this happen:


782
00:24:07,416 --> 00:24:09,896
first, the MLModel file, which


783
00:24:09,896 --> 00:24:11,366
stores the particular parameters


784
00:24:11,366 --> 00:24:13,686
needed to apply this style; and


785
00:24:13,686 --> 00:24:15,626
second, the inference engine,


786
00:24:15,766 --> 00:24:17,546
which takes in the MLModel and


787
00:24:17,546 --> 00:24:18,746
the image and performs the


788
00:24:18,746 --> 00:24:20,096
calculations necessary to


789
00:24:20,166 --> 00:24:20,886
produce the result.


790
00:24:21,886 --> 00:24:23,006
So let's peek under the hood of


791
00:24:23,006 --> 00:24:24,286
this inference engine and see


792
00:24:24,286 --> 00:24:25,416
how we leverage Apple's


793
00:24:25,416 --> 00:24:27,146
technology to perform this style


794
00:24:27,146 --> 00:24:28,416
transfer efficiently.


795
00:24:30,156 --> 00:24:31,606
This model is an example of a


796
00:24:31,606 --> 00:24:33,126
neural network, which consists


797
00:24:33,126 --> 00:24:34,466
of a series of mathematical


798
00:24:34,466 --> 00:24:35,866
operations called layers.


799
00:24:36,406 --> 00:24:37,566
Each layer applies some


800
00:24:37,566 --> 00:24:39,026
transformation to the image,


801
00:24:39,026 --> 00:24:40,376
finally resulting in the


802
00:24:40,376 --> 00:24:41,566
stylized output.


803
00:24:41,996 --> 00:24:43,996
The model stores weights for


804
00:24:43,996 --> 00:24:45,506
each layer which determine the


805
00:24:45,556 --> 00:24:47,086
particular transformation and


806
00:24:47,086 --> 00:24:48,106
the style that we are going to


807
00:24:48,106 --> 00:24:48,496
apply.


808
00:24:49,576 --> 00:24:50,696
The Core ML neural network


809
00:24:50,696 --> 00:24:52,366
inference engine has highly


810
00:24:52,366 --> 00:24:53,866
optimized implementations for


811
00:24:53,866 --> 00:24:54,776
each of these layers.


812
00:24:55,176 --> 00:24:57,236
On the GPU, we use MTL shaders.


813
00:24:57,466 --> 00:24:58,686
On the CPU we can use


814
00:24:58,686 --> 00:24:59,946
Accelerate, the proficient


815
00:24:59,946 --> 00:25:00,676
calculation.


816
00:25:01,296 --> 00:25:02,626
And we can dispatch different


817
00:25:02,626 --> 00:25:03,836
parts of the computation to


818
00:25:03,836 --> 00:25:04,976
different pieces of hardware


819
00:25:04,976 --> 00:25:06,526
dynamically depending on the


820
00:25:06,526 --> 00:25:08,256
model, the device state, and


821
00:25:08,256 --> 00:25:08,986
other factors.


822
00:25:10,156 --> 00:25:12,736
We can also find opportunities


823
00:25:12,736 --> 00:25:14,406
to fuse layers in the network,


824
00:25:14,406 --> 00:25:15,836
resulting in fewer overall


825
00:25:15,836 --> 00:25:17,156
computations being needed.


826
00:25:18,206 --> 00:25:20,336
We are able to optimize here


827
00:25:20,336 --> 00:25:21,916
because we know what's going on.


828
00:25:22,206 --> 00:25:23,196
We know the details of the


829
00:25:23,196 --> 00:25:24,736
model; they are contained in the


830
00:25:24,736 --> 00:25:26,216
MLModel file that you provided


831
00:25:26,216 --> 00:25:26,576
to us.


832
00:25:27,016 --> 00:25:28,126
And we know the details of the


833
00:25:28,126 --> 00:25:30,036
inference engine and the device


834
00:25:30,176 --> 00:25:32,316
because we designed them.


835
00:25:32,316 --> 00:25:33,716
We can take care of all of these


836
00:25:33,716 --> 00:25:35,856
optimizations for you, and you


837
00:25:35,856 --> 00:25:37,376
can focus on delivering the best


838
00:25:37,376 --> 00:25:38,656
user experience in your app.


839
00:25:39,086 --> 00:25:41,586
But what about your workload?


840
00:25:42,436 --> 00:25:44,136
What about, in particular, if


841
00:25:44,136 --> 00:25:45,286
you need to make multiple


842
00:25:45,286 --> 00:25:45,986
predictions?


843
00:25:47,196 --> 00:25:48,406
If Core ML doesn't know about


844
00:25:48,406 --> 00:25:50,186
it, then Core ML can't optimize


845
00:25:50,186 --> 00:25:50,486
for it.


846
00:25:51,486 --> 00:25:53,096
So in the past, if you had a


847
00:25:53,096 --> 00:25:55,886
workload like this, you needed


848
00:25:55,886 --> 00:25:57,356
to do something like this: a


849
00:25:57,356 --> 00:25:58,926
simple for loop wrapped around a


850
00:25:58,926 --> 00:26:00,436
call to the existing Core ML


851
00:26:00,436 --> 00:26:01,246
prediction API.


852
00:26:01,246 --> 00:26:03,416
So you'd loop over some array of


853
00:26:03,416 --> 00:26:04,766
inputs and produce an array of


854
00:26:04,766 --> 00:26:05,336
outputs.


855
00:26:05,716 --> 00:26:08,166
Let's take a closer look at what


856
00:26:08,166 --> 00:26:10,216
happens under the hood when this


857
00:26:10,216 --> 00:26:11,206
-- when we are doing this.


858
00:26:12,146 --> 00:26:13,636
For each image, we will need to


859
00:26:13,636 --> 00:26:15,096
do some kind of preprocessing


860
00:26:15,096 --> 00:26:15,366
work.


861
00:26:15,686 --> 00:26:16,876
If nothing else, we need to send


862
00:26:16,876 --> 00:26:18,216
the data down to the GPU.


863
00:26:19,066 --> 00:26:20,316
Once we have done that, we can


864
00:26:20,316 --> 00:26:21,976
do the calculation and produce


865
00:26:21,976 --> 00:26:22,816
the output image.


866
00:26:23,046 --> 00:26:23,526
But then there is a


867
00:26:23,526 --> 00:26:25,546
postprocessing step in which we


868
00:26:25,546 --> 00:26:26,736
need to retrieve the data from


869
00:26:26,736 --> 00:26:28,046
the GPU and return it to your


870
00:26:28,046 --> 00:26:28,296
app.


871
00:26:29,816 --> 00:26:31,176
The key to improving this


872
00:26:31,176 --> 00:26:32,716
picture is to eliminate the


873
00:26:32,716 --> 00:26:34,526
bubbles in the GPU pipeline.


874
00:26:35,896 --> 00:26:36,876
This results in greater


875
00:26:36,876 --> 00:26:38,136
performance for two major


876
00:26:38,136 --> 00:26:38,726
reasons.


877
00:26:38,926 --> 00:26:40,696
First, since there is no time


878
00:26:40,696 --> 00:26:42,306
when the GPU is idle the overall


879
00:26:42,306 --> 00:26:43,516
compute time is reduced.


880
00:26:44,066 --> 00:26:45,996
And second, because the GPU is


881
00:26:45,996 --> 00:26:47,856
kept working continuously, it's


882
00:26:47,856 --> 00:26:49,006
able to operate in a higher


883
00:26:49,006 --> 00:26:51,036
performance state and reduce the


884
00:26:51,036 --> 00:26:52,886
time necessary to compute each


885
00:26:52,886 --> 00:26:54,146
particular output.


886
00:26:55,616 --> 00:26:56,596
But so much of the appeal in


887
00:26:56,596 --> 00:26:57,806
Core ML is that you don't have


888
00:26:57,806 --> 00:26:59,236
to worry about any details like


889
00:26:59,236 --> 00:26:59,706
this at all.


890
00:27:00,106 --> 00:27:01,706
In fact, for your app all you


891
00:27:01,706 --> 00:27:02,666
are really concerned with for


892
00:27:02,666 --> 00:27:05,336
your users is going from a long


893
00:27:05,336 --> 00:27:06,636
time to get results to a short


894
00:27:06,676 --> 00:27:06,886
time.


895
00:27:07,846 --> 00:27:10,096
So this year we are introducing


896
00:27:10,096 --> 00:27:11,796
a new batch API that will allow


897
00:27:11,796 --> 00:27:13,026
you to do exactly this.


898
00:27:14,096 --> 00:27:15,336
Where before you needed to loop


899
00:27:15,336 --> 00:27:16,386
over your inputs and call


900
00:27:16,386 --> 00:27:18,056
separate predictions, the new


901
00:27:18,056 --> 00:27:19,156
API is very simple.


902
00:27:20,466 --> 00:27:22,516
One-line predictions, it


903
00:27:22,516 --> 00:27:24,366
consumes an input -- an array of


904
00:27:24,366 --> 00:27:25,946
inputs and produces an array of


905
00:27:25,946 --> 00:27:26,566
outputs.


906
00:27:26,936 --> 00:27:27,826
Core ML will take care of the


907
00:27:27,826 --> 00:27:27,976
rest.


908
00:27:28,516 --> 00:27:34,336
[ Applause ]


909
00:27:34,836 --> 00:27:35,716
So let's see it in action.


910
00:27:36,446 --> 00:27:38,696
So in keeping with our style


911
00:27:38,696 --> 00:27:40,386
transfer example, let's look at


912
00:27:40,386 --> 00:27:41,376
the case where we wanted to


913
00:27:41,376 --> 00:27:42,776
apply a style to our entire


914
00:27:42,776 --> 00:27:43,586
photo library.


915
00:27:43,956 --> 00:27:45,046
So here I have a simple app


916
00:27:45,116 --> 00:27:46,066
that's going to do just that.


917
00:27:46,066 --> 00:27:47,926
I am going to apply a style to


918
00:27:47,926 --> 00:27:48,946
200 images.


919
00:27:49,206 --> 00:27:51,396
On the left, as in your left,


920
00:27:51,976 --> 00:27:54,026
there is an implementation using


921
00:27:54,026 --> 00:27:55,526
last year's API in a for loop.


922
00:27:55,936 --> 00:27:57,636
And on the right we have the new


923
00:27:57,636 --> 00:27:58,276
batch API.


924
00:27:58,626 --> 00:27:59,346
So let's get started.


925
00:28:00,466 --> 00:28:01,156
We are off.


926
00:28:03,596 --> 00:28:04,606
And we can see the new is


927
00:28:04,606 --> 00:28:05,176
already done.


928
00:28:05,176 --> 00:28:06,756
We'll wait a moment for last


929
00:28:06,756 --> 00:28:08,916
year's technology, and there we


930
00:28:08,916 --> 00:28:08,983
go.


931
00:28:09,516 --> 00:28:14,796
[ Applause ]


932
00:28:15,296 --> 00:28:16,556
In this example we see a


933
00:28:16,556 --> 00:28:18,006
noticeable improvement with the


934
00:28:18,006 --> 00:28:18,756
new batch API.


935
00:28:19,136 --> 00:28:20,836
And in general, the improvement


936
00:28:20,836 --> 00:28:22,096
you'll see in your app depends


937
00:28:22,096 --> 00:28:23,906
on the model and the device and


938
00:28:23,906 --> 00:28:24,546
the workload.


939
00:28:25,016 --> 00:28:26,186
But if you have a large number


940
00:28:26,186 --> 00:28:27,746
of predictions to call, use the


941
00:28:27,746 --> 00:28:29,606
new API and give Core ML every


942
00:28:29,606 --> 00:28:30,606
opportunity to accelerate your


943
00:28:30,606 --> 00:28:30,976
computation.


944
00:28:35,666 --> 00:28:36,796
Of course, the most


945
00:28:36,796 --> 00:28:37,806
high-performance app in the


946
00:28:37,806 --> 00:28:39,746
world isn't terribly exciting if


947
00:28:39,746 --> 00:28:40,596
it's not delivering an


948
00:28:40,596 --> 00:28:41,846
experience that you want for


949
00:28:41,846 --> 00:28:42,526
your users.


950
00:28:43,666 --> 00:28:44,966
We want to ensure that no matter


951
00:28:44,966 --> 00:28:46,846
what that experience is, or what


952
00:28:46,846 --> 00:28:48,546
it could be in the future, Core


953
00:28:48,546 --> 00:28:50,066
ML will be just as performant


954
00:28:50,066 --> 00:28:51,336
and simple to use as ever.


955
00:28:52,486 --> 00:28:53,286
But the field of machine


956
00:28:53,286 --> 00:28:54,656
learning is growing rapidly.


957
00:28:55,186 --> 00:28:56,086
How will we keep up?


958
00:28:56,786 --> 00:28:57,926
And just how rapidly?


959
00:28:57,976 --> 00:28:59,406
Well let me tell you a little


960
00:28:59,406 --> 00:29:00,636
bit of a personal story about


961
00:29:00,666 --> 00:29:00,876
that.


962
00:29:02,246 --> 00:29:03,496
Let's take a look at a


963
00:29:03,496 --> 00:29:05,196
deceptively simple question that


964
00:29:05,196 --> 00:29:06,346
we can answer with machine


965
00:29:06,346 --> 00:29:06,626
learning.


966
00:29:07,686 --> 00:29:09,256
Given an image, what I want to


967
00:29:09,256 --> 00:29:11,536
know: Are there any horses in


968
00:29:11,536 --> 00:29:11,603
it?


969
00:29:12,576 --> 00:29:14,346
So I think I heard a chuckle or


970
00:29:14,346 --> 00:29:14,626
two.


971
00:29:14,626 --> 00:29:15,666
Maybe this seems like kind of a


972
00:29:15,666 --> 00:29:16,836
silly challenge problem.


973
00:29:16,836 --> 00:29:18,626
Small children love this, by the


974
00:29:18,626 --> 00:29:18,856
way.


975
00:29:19,086 --> 00:29:22,056
But -- so way, way back in the


976
00:29:22,106 --> 00:29:23,356
past, when I was first starting


977
00:29:23,356 --> 00:29:24,446
graduate school and I was


978
00:29:24,446 --> 00:29:25,746
thinking about this problem and


979
00:29:25,746 --> 00:29:26,736
first learning about machine


980
00:29:26,736 --> 00:29:28,476
learning, my insights on the top


981
00:29:28,476 --> 00:29:29,426
came down to something like


982
00:29:29,426 --> 00:29:32,306
this: I don't know -- seems


983
00:29:32,306 --> 00:29:32,546
hard.


984
00:29:33,226 --> 00:29:34,276
I don't really have any good


985
00:29:34,276 --> 00:29:34,776
idea for you.


986
00:29:35,896 --> 00:29:38,106
So a few years pass.


987
00:29:38,266 --> 00:29:39,716
I get older, hopefully a little


988
00:29:39,716 --> 00:29:40,316
bit wiser.


989
00:29:40,316 --> 00:29:41,466
But certainly the field is


990
00:29:41,466 --> 00:29:42,676
moving very, very quickly,


991
00:29:42,676 --> 00:29:43,836
because there started to be a


992
00:29:43,836 --> 00:29:45,176
lot of exciting new results


993
00:29:45,176 --> 00:29:46,686
using deep neural networks.


994
00:29:47,736 --> 00:29:48,926
And so then my view on this


995
00:29:48,926 --> 00:29:49,676
problem changed.


996
00:29:49,676 --> 00:29:50,716
And suddenly, wow, this


997
00:29:50,716 --> 00:29:52,036
cutting-edge research can really


998
00:29:52,036 --> 00:29:53,366
answer these kind of questions,


999
00:29:53,366 --> 00:29:54,756
and computers can catch up with


1000
00:29:54,756 --> 00:29:55,556
small children and horse


1001
00:29:55,556 --> 00:29:56,196
recognition technology.


1002
00:29:56,196 --> 00:29:56,976
What an exciting development.


1003
00:29:57,516 --> 00:29:59,866
[ Laughter ]


1004
00:30:00,366 --> 00:30:02,366
So a few more years pass.


1005
00:30:02,486 --> 00:30:03,856
Now I work at Apple, and my


1006
00:30:03,856 --> 00:30:05,226
perspective on this problem has


1007
00:30:05,226 --> 00:30:06,476
changed again.


1008
00:30:06,476 --> 00:30:09,256
Now, just grab Create ML.


1009
00:30:09,436 --> 00:30:10,656
The UI is lovely.


1010
00:30:10,656 --> 00:30:11,696
You'll have a horse classifier


1011
00:30:11,696 --> 00:30:12,526
in just a few minutes.


1012
00:30:13,366 --> 00:30:15,046
So, you know, if you are a


1013
00:30:15,046 --> 00:30:16,226
machine learning expert, maybe


1014
00:30:16,226 --> 00:30:17,136
you are looking at this and you


1015
00:30:17,136 --> 00:30:18,016
are thinking, "Oh, this guy


1016
00:30:18,016 --> 00:30:18,926
doesn't know what he is talking


1017
00:30:18,926 --> 00:30:19,196
about.


1018
00:30:19,196 --> 00:30:20,506
You know, in 2007 I knew how to


1019
00:30:20,506 --> 00:30:21,376
solve that problem.


1020
00:30:21,376 --> 00:30:23,336
In 2012 I'd solved it a hundred


1021
00:30:23,336 --> 00:30:23,646
times."


1022
00:30:24,506 --> 00:30:25,076
Not my point.


1023
00:30:25,756 --> 00:30:27,146
If you are someone who cares


1024
00:30:27,146 --> 00:30:29,446
about long-lasting, high-quality


1025
00:30:29,446 --> 00:30:30,976
software, this should make you


1026
00:30:30,976 --> 00:30:32,796
nervous, because in 11 years we


1027
00:30:32,796 --> 00:30:34,136
have seen the entire picture of


1028
00:30:34,136 --> 00:30:35,126
this problem turn over.


1029
00:30:36,246 --> 00:30:37,386
So let's take a look at a few


1030
00:30:37,386 --> 00:30:38,676
more features in Core ML that


1031
00:30:38,676 --> 00:30:40,146
can help set your mind at ease.


1032
00:30:41,606 --> 00:30:43,346
To do that, let's open the hood


1033
00:30:43,346 --> 00:30:44,586
once again and peek at one of


1034
00:30:44,586 --> 00:30:46,176
these new horse finder models,


1035
00:30:46,656 --> 00:30:48,406
which is, once again, a neural


1036
00:30:48,406 --> 00:30:48,766
network.


1037
00:30:49,526 --> 00:30:51,106
As we have illustrated before,


1038
00:30:51,306 --> 00:30:52,696
the neural network consists of a


1039
00:30:52,696 --> 00:30:54,266
series of highly optimized


1040
00:30:54,266 --> 00:30:54,696
layers.


1041
00:30:54,746 --> 00:30:56,266
It is a series of layers, and we


1042
00:30:56,266 --> 00:30:57,176
have highly optimized


1043
00:30:57,176 --> 00:30:58,586
implementations for each of them


1044
00:30:58,626 --> 00:30:59,916
in our inference engine.


1045
00:31:00,656 --> 00:31:02,396
Our list of supported operations


1046
00:31:02,396 --> 00:31:03,806
is large and always growing,


1047
00:31:04,306 --> 00:31:05,416
trying to keep up with new


1048
00:31:05,416 --> 00:31:06,506
developments in the field.


1049
00:31:07,436 --> 00:31:08,746
But what if there is a layer


1050
00:31:08,746 --> 00:31:10,116
that just isn't supported in


1051
00:31:10,116 --> 00:31:10,646
Core ML?


1052
00:31:11,256 --> 00:31:14,476
In the past, you either needed


1053
00:31:14,476 --> 00:31:15,926
to wait or you needed a


1054
00:31:15,926 --> 00:31:16,516
different model.


1055
00:31:16,516 --> 00:31:18,866
But what if this layer is the


1056
00:31:19,126 --> 00:31:20,596
key horse-finding layer?


1057
00:31:21,076 --> 00:31:22,166
This is the breakthrough that


1058
00:31:22,166 --> 00:31:23,626
your horse app was waiting for.


1059
00:31:23,966 --> 00:31:24,836
Can you afford to wait?


1060
00:31:26,546 --> 00:31:27,816
Given the speed of machine


1061
00:31:27,816 --> 00:31:28,566
learning, this could be a


1062
00:31:28,566 --> 00:31:29,426
serious obstacle.


1063
00:31:31,216 --> 00:31:33,376
So we introduced custom layers


1064
00:31:33,376 --> 00:31:34,406
for neural network models.


1065
00:31:35,076 --> 00:31:37,176
Now if a neural network layer is


1066
00:31:37,176 --> 00:31:38,866
missing, you can provide an


1067
00:31:38,866 --> 00:31:40,926
implementation with -- will mesh


1068
00:31:41,116 --> 00:31:42,326
seamlessly with the rest of the


1069
00:31:42,326 --> 00:31:43,186
Core ML model.


1070
00:31:44,026 --> 00:31:45,606
Inside the model, the custom


1071
00:31:45,606 --> 00:31:46,876
layer stores the name of an


1072
00:31:46,876 --> 00:31:48,166
implementing class -- the


1073
00:31:48,166 --> 00:31:49,726
AAPLCustomHorseLayer in this


1074
00:31:49,726 --> 00:31:50,126
case.


1075
00:31:50,906 --> 00:31:52,626
The implementation class fills


1076
00:31:52,626 --> 00:31:53,656
the role of the missing


1077
00:31:53,656 --> 00:31:54,976
implementation in the inference


1078
00:31:54,976 --> 00:31:55,286
engine.


1079
00:31:55,906 --> 00:31:56,936
Just like the layer is built


1080
00:31:56,936 --> 00:31:59,406
into Core ML, the implementation


1081
00:31:59,406 --> 00:32:00,856
provided here should be general


1082
00:32:00,856 --> 00:32:02,336
and applicable to any instance


1083
00:32:02,336 --> 00:32:02,916
of the new layer.


1084
00:32:04,736 --> 00:32:05,986
It simply needs to be included


1085
00:32:05,986 --> 00:32:07,206
in your app at runtime.


1086
00:32:07,706 --> 00:32:08,996
Then the parameters for this


1087
00:32:08,996 --> 00:32:10,196
particular layer are


1088
00:32:10,196 --> 00:32:11,946
encapsulated in the ML model


1089
00:32:12,106 --> 00:32:13,466
with the rest of the information


1090
00:32:13,466 --> 00:32:14,136
about the model.


1091
00:32:15,996 --> 00:32:17,196
Implementing a custom layer is


1092
00:32:17,196 --> 00:32:17,476
simple.


1093
00:32:18,136 --> 00:32:19,576
We expose an MLCustomLayer


1094
00:32:19,576 --> 00:32:20,196
protocol.


1095
00:32:20,556 --> 00:32:21,816
You simply provide methods to


1096
00:32:21,816 --> 00:32:23,356
initialize the layer based on


1097
00:32:23,356 --> 00:32:24,846
the data stored in the ML model.


1098
00:32:26,096 --> 00:32:27,176
You'll need to provide a method


1099
00:32:27,176 --> 00:32:28,516
that tells us how much space to


1100
00:32:28,516 --> 00:32:29,826
allocate for the outputs of the


1101
00:32:29,826 --> 00:32:31,846
layer, and then a method that


1102
00:32:31,846 --> 00:32:32,846
does the computation.


1103
00:32:34,676 --> 00:32:36,596
Plus, you can add this


1104
00:32:36,596 --> 00:32:38,406
flexibility without sacrificing


1105
00:32:38,406 --> 00:32:39,626
the performance of your model as


1106
00:32:39,626 --> 00:32:39,946
a whole.


1107
00:32:40,976 --> 00:32:41,996
The protocol includes an


1108
00:32:41,996 --> 00:32:43,336
optional method, which allows


1109
00:32:43,336 --> 00:32:44,506
you to provide us with a MTL


1110
00:32:44,506 --> 00:32:45,946
shader implementation of your


1111
00:32:45,946 --> 00:32:47,436
model -- of the layer, excuse


1112
00:32:47,436 --> 00:32:47,556
me.


1113
00:32:47,936 --> 00:32:50,056
If you give us this, then it can


1114
00:32:50,056 --> 00:32:51,556
be encoded in the same command


1115
00:32:51,556 --> 00:32:52,796
buffer as the rest of the Core


1116
00:32:52,796 --> 00:32:53,866
ML computation.


1117
00:32:53,866 --> 00:32:55,026
So there is no extra overhead


1118
00:32:55,026 --> 00:32:56,196
from additional encodings or


1119
00:32:56,196 --> 00:32:57,346
multiple trips to and from the


1120
00:32:57,346 --> 00:32:57,786
GPU.


1121
00:32:58,536 --> 00:32:59,826
If you don't provide this, then


1122
00:32:59,826 --> 00:33:01,026
we'll simply evaluate the layer


1123
00:33:01,026 --> 00:33:02,546
on the CPU with no other work on


1124
00:33:02,546 --> 00:33:03,506
your part.


1125
00:33:04,256 --> 00:33:06,016
So no matter how quickly


1126
00:33:06,016 --> 00:33:07,186
advancements in neural network


1127
00:33:07,186 --> 00:33:08,466
models may happen, you have a


1128
00:33:08,466 --> 00:33:09,716
way to keep up with Core ML.


1129
00:33:10,556 --> 00:33:11,936
But there are limitations.


1130
00:33:12,836 --> 00:33:14,196
Custom layers only work for


1131
00:33:14,196 --> 00:33:15,476
neural network models, and they


1132
00:33:15,476 --> 00:33:16,816
only take inputs and outputs


1133
00:33:16,816 --> 00:33:18,066
which are ML MultiArrays.


1134
00:33:18,066 --> 00:33:19,986
This is a natural way to


1135
00:33:19,986 --> 00:33:21,326
interact with neural networks.


1136
00:33:21,686 --> 00:33:22,706
But the machine learning field


1137
00:33:22,706 --> 00:33:24,406
is hardly restricted to only


1138
00:33:24,406 --> 00:33:25,556
advancing in this area.


1139
00:33:26,656 --> 00:33:27,666
In fact, when I was first


1140
00:33:27,666 --> 00:33:28,406
learning about image


1141
00:33:28,406 --> 00:33:29,996
recognition, almost no one was


1142
00:33:29,996 --> 00:33:31,416
talking about neural networks as


1143
00:33:31,416 --> 00:33:32,626
a solution to that problem.


1144
00:33:32,986 --> 00:33:34,006
And you can see today it's the


1145
00:33:34,006 --> 00:33:37,186
absolute state of the art.


1146
00:33:37,186 --> 00:33:38,686
And it's not hard to imagine


1147
00:33:38,686 --> 00:33:39,896
machine-learning-enabled app


1148
00:33:39,896 --> 00:33:41,426
experiences where custom layers


1149
00:33:41,426 --> 00:33:42,426
simply wouldn't fit.


1150
00:33:42,906 --> 00:33:44,666
For instance, a machine-learning


1151
00:33:44,666 --> 00:33:46,246
app might use a neural network


1152
00:33:46,246 --> 00:33:47,446
to embed an image in some


1153
00:33:47,446 --> 00:33:49,356
similarity space, then look up


1154
00:33:49,356 --> 00:33:50,576
similar images using a


1155
00:33:50,576 --> 00:33:51,776
nearest-neighbor method or


1156
00:33:51,776 --> 00:33:53,456
locality-sensitive hashing -- or


1157
00:33:53,456 --> 00:33:54,626
even some other approach.


1158
00:33:56,896 --> 00:33:58,306
A model might combine audio and


1159
00:33:58,306 --> 00:33:59,736
motion data to provide a bit of


1160
00:33:59,736 --> 00:34:01,096
needed encouragement to someone


1161
00:34:01,096 --> 00:34:02,126
who doesn't always close his


1162
00:34:02,126 --> 00:34:02,506
rings.


1163
00:34:04,826 --> 00:34:06,336
Or even a completely new model


1164
00:34:06,336 --> 00:34:07,776
type we haven't even imagined


1165
00:34:07,776 --> 00:34:08,886
yet that enables novel


1166
00:34:08,886 --> 00:34:10,286
experiences for your users.


1167
00:34:11,076 --> 00:34:12,266
In all these cases, it would be


1168
00:34:12,266 --> 00:34:13,116
great if we could have the


1169
00:34:13,116 --> 00:34:14,545
simplicity and portability of


1170
00:34:14,545 --> 00:34:16,496
Core ML without having to


1171
00:34:16,496 --> 00:34:18,106
sacrifice the flexibility to


1172
00:34:18,106 --> 00:34:19,096
keep up with the field.


1173
00:34:19,815 --> 00:34:22,005
So we are introducing custom


1174
00:34:22,005 --> 00:34:22,505
models.


1175
00:34:23,496 --> 00:34:25,056
A Core ML custom model allows


1176
00:34:25,056 --> 00:34:26,045
you to encapsulate the


1177
00:34:26,045 --> 00:34:27,315
implementation of a part of a


1178
00:34:27,315 --> 00:34:28,576
computation that's missing


1179
00:34:28,735 --> 00:34:29,686
inside Core ML.


1180
00:34:30,436 --> 00:34:32,045
Just like for custom layers, the


1181
00:34:32,045 --> 00:34:33,286
model stores the name of an


1182
00:34:33,286 --> 00:34:34,456
implementation class.


1183
00:34:34,956 --> 00:34:36,216
The class fills the role of the


1184
00:34:36,216 --> 00:34:37,606
general inference engine for


1185
00:34:37,606 --> 00:34:38,656
this type of model.


1186
00:34:39,116 --> 00:34:40,906
Then the parameters are stored


1187
00:34:40,906 --> 00:34:42,235
in the ML Model just like


1188
00:34:42,266 --> 00:34:42,626
before.


1189
00:34:43,386 --> 00:34:44,826
This allows the model to be


1190
00:34:44,826 --> 00:34:46,366
updated as an asset in your app


1191
00:34:46,366 --> 00:34:48,286
without having to touch code.


1192
00:34:50,005 --> 00:34:51,806
And implementing a custom model


1193
00:34:51,806 --> 00:34:52,556
is simple as well.


1194
00:34:52,896 --> 00:34:54,016
We expose a protocol,


1195
00:34:54,065 --> 00:34:55,216
MLCustomModel.


1196
00:34:55,466 --> 00:34:56,485
You provide methods to


1197
00:34:56,485 --> 00:34:57,876
initialize based on the data


1198
00:34:57,876 --> 00:34:59,126
stored in the ML Model.


1199
00:34:59,126 --> 00:35:01,076
And you provide a method to


1200
00:35:01,076 --> 00:35:02,206
compute the prediction on an


1201
00:35:02,206 --> 00:35:02,656
input.


1202
00:35:02,976 --> 00:35:04,626
There is an optional method to


1203
00:35:04,626 --> 00:35:06,146
provide a batch implementation


1204
00:35:06,146 --> 00:35:07,276
if there are opportunities in


1205
00:35:07,276 --> 00:35:08,776
this particular model type to


1206
00:35:08,776 --> 00:35:10,006
have optimizations there.


1207
00:35:10,286 --> 00:35:11,276
And if not, we'll call the


1208
00:35:11,276 --> 00:35:13,126
single prediction in a for loop.


1209
00:35:14,026 --> 00:35:16,046
And using a customized model in


1210
00:35:16,046 --> 00:35:17,316
your app is largely the same


1211
00:35:17,316 --> 00:35:18,696
workflow as any other Core ML


1212
00:35:18,696 --> 00:35:19,066
model.


1213
00:35:19,646 --> 00:35:21,156
In Xcode, a model with


1214
00:35:21,156 --> 00:35:22,936
customized components will have


1215
00:35:22,936 --> 00:35:24,986
a dependency section listing the


1216
00:35:24,986 --> 00:35:26,286
names of the implementations


1217
00:35:26,286 --> 00:35:27,366
needed along with a short


1218
00:35:27,366 --> 00:35:27,956
description.


1219
00:35:28,576 --> 00:35:29,816
Just include these in your app,


1220
00:35:29,936 --> 00:35:30,836
and you are ready to go.


1221
00:35:31,696 --> 00:35:33,616
The prediction API is unchanged,


1222
00:35:33,616 --> 00:35:34,946
whether for single predictions


1223
00:35:34,996 --> 00:35:35,646
or batch.


1224
00:35:37,016 --> 00:35:39,476
So custom layers and custom


1225
00:35:39,476 --> 00:35:41,086
models allow you to use the


1226
00:35:41,086 --> 00:35:42,806
power and simplicity of Core ML


1227
00:35:42,806 --> 00:35:43,896
without sacrificing the


1228
00:35:43,896 --> 00:35:45,676
flexibility needed to keep up


1229
00:35:45,676 --> 00:35:46,886
with the fast-paced area of


1230
00:35:46,886 --> 00:35:47,496
machine learning.


1231
00:35:48,566 --> 00:35:49,896
For new neural network layers,


1232
00:35:50,096 --> 00:35:52,016
custom layers allow you to make


1233
00:35:52,066 --> 00:35:53,566
use of the many optimizations


1234
00:35:53,566 --> 00:35:54,716
already present in the neural


1235
00:35:54,716 --> 00:35:56,176
network inference engine in Core


1236
00:35:56,176 --> 00:35:56,446
ML.


1237
00:35:57,096 --> 00:35:58,956
Custom models are more flexible


1238
00:35:59,816 --> 00:36:01,686
for types and functionality, but


1239
00:36:01,686 --> 00:36:02,656
they do require more


1240
00:36:02,656 --> 00:36:04,006
implementation work on your


1241
00:36:05,296 --> 00:36:05,436
part.


1242
00:36:05,806 --> 00:36:07,316
Both forms of customization


1243
00:36:07,316 --> 00:36:08,916
allow you to encapsulate model


1244
00:36:08,916 --> 00:36:10,346
parameters in an ML model,


1245
00:36:10,826 --> 00:36:12,176
making the model portable and


1246
00:36:12,176 --> 00:36:13,156
your code simpler.


1247
00:36:14,516 --> 00:36:16,816
And we've only been able to


1248
00:36:16,816 --> 00:36:18,146
touch on a few of the great new


1249
00:36:18,146 --> 00:36:19,506
features in Core ML 2.


1250
00:36:20,146 --> 00:36:22,336
Please download the beta, try


1251
00:36:22,336 --> 00:36:22,976
them out for yourself.


1252
00:36:27,066 --> 00:36:28,556
Core ML has many great new


1253
00:36:28,556 --> 00:36:29,926
features to reduce your app


1254
00:36:29,926 --> 00:36:31,916
size, improve performance, and


1255
00:36:31,916 --> 00:36:33,136
ensure flexibility and


1256
00:36:33,136 --> 00:36:34,476
compatibility with the latest


1257
00:36:34,476 --> 00:36:35,406
developments in machine


1258
00:36:35,406 --> 00:36:35,686
learning.


1259
00:36:36,496 --> 00:36:37,886
We showed you how quantization


1260
00:36:37,886 --> 00:36:39,676
can reduce model size, how the


1261
00:36:39,676 --> 00:36:41,336
new batch API can enable more


1262
00:36:41,336 --> 00:36:43,156
efficient processing, and how


1263
00:36:43,156 --> 00:36:44,936
custom layers and custom models


1264
00:36:44,936 --> 00:36:46,226
can help you bring cutting-edge


1265
00:36:46,226 --> 00:36:47,446
machine learning to your app.


1266
00:36:48,416 --> 00:36:49,756
Combined with our great new tool


1267
00:36:49,756 --> 00:36:51,236
for training models in Create


1268
00:36:51,236 --> 00:36:52,746
ML, there are more ways than


1269
00:36:52,746 --> 00:36:54,596
ever to add ML-enabled features


1270
00:36:54,596 --> 00:36:56,176
to your app and support great


1271
00:36:56,176 --> 00:36:57,736
new experiences for your users.


1272
00:36:59,576 --> 00:37:01,376
After just a short break, we'll


1273
00:37:01,376 --> 00:37:02,986
be back right here to take a


1274
00:37:02,986 --> 00:37:04,066
deeper look at some of these


1275
00:37:04,066 --> 00:37:04,626
features.


1276
00:37:04,896 --> 00:37:06,286
In particular, we'll show you


1277
00:37:06,286 --> 00:37:07,686
how to use our Core ML Tools


1278
00:37:07,686 --> 00:37:09,576
software to start reducing model


1279
00:37:09,576 --> 00:37:11,596
sizes and customizing your user


1280
00:37:11,596 --> 00:37:13,096
experiences with Core ML today.


1281
00:37:13,096 --> 00:37:13,976
Thank you.


1282
00:37:14,516 --> 00:37:20,500
[ Applause ]

