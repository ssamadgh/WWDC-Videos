1
00:00:07,516 --> 00:00:17,500
[ Music ]


2
00:00:21,516 --> 00:00:26,846
[ Applause ]


3
00:00:27,346 --> 00:00:27,986
>> Welcome.


4
00:00:28,846 --> 00:00:31,776
This is Part 2 of our
What's New in Metal session.


5
00:00:32,485 --> 00:00:36,746
My name is Charles Brissart,
and I'm a GPU Software Engineer,


6
00:00:36,746 --> 00:00:41,046
and together with my colleague,
Dan Omachi and Ana Tikhonova,


7
00:00:41,046 --> 00:00:43,726
I will be telling you about
some of our new features.


8
00:00:45,076 --> 00:00:47,876
But first, let's take a look


9
00:00:48,326 --> 00:00:50,756
at the other Metal
session at the WWDC.


10
00:00:51,716 --> 00:00:56,456
The first two sessions I call
Adopting Metal uncovered some


11
00:00:56,456 --> 00:00:59,386
of the basic concepts
of Metal as well


12
00:00:59,386 --> 00:01:01,336
as some more advanced
considerations.


13
00:01:02,956 --> 00:01:06,806
The What's New in Metal session
covered our new features.


14
00:01:08,046 --> 00:01:12,596
Finally, the Advanced Shadow
Optimization session will tell


15
00:01:12,596 --> 00:01:15,056
you how to get the best
performance out of your shaders.


16
00:01:15,916 --> 00:01:21,746
So this morning you were
told about tessellation,


17
00:01:21,916 --> 00:01:25,466
resource heaps, memoryless
render targets as well


18
00:01:25,466 --> 00:01:27,856
as some improvement
for GPU tools.


19
00:01:29,636 --> 00:01:32,776
This afternoon we'll tell you
about function specialization,


20
00:01:33,096 --> 00:01:38,076
function resource read-writes,
wide color, texture assets,


21
00:01:38,526 --> 00:01:41,736
as well as some addition to
the Metal Performance Shaders.


22
00:01:41,736 --> 00:01:46,726
So let's get started with
function specialization.


23
00:01:48,856 --> 00:01:51,566
It is a common pattern
in a rendering engine


24
00:01:52,336 --> 00:01:55,546
to define a few complex
master functions


25
00:01:55,816 --> 00:01:58,876
and then use those master
functions to generator minimum


26
00:01:58,876 --> 00:02:00,646
of specialized simple functions.


27
00:02:01,706 --> 00:02:05,586
The idea is that the
master function allow you


28
00:02:05,586 --> 00:02:10,485
to avoid duplicating card while
the specialized function are


29
00:02:10,485 --> 00:02:14,086
simpler on those as a result
of better performance.


30
00:02:15,796 --> 00:02:17,056
So let's take an example.


31
00:02:17,306 --> 00:02:20,386
If we are trying to write a
material function you could


32
00:02:20,386 --> 00:02:24,306
write a master function
that implements every aspect


33
00:02:24,536 --> 00:02:26,376
of any material that
you might need.


34
00:02:27,536 --> 00:02:30,466
But then, if you are trying
to implement a shiny --


35
00:02:30,736 --> 00:02:32,176
a simple shiny material,


36
00:02:32,646 --> 00:02:34,336
you would probably
not need reflection,


37
00:02:34,466 --> 00:02:36,286
but you will need a
specular highlight.


38
00:02:37,876 --> 00:02:39,986
If you implement a
reflected material


39
00:02:39,986 --> 00:02:42,676
on the other hand you will
need to add reflection


40
00:02:42,986 --> 00:02:44,756
on also the specular highlights.


41
00:02:45,606 --> 00:02:48,856
Our transition material will
need subsurface scattering,


42
00:02:48,896 --> 00:02:50,196
but probably no reflection


43
00:02:50,196 --> 00:02:53,536
or may be no specular
highlights either, and so on.


44
00:02:53,946 --> 00:02:54,766
You get the idea.


45
00:02:55,986 --> 00:02:59,766
So this is typically implemented
using preprocessor macros.


46
00:03:00,216 --> 00:03:05,566
The master function is
compiled with a set of values


47
00:03:05,566 --> 00:03:07,766
for the macro to create
a specialized function.


48
00:03:08,286 --> 00:03:12,146
This can be done at runtime,
but this is expensive.


49
00:03:12,146 --> 00:03:15,926
You can also try to
precompile every single variant


50
00:03:15,926 --> 00:03:20,356
of the precompiled function, but
-- and then store them in Metal,


51
00:03:20,406 --> 00:03:22,376
but this requires
a lot of storage


52
00:03:22,376 --> 00:03:24,166
because you can have
many, many variants,


53
00:03:24,316 --> 00:03:25,876
or maybe you don't know
which one you will need.


54
00:03:27,676 --> 00:03:30,486
Another approach is to
use runtime constants.


55
00:03:31,626 --> 00:03:34,776
Runtime constants avoid the need
to recompile your functions.


56
00:03:35,066 --> 00:03:37,586
However, you need to
evaluate the values


57
00:03:37,586 --> 00:03:39,146
of the constant at runtime.


58
00:03:40,516 --> 00:03:43,106
That will impact the
performance of your shaders.


59
00:03:43,106 --> 00:03:46,536
So we are proposing a new way


60
00:03:46,536 --> 00:03:49,736
to create specialized
functions using what we call


61
00:03:49,736 --> 00:03:50,656
function constants.


62
00:03:51,446 --> 00:03:53,886
So function constants
are constants


63
00:03:54,506 --> 00:03:57,286
that are defined directly in
the Metal shading language


64
00:03:57,676 --> 00:04:01,116
and can be compiled into IR
and stored in the Metal lib.


65
00:04:01,946 --> 00:04:05,516
Then at runtime you can provide
the value of the constant


66
00:04:05,896 --> 00:04:07,846
to create a specialized
function.


67
00:04:08,746 --> 00:04:11,526
The advantage of
this approach is


68
00:04:11,526 --> 00:04:13,866
that you can compile the
master function offline


69
00:04:13,866 --> 00:04:15,096
and store it in the Metal lib.


70
00:04:15,816 --> 00:04:17,166
The storage requirement is small


71
00:04:17,166 --> 00:04:19,356
because you only store
the master functions.


72
00:04:20,076 --> 00:04:23,146
And since we run a
quick optimization pass


73
00:04:23,146 --> 00:04:24,926
when we create the
specialized function,


74
00:04:25,446 --> 00:04:27,216
you still get the
best performance.


75
00:04:28,026 --> 00:04:31,036
So let's look at an example.


76
00:04:31,716 --> 00:04:34,146
This is what a master
function could look


77
00:04:34,146 --> 00:04:35,716
like using a preprocessor macro.


78
00:04:36,176 --> 00:04:38,566
Of course, this is
a simple example.


79
00:04:38,926 --> 00:04:40,696
A real one would be
much more complex.


80
00:04:41,266 --> 00:04:45,426
As you can see, different parts
of the code surrounded by what


81
00:04:45,426 --> 00:04:49,076
if statements so that
you can eliminate


82
00:04:49,146 --> 00:04:50,256
that section of the code.


83
00:04:51,766 --> 00:04:54,206
Here is what it would look
like with function constant.


84
00:04:54,796 --> 00:04:57,696
As you can see at the top,
we are defining a number


85
00:04:57,696 --> 00:04:59,886
of constants, and then
we use them in the code.


86
00:05:00,696 --> 00:05:04,326
To define the constants, you use
the constant keyword followed


87
00:05:04,326 --> 00:05:08,476
by the type, in this case
Boolean, and finally the name


88
00:05:08,476 --> 00:05:12,236
of the constant and the
function constant attribute.


89
00:05:12,806 --> 00:05:16,026
The function constant attribute
specifies that the value


90
00:05:16,026 --> 00:05:18,846
of the constant is not going
to be provided at compile time


91
00:05:19,156 --> 00:05:20,926
but will be provided at runtime


92
00:05:20,926 --> 00:05:22,616
when we create the
specialized function.


93
00:05:23,386 --> 00:05:25,546
You should also note that
we are passing an index.


94
00:05:26,166 --> 00:05:28,926
That index can be used
in addition to the name


95
00:05:28,926 --> 00:05:32,646
to identify the constant when we
create the specialized function


96
00:05:32,646 --> 00:05:33,326
at runtime.


97
00:05:34,846 --> 00:05:37,676
You can then use the
constant anywhere in your code


98
00:05:37,676 --> 00:05:38,876
like your normal constant.


99
00:05:39,426 --> 00:05:42,046
Here we have a simple if
statement that is used


100
00:05:42,046 --> 00:05:44,136
to conditionalize
part of the code.


101
00:05:45,426 --> 00:05:48,946
So once you've created your
master function and compiled it


102
00:05:48,946 --> 00:05:50,226
and stored it in a Metal lib,


103
00:05:50,706 --> 00:05:53,706
you need to at runtime
create specialized functions.


104
00:05:53,706 --> 00:05:56,506
So you need to provide the
values of the constant.


105
00:05:57,456 --> 00:06:00,946
To do that, we use an MTL
function constant values object


106
00:06:01,276 --> 00:06:03,396
that will solve the values
of multiple constants.


107
00:06:04,106 --> 00:06:07,956
Once we created the object,
we can then set the values


108
00:06:07,956 --> 00:06:13,106
of a constant either by
name, by index, or by name.


109
00:06:15,006 --> 00:06:17,456
Once we have created an object,


110
00:06:17,726 --> 00:06:20,396
we can then create the
specialized function


111
00:06:20,396 --> 00:06:22,746
by simply coding the
new function with names


112
00:06:23,176 --> 00:06:27,496
and constant values on the
library, providing the name


113
00:06:27,496 --> 00:06:32,446
of the master function as well
as the values we just filled.


114
00:06:33,016 --> 00:06:36,986
This will return a regular MTL
function that can then be used


115
00:06:36,986 --> 00:06:40,856
to create compute pipeline
or render pipeline depending


116
00:06:40,856 --> 00:06:41,946
on the type of the function.


117
00:06:42,456 --> 00:06:45,696
So to better understand
how this works,


118
00:06:45,696 --> 00:06:47,906
let's look at the
compilation pipeline.


119
00:06:48,576 --> 00:06:52,646
So at build time, you use the
source of your master function


120
00:06:53,336 --> 00:06:55,666
and compile it and
store into a Metal lib.


121
00:06:56,586 --> 00:07:01,146
At runtime you load
the Metal lib


122
00:07:01,146 --> 00:07:06,816
and create a new function using
the MTL function constant values


123
00:07:06,996 --> 00:07:08,366
to specialize the function.


124
00:07:09,316 --> 00:07:11,596
At this point, we
run some optimization


125
00:07:11,596 --> 00:07:13,716
to eliminate any code
that's not used anymore,


126
00:07:14,366 --> 00:07:17,096
and then we have an interior
function that we can use


127
00:07:17,666 --> 00:07:21,246
to create a render pipeline
or a compute pipeline.


128
00:07:21,826 --> 00:07:27,976
You can declare constants
of any scalar or vector type


129
00:07:27,976 --> 00:07:30,216
that is [inaudible]
in Metal , so float,


130
00:07:30,216 --> 00:07:32,326
half, int, uint, and so on.


131
00:07:32,886 --> 00:07:35,576
Here we are defining
half4 color.


132
00:07:37,076 --> 00:07:41,936
You can also create intermediate
constants using the value


133
00:07:41,936 --> 00:07:43,036
of function constants.


134
00:07:43,406 --> 00:07:45,906
Here we're defining
a Boolean constant


135
00:07:45,906 --> 00:07:48,456
that has the opposite value
of a function constant a.


136
00:07:49,496 --> 00:07:51,936
Here we are calculating a
value based on the value


137
00:07:51,936 --> 00:07:53,506
of the value function constant.


138
00:07:54,396 --> 00:07:58,376
We can also have
optional constants.


139
00:07:58,996 --> 00:08:01,596
Optional constants are constants
for which you don't need


140
00:08:01,596 --> 00:08:04,346
to always provide the value when
you specialize the function.


141
00:08:04,876 --> 00:08:07,616
This is exactly the same
thing as using a what ifdef


142
00:08:07,616 --> 00:08:09,746
in your code when using
preprocessor macros.


143
00:08:10,866 --> 00:08:14,066
To do this, you use the if
function constant defined built


144
00:08:14,066 --> 00:08:17,106
in that will return true if
the value has been provided


145
00:08:17,466 --> 00:08:20,576
and false if otherwise.


146
00:08:22,356 --> 00:08:24,876
You can also use
function constant to add


147
00:08:24,876 --> 00:08:27,176
or eliminate arguments
from function.


148
00:08:27,986 --> 00:08:31,796
This is useful to avoid, to
making sure you don't have


149
00:08:31,796 --> 00:08:34,135
to bind a buffer or texture


150
00:08:34,135 --> 00:08:35,696
if you know it's not
going to be used.


151
00:08:36,346 --> 00:08:39,806
It's also useful to replace
the type of an argument,


152
00:08:40,336 --> 00:08:41,506
and we'll talk about --


153
00:08:41,506 --> 00:08:43,706
we'll talk more about this
in the next couple of slides.


154
00:08:44,916 --> 00:08:46,286
So here we have an example.


155
00:08:46,906 --> 00:08:51,906
This is a vertex function that
can implement skinning depending


156
00:08:51,906 --> 00:08:54,126
of the value of the
doSkinning constant.


157
00:08:55,166 --> 00:09:00,986
The first argument of the
function is the matrices buffer


158
00:09:01,156 --> 00:09:02,996
that will exist depending


159
00:09:02,996 --> 00:09:06,336
on whether the doSkinning
constant is true or false.


160
00:09:06,806 --> 00:09:09,806
We use the function
constant attribute to qualify


161
00:09:09,806 --> 00:09:11,676
that argument as being optional.


162
00:09:12,806 --> 00:09:17,076
In the code, you still need to
use the same function constant


163
00:09:17,336 --> 00:09:19,446
to protect the code
that's using that argument.


164
00:09:20,306 --> 00:09:23,106
So here we use doSkinning
in the if statement,


165
00:09:23,696 --> 00:09:29,706
and then we can use the
matrices safely in our code.


166
00:09:30,296 --> 00:09:34,626
You can as well use function
constant to eliminate arguments


167
00:09:34,626 --> 00:09:35,786
from the stage in struct.


168
00:09:37,176 --> 00:09:39,046
Here, we have two
color arguments.


169
00:09:39,476 --> 00:09:44,696
The first color argument
as type float4 on these use


170
00:09:44,696 --> 00:09:46,886
for attributes, that
is attribute 1.


171
00:09:48,206 --> 00:09:54,096
The second lowp color is a
lower precision color half4


172
00:09:55,146 --> 00:09:57,926
but is overriding the
same attribute index.


173
00:09:59,046 --> 00:10:00,976
So you can have either
one or the other.


174
00:10:01,046 --> 00:10:04,016
These are used to
specifically change the type


175
00:10:04,506 --> 00:10:07,066
of the color attributes
in your code.


176
00:10:08,206 --> 00:10:12,196
There are some limitations with
function constants, namely,


177
00:10:12,196 --> 00:10:15,746
you cannot really change the
layout of a struct in memory,


178
00:10:16,856 --> 00:10:18,966
and that can be a problem
because you might want


179
00:10:18,966 --> 00:10:22,696
to have different constants for
different shaders and so on.


180
00:10:23,956 --> 00:10:25,336
But you can work around that


181
00:10:25,576 --> 00:10:28,446
but adding multiple arguments
with different types.


182
00:10:29,086 --> 00:10:31,746
So in this example, we
have two buffer arguments


183
00:10:32,566 --> 00:10:34,616
that are using buffer index 1.


184
00:10:35,336 --> 00:10:37,036
They are controlled
by function constants,


185
00:10:37,036 --> 00:10:38,806
use ConstantA and ConstantB.


186
00:10:41,606 --> 00:10:46,626
So these are used to
select one or the other.


187
00:10:46,626 --> 00:10:50,406
Note that we have -- we use
an intermediate constant


188
00:10:50,406 --> 00:10:52,756
that is the opposite
of the first constant


189
00:10:52,886 --> 00:10:54,116
to make sure only one


190
00:10:54,116 --> 00:10:56,196
of the arguments will
exist at a given time.


191
00:10:56,656 --> 00:10:59,756
So in summary, you can
use function constant


192
00:11:00,196 --> 00:11:02,626
to create specialized
function at runtime.


193
00:11:03,226 --> 00:11:07,546
It avoids front end compilation,
and because we only use --


194
00:11:07,546 --> 00:11:09,796
and it only uses fast
optimization phase


195
00:11:09,796 --> 00:11:11,066
to eliminate unused code.


196
00:11:11,606 --> 00:11:14,346
The storage is compact
because you only need


197
00:11:14,346 --> 00:11:16,506
to store the master
function in your library.


198
00:11:17,356 --> 00:11:18,756
You don't have to
ship your source.


199
00:11:18,756 --> 00:11:20,246
It can only ship the IR.


200
00:11:20,246 --> 00:11:23,376
And finally, the unused
code is eliminated,


201
00:11:23,376 --> 00:11:24,806
which gives you the
best performance.


202
00:11:25,796 --> 00:11:31,036
So let's now talk about
function resource read-writes.


203
00:11:31,596 --> 00:11:35,076
So we're introducing
two new features,


204
00:11:35,686 --> 00:11:37,536
function buffered read-writes


205
00:11:37,536 --> 00:11:39,486
and function texture
read-writes.


206
00:11:40,826 --> 00:11:44,006
Function buffered read-writes
is the ability to read and write


207
00:11:44,196 --> 00:11:49,146
to a buffer from any function
type and also the ability


208
00:11:49,146 --> 00:11:51,636
to use atomic operations
on those buffers


209
00:11:51,636 --> 00:11:52,646
from any function type.


210
00:11:53,436 --> 00:11:56,276
As you guessed, function texture
read-writes is the ability


211
00:11:56,276 --> 00:12:00,166
to read and write to texture
from any function type.


212
00:12:01,526 --> 00:12:05,996
Function buffer read-writes
is available on iOS


213
00:12:06,306 --> 00:12:09,446
with a 9 processor and macOS.


214
00:12:10,116 --> 00:12:13,256
Function texture read-writes
is available on macOS.


215
00:12:13,256 --> 00:12:17,766
So let's talk about function
buffered read-writes.


216
00:12:18,276 --> 00:12:19,306
So what's new here?


217
00:12:19,596 --> 00:12:21,896
What's new is the
ability to write to buffer


218
00:12:21,896 --> 00:12:25,816
from fragment function as well
as using an atomic operation


219
00:12:25,816 --> 00:12:27,466
in the text and fragment
function.


220
00:12:28,226 --> 00:12:31,026
These can be used to
implement such things


221
00:12:31,026 --> 00:12:35,186
as order-independent
transparency, building lists


222
00:12:35,186 --> 00:12:37,586
of lights that affect
the given tile,


223
00:12:38,036 --> 00:12:39,546
or simply to debug your shaders.


224
00:12:41,306 --> 00:12:43,676
So let's look at
the simple example.


225
00:12:44,016 --> 00:12:47,196
Let's say we want to
write the position


226
00:12:47,516 --> 00:12:49,566
of the visible fragments
we are rendering.


227
00:12:50,096 --> 00:12:51,706
It could look like this.


228
00:12:52,506 --> 00:12:54,006
So we have a fragment function


229
00:12:54,596 --> 00:12:56,286
to which we pass
an output buffer.


230
00:12:56,286 --> 00:12:58,856
The output buffer is
where we are going


231
00:12:58,856 --> 00:13:02,376
to store the position
of the fragments.


232
00:13:02,596 --> 00:13:07,346
Then we have a counter, so
another buffer that we start


233
00:13:07,346 --> 00:13:10,106
after [inaudible] that we
use to find the position


234
00:13:10,426 --> 00:13:12,046
into the buffer,
the first buffer,


235
00:13:12,046 --> 00:13:14,126
to which we want to write.


236
00:13:15,166 --> 00:13:18,616
We can then use an atomic
preparation to count the number


237
00:13:18,616 --> 00:13:20,806
of fragments with that
has been already written


238
00:13:20,806 --> 00:13:22,396
to get an index in the buffer.


239
00:13:22,926 --> 00:13:25,886
And then we can write into
the buffer the position


240
00:13:25,886 --> 00:13:26,606
of the fragments.


241
00:13:27,016 --> 00:13:30,926
So this looks pretty good,
but there is a small problem.


242
00:13:32,416 --> 00:13:36,296
The depth and stencil
test when you're writing


243
00:13:36,296 --> 00:13:38,786
to buffer is actually
always exhibited


244
00:13:38,786 --> 00:13:39,956
after the fragment shader.


245
00:13:40,646 --> 00:13:44,426
So this is a problem
because we are going


246
00:13:44,426 --> 00:13:46,406
to still perform the
rights to the buffer,


247
00:13:46,406 --> 00:13:47,566
which is not what we want.


248
00:13:47,566 --> 00:13:49,206
We only want the
visible fragments.


249
00:13:50,116 --> 00:13:52,436
It's also something to be aware


250
00:13:52,436 --> 00:13:54,426
of because it will
impact your performance.


251
00:13:54,496 --> 00:13:57,336
That means we don't have any
early Z optimization here,


252
00:13:57,416 --> 00:13:59,686
so we are going to
exhibit fragment shader


253
00:13:59,686 --> 00:14:01,756
when we probably
wouldn't want to.


254
00:14:02,936 --> 00:14:06,326
Fortunately we have a new
function qualifier early


255
00:14:06,326 --> 00:14:09,846
fragment test that can be
used to force the depth


256
00:14:09,846 --> 00:14:12,906
and stencil test to appear
before the fragment shader.


257
00:14:12,906 --> 00:14:18,446
As a result, if the depth test
fail, we will skip the execution


258
00:14:18,446 --> 00:14:22,096
of the fragment shader and
thus not write to the buffer.


259
00:14:22,096 --> 00:14:26,526
So this is what we need here,
to reach the final function


260
00:14:27,126 --> 00:14:30,296
with the early fragment test
attribute which otherwise


261
00:14:30,296 --> 00:14:34,766
to only execute the function
when the fragments are visible.


262
00:14:34,766 --> 00:14:39,976
Now let's talk about
function texture read-writes.


263
00:14:40,826 --> 00:14:45,426
So what's new is the ability to
write to texture from the vertex


264
00:14:45,426 --> 00:14:49,806
and fragment functions as well
as the ability to read and write


265
00:14:50,516 --> 00:14:52,606
to a texture from
a single function.


266
00:14:53,376 --> 00:14:55,836
This can be used, for
instance, to save memory


267
00:14:55,836 --> 00:14:57,916
when implementing post
processing effects


268
00:14:58,376 --> 00:15:01,046
by using the same texture
on both input and output.


269
00:15:04,156 --> 00:15:06,056
So writing to texture
is fairly simple.


270
00:15:06,476 --> 00:15:09,866
You just define your texture
with the access qualifier write,


271
00:15:09,866 --> 00:15:12,656
and then you can
write to your texture.


272
00:15:13,266 --> 00:15:18,786
Read-write texture, a texture
to which you can both --


273
00:15:18,786 --> 00:15:21,486
that you can both read
and write in your shader.


274
00:15:21,486 --> 00:15:25,486
Only a limited number of formats
is reported for those textures.


275
00:15:26,316 --> 00:15:29,466
To use the read-write texture
you will use the access


276
00:15:29,466 --> 00:15:34,376
qualifier of read-write, and
then you can read to the texture


277
00:15:35,236 --> 00:15:39,146
and write to it in your shader.


278
00:15:39,146 --> 00:15:41,546
However, you have to be careful
when you write to the texture


279
00:15:41,546 --> 00:15:44,376
if you want to read the results,


280
00:15:44,376 --> 00:15:47,566
if you want to read the same
pixel again in your shader.


281
00:15:48,046 --> 00:15:51,086
In this case, you need
to use a texture fence.


282
00:15:51,596 --> 00:15:53,186
The texture fence will ensure


283
00:15:53,186 --> 00:15:56,106
that the writes have
been committed to memory


284
00:15:56,106 --> 00:15:58,146
so that you can read
the proper value.


285
00:15:59,366 --> 00:16:06,086
Here, we write to a given pixel,
and then we use a texture fence


286
00:16:06,216 --> 00:16:08,916
to make sure we can
read that value again


287
00:16:09,176 --> 00:16:11,546
and then we can finally
read the value.


288
00:16:11,546 --> 00:16:14,996
We should also be
careful with texture fence


289
00:16:14,996 --> 00:16:17,686
because they only apply
on a single SIMD thread,


290
00:16:18,786 --> 00:16:20,886
which means that if you have
two threads that are writing


291
00:16:20,886 --> 00:16:23,876
to a texture and the
second thread is trying


292
00:16:23,956 --> 00:16:28,386
to read the value that was
written by the first thread,


293
00:16:29,666 --> 00:16:32,806
even after a texture
fence, this will not work.


294
00:16:33,786 --> 00:16:38,886
What will work is if each thread
is reading the pixel values


295
00:16:38,886 --> 00:16:41,426
that it was writing
to but not the ones


296
00:16:41,426 --> 00:16:42,576
that are written
by other threads.


297
00:16:43,736 --> 00:16:47,376
So one note about reading,
we talked a lot about writing


298
00:16:47,376 --> 00:16:48,806
to buffers and textures.


299
00:16:49,526 --> 00:16:51,706
With vertex and fragment
functions,


300
00:16:51,786 --> 00:16:52,826
you have to be careful.


301
00:16:53,996 --> 00:16:56,736
In this example, fragment
function is trying to write --


302
00:16:56,736 --> 00:16:58,646
is writing to a buffer


303
00:16:58,646 --> 00:17:01,066
and a vertex function is
trying to read the results.


304
00:17:01,066 --> 00:17:02,476
However, this is
not going to work


305
00:17:02,476 --> 00:17:05,076
because of having the
same RenderCommandEncoder.


306
00:17:05,965 --> 00:17:10,336
To fix this, we need to use
two RenderCommandEncoder.


307
00:17:11,435 --> 00:17:13,326
The fragment function
writes to the buffer


308
00:17:13,326 --> 00:17:16,246
in the first
RenderCommandEncoder while the


309
00:17:16,246 --> 00:17:17,386
texture -- the vertex function


310
00:17:17,386 --> 00:17:19,056
in the second
RenderCommandEncoder can finally


311
00:17:19,056 --> 00:17:20,996
read the result and
get proper results.


312
00:17:21,606 --> 00:17:24,816
You should note that
with compute shader,


313
00:17:24,816 --> 00:17:26,715
this is not necessary.


314
00:17:26,715 --> 00:17:29,036
It can be done the same
compute CommandEncoder.


315
00:17:30,256 --> 00:17:32,876
So in summary, we
introduced two new features,


316
00:17:33,406 --> 00:17:36,626
function buffer read-writes and
function texture read-writes.


317
00:17:36,956 --> 00:17:40,686
You can use early fragment
tests to make sure the depth


318
00:17:40,686 --> 00:17:44,086
and stencil test is done
because the execution


319
00:17:44,086 --> 00:17:44,856
of the fragment shader.


320
00:17:45,856 --> 00:17:48,616
You should use a texture fence
if you are trying to read data


321
00:17:48,616 --> 00:17:52,526
from a read-write texture
that you have been writing to.


322
00:17:52,826 --> 00:17:56,276
And finally, when using vertex
and fragment shader to write


323
00:17:56,276 --> 00:17:58,716
to buffers, you need
to make sure


324
00:17:58,716 --> 00:18:00,486
to use a different
RenderCommandEncoder


325
00:18:00,486 --> 00:18:01,926
when you want to
read the results.


326
00:18:03,276 --> 00:18:08,416
So with this, I will hand the
stage to Dan Omachi to talk


327
00:18:08,416 --> 00:18:09,596
to you about wide color.


328
00:18:10,516 --> 00:18:13,546
[ Applause ]


329
00:18:14,046 --> 00:18:14,466
>> Thank you, Charles.


330
00:18:14,466 --> 00:18:14,776
Thank you.


331
00:18:15,646 --> 00:18:17,416
As Charles mentioned,
my name is Dan Omachi.


332
00:18:17,766 --> 00:18:20,746
I work as an engineer in Apple's
GPU Software Frameworks Team


333
00:18:21,336 --> 00:18:22,896
and I'd like to start
off talking to you


334
00:18:22,896 --> 00:18:25,716
about color management,
which isn't a topic


335
00:18:27,256 --> 00:18:31,016
that all developers are
actually familiar with.


336
00:18:32,266 --> 00:18:36,476
So if you are an
artist at either the --


337
00:18:36,476 --> 00:18:39,926
either a texture artist
creating assets for a game


338
00:18:40,416 --> 00:18:43,336
or a photographer editing
photos for distribution,


339
00:18:44,156 --> 00:18:46,276
you would have a particular
color scheme in mind,


340
00:18:46,276 --> 00:18:49,506
and you'd choose
colors pretty carefully.


341
00:18:50,366 --> 00:18:55,376
And you'd want consistency
regardless of the display


342
00:18:55,796 --> 00:18:57,556
on which your content is viewed.


343
00:18:58,576 --> 00:19:00,686
Now it's our responsibility
as developers


344
00:19:00,686 --> 00:19:04,106
and software engineers to
guarantee that consistency.


345
00:19:04,386 --> 00:19:08,616
If you're using a high level
framework like SceneKit,


346
00:19:08,846 --> 00:19:12,866
SpriteKit, or Core Graphics,
much of this work is done


347
00:19:12,866 --> 00:19:14,416
for you, and you


348
00:19:14,416 --> 00:19:17,336
as app developers don't
need to think about it.


349
00:19:17,946 --> 00:19:20,786
Metal, however, is a
much lower level API.


350
00:19:22,806 --> 00:19:26,086
This offers increased
performance and some flexibility


351
00:19:26,086 --> 00:19:30,086
but also places some of this
responsibility in your hands.


352
00:19:32,096 --> 00:19:32,696
So why now?


353
00:19:33,826 --> 00:19:36,066
You've been able to
use different displays


354
00:19:36,446 --> 00:19:37,916
with different color spaces


355
00:19:38,456 --> 00:19:40,456
with Apple devices
for many years now.


356
00:19:41,786 --> 00:19:46,246
Well, late last year, Apple
introduced a couple of iMacs


357
00:19:46,656 --> 00:19:50,256
with a display capable
of rendering colors


358
00:19:50,536 --> 00:19:52,296
in the P3 color space.


359
00:19:52,846 --> 00:19:57,956
And in April, we introduced
the 9.7-inch iPad Pro,


360
00:19:58,436 --> 00:20:00,566
which also has a P3 display.


361
00:20:00,566 --> 00:20:03,726
So what is the P3 color space?


362
00:20:04,196 --> 00:20:06,326
Well, this is a chromaticity
diagram,


363
00:20:06,496 --> 00:20:10,266
and conceptually this
represents all of the colors


364
00:20:10,346 --> 00:20:13,746
in the visual spectrum, in
other words, all the colors


365
00:20:13,906 --> 00:20:15,816
that the normal human
eye can see.


366
00:20:17,666 --> 00:20:20,786
Of that, within this
triangle are colors


367
00:20:21,116 --> 00:20:25,706
that a standard sRGB
display can represent.


368
00:20:26,336 --> 00:20:32,406
The P3 display is able
to represent colors


369
00:20:32,406 --> 00:20:35,106
of a much broader variety.


370
00:20:36,676 --> 00:20:39,146
So here's how it works on macOS.


371
00:20:41,186 --> 00:20:44,116
We want you to be able to
render in any color space


372
00:20:45,696 --> 00:20:50,076
and as I mentioned, high level
frameworks take care of this,


373
00:20:50,076 --> 00:20:52,116
this job of color
management for you


374
00:20:52,396 --> 00:20:54,756
by performing an operation
called color matching


375
00:20:55,066 --> 00:20:58,516
where your color and one
color space is matched to that


376
00:20:58,516 --> 00:21:01,816
of the display color space
so that the same intensity


377
00:21:02,106 --> 00:21:04,286
on the display regardless
of the color space


378
00:21:04,286 --> 00:21:06,716
that you're working
in is displayed.


379
00:21:08,276 --> 00:21:13,626
Now, Metal views by default
are not color managed.


380
00:21:14,516 --> 00:21:16,176
This color match
operation is skipped,


381
00:21:16,286 --> 00:21:20,546
and this generally offers
increased performance.


382
00:21:21,596 --> 00:21:25,966
So by default, you're
ignoring the color profile


383
00:21:25,966 --> 00:21:28,806
of the display, and therefore,


384
00:21:28,806 --> 00:21:33,476
the display will interpret
colors in its own color space.


385
00:21:34,966 --> 00:21:38,326
Now, this means that sRGB
colors will be interpreted


386
00:21:38,326 --> 00:21:42,296
as P3 colors, and rendering will
be inconsistent between the two.


387
00:21:42,516 --> 00:21:46,616
So if this is your application
with an sRGB drawable


388
00:21:47,576 --> 00:21:52,916
and this is the display, well,
when you call present drawable,


389
00:21:53,556 --> 00:21:56,356
these colors become
much saturated.


390
00:21:57,036 --> 00:21:57,906
So why does this happen?


391
00:21:58,166 --> 00:22:01,086
Well, let's go back to
our chromaticity diagram.


392
00:22:02,076 --> 00:22:06,476
This is the most green
color that you can represent


393
00:22:06,526 --> 00:22:11,356
in the sRGB color space,
and in a fragment shader,


394
00:22:12,156 --> 00:22:15,636
you'd represent this as
0.0 in the red channel,


395
00:22:15,906 --> 00:22:20,036
1.0 in the green channel
and 0.0 in the blue channel.


396
00:22:20,856 --> 00:22:24,746
Well, the P3 Display
just takes that raw value


397
00:22:24,746 --> 00:22:25,926
and interprets it,


398
00:22:25,926 --> 00:22:29,716
and it basically thinks
that it's a P3 color.


399
00:22:30,316 --> 00:22:34,806
So you're getting the most
green color of a P3 Display,


400
00:22:35,086 --> 00:22:37,416
which happens to be a
different green color.


401
00:22:38,326 --> 00:22:43,546
Now, for content creation
apps, it's pretty critical


402
00:22:43,546 --> 00:22:48,266
that you get this right because
artists have used careful


403
00:22:48,266 --> 00:22:51,246
consideration to
render their colors.


404
00:22:51,246 --> 00:23:00,696
For games, the effect is more
subtle, but if your designers


405
00:23:01,066 --> 00:23:07,406
and artists are looking for this
dark and gritty theme, well,


406
00:23:07,406 --> 00:23:10,296
they're going to be disappointed
when it looks much more cheerful


407
00:23:10,296 --> 00:23:12,616
and happy when you
plug in a P3 Display.


408
00:23:13,816 --> 00:23:16,636
Also, this problem can get worse


409
00:23:16,976 --> 00:23:20,966
as the industry moves towards
even wider gamut displays.


410
00:23:22,816 --> 00:23:27,956
So, the solution is
really quite simple.


411
00:23:28,926 --> 00:23:33,696
You enable color management
on the NSWindow or CAMetal


412
00:23:33,806 --> 00:23:36,696
by setting the color space
to your working color space,


413
00:23:36,696 --> 00:23:38,776
probably the sRGB color space.


414
00:23:39,446 --> 00:23:43,146
This causes the OS to
perform a color match as part


415
00:23:43,146 --> 00:23:48,656
of its window server's
normal compositing pass.


416
00:23:48,786 --> 00:23:51,136
So if here's your
display, or excuse me,


417
00:23:51,136 --> 00:23:53,286
here's your application
with sRGB drawable


418
00:23:53,696 --> 00:23:54,746
and here's the display,


419
00:23:56,506 --> 00:23:59,406
the window server takes your
drawable when you call present


420
00:23:59,406 --> 00:24:06,936
and performs the color match
before slapping it on the glass.


421
00:24:07,056 --> 00:24:09,656
Now, all right, so now
you've got that consistency.


422
00:24:09,776 --> 00:24:12,346
What if you want to
adopt wide color?


423
00:24:13,066 --> 00:24:18,876
You want to purposefully render
those more intense colors a wide


424
00:24:18,996 --> 00:24:21,116
gamut display is only
capable of rendering.


425
00:24:21,746 --> 00:24:25,206
Well, first of all, you
need to create some content.


426
00:24:25,206 --> 00:24:27,716
You need your artist to
create wider content,


427
00:24:28,856 --> 00:24:33,826
and for that we recommend
using the extended range sRGB


428
00:24:33,896 --> 00:24:36,166
color space.


429
00:24:37,436 --> 00:24:41,796
This allows existing assets that
aren't offered for wide color


430
00:24:42,036 --> 00:24:44,106
to continue working
as they have,


431
00:24:44,666 --> 00:24:47,776
and your shader pipelines don't
need to do anything different.


432
00:24:49,186 --> 00:24:53,896
However, your artists can
create new wider color assets


433
00:24:54,346 --> 00:24:56,606
that will provide much
more intense colors.


434
00:24:57,946 --> 00:25:03,356
So what exactly is the
extended range sRGB?


435
00:25:03,356 --> 00:25:08,666
Well here's the sRGB
triangle and here's P3.


436
00:25:10,946 --> 00:25:13,726
Extended range sRGB
just goes out infinitely


437
00:25:14,116 --> 00:25:19,436
in all directions, meaning
values outside of 0 to 1


438
00:25:19,736 --> 00:25:24,076
in your shader represent
values that can only be viewed


439
00:25:24,466 --> 00:25:28,316
on a wider than sRGB
color display.


440
00:25:30,516 --> 00:25:33,966
So I mentioned values
outside of 0 to 1.


441
00:25:34,406 --> 00:25:37,826
This means that you will need to
use floating point pixel formats


442
00:25:38,066 --> 00:25:43,016
to express such values, and for
source textures we recommend a


443
00:25:43,436 --> 00:25:44,516
couple of formats.


444
00:25:45,136 --> 00:25:48,156
You can use the BC6H
floating point format.


445
00:25:48,486 --> 00:25:50,916
It's a compressed format
offering high performance


446
00:25:51,116 --> 00:25:54,456
as well as the pack float
and shared exponent formats.


447
00:25:55,406 --> 00:25:59,216
For your render targets, you
can use this pack float format


448
00:25:59,866 --> 00:26:04,436
or the RGBA half-float
format, allowing you


449
00:26:04,596 --> 00:26:07,256
to specify these
more intense colors.


450
00:26:09,346 --> 00:26:12,016
Color management on
iOS is a bit simpler.


451
00:26:12,956 --> 00:26:15,816
You always render in
the sRGB color space,


452
00:26:17,446 --> 00:26:19,526
even when targeting
a P3 Display.


453
00:26:19,526 --> 00:26:22,846
Colors are automatically matched
with no performance penalty.


454
00:26:24,206 --> 00:26:28,346
And if you want to use wide
colors, you can make use


455
00:26:28,406 --> 00:26:29,856
of some new pixel formats


456
00:26:31,046 --> 00:26:33,516
that are natively
readable by the display.


457
00:26:34,106 --> 00:26:36,466
There's no compositing
operation that needs to happen.


458
00:26:37,826 --> 00:26:40,656
They can be gamma encoded,
offering better blacks


459
00:26:40,656 --> 00:26:43,716
and allowing you to do linear
blending in your shaders,


460
00:26:44,566 --> 00:26:47,796
and they're efficient for
use as source textures.


461
00:26:48,106 --> 00:26:49,176
All right.


462
00:26:49,176 --> 00:26:52,096
Here are the bit layouts
of these new formats.


463
00:26:52,306 --> 00:26:56,386
So, there are -- there
is a 32-bit RGB format


464
00:26:56,806 --> 00:27:01,556
with 10 bits per channel
and also an RGBA format


465
00:27:01,876 --> 00:27:05,096
with 10 bits per channel
spread across 64 bits.


466
00:27:05,876 --> 00:27:11,016
Now, this, the values
of this 10 bits are --


467
00:27:11,016 --> 00:27:13,276
can express values
outside of 0 to 1.


468
00:27:13,686 --> 00:27:20,276
Values from 0 to 384 represent
negative values, 384 to 894,


469
00:27:20,576 --> 00:27:24,506
the next 510 values, represent
values between 0 and 1


470
00:27:24,826 --> 00:27:28,416
and those greater than
894 represent these more


471
00:27:28,416 --> 00:27:29,186
intense values.


472
00:27:30,386 --> 00:27:37,426
Now, note here that the RGBA
pixel format is twice as large


473
00:27:37,426 --> 00:27:40,406
and therefore uses twice
as much memory and twice


474
00:27:40,406 --> 00:27:43,756
as much bandwidth
as this RGB format.


475
00:27:44,746 --> 00:27:49,096
So, in general, we recommend
that you use this only


476
00:27:49,096 --> 00:27:53,396
in the CAMetal Layer if
you need destination alpha.


477
00:27:54,056 --> 00:27:57,986
All right, so you've made
the decision that you want


478
00:27:57,986 --> 00:28:00,296
to create some wide
gamut content.


479
00:28:00,946 --> 00:28:03,456
How can you do this?


480
00:28:03,456 --> 00:28:04,686
Well, you have an artist --


481
00:28:05,606 --> 00:28:09,156
author using image
editor on macOS,


482
00:28:09,156 --> 00:28:13,376
which supports the P3 color
space, such as Adobe Photoshop.


483
00:28:14,256 --> 00:28:19,496
You can save that image as
a 16-bit per channel PNG


484
00:28:19,496 --> 00:28:24,376
or JPEG using the
display P3 color profile.


485
00:28:24,776 --> 00:28:26,546
Now, once you've got this image,


486
00:28:27,026 --> 00:28:28,926
how do you create
textures from it?


487
00:28:29,766 --> 00:28:30,976
Well, you've got
two solutions here.


488
00:28:31,566 --> 00:28:35,616
The first is you can create your
own asset conditioning tool,


489
00:28:36,306 --> 00:28:42,596
and from that 16-bit per channel
Display P3 image you can convert


490
00:28:43,046 --> 00:28:49,556
using the extended sRGB floating
point color space using either


491
00:28:49,556 --> 00:28:51,526
the ImageIO or vImage
frameworks.


492
00:28:52,106 --> 00:28:55,836
And then from that on
macOS, you'd convert to one


493
00:28:55,836 --> 00:28:58,186
of those floating point pixel
formats I mentioned earlier,


494
00:28:58,726 --> 00:29:00,576
and on iOS you'd convert to one


495
00:29:00,576 --> 00:29:03,196
of those extended range pixel
formats I just mentioned.


496
00:29:03,196 --> 00:29:06,206
All right, so that's option one


497
00:29:06,206 --> 00:29:08,396
if you really want
explicit control


498
00:29:08,396 --> 00:29:09,746
of how your textures are built.


499
00:29:11,636 --> 00:29:14,546
The next option is
to use Xcode support


500
00:29:14,746 --> 00:29:16,806
for textures in asset
catalogues.


501
00:29:17,536 --> 00:29:21,246
With that, will automatically
create extended range sRGB


502
00:29:21,246 --> 00:29:24,236
textures for devices
with a P3 Display,


503
00:29:24,236 --> 00:29:26,756
and I'll talk a little bit more


504
00:29:26,756 --> 00:29:28,886
about asset catalogues
right now.


505
00:29:29,506 --> 00:29:36,006
So for a while now you've been
able to put icons and images


506
00:29:36,006 --> 00:29:39,336
into an asset catalogue
within your Xcode project.


507
00:29:40,706 --> 00:29:45,156
Last year, we introduced app
thinning whereby you can create


508
00:29:45,156 --> 00:29:46,406
a specialized version


509
00:29:46,806 --> 00:29:50,076
for various devices based
upon device capability


510
00:29:50,076 --> 00:29:53,396
such as the amount of memory,
the graphics features set,


511
00:29:54,186 --> 00:30:00,746
or the type of device, whether
it be an iPad, Mac or TV


512
00:30:00,866 --> 00:30:03,676
or watch or even
phone, of course.


513
00:30:05,006 --> 00:30:08,906
And when your app was
downloaded, you download


514
00:30:08,906 --> 00:30:12,486
and install only the single
version of that assess made


515
00:30:12,486 --> 00:30:15,806
for that device with the
capabilities you specified.


516
00:30:16,446 --> 00:30:20,476
The asset was compressed over
the wire and on the device,


517
00:30:20,766 --> 00:30:24,556
saving a lot of storage
on the user's device,


518
00:30:25,366 --> 00:30:28,856
and there were numerous APIs,


519
00:30:28,856 --> 00:30:31,676
which offer efficient
access to those assets.


520
00:30:32,696 --> 00:30:36,856
So now we've added texture
sets to these asset catalogues.


521
00:30:37,726 --> 00:30:39,446
So what does this offer?


522
00:30:39,446 --> 00:30:41,796
Well, storage for mipmap levels.


523
00:30:42,136 --> 00:30:45,466
Textures are more
than just 2D images.


524
00:30:46,656 --> 00:30:50,646
You can perform offline mipmap
generation within Xcode,


525
00:30:51,986 --> 00:30:55,106
will automatically color
match this texture.


526
00:30:55,416 --> 00:30:59,656
So if it's a wide gamut texture
in some different color space,


527
00:30:59,996 --> 00:31:04,616
will perform a color
matching operation to the sRGB


528
00:31:04,616 --> 00:31:07,246
or extended range
sRGB color space.


529
00:31:07,776 --> 00:31:13,306
And I think the most important
feature of this ability here is


530
00:31:13,536 --> 00:31:16,706
that we can choose the
most optimal pixel format


531
00:31:17,136 --> 00:31:20,446
for every device on
which your app can run.


532
00:31:20,756 --> 00:31:25,026
So on newer devices that support
ASTC texture compression,


533
00:31:25,756 --> 00:31:27,346
we can use that format.


534
00:31:27,866 --> 00:31:29,956
On older devices which
don't support that,


535
00:31:30,176 --> 00:31:33,406
we can choose either
a noncompressed format


536
00:31:33,756 --> 00:31:35,526
or some other compressed format.


537
00:31:36,526 --> 00:31:38,956
Additionally, we can
choose a wide color format


538
00:31:39,556 --> 00:31:44,786
for devices with a P3 Display.


539
00:31:44,916 --> 00:31:46,136
So here's the basic workflow.


540
00:31:47,486 --> 00:31:51,156
You create texture
sets within Xcode.


541
00:31:51,606 --> 00:31:54,996
You assign a name to the
set, a unique identifier.


542
00:31:56,236 --> 00:31:59,116
You'll add an image and
indicate basically how


543
00:31:59,116 --> 00:32:01,676
that texture will be used,
whether it's a color texture


544
00:32:02,136 --> 00:32:07,516
or some other type of data like
a normal map or a height map.


545
00:32:07,516 --> 00:32:11,616
Then, you'll -- can
create this texture.


546
00:32:11,616 --> 00:32:13,966
Xcode will build this texture


547
00:32:13,966 --> 00:32:15,526
and deliver it to
your application.


548
00:32:15,656 --> 00:32:19,646
Now, you can create these
texture sets via the Xcode UI


549
00:32:19,646 --> 00:32:21,296
or programmatically.


550
00:32:21,836 --> 00:32:26,876
Once your texture is on the
device, you can supply the name


551
00:32:27,026 --> 00:32:30,966
to MetalKit, and MetalKit
will build a texture,


552
00:32:31,086 --> 00:32:34,666
a Metal texture,
from that asset.


553
00:32:35,576 --> 00:32:38,806
So I'd like to walk you
through the Xcode workflow


554
00:32:38,986 --> 00:32:41,436
to introduce some of
these concepts to you.


555
00:32:43,136 --> 00:32:48,386
So, you'll first select
the asset catalogue


556
00:32:48,526 --> 00:32:51,736
in your projects
navigator sidebar


557
00:32:52,326 --> 00:32:56,196
and then hit this plus button
here, which brings up this menu.


558
00:32:56,606 --> 00:33:00,436
Now, here's where you can create
the various types of sets.


559
00:33:00,436 --> 00:33:06,686
There are image sets, icon
sets, generic data sets,


560
00:33:06,686 --> 00:33:11,506
as well as texture and
cube map texture sets.


561
00:33:12,806 --> 00:33:15,366
So once you've created
your texture set,


562
00:33:15,956 --> 00:33:17,726
you need to name it.


563
00:33:18,076 --> 00:33:20,956
Now, your naming
hierarchy need not be flat.


564
00:33:21,386 --> 00:33:24,526
If you have a number of textures
that are called base texture,


565
00:33:24,716 --> 00:33:27,956
one for each object, you can
create a folder for each object


566
00:33:28,246 --> 00:33:31,776
and stuff your base texture
for that object in that folder,


567
00:33:32,806 --> 00:33:36,326
and your hierarchy can be
as complex as you'd like.


568
00:33:37,266 --> 00:33:43,166
You add your image, and then
you set the interpretation.


569
00:33:43,716 --> 00:33:45,226
Now there are three
options here.


570
00:33:45,756 --> 00:33:49,866
Color, in color NonPremultiplied
perform this color


571
00:33:49,866 --> 00:33:51,106
match operation.


572
00:33:52,066 --> 00:33:56,136
The NonPremultiplied option
will multiply the alpha channel


573
00:33:56,196 --> 00:33:57,726
by your R, B, and G --


574
00:33:57,936 --> 00:34:01,196
RGB channels before
building the texture.


575
00:34:02,096 --> 00:34:08,716
The data option here will
-- is used for normal maps,


576
00:34:08,716 --> 00:34:15,005
height maps, roughness maps,
textures of noncolor type.


577
00:34:15,916 --> 00:34:17,576
Now, this is all you need to do.


578
00:34:18,136 --> 00:34:21,136
Xcode will go off and
build various versions


579
00:34:21,136 --> 00:34:26,536
of this texture, and it
will pick the most optimal


580
00:34:26,536 --> 00:34:27,206
pixel format.


581
00:34:28,966 --> 00:34:33,466
You can, however, have
more explicit control.


582
00:34:33,826 --> 00:34:35,846
You can select any number
of these traits here,


583
00:34:37,366 --> 00:34:39,516
which will open up
a number of buckets


584
00:34:39,516 --> 00:34:40,946
that you can select
to customize.


585
00:34:41,906 --> 00:34:44,826
You can add different
images for each version.


586
00:34:45,196 --> 00:34:47,516
You probably wouldn't
use a different image,


587
00:34:47,616 --> 00:34:49,906
but may be a different
size of an image.


588
00:34:49,906 --> 00:34:53,005
So on a device with
lots of memory,


589
00:34:53,036 --> 00:34:55,536
you can use a bigger
texture, and a device


590
00:34:55,536 --> 00:34:58,526
with a smaller memory, you would
use a much smaller texture.


591
00:34:58,986 --> 00:35:05,226
And then you can specify how
or whether you want mipmaps.


592
00:35:05,896 --> 00:35:08,826
The all option will
generate mipmaps all the way


593
00:35:08,826 --> 00:35:14,436
down to the 1 by 1 level and the
fixed option here will give you


594
00:35:14,436 --> 00:35:17,866
some more explicit control,
such as whether you want


595
00:35:17,866 --> 00:35:21,406
to use a max level and
also whether you want


596
00:35:21,406 --> 00:35:24,366
to have different
images for each level.


597
00:35:24,936 --> 00:35:29,456
And finally, you can override
our automatic selection


598
00:35:30,026 --> 00:35:33,226
of pixel formats.


599
00:35:33,226 --> 00:35:37,266
Now I mentioned that you can
programmatically create these


600
00:35:37,266 --> 00:35:38,036
texture sets.


601
00:35:38,856 --> 00:35:40,886
You don't really want to
go through the Xcode UI


602
00:35:41,396 --> 00:35:44,036
if you've got thousands
of assets.


603
00:35:44,926 --> 00:35:48,336
So there's a pretty
simple directory structure,


604
00:35:48,336 --> 00:35:50,686
and within that directory
structure are a number


605
00:35:50,686 --> 00:35:52,036
of JSON files.


606
00:35:52,636 --> 00:35:57,806
Now these files and directory
structure is fully documented


607
00:35:58,056 --> 00:36:02,146
on the asset catalogue
reference.


608
00:36:02,656 --> 00:36:05,216
So you can create your own
asset conditioning tool


609
00:36:05,446 --> 00:36:08,906
to set up your texture set.


610
00:36:09,876 --> 00:36:12,726
So once you've got this
asset on the device,


611
00:36:12,726 --> 00:36:14,156
how do you make use of it?


612
00:36:14,326 --> 00:36:19,086
Well, you create a MetalKit
texture loader supplying your


613
00:36:19,086 --> 00:36:24,136
Metal device, and then
you supply the name along


614
00:36:24,136 --> 00:36:26,796
with its hierarchy
to the texture loader


615
00:36:27,216 --> 00:36:29,396
and MetalKit will go off
and build that texture.


616
00:36:29,696 --> 00:36:32,636
You can supply a couple
of other options here


617
00:36:32,636 --> 00:36:34,976
such as scale factor if
you have different versions


618
00:36:34,976 --> 00:36:39,976
of the texture for different
scale factors or the bundle


619
00:36:40,046 --> 00:36:42,446
if the asset catalogue is


620
00:36:42,496 --> 00:36:44,266
in something other
than the main bundle.


621
00:36:44,266 --> 00:36:45,406
There are also a couple


622
00:36:45,406 --> 00:36:49,356
of options here that
you can specify.


623
00:36:49,726 --> 00:36:53,226
So I'd really like you to
pay attention to color space


624
00:36:53,586 --> 00:36:55,586
and set your apps apart


625
00:36:55,676 --> 00:36:59,896
by creating content
with wide color.


626
00:37:00,886 --> 00:37:04,306
Asset catalogues can help
you achieve that goal.


627
00:37:04,306 --> 00:37:07,556
As well, they provide a
number of other features


628
00:37:07,826 --> 00:37:09,556
which you can make use of,


629
00:37:09,646 --> 00:37:11,876
such as optimal pixel
format selection.


630
00:37:12,786 --> 00:37:18,386
I'd like to have my colleague
Anna Tikhonova up here to talk


631
00:37:18,386 --> 00:37:19,876
about some exciting improvements


632
00:37:20,146 --> 00:37:21,976
to the Metal Performance
Shaders framework.


633
00:37:22,516 --> 00:37:27,796
[ Applause ]


634
00:37:28,296 --> 00:37:29,616
>> Hi. Good afternoon.


635
00:37:30,196 --> 00:37:31,726
Thank you, Dan, for
the introduction.


636
00:37:31,846 --> 00:37:33,266
As Dan said, my name is Anna.


637
00:37:33,366 --> 00:37:35,416
I'm an engineer on
the GPU Software Team.


638
00:37:35,506 --> 00:37:37,386
So let's talk about
some new additions


639
00:37:37,456 --> 00:37:38,686
to the Metal Performance
Shaders.


640
00:37:40,686 --> 00:37:43,336
We introduced the Metal
Performance Shaders framework


641
00:37:43,336 --> 00:37:45,766
last year in the What's
New in Metal Part 2 talk.


642
00:37:46,006 --> 00:37:47,636
If you haven't seen
that session,


643
00:37:47,746 --> 00:37:49,256
you should definitely
check out the video.


644
00:37:50,336 --> 00:37:52,096
But just to give
you a quick recap,


645
00:37:52,626 --> 00:37:55,176
the Metal Performance Shaders
framework is the framework


646
00:37:55,176 --> 00:37:58,436
of optimized high performance
data parallel algorithms


647
00:37:58,436 --> 00:37:59,536
for the GPU in Metal .


648
00:38:00,896 --> 00:38:02,866
The algorithms are
optimized for iOS,


649
00:38:03,056 --> 00:38:06,036
and they have been available
for you since iOS 9, for the A8


650
00:38:06,036 --> 00:38:07,836
and now the A9 processors.


651
00:38:08,846 --> 00:38:12,066
The framework is designed
to integrate easily


652
00:38:12,176 --> 00:38:15,386
into your Metal applications
and be very simple to use.


653
00:38:16,536 --> 00:38:19,436
It should be as simple as
calling a library function.


654
00:38:20,066 --> 00:38:23,776
So last year, we talked
about following a list


655
00:38:23,776 --> 00:38:27,796
of supported image operations,
and you should watch the video


656
00:38:27,986 --> 00:38:30,066
for lots of details
and examples.


657
00:38:30,716 --> 00:38:32,996
But this year, we've added
some more cool stuff for you.


658
00:38:34,396 --> 00:38:36,846
We've added wide color
conversion, which you can use


659
00:38:36,846 --> 00:38:39,596
to convert your Metal textures
between different color spaces.


660
00:38:40,456 --> 00:38:45,326
You can convert between
RGB, sRGB, grayscale, CMYK,


661
00:38:45,636 --> 00:38:49,106
C3 and any color
space you define.


662
00:38:49,826 --> 00:38:53,536
We've also added Gaussian
pyramids, which you can use


663
00:38:53,536 --> 00:38:56,146
to create multiscaler
presentations of image data


664
00:38:56,386 --> 00:38:58,796
on the GPU to enable
multiscale algorithms.


665
00:38:59,726 --> 00:39:03,226
They can also be used for
common optical flow algorithms,


666
00:39:03,886 --> 00:39:06,666
image blending, and
high-quality mipmap generation.


667
00:39:08,006 --> 00:39:11,196
And finally, we've added
convolutional neural networks,


668
00:39:11,406 --> 00:39:13,166
or CNNs, which are used


669
00:39:13,356 --> 00:39:15,476
to accelerate deep
learning algorithms.


670
00:39:16,376 --> 00:39:18,286
This is going to be the
main topic of this talk.


671
00:39:18,586 --> 00:39:20,046
So let's just dive right in.


672
00:39:20,636 --> 00:39:23,026
First of all, what
is deep learning?


673
00:39:24,336 --> 00:39:27,436
Deep learning is a field of
machine learning which goal is


674
00:39:27,436 --> 00:39:28,356
to answer this question.


675
00:39:28,666 --> 00:39:31,896
Can a machine do the same
task that a human can do?


676
00:39:32,346 --> 00:39:34,346
Well, what types of
tasks am I talking about?


677
00:39:35,176 --> 00:39:36,946
Each one of you has an
iPhone in your pocket.


678
00:39:37,116 --> 00:39:38,996
You probably took a
few pictures today,


679
00:39:39,436 --> 00:39:42,896
and all of us are constantly
exposed to images and videos


680
00:39:42,896 --> 00:39:46,076
on the Web every day, on
news sites, on social media.


681
00:39:47,436 --> 00:39:50,866
When you see an image, you
know instantly what is depicted


682
00:39:50,866 --> 00:39:51,136
on it.


683
00:39:51,796 --> 00:39:53,006
You can detect faces.


684
00:39:53,496 --> 00:39:55,206
If you know these
people, you can tag them.


685
00:39:55,366 --> 00:39:56,606
You can annotate this image.


686
00:39:56,936 --> 00:39:59,076
And this works well
for a single image,


687
00:40:00,006 --> 00:40:03,246
but what if you have more
images and even more images?


688
00:40:03,416 --> 00:40:06,946
Think about all of the images
uploaded to the Web every day.


689
00:40:07,716 --> 00:40:10,116
No human can hand
annotate this many images.


690
00:40:10,656 --> 00:40:12,766
So deep learning is a technique


691
00:40:12,846 --> 00:40:14,256
for solving these
kinds of problems.


692
00:40:15,566 --> 00:40:18,176
It can be used for sifting
through large amounts of data


693
00:40:18,356 --> 00:40:21,536
and for answering questions
such as, "Who's in this image?"


694
00:40:21,536 --> 00:40:22,426
And "Where was it taken?"


695
00:40:23,026 --> 00:40:25,926
But I'm using image-based
examples in this talk


696
00:40:25,926 --> 00:40:26,906
because they are visual.


697
00:40:27,096 --> 00:40:29,846
So they are a great fit for
this type of a presentation,


698
00:40:30,246 --> 00:40:31,466
but I just want to mention


699
00:40:31,746 --> 00:40:33,656
that deep learning
algorithms can be used


700
00:40:33,656 --> 00:40:34,736
for other types of data.


701
00:40:34,956 --> 00:40:39,356
For example, other types
of signal like audio


702
00:40:39,506 --> 00:40:41,576
to do speech recognition
and haptics


703
00:40:41,666 --> 00:40:42,886
to create the sense of touch.


704
00:40:45,316 --> 00:40:47,436
Deep learning algorithms
have two phases.


705
00:40:48,186 --> 00:40:49,916
The first one is
the training phase.


706
00:40:50,486 --> 00:40:53,426
So let's talk about it,
give a specific example.


707
00:40:53,846 --> 00:40:56,296
So image that you
want train your system


708
00:40:56,386 --> 00:40:58,176
to categorize images
into classes.


709
00:40:58,696 --> 00:40:59,626
This is an image of a cat.


710
00:40:59,946 --> 00:41:01,026
This is an image of a dog.


711
00:41:01,136 --> 00:41:02,186
This is the image of a rabbit.


712
00:41:03,346 --> 00:41:07,186
This is a labor intensive task
that requires a large number


713
00:41:07,186 --> 00:41:10,596
of images, hand-labeled
annotated images


714
00:41:10,596 --> 00:41:12,706
for each one of these
categories.


715
00:41:13,846 --> 00:41:16,706
So for example, if you
want to train your system


716
00:41:16,706 --> 00:41:20,396
to recognize cats, you need to
feed it a large number of images


717
00:41:20,396 --> 00:41:24,146
of cats all labeled, and
same for your rabbits


718
00:41:24,226 --> 00:41:26,356
and all the other animals
that you want your system


719
00:41:26,356 --> 00:41:29,666
to be able to recognize.


720
00:41:30,256 --> 00:41:34,436
This is a one-time
computationally expensive step.


721
00:41:34,836 --> 00:41:36,466
It's usually done offline,
and there are plenty


722
00:41:36,466 --> 00:41:38,116
of training packages
available out there.


723
00:41:38,596 --> 00:41:42,156
The result of the training
phase is trained parameters.


724
00:41:42,956 --> 00:41:45,186
So I will not talk
about them right now,


725
00:41:45,186 --> 00:41:46,596
but we will get back
to them later.


726
00:41:48,246 --> 00:41:51,266
The trained parameters are
required for the next phase,


727
00:41:51,396 --> 00:41:52,456
which is the inference phase.


728
00:41:53,536 --> 00:41:56,686
This is the phase where
your system is presented


729
00:41:56,686 --> 00:41:59,496
with a new image that has
never seen before, and it needs


730
00:41:59,496 --> 00:42:00,716
to classify in real-time.


731
00:42:00,936 --> 00:42:03,966
So in this example, the system
correctly classified this image


732
00:42:03,996 --> 00:42:05,096
as an image of a cat.


733
00:42:06,636 --> 00:42:09,446
We provide GPU acceleration
for the inference phase.


734
00:42:10,076 --> 00:42:12,646
Specifically, we give
you the building blocks


735
00:42:12,996 --> 00:42:15,686
to build your inference
networks for the GPU.


736
00:42:17,266 --> 00:42:20,246
So let's now talk about what
are the convolutional neural


737
00:42:20,246 --> 00:42:24,976
networks and what are these
building blocks we provide?


738
00:42:25,496 --> 00:42:27,456
The convolutional
neural networks, or CNNs,


739
00:42:27,696 --> 00:42:30,706
are biologically
inspired and designed


740
00:42:30,706 --> 00:42:32,146
to resemble the visual cortex.


741
00:42:33,336 --> 00:42:36,826
When our brain processes visual
input, the first hierarchy


742
00:42:36,826 --> 00:42:38,686
of neurons that receive
information


743
00:42:38,826 --> 00:42:42,526
in the visual cortex are
sensitive to specific edges


744
00:42:42,526 --> 00:42:45,296
or blobs of color, while
the brain regions further


745
00:42:45,296 --> 00:42:49,256
down the visual pipeline respond
to more complex structures


746
00:42:49,496 --> 00:42:51,066
like faces or kinds of animals.


747
00:42:51,676 --> 00:42:53,326
So in a very similar way,


748
00:42:54,146 --> 00:42:57,246
the convolutional neural
networks are organized


749
00:42:57,486 --> 00:43:00,286
into layers of neurons
which are trained


750
00:43:00,356 --> 00:43:02,646
to recognize increasingly
complex features.


751
00:43:04,846 --> 00:43:08,426
So the first layers are trained
to recognize low level features


752
00:43:08,836 --> 00:43:11,216
like edges and blobs of color,


753
00:43:11,636 --> 00:43:13,566
while the subsequent
layers are trained


754
00:43:13,566 --> 00:43:15,156
to recognize higher
level features.


755
00:43:15,156 --> 00:43:17,216
So for example, if we
are doing face detection,


756
00:43:17,566 --> 00:43:20,026
then will have layers that will
recognize features like noses,


757
00:43:20,326 --> 00:43:23,716
eyes, cheeks, and then
combination of these features,


758
00:43:23,816 --> 00:43:24,966
and then finally faces.


759
00:43:26,586 --> 00:43:30,116
And then the final few layers
combine all the generated


760
00:43:30,116 --> 00:43:33,106
information to produce the
final output for the network,


761
00:43:33,346 --> 00:43:35,886
such as the probability that
there is a face in the image.


762
00:43:36,986 --> 00:43:38,336
And I keep mentioning features.


763
00:43:39,026 --> 00:43:43,626
Think of a feature as a
filter that filters the input


764
00:43:43,626 --> 00:43:45,066
for that feature,
such as a nose,


765
00:43:45,786 --> 00:43:48,816
and if that information is
found, it's passed along.


766
00:43:49,686 --> 00:43:52,966
If that feature is found, this
information is passed along


767
00:43:53,356 --> 00:43:54,366
to the subsequent layers.


768
00:43:54,916 --> 00:43:57,246
And, of course, we need to
look for many such features.


769
00:43:57,246 --> 00:43:59,946
So if we're doing face
detection, then looking


770
00:43:59,946 --> 00:44:01,946
for just noses is
simply not enough.


771
00:44:02,306 --> 00:44:04,906
We also need to look for other
facial features like cheeks,


772
00:44:05,406 --> 00:44:07,486
eyes, and then combinations
of such features.


773
00:44:07,916 --> 00:44:09,816
So we need many of
these feature filters.


774
00:44:11,736 --> 00:44:14,306
So now that I've covered
convolutional neural networks,


775
00:44:14,886 --> 00:44:16,646
let's talk about the
building blocks we'll provide.


776
00:44:17,436 --> 00:44:19,866
The first building
block is your data.


777
00:44:20,746 --> 00:44:24,176
We want you to use MPS images
and MPS temporary images,


778
00:44:24,266 --> 00:44:27,516
which we added specifically to
support convolutional networks.


779
00:44:28,246 --> 00:44:31,346
They provide and optimize layout
for your data, for your input


780
00:44:31,346 --> 00:44:32,436
and intermediate results.


781
00:44:32,436 --> 00:44:37,986
Think of MPS temporary images
as light-weight MPS images,


782
00:44:39,146 --> 00:44:41,716
which we want you to
use for image data


783
00:44:41,836 --> 00:44:43,046
with a transient lifetime.


784
00:44:44,286 --> 00:44:49,216
MPS temporary images are built
using the Metal resource heaps,


785
00:44:49,386 --> 00:44:54,536
which were described in the
Part 1 of these sessions.


786
00:44:55,546 --> 00:44:58,146
They address some of
the reused cache memory,


787
00:44:59,196 --> 00:45:02,866
and they avoid expensive
allocation


788
00:45:02,866 --> 00:45:04,686
and deallocation of
texture resources.


789
00:45:05,116 --> 00:45:06,936
So the goal is to save
you lots of memory


790
00:45:07,546 --> 00:45:10,196
and to help you manage
intermediate resources.


791
00:45:11,056 --> 00:45:14,946
We also provide a collection
of layers, which you can use


792
00:45:15,266 --> 00:45:17,376
to create your inference
networks.


793
00:45:17,986 --> 00:45:20,436
But you may be thinking
right now, "How do I know


794
00:45:20,756 --> 00:45:22,496
which building blocks
I actually need


795
00:45:22,756 --> 00:45:24,496
to build my own inference
network?"


796
00:45:25,856 --> 00:45:28,666
So the answer is
trained parameters.


797
00:45:29,626 --> 00:45:32,806
The trained parameters, I
mentioned them previously


798
00:45:32,806 --> 00:45:34,456
when we talked about
the training phase.


799
00:45:34,666 --> 00:45:37,786
The trained parameters give
you a complete recipe for how


800
00:45:37,786 --> 00:45:39,216
to build your inference
networks.


801
00:45:39,926 --> 00:45:42,976
They tell you how many
layers you will have,


802
00:45:43,276 --> 00:45:45,756
what kind they will be, in
which order they will appear,


803
00:45:45,756 --> 00:45:48,236
and you also get all those
feature filters for every layer.


804
00:45:50,286 --> 00:45:53,586
So we take care of everything
under the hood to make sure


805
00:45:53,586 --> 00:45:56,406
that the networks you build
using these building blocks have


806
00:45:56,496 --> 00:45:59,156
the best possible
performance on all iOS GPUs.


807
00:45:59,516 --> 00:46:01,736
All you have to do
is to mine your data


808
00:46:02,206 --> 00:46:04,536
into this optimized
layout that we provide


809
00:46:05,276 --> 00:46:08,216
and to call library
functions to create the layers


810
00:46:08,216 --> 00:46:09,026
that make up your network.


811
00:46:10,636 --> 00:46:14,106
So now let's discuss all these
building blocks in more detail,


812
00:46:14,386 --> 00:46:17,006
but let's do it in a context
of a specific example.


813
00:46:20,416 --> 00:46:25,516
So in this demo, I have a system


814
00:46:25,576 --> 00:46:27,726
that has been trained
to detect smiles.


815
00:46:28,736 --> 00:46:30,256
And what we'll have is


816
00:46:30,256 --> 00:46:33,716
in real-time the system will
detect whether I am smiling


817
00:46:33,716 --> 00:46:33,956
or not.


818
00:46:34,066 --> 00:46:35,736
So I will first smile,
and then I will frown,


819
00:46:36,306 --> 00:46:37,976
and you will see the
system report just that.


820
00:46:44,516 --> 00:46:46,946
[ Laughter ]


821
00:46:47,446 --> 00:46:48,446
All right.


822
00:46:48,446 --> 00:46:48,976
So that [inaudible] my demo.


823
00:46:49,516 --> 00:46:54,646
[ Applause ]


824
00:46:55,146 --> 00:46:58,376
Okay. So now let's take a
look at the building blocks


825
00:46:58,376 --> 00:47:00,726
that I needed to build
this kind of a network.


826
00:47:01,236 --> 00:47:03,206
So the first building
block we're going to talk


827
00:47:03,206 --> 00:47:04,836
about is the convolution layer.


828
00:47:05,666 --> 00:47:08,206
It's the core building block of
convolutional neural networks,


829
00:47:08,536 --> 00:47:11,156
and its goal is to
recognize features and input.


830
00:47:11,156 --> 00:47:12,946
And it's called a
convolutional layer


831
00:47:13,256 --> 00:47:15,356
because it performs a
convolution on the input.


832
00:47:15,746 --> 00:47:18,536
So let's recall how
regular convolution works.


833
00:47:18,626 --> 00:47:21,176
You have your input and your
output and in this case a 5


834
00:47:21,176 --> 00:47:22,966
by 5 pixel filter
with some weight.


835
00:47:23,716 --> 00:47:26,086
And in order to compute
the value of this pixel


836
00:47:26,526 --> 00:47:27,476
in your output, you need


837
00:47:27,476 --> 00:47:29,246
to convolve the filter
with the input.


838
00:47:30,636 --> 00:47:31,176
Pretty easy.


839
00:47:32,036 --> 00:47:34,616
The convolution layer
is a generalization


840
00:47:34,646 --> 00:47:35,776
of regular convolution.


841
00:47:36,746 --> 00:47:38,476
It allows you to have
multiple filters.


842
00:47:39,286 --> 00:47:42,306
The different filters are
applied to the input separately,


843
00:47:42,486 --> 00:47:44,106
resulting in different
output channels.


844
00:47:44,206 --> 00:47:45,706
So if you have 16 filters.


845
00:47:45,706 --> 00:47:47,596
That means you have
16 output channels.


846
00:47:48,106 --> 00:47:51,426
So in order to get the value of
this pixel in the first channel


847
00:47:51,426 --> 00:47:54,216
of the output, you need
to take the first filter


848
00:47:54,456 --> 00:47:56,446
and convolve it with the input.


849
00:47:56,446 --> 00:48:00,416
And in order to get the value of
this pixel in the second channel


850
00:48:00,416 --> 00:48:02,836
of the output, you need
to take the second filter


851
00:48:03,116 --> 00:48:04,336
and convolve it with your input.


852
00:48:05,166 --> 00:48:07,546
Of course, in our examples,


853
00:48:07,596 --> 00:48:09,896
mild detection we are
dealing with color images.


854
00:48:10,226 --> 00:48:13,376
So that means that your input
actually has three separate


855
00:48:13,376 --> 00:48:16,856
channels, and just because
of how convolutional neural


856
00:48:16,856 --> 00:48:20,886
networks work, you need
three sets of 16 filters


857
00:48:21,326 --> 00:48:23,846
where you have one set
for each input channel.


858
00:48:24,956 --> 00:48:28,166
And then you apply
the different filters


859
00:48:29,096 --> 00:48:37,106
to separate input channels
and combine the results


860
00:48:37,856 --> 00:48:42,616
to get a single output value.


861
00:48:43,366 --> 00:48:45,426
So this is how you
would create one


862
00:48:45,426 --> 00:48:48,006
of these convolution
layers in our framework.


863
00:48:48,756 --> 00:48:51,596
You first create a descriptor
and specify such parameters


864
00:48:51,706 --> 00:48:54,516
as the width and height of the
filters you're going to use


865
00:48:54,906 --> 00:48:57,086
and then the number of
input and output channels.


866
00:48:57,846 --> 00:49:02,236
And then you create
a convolution layer


867
00:49:02,236 --> 00:49:06,876
from this descriptor and
provide the actual data


868
00:49:07,226 --> 00:49:08,786
for the feature filters,
which you get


869
00:49:08,836 --> 00:49:09,866
from the trained parameters.


870
00:49:12,846 --> 00:49:14,896
The next layer we are going to
talk about is the pooling layer.


871
00:49:15,866 --> 00:49:17,906
The function of the
pooling layer is


872
00:49:17,906 --> 00:49:21,546
to progressively reduce the
spatial size of the network,


873
00:49:21,686 --> 00:49:23,306
which reduces the
amount of competition


874
00:49:23,306 --> 00:49:24,296
for the subsequent layers.


875
00:49:24,686 --> 00:49:26,626
And it's common to
insert a pooling of the


876
00:49:26,626 --> 00:49:28,726
in between successive
convolution layers.


877
00:49:29,646 --> 00:49:33,906
Another function of the
pooling layer is to summarize


878
00:49:33,956 --> 00:49:37,546
or condense information
in a region of the input,


879
00:49:37,546 --> 00:49:41,896
and it would provide two pooling
operations, maximum and average.


880
00:49:42,816 --> 00:49:48,496
So in this example, we take a 2
by 2 pixel region of the input.


881
00:49:49,536 --> 00:49:53,786
We take the maximum value
and store it as our output.


882
00:49:54,356 --> 00:49:58,306
And this is the API
you need to use


883
00:49:58,306 --> 00:50:00,706
in the Metal Performance
Shaders framework to create one


884
00:50:00,706 --> 00:50:01,586
of these pooling layers.


885
00:50:02,086 --> 00:50:04,516
It's common to use
the max operation


886
00:50:06,226 --> 00:50:10,926
with a filter size of 2 by 2.


887
00:50:11,316 --> 00:50:14,416
The fully connected layer is
a layer where every neuron


888
00:50:14,446 --> 00:50:17,466
in the input is connected to
every neuron in the output.


889
00:50:18,206 --> 00:50:20,926
But think about it as a special
type of a convolution layer


890
00:50:21,556 --> 00:50:24,836
where the filter size is
the same as your input size.


891
00:50:24,836 --> 00:50:28,306
So in this example, we have
a filter of the same size


892
00:50:28,306 --> 00:50:30,466
as the input, and
we convolve them


893
00:50:30,546 --> 00:50:32,036
to get a single output value.


894
00:50:32,256 --> 00:50:35,736
So in this architecture,
the convolution


895
00:50:35,736 --> 00:50:38,666
and pooling layers operate
on regions of input,


896
00:50:38,976 --> 00:50:41,286
while the fully connected
layer can be used


897
00:50:41,496 --> 00:50:44,846
to aggregate information
from across the entire input.


898
00:50:45,506 --> 00:50:47,986
It's usually one of the
last layers in your network,


899
00:50:48,096 --> 00:50:50,746
and this is where your final
decision-making is taking place


900
00:50:51,216 --> 00:50:55,346
and you create -- you generate
the output for the network,


901
00:50:55,846 --> 00:50:58,276
such as the probability that
there's a smile in the image.


902
00:50:58,896 --> 00:51:03,276
And this is how you
would create one


903
00:51:03,276 --> 00:51:04,506
of these fully connected layers


904
00:51:04,676 --> 00:51:06,496
in the Metal Performance
Shaders framework.


905
00:51:07,196 --> 00:51:08,826
You create a convolution
descriptor


906
00:51:08,826 --> 00:51:11,016
because this is a special
type of a convolution layer,


907
00:51:11,446 --> 00:51:13,416
and then you create a
fully connected layer


908
00:51:14,176 --> 00:51:15,026
from this descriptor.


909
00:51:15,616 --> 00:51:18,156
We'll also provide
some additional layers,


910
00:51:18,156 --> 00:51:20,636
which I'm not going to cover
in detail in this presentation


911
00:51:21,156 --> 00:51:23,266
but they are described
in our documentation.


912
00:51:23,526 --> 00:51:26,666
We provide the neural
layer, which is usually used


913
00:51:26,666 --> 00:51:28,516
in conjunction with
the convolution layer,


914
00:51:28,756 --> 00:51:31,796
and we also provide the soft
max and normalization layers.


915
00:51:32,816 --> 00:51:35,196
So now that we've
covered all of the layers,


916
00:51:35,606 --> 00:51:36,626
let's talk about your data.


917
00:51:37,386 --> 00:51:39,396
I mentioned that you
should be using MPS images.


918
00:51:40,006 --> 00:51:40,856
So what are they really?


919
00:51:42,356 --> 00:51:45,386
Most of you are already
familiar with Metal textures.


920
00:51:45,526 --> 00:51:50,596
So this is a 2D Metal
texture with multiple channels


921
00:51:50,906 --> 00:51:53,326
where every channel corresponds
to a color channel and alpha.


922
00:51:54,276 --> 00:51:56,906
And I mentioned in my
previous examples that we need


923
00:51:56,906 --> 00:52:00,026
to create images with
multiple channels,


924
00:52:00,706 --> 00:52:01,946
for example, 32 channels.


925
00:52:02,286 --> 00:52:04,046
If we have 32 feature filters,


926
00:52:04,296 --> 00:52:05,706
we need to create
an output channel --


927
00:52:05,706 --> 00:52:08,486
an output image that
has 32 channels.


928
00:52:08,726 --> 00:52:09,466
So how do we do this?


929
00:52:10,386 --> 00:52:15,456
So an MPS image is really
a Metal 2D array texture


930
00:52:15,456 --> 00:52:16,376
with multiple slices.


931
00:52:16,946 --> 00:52:18,736
And when you're creating
an MPS image,


932
00:52:19,206 --> 00:52:21,016
all you really should
care about is


933
00:52:21,016 --> 00:52:25,006
that you are creating an image
with 32 -- with 32 channels.


934
00:52:25,546 --> 00:52:29,606
But sometimes you may need to
reach the MPS image data back


935
00:52:29,636 --> 00:52:31,586
to the CPU, or you may want


936
00:52:31,586 --> 00:52:35,496
to use an existing Metal 2D
array texture as your MPS image.


937
00:52:35,916 --> 00:52:37,636
So for those cases,
you need to know


938
00:52:37,846 --> 00:52:41,656
that we use a special
packed layout for your data.


939
00:52:42,146 --> 00:52:44,106
So every pixel in a slice


940
00:52:44,106 --> 00:52:47,156
of the structure contains
the data for four channels.


941
00:52:48,726 --> 00:52:52,516
So a 32-channel image would
really just have eight slices.


942
00:52:53,356 --> 00:52:57,046
And this is the API you
need to use to create one


943
00:52:57,046 --> 00:52:59,066
of the MPS images
in our framework.


944
00:52:59,606 --> 00:53:02,616
You first create a descriptor
and specify such parameters


945
00:53:02,656 --> 00:53:07,016
as the channel for data format
with the height of the image


946
00:53:07,266 --> 00:53:08,686
and the number of channels.


947
00:53:09,886 --> 00:53:11,456
And then you create an MPS image


948
00:53:11,606 --> 00:53:13,146
from this descriptor,
pretty simple.


949
00:53:14,376 --> 00:53:18,066
Of course, if you have
small input images,


950
00:53:18,136 --> 00:53:20,476
then you should batch them
to better utilize the GPU,


951
00:53:21,156 --> 00:53:24,066
and we provide a simple
mechanism for you to do this.


952
00:53:24,546 --> 00:53:28,696
So in this example, we create
an array of 100 MPS images.


953
00:53:30,596 --> 00:53:33,136
Okay, so now that we've
covered all the layers,


954
00:53:33,136 --> 00:53:35,866
we've covered data, and
now let's take a look


955
00:53:35,866 --> 00:53:39,426
at the actual network you need
to build to do smile detection.


956
00:53:40,166 --> 00:53:42,666
So we start with our
inputs, and now we're going


957
00:53:42,666 --> 00:53:45,126
to use the trained parameters
that I keep mentioning


958
00:53:46,096 --> 00:53:47,766
to help us build this network.


959
00:53:48,346 --> 00:53:51,446
So the trained parameters
tell us that the first layer


960
00:53:51,546 --> 00:53:53,786
in this network is going
to be a convolution layer,


961
00:53:54,056 --> 00:53:56,066
which takes a three-channel
images input


962
00:53:56,426 --> 00:53:58,286
and outputs a 16-channel image.


963
00:53:59,656 --> 00:54:03,456
The trained parameters also give
us the three sets of 16 filters


964
00:54:03,896 --> 00:54:07,796
for this layer, and these
colorful blue images show you


965
00:54:08,726 --> 00:54:12,136
the visualization of
the output channels


966
00:54:12,436 --> 00:54:14,496
after the filters have
been applied to the input.


967
00:54:16,686 --> 00:54:18,596
The next layer is
a pooling layer,


968
00:54:18,796 --> 00:54:22,336
which reduces the spatial
resolution of the output


969
00:54:22,336 --> 00:54:25,406
of the convolution layer by a
factor of two in each dimension.


970
00:54:27,166 --> 00:54:28,346
The trained parameters tell us


971
00:54:28,346 --> 00:54:30,706
that the next layer is
another convolution layer,


972
00:54:31,056 --> 00:54:33,116
which takes a 16-channel
images input


973
00:54:33,336 --> 00:54:37,046
and outputs a 16-channel image,
which is further down reduced


974
00:54:37,046 --> 00:54:38,596
in size by the next
pooling layer,


975
00:54:39,206 --> 00:54:42,396
and so on until we
get to our output.


976
00:54:43,616 --> 00:54:46,656
As you can see, this
network has a series


977
00:54:46,656 --> 00:54:49,576
of convolution layers
followed by the pooling layers,


978
00:54:50,146 --> 00:54:53,136
and the last two layers are
the fully connected layers,


979
00:54:53,296 --> 00:54:56,546
which generate the final
output for your network.


980
00:54:58,126 --> 00:55:00,516
So now that we know what this
network should look like,


981
00:55:00,516 --> 00:55:03,936
and this is very common for a
convolutional neural network


982
00:55:03,936 --> 00:55:07,036
for inference, so now
let's write the code


983
00:55:07,366 --> 00:55:08,596
to create it in our framework.


984
00:55:09,746 --> 00:55:12,866
So the first step is
to create the layers.


985
00:55:13,576 --> 00:55:15,736
Once again, the trained
parameters tell us that we need


986
00:55:15,916 --> 00:55:20,376
to have four convolution layers
in our network and I'm showing


987
00:55:20,376 --> 00:55:23,026
that the code had to create
one of them for simplicity


988
00:55:23,406 --> 00:55:25,966
but as you can see, I'm
using exactly the same API


989
00:55:26,586 --> 00:55:27,636
that I've showed you before.


990
00:55:28,356 --> 00:55:30,596
Then we need to create
our pooling layer.


991
00:55:31,426 --> 00:55:33,886
We just need one because
we're always going


992
00:55:33,886 --> 00:55:37,136
to be using the max operation
with a filter size of 2 by 2.


993
00:55:38,166 --> 00:55:40,716
And we also need to create
two fully connected layers,


994
00:55:41,036 --> 00:55:42,896
and once again I'm only
showing you the code


995
00:55:42,896 --> 00:55:44,416
for one for simplicity.


996
00:55:45,226 --> 00:55:48,276
And now, we need to take
care of our input and output.


997
00:55:48,726 --> 00:55:51,546
In this particular
example, I'm assuming


998
00:55:51,546 --> 00:55:55,286
that we have an existing Metal
app and you have some textures


999
00:55:55,286 --> 00:55:57,196
that you would like to use
for your input and output,


1000
00:55:57,616 --> 00:56:01,396
and this is the API that you
need to use to create MPS images


1001
00:56:01,766 --> 00:56:03,226
from existing Metal textures.


1002
00:56:03,846 --> 00:56:08,336
And so the last step is
to encode all your layers


1003
00:56:08,716 --> 00:56:11,556
into an existing command
buffer in the order prescribed


1004
00:56:11,616 --> 00:56:12,596
by the trained parameters.


1005
00:56:14,686 --> 00:56:18,256
So we have our input and our
outputs, and now we notice


1006
00:56:18,256 --> 00:56:21,156
that we need one more
thing to take care of.


1007
00:56:21,156 --> 00:56:24,306
We need to store the output
of the first layer somewhere.


1008
00:56:24,476 --> 00:56:28,276
So let's use MPS
temporary images for that.


1009
00:56:28,466 --> 00:56:31,106
This is how you would create
an MPS temporary image.


1010
00:56:31,526 --> 00:56:33,306
As you can see, this
is very similar


1011
00:56:33,306 --> 00:56:35,596
to the way you would
create a regular MPS image.


1012
00:56:36,706 --> 00:56:40,286
And now we immediately use it
when we encode the first layer.


1013
00:56:40,896 --> 00:56:43,756
And the temporary image
will go away as soon


1014
00:56:43,756 --> 00:56:45,096
as the command buffer
is submitted.


1015
00:56:45,986 --> 00:56:47,496
And then we continue.


1016
00:56:47,496 --> 00:56:50,396
We create another temporary
image to store the output


1017
00:56:50,396 --> 00:56:54,516
of the second layer, and so
on until we get to our output.


1018
00:56:56,026 --> 00:56:56,446
That's it.


1019
00:56:56,446 --> 00:56:59,586
And just to tie it
all back together,


1020
00:57:00,146 --> 00:57:03,606
the order in which you encode
the layers matches the network


1021
00:57:03,606 --> 00:57:06,016
diagram that I showed
you earlier exactly,


1022
00:57:06,596 --> 00:57:09,016
so starting from the input
and all the way to the output.


1023
00:57:10,156 --> 00:57:12,876
So now we worked through
a pretty simple example.


1024
00:57:13,356 --> 00:57:15,146
Let's look at a more
complex one.


1025
00:57:17,006 --> 00:57:18,956
We've ported the
inception inference network


1026
00:57:18,956 --> 00:57:21,746
from tensor flow to run
using the Metal Performance


1027
00:57:21,746 --> 00:57:22,426
Shaders framework.


1028
00:57:23,506 --> 00:57:26,056
This is a very commonly
used inference network


1029
00:57:26,056 --> 00:57:28,776
for object detection, and
this is the full diagram


1030
00:57:29,626 --> 00:57:30,216
for this network.


1031
00:57:31,366 --> 00:57:34,516
As you can see, this
network is a lot more complex


1032
00:57:34,546 --> 00:57:35,846
that the previous
one I showed you.


1033
00:57:36,686 --> 00:57:37,916
It has over 100 layers.


1034
00:57:38,296 --> 00:57:40,296
But just to remind you,
all you have to do is


1035
00:57:40,296 --> 00:57:42,466
to call some library functions
to create these layers.


1036
00:57:43,676 --> 00:57:45,976
And now first, let's take a
look at this network in action.


1037
00:57:51,046 --> 00:57:53,726
So here I have a collection of
images of different objects,


1038
00:57:54,226 --> 00:57:56,086
and as soon as I
tap on this image,


1039
00:57:56,446 --> 00:58:00,446
we will run the inference
network in real-time


1040
00:58:00,446 --> 00:58:02,686
and it will report
the top five guesses


1041
00:58:02,686 --> 00:58:04,476
for what it thinks
this object is.


1042
00:58:05,236 --> 00:58:07,836
So the top guess is
that it's a zebra.


1043
00:58:09,606 --> 00:58:14,306
Then this is a pickup
truck, and this a volcano.


1044
00:58:14,646 --> 00:58:16,886
So that looks pretty good
to me, but of course,


1045
00:58:17,606 --> 00:58:20,256
let's do a real live demo
right here on this stage.


1046
00:58:20,946 --> 00:58:24,816
And we'll take a picture
of this water bottle,


1047
00:58:24,816 --> 00:58:30,866
and let's use this
image, water bottle.


1048
00:58:31,516 --> 00:58:39,746
[ Applause ]


1049
00:58:40,246 --> 00:58:42,996
So what I wanted to show
you with this live demo is


1050
00:58:42,996 --> 00:58:46,966
that even a large network
with over 100 layers can run


1051
00:58:47,016 --> 00:58:49,616
in real-time using the Metal
Performance Shaders framework,


1052
00:58:50,166 --> 00:58:50,976
but this is not all.


1053
00:58:51,766 --> 00:58:54,866
I also want to talk about
the memory savings we got


1054
00:58:54,966 --> 00:58:57,446
from using MPS temporary
images in this demo.


1055
00:58:58,286 --> 00:59:01,996
So in the first version of
this demo, we used MPS images


1056
00:59:02,146 --> 00:59:04,726
to store intermediate
results, and we ended


1057
00:59:04,726 --> 00:59:08,796
up needing 74 MPS
images totaling in size


1058
00:59:09,256 --> 00:59:12,076
over 80 megabytes for
the entire network.


1059
00:59:12,786 --> 00:59:15,356
And of course, you don't
have to use 74 images.


1060
00:59:15,356 --> 00:59:18,466
You can come up with your
own clever scheme for how


1061
00:59:18,466 --> 00:59:22,766
to reuse these images, but
this means more stuff to manage


1062
00:59:22,766 --> 00:59:25,306
in your code, and we want to
make sure that our framework is


1063
00:59:25,306 --> 00:59:27,126
as easy for you to
use as possible.


1064
00:59:28,006 --> 00:59:29,586
So in the second
version of the demo,


1065
00:59:30,066 --> 00:59:33,936
we replaced all the MPS images
with MPS temporary images,


1066
00:59:34,876 --> 00:59:36,566
and this gave us
several advantages.


1067
00:59:37,016 --> 00:59:40,076
The first one is reduced
CPU cost in terms of time


1068
00:59:40,076 --> 00:59:45,486
and energy, but also creating
74 temporary images resulted


1069
00:59:45,526 --> 00:59:50,286
in just 5 underlying memory
allocations, totaling just


1070
00:59:50,286 --> 00:59:53,986
over 20 megabytes and this
is 76% of memory savings.


1071
00:59:54,616 --> 00:59:55,296
That's pretty huge.


1072
00:59:56,516 --> 01:00:00,446
So what I showed you with
these two live demos is


1073
01:00:00,486 --> 01:00:02,816
that the Metal Performance
Shaders framework provides


1074
01:00:03,216 --> 01:00:06,356
complete support for building
convolutional neural networks


1075
01:00:06,466 --> 01:00:09,836
for inference, and it's
optimized iOS GPU use.


1076
01:00:10,296 --> 01:00:12,606
So please, use the
convolutional neural networks


1077
01:00:13,086 --> 01:00:15,266
to build some cool apps.


1078
01:00:15,776 --> 01:00:18,806
So this is the end of
What's New in Metal talks,


1079
01:00:19,006 --> 01:00:22,656
and if you haven't seen the
first session, please check


1080
01:00:22,656 --> 01:00:25,496
out the video so you can learn
about such cool new features


1081
01:00:25,536 --> 01:00:29,306
as tessellation, resource heaps,
and memoryless render targets


1082
01:00:29,626 --> 01:00:30,986
and improvements to our tools.


1083
01:00:31,886 --> 01:00:37,136
In this session, we talked
about function specialization


1084
01:00:37,136 --> 01:00:39,636
and function resource
read-writes, white color


1085
01:00:39,686 --> 01:00:42,026
and texture assets,
and new additions


1086
01:00:42,026 --> 01:00:44,166
to the Metal performance
tools, concentrating


1087
01:00:44,166 --> 01:00:45,506
on convolutional
neural networks.


1088
01:00:46,816 --> 01:00:50,136
For more information about this
session, please go to this URL.


1089
01:00:51,656 --> 01:00:53,946
You can catch the
video and get links


1090
01:00:53,996 --> 01:00:56,206
to related documentation
and sample code.


1091
01:00:57,696 --> 01:00:59,806
And here's some information
on the related sessions.


1092
01:01:01,066 --> 01:01:03,006
You could always
check out the videos


1093
01:01:03,456 --> 01:01:05,656
of the past Metal
sessions online,


1094
01:01:06,006 --> 01:01:08,836
but you can also catch
an advanced Metal shader


1095
01:01:08,836 --> 01:01:12,576
optimization talk later today,
and just note the location


1096
01:01:12,576 --> 01:01:14,406
of this talk has
changed to Knob Hill.


1097
01:01:15,986 --> 01:01:18,456
Tomorrow, you have an
opportunity to catch the Working


1098
01:01:18,456 --> 01:01:21,046
with White Color talk
and the Neural Networks


1099
01:01:21,046 --> 01:01:22,976
and Accelerate talk
where you can learn how


1100
01:01:22,976 --> 01:01:23,956
to create neural networks


1101
01:01:23,956 --> 01:01:26,546
for the CPU using the
Accelerate framework.


1102
01:01:27,316 --> 01:01:28,826
So thank you very
much for coming,


1103
01:01:28,936 --> 01:01:30,916
and I hope you have
a great WWDC.


1104
01:01:31,508 --> 01:01:33,508
[ Applause ]

