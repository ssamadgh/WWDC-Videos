1
00:00:25,076 --> 00:00:27,016
>> Hello, good afternoon and


2
00:00:27,016 --> 00:00:28,686
welcome to session 509,


3
00:00:29,026 --> 00:00:30,836
Introducing AirPlay 2 Unlocking


4
00:00:30,836 --> 00:00:31,656
Multi-Room Audio.


5
00:00:32,195 --> 00:00:33,886
My name is David Saracino, I'm a


6
00:00:33,886 --> 00:00:34,666
member of the AirPlay


7
00:00:34,666 --> 00:00:36,286
engineering team, and I'm very


8
00:00:36,286 --> 00:00:37,286
excited to be here this


9
00:00:37,286 --> 00:00:38,486
afternoon to talk to you about


10
00:00:38,486 --> 00:00:39,576
the many new features we've


11
00:00:39,576 --> 00:00:41,126
added to audio to AirPlay over


12
00:00:41,126 --> 00:00:42,756
the last year, and how you can


13
00:00:42,756 --> 00:00:45,006
adopt them in your app.


14
00:00:45,396 --> 00:00:46,466
But before we get into that


15
00:00:46,466 --> 00:00:47,686
adoption and how you can build


16
00:00:47,686 --> 00:00:49,056
this into your app, let's first


17
00:00:49,056 --> 00:00:50,926
take a quick survey of AirPlay


18
00:00:51,226 --> 00:00:52,596
today, and where we're taking it


19
00:00:52,596 --> 00:00:54,056
with AirPlay 2.


20
00:00:54,636 --> 00:00:56,876
So, with AirPlay today, you can


21
00:00:56,876 --> 00:00:58,616
wirelessly send your myriad


22
00:00:58,616 --> 00:01:00,906
screen, audio, or video content


23
00:01:01,196 --> 00:01:02,826
from just about any Apple device


24
00:01:03,066 --> 00:01:04,556
to an Apple TV or AirPlay


25
00:01:04,556 --> 00:01:04,956
speaker.


26
00:01:06,296 --> 00:01:07,596
Over the past year, we've added


27
00:01:07,596 --> 00:01:08,896
so many features to the audio


28
00:01:08,896 --> 00:01:10,276
domain that that's where we're


29
00:01:10,276 --> 00:01:11,166
going to spend the majority of


30
00:01:11,166 --> 00:01:11,506
our talk.


31
00:01:12,996 --> 00:01:14,146
And because we've added so many


32
00:01:14,146 --> 00:01:15,296
features to the audio domain


33
00:01:15,646 --> 00:01:17,326
we're introducing this as a new


34
00:01:17,326 --> 00:01:18,826
feature called AirPlay 2.


35
00:01:21,106 --> 00:01:22,216
So, what is AirPlay 2?


36
00:01:22,216 --> 00:01:25,166
Well with AirPlay 2, you can


37
00:01:25,166 --> 00:01:27,176
still wirelessly send the audio


38
00:01:27,176 --> 00:01:28,726
content from your app to an


39
00:01:28,726 --> 00:01:29,406
AirPlay speaker.


40
00:01:29,926 --> 00:01:32,436
But additionally, with AirPlay


41
00:01:32,436 --> 00:01:34,986
2, you can send that content to


42
00:01:34,986 --> 00:01:37,346
multiple AirPlay 2 speakers with


43
00:01:37,346 --> 00:01:40,546
very tight sync.


44
00:01:40,726 --> 00:01:41,986
Additionally, in AirPlay 2,


45
00:01:42,316 --> 00:01:43,346
we've enhanced the audio


46
00:01:43,346 --> 00:01:44,826
buffering on the AirPlay 2


47
00:01:44,826 --> 00:01:46,616
speakers so that your content


48
00:01:46,616 --> 00:01:47,876
can play with the high degree of


49
00:01:47,876 --> 00:01:49,586
robustness, reliability, and


50
00:01:49,586 --> 00:01:50,416
responsiveness.


51
00:01:51,306 --> 00:01:53,946
And lastly, we're introducing


52
00:01:53,946 --> 00:01:54,906
multi-device control.


53
00:01:55,336 --> 00:01:57,016
And this will allow multiple


54
00:01:57,016 --> 00:01:58,766
Apple devices in your home to


55
00:01:58,766 --> 00:02:00,216
interact with the audio that's


56
00:02:00,216 --> 00:02:01,226
being streamed throughout your


57
00:02:02,086 --> 00:02:02,206
house.


58
00:02:03,336 --> 00:02:04,596
So, where is AirPlay 2


59
00:02:04,596 --> 00:02:05,106
supported?


60
00:02:05,836 --> 00:02:07,406
Well I'm happy to say if you


61
00:02:07,406 --> 00:02:10,015
have iOS, tvOS, or macOS app,


62
00:02:10,485 --> 00:02:11,556
you can take the steps that I'm


63
00:02:11,556 --> 00:02:13,686
about to outline today and build


64
00:02:13,686 --> 00:02:14,686
them into your app to take


65
00:02:14,686 --> 00:02:15,776
advantage of AirPlay 2.


66
00:02:16,326 --> 00:02:19,706
And when you do, your app will


67
00:02:19,706 --> 00:02:21,156
be able to play on a very wide


68
00:02:21,156 --> 00:02:23,086
ecosystem of AirPlay 2 speakers,


69
00:02:23,396 --> 00:02:25,136
including the HomePod, latest


70
00:02:25,136 --> 00:02:26,996
generation Apple TV, and


71
00:02:26,996 --> 00:02:28,436
third-party AirPlay 2 speakers


72
00:02:28,436 --> 00:02:29,206
that will soon be coming to


73
00:02:29,206 --> 00:02:29,526
market.


74
00:02:29,966 --> 00:02:32,666
So, that's the high-level


75
00:02:32,666 --> 00:02:34,126
overview of AirPlay and AirPlay


76
00:02:34,126 --> 00:02:34,386
2.


77
00:02:35,306 --> 00:02:36,366
Let's look at our agenda for the


78
00:02:36,366 --> 00:02:38,436
rest of the talk.


79
00:02:38,646 --> 00:02:39,696
First, we're going to talk about


80
00:02:39,786 --> 00:02:41,146
basic AirPlay 2 adoption.


81
00:02:41,436 --> 00:02:42,636
The steps that you need to do


82
00:02:42,636 --> 00:02:44,346
take in order to get AirPlay 2


83
00:02:44,346 --> 00:02:45,546
inside your app.


84
00:02:46,446 --> 00:02:47,706
Next, we're going to focus on


85
00:02:47,706 --> 00:02:49,036
some advanced playback scenarios


86
00:02:49,036 --> 00:02:50,356
that you may encounter as you


87
00:02:50,356 --> 00:02:51,276
adopt AirPlay 2.


88
00:02:52,096 --> 00:02:53,186
And lastly, we're going to talk


89
00:02:53,186 --> 00:02:54,166
about the availability of


90
00:02:54,166 --> 00:02:55,376
AirPlay 2.


91
00:02:57,396 --> 00:03:00,026
So, let's begin our talk about


92
00:03:00,026 --> 00:03:00,926
AirPlay 2 adoption.


93
00:03:02,146 --> 00:03:04,416
So, in order to adopt AirPlay 2


94
00:03:04,416 --> 00:03:05,886
in your app, there are basically


95
00:03:05,956 --> 00:03:07,176
four steps you need to take.


96
00:03:08,246 --> 00:03:10,386
First, you should identify your


97
00:03:10,386 --> 00:03:12,116
app as presenting long-form


98
00:03:12,116 --> 00:03:12,476
audio.


99
00:03:12,876 --> 00:03:14,176
I'll explain what long-form


100
00:03:14,176 --> 00:03:17,116
audio is in a minute.


101
00:03:17,276 --> 00:03:18,796
Next, you should add an AirPlay


102
00:03:18,796 --> 00:03:20,586
picker inside your app.


103
00:03:21,686 --> 00:03:23,506
And third, you should integrate


104
00:03:23,506 --> 00:03:24,356
with certain parts of the


105
00:03:24,356 --> 00:03:25,566
MediaPlayer framework.


106
00:03:26,126 --> 00:03:29,036
And lastly, you should adopt one


107
00:03:29,036 --> 00:03:30,306
of the playback APIs that are


108
00:03:30,306 --> 00:03:31,776
specifically designed to take


109
00:03:31,776 --> 00:03:32,906
advantage of the enhanced


110
00:03:32,906 --> 00:03:35,016
buffering in AirPlay 2.


111
00:03:35,776 --> 00:03:37,036
So, let's walk through each of


112
00:03:37,036 --> 00:03:37,736
these individually.


113
00:03:39,556 --> 00:03:41,836
First, identifying yourself as


114
00:03:41,836 --> 00:03:43,106
presenting long-form audio


115
00:03:43,106 --> 00:03:43,596
content.


116
00:03:44,416 --> 00:03:45,596
So, what is long-form audio


117
00:03:45,596 --> 00:03:46,066
content?


118
00:03:46,546 --> 00:03:48,206
Well long-form audio is anything


119
00:03:48,206 --> 00:03:49,796
like, is content like music,


120
00:03:50,166 --> 00:03:52,946
podcasts, or audiobooks, as


121
00:03:52,946 --> 00:03:54,246
distinct from things like system


122
00:03:54,246 --> 00:03:56,246
sounds on a Mac.


123
00:03:56,736 --> 00:03:58,156
Now, identifying yourself as


124
00:03:58,156 --> 00:03:59,726
presenting long-form audio


125
00:03:59,726 --> 00:04:00,916
content is very easy.


126
00:04:01,736 --> 00:04:02,916
You simply set your


127
00:04:02,916 --> 00:04:04,986
application's AVAudioSession's


128
00:04:05,116 --> 00:04:06,816
route sharing policy to be


129
00:04:06,816 --> 00:04:07,386
long-form.


130
00:04:08,776 --> 00:04:10,166
Now here's a code snippet of how


131
00:04:10,166 --> 00:04:11,086
you do that on iOS.


132
00:04:11,966 --> 00:04:12,916
This is something that many of


133
00:04:12,916 --> 00:04:14,786
you iOS developers are likely


134
00:04:14,786 --> 00:04:16,685
familiar with setting your


135
00:04:16,685 --> 00:04:18,656
category and mode, and there's


136
00:04:18,656 --> 00:04:20,226
just a new parameter here which


137
00:04:20,226 --> 00:04:21,305
is routeSharingPolicy:


138
00:04:21,706 --> 00:04:22,256
.longform.


139
00:04:23,526 --> 00:04:24,816
As I said, this has been around


140
00:04:24,816 --> 00:04:26,556
on iOS for a little while.


141
00:04:26,556 --> 00:04:27,676
This class is actually,


142
00:04:27,676 --> 00:04:29,386
AVAudioSession is new altogether


143
00:04:29,386 --> 00:04:31,436
on macOS, and I'm not going to


144
00:04:31,436 --> 00:04:32,496
show a code snippet, but it's


145
00:04:32,496 --> 00:04:34,336
even simpler than what you have


146
00:04:34,336 --> 00:04:35,186
to do on iOS.


147
00:04:35,586 --> 00:04:37,046
You simply set the route sharing


148
00:04:37,046 --> 00:04:37,966
policy to long-form.


149
00:04:39,096 --> 00:04:40,866
And for more information on this


150
00:04:40,866 --> 00:04:42,956
new route sharing policy and


151
00:04:42,956 --> 00:04:44,446
what it can do for your app in


152
00:04:44,446 --> 00:04:45,216
regards to the rest of the


153
00:04:45,216 --> 00:04:47,266
system, for example allow the


154
00:04:47,266 --> 00:04:49,086
user to take a phone call while


155
00:04:49,086 --> 00:04:51,716
you're app is AirPlay, using


156
00:04:51,716 --> 00:04:53,226
AirPlay 2 to stream to speakers,


157
00:04:53,646 --> 00:04:54,826
I encourage you to take a look


158
00:04:54,826 --> 00:04:55,916
at this session from earlier in


159
00:04:55,916 --> 00:04:57,496
the week called What's New in


160
00:04:57,496 --> 00:04:57,846
Audio.


161
00:04:57,876 --> 00:05:00,976
All right, so that's the first


162
00:05:00,976 --> 00:05:03,196
step you need to do to take


163
00:05:03,196 --> 00:05:04,266
advantage of AirPlay 2.


164
00:05:04,826 --> 00:05:06,556
The second is to add an AirPlay


165
00:05:06,556 --> 00:05:08,346
picker inside your app.


166
00:05:09,016 --> 00:05:11,916
This will allow users to route


167
00:05:11,916 --> 00:05:13,926
their content to AirPlay


168
00:05:13,926 --> 00:05:15,386
speakers without leaving the


169
00:05:15,386 --> 00:05:17,386
confines of your app.


170
00:05:17,716 --> 00:05:18,966
And doing this is very easy.


171
00:05:18,966 --> 00:05:21,966
You simply adopt a new API


172
00:05:22,396 --> 00:05:23,416
called AVKit's


173
00:05:23,416 --> 00:05:24,846
AVRoutePickerView.


174
00:05:25,426 --> 00:05:26,756
You simply add this view to your


175
00:05:26,756 --> 00:05:27,496
view hierarchy.


176
00:05:28,776 --> 00:05:30,606
And you may want to control when


177
00:05:30,606 --> 00:05:32,206
this is shown, for example only


178
00:05:32,206 --> 00:05:34,376
show it if there actually is an


179
00:05:34,376 --> 00:05:36,006
AirPlay 2 speaker or an AirPlay


180
00:05:36,006 --> 00:05:37,366
speaker available to be routed


181
00:05:37,366 --> 00:05:37,606
to.


182
00:05:38,246 --> 00:05:39,916
And you can do that by adopting


183
00:05:39,916 --> 00:05:42,046
AVFoundation's AVRouteDetector


184
00:05:42,146 --> 00:05:43,406
which will tell you if there are


185
00:05:43,406 --> 00:05:44,666
speakers, if there are routes


186
00:05:44,666 --> 00:05:45,206
available.


187
00:05:46,636 --> 00:05:49,786
These API, they're new in the


188
00:05:49,786 --> 00:05:53,206
latest OS releases and they're


189
00:05:53,206 --> 00:05:55,546
available on macOS, iOS, and


190
00:05:55,546 --> 00:05:56,036
tvOS.


191
00:05:57,066 --> 00:05:58,276
And a note to all of you iOS


192
00:05:58,276 --> 00:05:59,426
developers who are currently


193
00:05:59,426 --> 00:06:01,496
using MPVolumeView, it will


194
00:06:01,496 --> 00:06:03,066
still work, but we are


195
00:06:03,066 --> 00:06:04,146
encouraging people to move to


196
00:06:04,146 --> 00:06:05,086
these API instead.


197
00:06:05,636 --> 00:06:08,236
So, that's the second thing you


198
00:06:08,236 --> 00:06:09,646
need to do to get your app up


199
00:06:09,646 --> 00:06:10,866
and running with AirPlay 2.


200
00:06:11,576 --> 00:06:13,426
The third thing you should do is


201
00:06:13,426 --> 00:06:14,786
to integrate your app with


202
00:06:14,786 --> 00:06:15,866
certain parts of the MediaPlayer


203
00:06:15,866 --> 00:06:16,326
framework.


204
00:06:16,836 --> 00:06:18,036
Now this is rather large, so


205
00:06:18,036 --> 00:06:18,986
there's basically two things


206
00:06:18,986 --> 00:06:20,566
that I'm specifically focusing


207
00:06:20,566 --> 00:06:20,936
on today.


208
00:06:20,936 --> 00:06:23,416
And these are things that will,


209
00:06:24,036 --> 00:06:26,496
these API will allow you to do


210
00:06:26,496 --> 00:06:27,726
things like display the


211
00:06:27,726 --> 00:06:29,896
currently playing album art on


212
00:06:29,896 --> 00:06:31,836
the lock screen or an Apple TV


213
00:06:32,776 --> 00:06:33,906
it's being AirPlayed to.


214
00:06:34,286 --> 00:06:35,556
And additionally, they will


215
00:06:35,556 --> 00:06:37,526
allow you to receive playback


216
00:06:37,526 --> 00:06:39,716
commands and pause commands from


217
00:06:39,716 --> 00:06:42,056
other devices on the network or


218
00:06:42,056 --> 00:06:43,846
from other accessories like


219
00:06:43,846 --> 00:06:44,336
headphones.


220
00:06:44,916 --> 00:06:47,066
And the two pieces of API that


221
00:06:47,066 --> 00:06:48,646
we need, we are asking you to


222
00:06:48,646 --> 00:06:49,036
adopt.


223
00:06:49,796 --> 00:06:50,516
The first is


224
00:06:50,516 --> 00:06:52,446
MPRemoteCommandCenter and this


225
00:06:52,446 --> 00:06:53,676
will allow you to receive those


226
00:06:53,676 --> 00:06:55,996
remote commands, and the second


227
00:06:56,276 --> 00:06:58,486
is MPNowPlayingInfoCenter which


228
00:06:58,486 --> 00:06:59,476
will allow you to inform the


229
00:06:59,476 --> 00:07:01,076
system metadata about the


230
00:07:01,076 --> 00:07:02,456
currently playing track.


231
00:07:02,626 --> 00:07:05,456
All right, so that's the third


232
00:07:05,456 --> 00:07:06,676
thing you need to do to get your


233
00:07:06,676 --> 00:07:08,086
app up and running with AirPlay


234
00:07:08,086 --> 00:07:08,336
2.


235
00:07:08,986 --> 00:07:10,536
The fourth this is you should


236
00:07:10,536 --> 00:07:11,946
adopt one of the playback APIs


237
00:07:11,946 --> 00:07:13,846
that are specifically designed


238
00:07:13,886 --> 00:07:15,666
to take advantage of enhanced


239
00:07:15,666 --> 00:07:16,916
buffering in AirPlay 2.


240
00:07:18,106 --> 00:07:18,926
And to motivate this


241
00:07:18,926 --> 00:07:20,786
conversation, let's take a look


242
00:07:20,786 --> 00:07:22,136
a little bit deeper at AirPlay


243
00:07:22,136 --> 00:07:24,526
audio and the buffer levels


244
00:07:24,646 --> 00:07:26,556
beginning with how AirPlay works


245
00:07:26,556 --> 00:07:26,876
today.


246
00:07:28,076 --> 00:07:30,266
So, AirPlay today is effectively


247
00:07:30,266 --> 00:07:32,506
a real-time stream of audio to


248
00:07:32,506 --> 00:07:33,826
which the speakers adds just a


249
00:07:33,826 --> 00:07:35,486
few seconds of buffering before


250
00:07:35,486 --> 00:07:36,506
playing out in real-time.


251
00:07:37,066 --> 00:07:39,936
Now, this works very well to a


252
00:07:39,936 --> 00:07:41,906
single speaker, and it works


253
00:07:41,906 --> 00:07:43,626
pretty well with lots of


254
00:07:43,626 --> 00:07:44,116
content.


255
00:07:45,136 --> 00:07:48,256
But, if we are able to restrict


256
00:07:48,256 --> 00:07:49,646
ourselves on only focusing on


257
00:07:49,646 --> 00:07:51,526
long-form content, like music,


258
00:07:51,596 --> 00:07:53,516
podcasts or audiobooks, we can


259
00:07:53,516 --> 00:07:56,356
probably do better.


260
00:07:56,526 --> 00:07:58,616
So, let's talk about the


261
00:07:58,616 --> 00:08:00,586
enhanced buffering in AirPlay 2.


262
00:08:01,186 --> 00:08:03,386
So, what am I talking about with


263
00:08:03,386 --> 00:08:04,946
enhanced buffering in AirPlay 2?


264
00:08:05,536 --> 00:08:09,326
Well, as the name implies, we've


265
00:08:09,326 --> 00:08:11,986
added very large buffer capacity


266
00:08:11,986 --> 00:08:13,516
on the AirPlay 2 speakers.


267
00:08:13,726 --> 00:08:14,566
Here I want you to think


268
00:08:14,566 --> 00:08:16,796
minutes, not seconds.


269
00:08:17,816 --> 00:08:18,746
Additionally, we're able to


270
00:08:18,746 --> 00:08:20,226
stream the audio content from


271
00:08:20,226 --> 00:08:21,486
your app to the speaker


272
00:08:21,486 --> 00:08:22,596
faster-than-real-time.


273
00:08:23,126 --> 00:08:25,416
And I think the benefits of this


274
00:08:25,416 --> 00:08:26,096
should be clear.


275
00:08:27,396 --> 00:08:28,956
The large buffering on the


276
00:08:28,956 --> 00:08:30,126
AirPlay speakers will add a


277
00:08:30,126 --> 00:08:32,236
large degree of robustness to


278
00:08:32,236 --> 00:08:33,135
the AirPlay session.


279
00:08:33,916 --> 00:08:35,106
You should be able to survive


280
00:08:35,106 --> 00:08:36,586
more typical network glitches,


281
00:08:36,586 --> 00:08:38,046
like walking, taking the trash


282
00:08:38,046 --> 00:08:39,226
out, or walking to the dead spot


283
00:08:39,226 --> 00:08:40,326
of the house, or even


284
00:08:40,326 --> 00:08:41,405
microwaving some popcorn.


285
00:08:43,336 --> 00:08:44,606
Additionally, this should


286
00:08:44,606 --> 00:08:46,106
provide more responsive playback


287
00:08:46,106 --> 00:08:46,806
experience.


288
00:08:47,556 --> 00:08:48,876
The real-time nature of existing


289
00:08:48,876 --> 00:08:50,016
AirPlay means that there's a


290
00:08:50,016 --> 00:08:51,816
fixed output latency that is


291
00:08:51,816 --> 00:08:53,376
related to the buffer level on


292
00:08:53,376 --> 00:08:54,196
the AirPlay speaker.


293
00:08:55,346 --> 00:08:57,046
So, when you hit play, with


294
00:08:57,046 --> 00:08:58,386
AirPlay 2 it should play a lot


295
00:08:58,386 --> 00:08:58,776
faster.


296
00:08:58,776 --> 00:08:59,726
When you hit skip, it should


297
00:08:59,726 --> 00:09:00,676
skip a lot faster.


298
00:09:01,726 --> 00:09:04,476
This will add a great, this is a


299
00:09:04,476 --> 00:09:06,366
great user experience for your


300
00:09:06,366 --> 00:09:08,586
users which is why we're really


301
00:09:08,586 --> 00:09:09,506
excited to talk about the


302
00:09:09,506 --> 00:09:10,196
enhanced buffering.


303
00:09:10,716 --> 00:09:14,106
And so in order to take


304
00:09:14,106 --> 00:09:15,336
advantage of the advanced


305
00:09:15,336 --> 00:09:16,766
buffering, you should adopt one


306
00:09:16,766 --> 00:09:17,466
of a few API.


307
00:09:18,646 --> 00:09:20,656
The first, is AVPlayer and


308
00:09:20,656 --> 00:09:21,416
AVQueuePlayer.


309
00:09:21,606 --> 00:09:23,696
The AVPlayer's set of playback


310
00:09:23,696 --> 00:09:23,976
API.


311
00:09:24,686 --> 00:09:26,016
These have been around for quite


312
00:09:26,016 --> 00:09:27,686
a while and they're your easiest


313
00:09:27,686 --> 00:09:29,196
route to AirPlay 2 in the


314
00:09:29,196 --> 00:09:29,906
enhanced buffering.


315
00:09:31,286 --> 00:09:33,146
The second API, set of API are


316
00:09:33,146 --> 00:09:34,556
new API that we're introducing


317
00:09:34,556 --> 00:09:35,736
today which are


318
00:09:35,736 --> 00:09:37,766
AVSampleBufferAudio Renderer and


319
00:09:37,766 --> 00:09:38,796
AVSampleBufferRender


320
00:09:38,796 --> 00:09:39,456
Synchronizer.


321
00:09:40,896 --> 00:09:42,536
These will prevent, present more


322
00:09:42,536 --> 00:09:44,786
flexibility to apps that need


323
00:09:45,396 --> 00:09:45,466
it.


324
00:09:46,096 --> 00:09:47,276
So, let's talk about these


325
00:09:47,276 --> 00:09:50,326
individually by first surveying


326
00:09:50,326 --> 00:09:51,516
AVPlayer and AVQueuePlayer.


327
00:09:52,666 --> 00:09:53,646
So, as I've said these have been


328
00:09:53,646 --> 00:09:54,896
around for quite a while, since


329
00:09:54,896 --> 00:09:56,516
iOS 4 on the iOS platforms.


330
00:09:57,596 --> 00:09:59,026
And because of that there's a


331
00:09:59,026 --> 00:10:00,316
ton of documentation on the


332
00:10:00,316 --> 00:10:01,716
developer website and a lot of


333
00:10:01,716 --> 00:10:02,516
sample code there.


334
00:10:03,046 --> 00:10:04,286
So, if you want a lot of detail


335
00:10:04,286 --> 00:10:06,326
on these, I encourage you to


336
00:10:06,326 --> 00:10:07,896
look to the developer website,


337
00:10:07,896 --> 00:10:09,466
because I'm just going to give a


338
00:10:09,466 --> 00:10:10,646
very high-level overview of


339
00:10:10,646 --> 00:10:11,486
these API today.


340
00:10:12,366 --> 00:10:14,316
So, what I'm going to do, is I'm


341
00:10:14,316 --> 00:10:15,876
going to walk through what it


342
00:10:15,876 --> 00:10:17,336
takes to build an app with this


343
00:10:17,336 --> 00:10:17,666
API.


344
00:10:18,196 --> 00:10:19,236
So, here you have your Client


345
00:10:19,236 --> 00:10:20,526
App, and if you want to build an


346
00:10:20,526 --> 00:10:21,706
app with an AVPlayer or


347
00:10:21,706 --> 00:10:23,186
AVQueuePlayer, the first thing


348
00:10:23,186 --> 00:10:24,356
you're going to do is your going


349
00:10:24,356 --> 00:10:25,926
instantiate one of the objects.


350
00:10:27,246 --> 00:10:28,716
Here I'm using an AVQueuePlayer


351
00:10:28,716 --> 00:10:29,916
but the steps are exactly the


352
00:10:29,916 --> 00:10:31,196
same if you use AVPlayer.


353
00:10:31,686 --> 00:10:35,576
Next, you're going to take a URL


354
00:10:35,576 --> 00:10:37,006
that points to the content that


355
00:10:37,006 --> 00:10:38,616
you want to play, and that


356
00:10:38,616 --> 00:10:40,316
content can be local or it can


357
00:10:40,316 --> 00:10:41,296
be in the cloud, it can be


358
00:10:41,296 --> 00:10:41,586
remote.


359
00:10:41,586 --> 00:10:43,296
You're going to take that URL,


360
00:10:43,296 --> 00:10:45,006
and you're going to wrap it in


361
00:10:45,006 --> 00:10:46,766
an AVAsset and you're going to


362
00:10:46,766 --> 00:10:48,246
wrap that AVAsset in an


363
00:10:48,246 --> 00:10:48,766
AVPlayerItem.


364
00:10:50,296 --> 00:10:51,786
Next, you're going to give that


365
00:10:51,786 --> 00:10:53,146
AVPlayerItem to the


366
00:10:53,146 --> 00:10:53,646
AVQueuePlayer.


367
00:10:54,246 --> 00:10:56,636
Now after you've handed it over,


368
00:10:56,636 --> 00:10:57,466
you're ready to initiate


369
00:10:57,466 --> 00:10:57,846
playback.


370
00:10:57,966 --> 00:11:00,536
And you do so but simply setting


371
00:11:00,536 --> 00:11:01,366
the rate of 1 on the


372
00:11:01,366 --> 00:11:03,856
AVQueuePlayer, which in response


373
00:11:03,856 --> 00:11:05,146
will begin downloading the audio


374
00:11:05,146 --> 00:11:07,496
data from wherever it resides,


375
00:11:08,036 --> 00:11:10,266
and then playing it out the


376
00:11:10,266 --> 00:11:10,806
speakers.


377
00:11:11,206 --> 00:11:15,876
So, this is the quick survey of


378
00:11:15,876 --> 00:11:17,346
AVPlayer and AVQueuePlayer, I


379
00:11:17,346 --> 00:11:18,466
should also note that this works


380
00:11:18,466 --> 00:11:19,876
really well for video content.


381
00:11:20,546 --> 00:11:21,316
So, if you want to play some


382
00:11:21,316 --> 00:11:22,686
video content, you can do it


383
00:11:22,686 --> 00:11:23,986
pretty easily with AVPlayer and


384
00:11:23,986 --> 00:11:24,446
AVQueuePlayer.


385
00:11:25,026 --> 00:11:29,056
So, just to reiterate, your


386
00:11:29,056 --> 00:11:31,036
easiest way to AirPlay 2 is


387
00:11:31,036 --> 00:11:31,866
using these API.


388
00:11:32,666 --> 00:11:34,486
But we recognize that this won't


389
00:11:34,486 --> 00:11:35,346
work for everybody.


390
00:11:35,746 --> 00:11:37,146
There's a certain class of audio


391
00:11:37,146 --> 00:11:38,166
playing apps that either


392
00:11:38,166 --> 00:11:39,736
require, that want to do their


393
00:11:39,736 --> 00:11:40,936
own IO possibly.


394
00:11:41,446 --> 00:11:43,916
Possibly their own DRM or even


395
00:11:43,976 --> 00:11:46,506
preprocessing on the media data


396
00:11:46,596 --> 00:11:47,456
before it's rendered out.


397
00:11:47,566 --> 00:11:48,866
They need more flexibility than


398
00:11:48,866 --> 00:11:50,216
the, than what these API


399
00:11:50,346 --> 00:11:50,716
provide.


400
00:11:51,176 --> 00:11:54,416
And for these developers we're


401
00:11:54,416 --> 00:11:55,986
introducing these new classes,


402
00:11:56,536 --> 00:11:58,616
AVSampleBufferAudio Renderer and


403
00:11:58,616 --> 00:11:59,586
AVSampleBufferRender


404
00:11:59,586 --> 00:12:00,236
Synchronizer.


405
00:12:01,766 --> 00:12:03,646
Now with these APIs when you use


406
00:12:03,646 --> 00:12:05,006
them to do playback, your app


407
00:12:05,006 --> 00:12:06,566
has additional responsibilities.


408
00:12:07,786 --> 00:12:09,536
So, first of all your app is


409
00:12:09,536 --> 00:12:10,846
responsible for sourcing and


410
00:12:10,846 --> 00:12:11,666
parsing the content.


411
00:12:12,156 --> 00:12:13,286
You need to go get it wherever


412
00:12:13,286 --> 00:12:14,796
it is, download it, read it off


413
00:12:14,796 --> 00:12:16,396
disc, whatever.


414
00:12:17,046 --> 00:12:18,596
And then your app has to parse


415
00:12:18,596 --> 00:12:20,806
it and feed the raw audio data,


416
00:12:20,806 --> 00:12:22,786
the raw audio buffers to the API


417
00:12:22,926 --> 00:12:23,436
for rendering.


418
00:12:24,936 --> 00:12:26,086
This is kind of similar to


419
00:12:26,086 --> 00:12:27,386
AudioQueue but it works better


420
00:12:27,386 --> 00:12:28,806
with the deeper buffers that we


421
00:12:28,806 --> 00:12:31,776
need for buffered AirPlay, for


422
00:12:31,916 --> 00:12:32,526
enhanced AirPlay.


423
00:12:32,826 --> 00:12:35,366
Because these are new, we're


424
00:12:35,366 --> 00:12:36,486
going to spend the remainder of


425
00:12:36,486 --> 00:12:37,666
this session talking about


426
00:12:37,976 --> 00:12:38,356
these.


427
00:12:38,496 --> 00:12:42,676
All right, so let's walk through


428
00:12:42,676 --> 00:12:44,156
a simple block diagram of how


429
00:12:44,156 --> 00:12:45,436
you might build an app using


430
00:12:45,436 --> 00:12:45,906
these API.


431
00:12:47,096 --> 00:12:48,706
So, again use your Client App,


432
00:12:49,436 --> 00:12:50,286
and the first thing you're going


433
00:12:50,286 --> 00:12:51,376
to do when building an app with


434
00:12:51,376 --> 00:12:52,046
these is you're going to


435
00:12:52,046 --> 00:12:52,896
instantiate an


436
00:12:53,286 --> 00:12:54,486
AVSampleBufferRender


437
00:12:54,486 --> 00:12:55,776
Synchronizer and


438
00:12:55,776 --> 00:12:57,176
AVSampleBufferAudio Renderer.


439
00:12:58,156 --> 00:12:59,656
Now of these classes, the


440
00:12:59,656 --> 00:13:01,516
AudioRenderer class is the one


441
00:13:01,516 --> 00:13:02,436
that is responsible for


442
00:13:02,436 --> 00:13:04,226
rendering the audio, and the


443
00:13:04,226 --> 00:13:05,686
Synchronizer class is the one


444
00:13:05,686 --> 00:13:06,766
that's responsible for


445
00:13:06,766 --> 00:13:08,146
establishing the media timeline.


446
00:13:09,416 --> 00:13:10,506
Now, there's good reason, you


447
00:13:10,506 --> 00:13:11,536
may be asking yourself, why are


448
00:13:11,536 --> 00:13:12,586
they two classes, why didn't you


449
00:13:12,586 --> 00:13:13,506
roll it into one?


450
00:13:13,626 --> 00:13:14,566
Well, I assure you there's a


451
00:13:14,566 --> 00:13:15,146
good reason.


452
00:13:15,146 --> 00:13:16,046
We'll talk about that later.


453
00:13:16,306 --> 00:13:17,126
But never the less,


454
00:13:17,616 --> 00:13:19,106
AudioRenderer it does the


455
00:13:19,106 --> 00:13:20,066
rendering of the audio.


456
00:13:20,186 --> 00:13:21,676
The Synchronizer establishes the


457
00:13:21,676 --> 00:13:23,636
media timeline.


458
00:13:24,156 --> 00:13:25,646
So, after you've instantiated


459
00:13:25,646 --> 00:13:27,786
these, you add the AudioRenderer


460
00:13:27,976 --> 00:13:28,866
to the Synchronizer.


461
00:13:29,436 --> 00:13:31,596
This tells the AudioRenderer to


462
00:13:31,596 --> 00:13:33,006
follow the media timeline


463
00:13:33,006 --> 00:13:34,816
established by the Synchronizer.


464
00:13:34,816 --> 00:13:38,386
Now, as you begin working with


465
00:13:38,386 --> 00:13:39,816
an AVSampleBufferAudio Renderer,


466
00:13:39,866 --> 00:13:40,836
one of the things it's going to


467
00:13:40,836 --> 00:13:41,766
do is it's going to tell you


468
00:13:41,766 --> 00:13:43,656
when it wants more media data.


469
00:13:44,376 --> 00:13:45,686
And when it tells you that it


470
00:13:45,686 --> 00:13:46,906
wants more media data, in


471
00:13:46,906 --> 00:13:48,686
response, you should feed it.


472
00:13:49,776 --> 00:13:50,796
So, give it some audio data.


473
00:13:51,416 --> 00:13:54,016
And after you've done that, you


474
00:13:54,016 --> 00:13:55,676
can begin playback by setting


475
00:13:55,676 --> 00:13:57,066
the rate of 1 on the


476
00:13:57,066 --> 00:13:57,706
Synchronizer.


477
00:13:58,216 --> 00:14:00,596
And after you've set the rate of


478
00:14:00,596 --> 00:14:02,476
1 on the Synchronizer, audio


479
00:14:02,476 --> 00:14:04,016
data will begin flowing out of


480
00:14:04,016 --> 00:14:04,676
the AudioRenderer.


481
00:14:05,226 --> 00:14:09,136
So, that's the basic high-level


482
00:14:09,136 --> 00:14:10,356
overview of how you build a


483
00:14:10,356 --> 00:14:11,646
playback engine, with


484
00:14:11,646 --> 00:14:13,216
AVSampleBufferAudio Renderer and


485
00:14:13,216 --> 00:14:14,036
AVSampleBufferRender


486
00:14:14,036 --> 00:14:14,656
Synchronizer.


487
00:14:15,136 --> 00:14:16,056
Now I'd like to invite my


488
00:14:16,056 --> 00:14:17,506
colleague Adam Sonnanstine up on


489
00:14:17,506 --> 00:14:19,236
stage to give a demo of this.


490
00:14:20,016 --> 00:14:21,196
[ Applause ]


491
00:14:21,196 --> 00:14:21,776
>> Thank you, David.


492
00:14:23,156 --> 00:14:24,566
I'm very happy to be here today


493
00:14:24,566 --> 00:14:26,836
to demonstrate AirPlay 2 and its


494
00:14:26,836 --> 00:14:28,356
enhanced reliability and


495
00:14:28,356 --> 00:14:29,016
robustness.


496
00:14:29,066 --> 00:14:31,236
I'm going to do that using this


497
00:14:31,236 --> 00:14:32,726
sample application that we've


498
00:14:32,726 --> 00:14:33,256
developed.


499
00:14:33,256 --> 00:14:35,206
And this application will be


500
00:14:35,266 --> 00:14:37,416
available as sample code shortly


501
00:14:37,416 --> 00:14:38,626
after the conference ends.


502
00:14:39,316 --> 00:14:40,916
And we also have an Apple TV on


503
00:14:40,916 --> 00:14:42,336
stage so I can demonstrate


504
00:14:42,646 --> 00:14:44,576
AirPlaying from the phone to the


505
00:14:44,576 --> 00:14:46,626
Apple TV both with current


506
00:14:46,626 --> 00:14:48,236
AirPlay and with AirPlay 2.


507
00:14:48,566 --> 00:14:50,136
So let's take a look at the app.


508
00:14:50,136 --> 00:14:51,816
You can see it's just a simple


509
00:14:51,816 --> 00:14:53,266
sort of stripped down music


510
00:14:53,266 --> 00:14:54,266
player interface.


511
00:14:54,816 --> 00:14:57,156
We have this nice AirPlay or


512
00:14:57,156 --> 00:14:59,226
sorry Control Center integration


513
00:14:59,566 --> 00:15:01,066
because I've adopted the


514
00:15:01,126 --> 00:15:02,786
MediaPlayer APIs that David


515
00:15:02,786 --> 00:15:03,496
mentioned before.


516
00:15:03,496 --> 00:15:05,106
So, I'm just going to go ahead


517
00:15:05,106 --> 00:15:05,946
and start playing.


518
00:15:05,946 --> 00:15:07,736
And so you can hear it, I'll


519
00:15:08,346 --> 00:15:10,426
start AirPlaying to my Apple TV.


520
00:15:10,996 --> 00:15:16,216
[music] Now this app has not yet


521
00:15:16,216 --> 00:15:18,486
been optimized for AirPlay 2 so


522
00:15:18,486 --> 00:15:19,646
I just want to give you a base


523
00:15:19,716 --> 00:15:21,746
line of what the performance is


524
00:15:21,746 --> 00:15:23,036
with the current AirPlay.


525
00:15:23,726 --> 00:15:25,806
And I'm going to do that, I want


526
00:15:25,806 --> 00:15:27,676
you to imagine that you are


527
00:15:27,956 --> 00:15:29,616
enjoying this music sitting on


528
00:15:29,616 --> 00:15:31,346
my couch in my living room, the


529
00:15:31,346 --> 00:15:32,646
music being streamed from my


530
00:15:32,646 --> 00:15:34,706
phone to my Apple TV and I


531
00:15:34,706 --> 00:15:36,296
decide that it's a good time to


532
00:15:36,436 --> 00:15:37,786
walk outside with the phone in


533
00:15:37,786 --> 00:15:39,476
my pocket to take out the trash,


534
00:15:40,366 --> 00:15:42,816
maybe to, walking right outside


535
00:15:42,816 --> 00:15:45,236
of my Wi-Fi range, and I'm going


536
00:15:45,236 --> 00:15:46,746
to simulate that by using this


537
00:15:46,806 --> 00:15:48,976
bag, which shields everything I


538
00:15:48,976 --> 00:15:50,596
put inside it from


539
00:15:50,596 --> 00:15:52,836
electromagnetic radiation going


540
00:15:52,836 --> 00:15:55,006
in and from coming out, and that


541
00:15:55,006 --> 00:15:56,176
includes Wi-Fi.


542
00:15:56,876 --> 00:16:00,166
So, I'll take my phone, and here


543
00:16:00,166 --> 00:16:03,096
I am walking outside and you can


544
00:16:03,096 --> 00:16:05,766
hear that almost immediately the


545
00:16:05,766 --> 00:16:06,656
music cuts out.


546
00:16:07,016 --> 00:16:07,996
You've probably experienced


547
00:16:07,996 --> 00:16:09,826
something like this before and


548
00:16:09,826 --> 00:16:11,046
if you're actually sitting in my


549
00:16:11,046 --> 00:16:12,376
living room, listening to music


550
00:16:12,626 --> 00:16:13,586
you're probably going to think


551
00:16:13,586 --> 00:16:14,906
I'm not a very good host.


552
00:16:15,716 --> 00:16:17,826
So, let's go back and I'll go


553
00:16:17,826 --> 00:16:19,146
ahead and pause the music.


554
00:16:20,696 --> 00:16:23,446
And we'll switch into Xcode here


555
00:16:23,446 --> 00:16:24,896
and I'll show you how you can do


556
00:16:25,246 --> 00:16:27,606
a couple of simple steps to opt


557
00:16:27,856 --> 00:16:30,446
your app, this app into AirPlay


558
00:16:30,446 --> 00:16:30,736
2.


559
00:16:31,716 --> 00:16:33,876
So, here we are in Xcode and I


560
00:16:33,876 --> 00:16:35,636
have just a very small snippet


561
00:16:35,636 --> 00:16:37,076
from my application here, the


562
00:16:37,076 --> 00:16:38,316
rest of it's implemented in


563
00:16:38,526 --> 00:16:39,386
other files.


564
00:16:39,776 --> 00:16:41,416
And I just have a function that


565
00:16:41,876 --> 00:16:43,946
the rest of the app is using to


566
00:16:44,176 --> 00:16:45,956
grab an object that it can use


567
00:16:45,956 --> 00:16:48,486
to actually do the audio


568
00:16:48,486 --> 00:16:49,016
playback.


569
00:16:49,506 --> 00:16:50,466
It's basically a factory


570
00:16:50,466 --> 00:16:50,866
function.


571
00:16:50,866 --> 00:16:52,446
And this can return an object


572
00:16:52,446 --> 00:16:53,956
that conforms to a protocol that


573
00:16:53,956 --> 00:16:54,956
I've defined elsewhere in the


574
00:16:54,956 --> 00:16:55,186
app.


575
00:16:55,606 --> 00:16:56,246
We don't need to look at the


576
00:16:56,246 --> 00:16:57,836
details, but it defines methods


577
00:16:57,836 --> 00:16:59,356
for things like play and pause


578
00:16:59,356 --> 00:17:00,966
and the sort of basic playback


579
00:17:00,966 --> 00:17:02,326
operations that a player should


580
00:17:02,326 --> 00:17:02,576
do.


581
00:17:03,276 --> 00:17:04,406
And we have an existing


582
00:17:04,406 --> 00:17:06,026
implementation of this protocol


583
00:17:06,106 --> 00:17:07,715
which is being used in the demo


584
00:17:07,715 --> 00:17:08,396
that we just saw.


585
00:17:08,396 --> 00:17:10,516
And what we're going to do right


586
00:17:10,516 --> 00:17:11,715
now is I'm going to create a


587
00:17:11,715 --> 00:17:14,566
brand-new implementation of this


588
00:17:14,566 --> 00:17:15,705
audio player, and I'm going to


589
00:17:15,705 --> 00:17:17,536
call it SampleBufferAudioPlayer


590
00:17:17,536 --> 00:17:19,425
because it's going to be built


591
00:17:19,425 --> 00:17:21,356
using AVSampleBufferAudio


592
00:17:21,356 --> 00:17:21,786
Renderer.


593
00:17:21,786 --> 00:17:23,596
I'm just going to preemptively


594
00:17:23,596 --> 00:17:25,096
swap that in so that the next


595
00:17:25,096 --> 00:17:26,886
time we launch the app, it will


596
00:17:26,886 --> 00:17:28,156
use the new implementation


597
00:17:28,156 --> 00:17:28,826
instead of the old


598
00:17:28,826 --> 00:17:29,526
implementation.


599
00:17:30,736 --> 00:17:32,386
So, as I mentioned, this class


600
00:17:32,386 --> 00:17:33,846
is going to be built on top of


601
00:17:33,846 --> 00:17:35,666
AVSampleBufferAudio Renderer,


602
00:17:35,896 --> 00:17:36,946
and we're also going to need to


603
00:17:37,046 --> 00:17:38,266
use AVSampleBufferRender


604
00:17:38,266 --> 00:17:39,016
Synchronizer.


605
00:17:39,296 --> 00:17:40,576
These are the classes that David


606
00:17:40,576 --> 00:17:41,786
just introduced to you.


607
00:17:42,376 --> 00:17:43,616
And I just want to reiterate


608
00:17:43,616 --> 00:17:45,766
what David said that AVPlayer


609
00:17:45,766 --> 00:17:46,796
would give you all of the


610
00:17:46,796 --> 00:17:48,176
benefits that I'm about to show


611
00:17:48,176 --> 00:17:49,626
you, but with a little bit less


612
00:17:49,626 --> 00:17:50,486
work on your part.


613
00:17:50,486 --> 00:17:51,536
So, if you're already using


614
00:17:51,536 --> 00:17:53,026
AVPlayer, or you think you can


615
00:17:53,026 --> 00:17:54,926
use AVPlayer, we recommend that


616
00:17:54,926 --> 00:17:55,416
you do that.


617
00:17:55,596 --> 00:17:57,036
For the rest of you, I want to


618
00:17:57,036 --> 00:17:58,176
show you how to use these new


619
00:17:58,176 --> 00:17:58,786
classes.


620
00:18:00,166 --> 00:18:03,716
So, once I have my state here,


621
00:18:03,716 --> 00:18:05,236
I'm going to hook up my


622
00:18:05,776 --> 00:18:06,816
AudioRenderer with my


623
00:18:06,816 --> 00:18:08,476
RenderSynchronizer using the


624
00:18:08,476 --> 00:18:10,066
addRenderer method, and I'll do


625
00:18:10,066 --> 00:18:11,046
that as soon as my


626
00:18:11,086 --> 00:18:12,516
SampleBufferAudioPlayer is


627
00:18:12,516 --> 00:18:12,936
created.


628
00:18:13,416 --> 00:18:15,796
And then we need to interact


629
00:18:15,796 --> 00:18:17,206
with the renderer, and we're


630
00:18:17,206 --> 00:18:18,396
going to do that by calling the


631
00:18:18,396 --> 00:18:20,486
requestMediaDataWhenReady


632
00:18:20,486 --> 00:18:20,786
method.


633
00:18:20,786 --> 00:18:22,496
And what this is going to do is


634
00:18:22,496 --> 00:18:24,236
it's going to call a closure in


635
00:18:24,236 --> 00:18:26,766
my app so that I can feed the


636
00:18:26,766 --> 00:18:28,206
AudioRenderer some more audio


637
00:18:28,206 --> 00:18:29,756
data whenever the AudioRenderer


638
00:18:29,756 --> 00:18:30,826
decides that it's ready to


639
00:18:30,826 --> 00:18:31,656
receive more data.


640
00:18:32,396 --> 00:18:35,296
And it will call this closure as


641
00:18:35,296 --> 00:18:37,116
often as it needs to to stay


642
00:18:37,116 --> 00:18:39,116
full of audio data.


643
00:18:40,796 --> 00:18:43,146
Now within my closure, I'm


644
00:18:43,146 --> 00:18:44,666
actually going to loop so that


645
00:18:44,986 --> 00:18:46,476
I'm going to keep appending more


646
00:18:46,476 --> 00:18:47,566
data as long as the


647
00:18:47,566 --> 00:18:49,426
AudioRenderer is still ready for


648
00:18:49,426 --> 00:18:50,166
more media data.


649
00:18:51,256 --> 00:18:52,886
Within my loop, the first thing


650
00:18:52,886 --> 00:18:55,156
I need to do is grab my next


651
00:18:55,156 --> 00:18:57,196
chunk of audio data and package


652
00:18:57,196 --> 00:18:58,716
it as a CMSampleBuffer.


653
00:18:59,366 --> 00:19:01,726
Now this method here is my app's


654
00:19:01,826 --> 00:19:04,116
own logic for doing this, for


655
00:19:04,116 --> 00:19:05,726
grabbing its next little bit of


656
00:19:05,726 --> 00:19:06,366
audio data.


657
00:19:06,776 --> 00:19:07,746
Your app will have your own


658
00:19:07,746 --> 00:19:09,216
version of this logic, maybe


659
00:19:09,216 --> 00:19:10,596
pulling the data off the network


660
00:19:10,596 --> 00:19:11,876
or decrypting it from disc.


661
00:19:11,876 --> 00:19:13,526
If you want to see the details


662
00:19:13,526 --> 00:19:15,426
of my version, once again, check


663
00:19:15,426 --> 00:19:16,376
out the sample code.


664
00:19:17,156 --> 00:19:19,056
And I have this set up to return


665
00:19:19,056 --> 00:19:20,796
an optional CMSampleBuffer and


666
00:19:20,796 --> 00:19:22,716
that's just so I can use a nil


667
00:19:22,716 --> 00:19:24,376
return value to signal that I've


668
00:19:24,376 --> 00:19:26,926
reached the end of data.


669
00:19:27,106 --> 00:19:28,976
Once I have my sample buffer, I


670
00:19:28,976 --> 00:19:30,966
turn around and enqueue it into


671
00:19:30,966 --> 00:19:32,046
the AudioRenderer using the


672
00:19:32,046 --> 00:19:32,896
enqueue method.


673
00:19:32,896 --> 00:19:35,026
And what this will do is it will


674
00:19:35,026 --> 00:19:37,146
hand the audio data off to the


675
00:19:37,146 --> 00:19:38,456
renderer so that it can schedule


676
00:19:38,456 --> 00:19:39,406
it to be played at the


677
00:19:39,406 --> 00:19:40,216
appropriate time.


678
00:19:41,286 --> 00:19:43,286
As I mentioned, I'm using a nil


679
00:19:43,566 --> 00:19:44,976
sample buffer to signal that I


680
00:19:44,976 --> 00:19:46,016
don't have any more data.


681
00:19:46,486 --> 00:19:47,776
And when that happens I need to


682
00:19:47,776 --> 00:19:48,716
explicitly tell the


683
00:19:48,716 --> 00:19:50,556
AudioRenderer to stop requesting


684
00:19:50,556 --> 00:19:51,416
more media data.


685
00:19:52,006 --> 00:19:53,406
If I don't do this, and I exit


686
00:19:53,406 --> 00:19:55,376
my closure, that AudioRenderer


687
00:19:55,376 --> 00:19:57,046
is just going to invoke the


688
00:19:57,046 --> 00:19:58,896
closure right away, when it's


689
00:19:58,896 --> 00:20:00,736
ready for data again, but since


690
00:20:00,736 --> 00:20:01,616
I don't have anything more to


691
00:20:01,616 --> 00:20:02,686
give it, I don't want that to


692
00:20:02,686 --> 00:20:03,036
happen.


693
00:20:03,106 --> 00:20:04,356
So, I need to tell it to stop.


694
00:20:06,046 --> 00:20:07,796
And this function here is pretty


695
00:20:07,796 --> 00:20:09,406
much your entire interaction


696
00:20:09,406 --> 00:20:10,896
with the AudioRenderer for a


697
00:20:10,896 --> 00:20:12,316
simple use case like this.


698
00:20:13,476 --> 00:20:14,376
Now, we need to do a couple


699
00:20:14,376 --> 00:20:15,146
things with the


700
00:20:15,146 --> 00:20:16,166
RenderSynchronizer.


701
00:20:16,706 --> 00:20:19,116
I have a play method, and all


702
00:20:19,116 --> 00:20:21,066
that play really is, is setting


703
00:20:21,066 --> 00:20:22,546
the RenderSynchronizer's rate to


704
00:20:22,546 --> 00:20:23,906
1, which will start playback.


705
00:20:23,906 --> 00:20:25,776
I have a little bit of prep work


706
00:20:25,776 --> 00:20:26,836
I need to do there, which we


707
00:20:26,836 --> 00:20:27,956
won't look at the details, but


708
00:20:27,956 --> 00:20:29,576
it does end up calling this


709
00:20:29,576 --> 00:20:30,826
start enqueueing method so you


710
00:20:30,826 --> 00:20:32,036
can sort of get a feel for how


711
00:20:32,036 --> 00:20:32,896
things flow here.


712
00:20:33,376 --> 00:20:35,096
And I have a little bit of UI


713
00:20:35,096 --> 00:20:36,636
updating to do every time I play


714
00:20:36,636 --> 00:20:38,326
or pause, so this method will


715
00:20:38,326 --> 00:20:39,886
dispatch back to the main queue


716
00:20:39,886 --> 00:20:41,336
in order to keep the UI up to


717
00:20:41,336 --> 00:20:41,586
date.


718
00:20:42,106 --> 00:20:44,566
Similarly, I have a pause


719
00:20:44,566 --> 00:20:45,736
method, and the only difference


720
00:20:45,736 --> 00:20:47,376
here is that it sets the


721
00:20:47,376 --> 00:20:49,376
renderSynchronizer's rate to 0.


722
00:20:50,356 --> 00:20:52,796
So, now you've seen all of the


723
00:20:52,796 --> 00:20:53,546
interaction that we're going to


724
00:20:53,546 --> 00:20:54,846
have with AVFoundation.


725
00:20:55,076 --> 00:20:56,396
I have one more bit of code I


726
00:20:56,396 --> 00:20:57,576
need to drop in, just to make


727
00:20:57,576 --> 00:20:59,326
sure my app will build and work


728
00:20:59,326 --> 00:21:00,836
correctly, which we won't look


729
00:21:00,836 --> 00:21:02,236
at the details of, but I'll just


730
00:21:02,236 --> 00:21:02,846
drop it there.


731
00:21:03,556 --> 00:21:05,586
And then, we will rebuild the


732
00:21:05,586 --> 00:21:06,166
application.


733
00:21:06,166 --> 00:21:09,036
And I seem to have some


734
00:21:10,356 --> 00:21:11,666
compilation errors.


735
00:21:12,296 --> 00:21:14,846
So, what I'm going to do is


736
00:21:15,696 --> 00:21:18,266
figure out what I did wrong.


737
00:21:31,666 --> 00:21:32,886
Well, why don't we just take


738
00:21:32,966 --> 00:21:37,476
this and, ah I think I know


739
00:21:37,476 --> 00:21:38,156
what's wrong here.


740
00:21:52,256 --> 00:21:55,436
Just drag that over here, and go


741
00:21:55,546 --> 00:21:58,606
back here and we'll try again.


742
00:21:58,606 --> 00:22:04,156
All right, so we got our build


743
00:22:04,156 --> 00:22:04,426
ready.


744
00:22:04,896 --> 00:22:06,186
And now we're relaunching the


745
00:22:06,186 --> 00:22:06,486
app.


746
00:22:07,156 --> 00:22:08,446
So, we'll switch back to our


747
00:22:08,446 --> 00:22:09,386
side by side view.


748
00:22:09,386 --> 00:22:13,006
And once the app finishes


749
00:22:13,046 --> 00:22:14,806
launching, I'm going to go ahead


750
00:22:14,806 --> 00:22:16,906
and restart the music playback.


751
00:22:18,716 --> 00:22:19,536
Here we go.


752
00:22:19,726 --> 00:22:23,916
And once the music starts


753
00:22:23,916 --> 00:22:25,646
playing [music] now we're


754
00:22:25,646 --> 00:22:28,156
playing using AirPlay 2 from the


755
00:22:28,156 --> 00:22:29,296
phone to the Apple TV.


756
00:22:29,946 --> 00:22:31,266
So, now I want to run the same


757
00:22:31,266 --> 00:22:33,656
scenario that I did before using


758
00:22:34,066 --> 00:22:34,956
our bag again.


759
00:22:35,606 --> 00:22:37,636
So, let's grab the phone.


760
00:22:39,636 --> 00:22:41,086
And we'll put it in the bag.


761
00:22:41,086 --> 00:22:42,606
Once again, here's me taking the


762
00:22:42,706 --> 00:22:44,676
phone outside, outside of Wi-Fi


763
00:22:44,676 --> 00:22:45,226
range.


764
00:22:46,426 --> 00:22:47,766
I'll fold up the bag nice and


765
00:22:47,766 --> 00:22:48,106
tight.


766
00:22:49,036 --> 00:22:50,476
And as you can see, where as in


767
00:22:50,476 --> 00:22:51,976
the first demo, the music cut


768
00:22:51,976 --> 00:22:53,846
out almost immediately, now that


769
00:22:53,846 --> 00:22:55,696
we're using AirPlay 2, the music


770
00:22:55,696 --> 00:22:57,146
can keep playing even through


771
00:22:57,146 --> 00:22:58,466
these sorts of minor Wi-Fi


772
00:22:58,466 --> 00:22:59,036
interruptions.


773
00:22:59,546 --> 00:23:00,796
So, that's the power of AirPlay


774
00:23:00,796 --> 00:23:02,816
2 using AVSampleBufferAudio


775
00:23:02,816 --> 00:23:03,146
Renderer.


776
00:23:03,516 --> 00:23:04,956
Thank you very much and back to


777
00:23:04,956 --> 00:23:05,256
David.


778
00:23:07,071 --> 00:23:09,071
[ Applause ]


779
00:23:09,126 --> 00:23:09,806
>> Thank you, Adam.


780
00:23:10,396 --> 00:23:12,136
So, now that we've shown, walked


781
00:23:12,136 --> 00:23:13,606
you through the steps to create


782
00:23:13,606 --> 00:23:14,576
a simple app with


783
00:23:14,576 --> 00:23:16,216
AVSampleBufferAudio Renderer and


784
00:23:16,216 --> 00:23:17,106
AVSampleBufferRender


785
00:23:17,106 --> 00:23:19,336
Synchronizer, let's move on to


786
00:23:19,336 --> 00:23:20,986
some more advanced playback


787
00:23:20,986 --> 00:23:22,406
scenarios that you may be, you


788
00:23:22,406 --> 00:23:24,106
may encounter when using these


789
00:23:24,106 --> 00:23:24,416
API.


790
00:23:24,696 --> 00:23:27,366
So, what we're going to talk


791
00:23:27,366 --> 00:23:29,456
about in this section is audio


792
00:23:29,456 --> 00:23:30,446
buffer levels of


793
00:23:30,726 --> 00:23:32,356
AVSampleBufferAudio Renderer.


794
00:23:32,356 --> 00:23:34,026
We're going to talk to about how


795
00:23:34,026 --> 00:23:36,146
to implement a seek, how to


796
00:23:36,146 --> 00:23:38,106
implement play queues, some of


797
00:23:38,106 --> 00:23:39,926
the support audio formats with


798
00:23:39,926 --> 00:23:41,476
AVSampleBufferAudio Renderer,


799
00:23:41,766 --> 00:23:42,806
and lastly, we're going to take


800
00:23:42,806 --> 00:23:44,446
a slight detour and talk about


801
00:23:44,446 --> 00:23:45,486
video synchronization.


802
00:23:47,736 --> 00:23:49,416
So, let's jump into the next


803
00:23:49,416 --> 00:23:51,496
section and talk about the audio


804
00:23:51,496 --> 00:23:52,276
buffer levels of


805
00:23:52,276 --> 00:23:53,566
AVSampleBufferAudio Renderer.


806
00:23:54,546 --> 00:23:55,696
So, what am I talking about?


807
00:23:55,696 --> 00:23:57,616
I'm talking about the fact that


808
00:23:57,616 --> 00:23:59,836
the amount of audio data request


809
00:23:59,836 --> 00:24:01,176
by an AVSampleBufferAudio


810
00:24:01,176 --> 00:24:03,496
Renderer of your app will vary


811
00:24:03,706 --> 00:24:05,886
depending on the current route.


812
00:24:07,156 --> 00:24:08,506
So, let's take a look at this


813
00:24:08,506 --> 00:24:09,016
pictorially.


814
00:24:09,526 --> 00:24:10,886
Here I've drawn a media timeline


815
00:24:10,886 --> 00:24:12,606
and I'm going to drop in the


816
00:24:12,606 --> 00:24:13,036
playhead.


817
00:24:14,456 --> 00:24:15,286
And when you're playing back


818
00:24:15,286 --> 00:24:17,496
locally, the AudioRenderer is


819
00:24:17,496 --> 00:24:18,566
only going to ask for a few


820
00:24:18,566 --> 00:24:20,076
seconds ahead of the playhead.


821
00:24:21,186 --> 00:24:23,256
So, that is where the, that is


822
00:24:23,256 --> 00:24:24,866
where you should enqueue to, as


823
00:24:24,866 --> 00:24:25,856
much as it asks for.


824
00:24:27,146 --> 00:24:29,066
And as you play, again you're


825
00:24:29,066 --> 00:24:30,036
just going to enqueue a few


826
00:24:30,036 --> 00:24:31,606
seconds ahead of the app.


827
00:24:32,996 --> 00:24:34,546
But suppose your user suddenly


828
00:24:34,546 --> 00:24:36,926
decides to route to an AirPlay 2


829
00:24:36,926 --> 00:24:37,326
speaker.


830
00:24:38,596 --> 00:24:40,246
Now when that happens, the


831
00:24:40,246 --> 00:24:42,146
AVSampleBufferAudio Renderer is


832
00:24:42,146 --> 00:24:43,766
going to ask for up to multiple


833
00:24:43,766 --> 00:24:45,016
minutes ahead of the playhead.


834
00:24:46,306 --> 00:24:47,746
And once again, when you hit


835
00:24:47,746 --> 00:24:49,276
play, you'll work multiple


836
00:24:49,276 --> 00:24:50,286
minutes ahead of the playhead.


837
00:24:51,376 --> 00:24:53,126
Now the key here, is that the


838
00:24:53,126 --> 00:24:54,866
amount of data requested by


839
00:24:54,866 --> 00:24:56,516
AVSampleBufferAudio Renderer


840
00:24:56,936 --> 00:24:58,586
varies depending on where the


841
00:24:58,586 --> 00:24:59,486
audio is currently routed.


842
00:24:59,486 --> 00:25:01,716
If it's routed locally, this


843
00:25:01,716 --> 00:25:03,126
will just be seconds, to


844
00:25:03,126 --> 00:25:04,716
Bluetooth seconds, to an AirPlay


845
00:25:04,716 --> 00:25:06,236
1 speaker seconds.


846
00:25:07,236 --> 00:25:08,806
But as soon as the user routes


847
00:25:08,806 --> 00:25:10,846
your audio content to an AirPlay


848
00:25:10,846 --> 00:25:13,616
2 speaker, that AudioRenderer is


849
00:25:13,616 --> 00:25:14,556
going to get really hungry and


850
00:25:14,556 --> 00:25:15,686
it's going to ask for multiple


851
00:25:15,686 --> 00:25:16,816
minutes of audio data.


852
00:25:18,096 --> 00:25:19,326
And the key here is that your


853
00:25:19,326 --> 00:25:20,376
app should be ready to handle


854
00:25:20,376 --> 00:25:21,016
these changes.


855
00:25:21,516 --> 00:25:25,146
All right, next let's talk about


856
00:25:25,146 --> 00:25:25,516
seek.


857
00:25:26,856 --> 00:25:27,936
So, what is a seek?


858
00:25:28,136 --> 00:25:29,856
Well a seek very quick, very


859
00:25:29,856 --> 00:25:31,816
simply, is manually changing the


860
00:25:31,816 --> 00:25:32,796
location of the playhead.


861
00:25:33,876 --> 00:25:35,046
So, again let's draw out our


862
00:25:35,046 --> 00:25:36,526
media timeline that we just saw


863
00:25:36,526 --> 00:25:37,366
a moment ago.


864
00:25:38,176 --> 00:25:40,006
Put in the playhead, and talk


865
00:25:40,006 --> 00:25:42,276
about local play or talk about


866
00:25:42,876 --> 00:25:44,036
standard playback scenario.


867
00:25:44,716 --> 00:25:46,836
User hits play, and then after


868
00:25:46,836 --> 00:25:48,286
it plays for a little bit the


869
00:25:48,286 --> 00:25:49,496
user decides you know what, I


870
00:25:49,496 --> 00:25:51,206
want to seek further into the


871
00:25:51,206 --> 00:25:51,576
track.


872
00:25:51,576 --> 00:25:52,956
So, they pick up the playhead


873
00:25:52,956 --> 00:25:54,206
and they drag it.


874
00:25:55,076 --> 00:25:56,506
So, they've asked for a seek.


875
00:25:57,046 --> 00:25:59,496
So, how do we handle this with


876
00:25:59,496 --> 00:26:00,806
AVSampleBufferAudio Renderer?


877
00:26:01,366 --> 00:26:02,626
Well it's actually really easy.


878
00:26:03,696 --> 00:26:04,496
The first thing that you're


879
00:26:04,496 --> 00:26:05,296
going to do is you're going to


880
00:26:05,296 --> 00:26:05,716
stop.


881
00:26:05,836 --> 00:26:07,606
You're going to stop playback of


882
00:26:07,606 --> 00:26:08,986
the AVSampleBufferRender


883
00:26:08,986 --> 00:26:10,326
Synchronizer and you're going to


884
00:26:10,326 --> 00:26:12,416
stop enqueueing media data into


885
00:26:12,416 --> 00:26:13,376
the AVSampleBufferAudio


886
00:26:13,376 --> 00:26:13,666
Renderer.


887
00:26:14,176 --> 00:26:17,226
Next, you're going to issue a


888
00:26:17,226 --> 00:26:18,356
flush on the


889
00:26:18,356 --> 00:26:19,956
SampleBufferAudioRenderer which


890
00:26:19,956 --> 00:26:21,556
will clear out all media data


891
00:26:21,556 --> 00:26:22,566
that's already been enqueued.


892
00:26:23,106 --> 00:26:25,716
And at that point, you're free


893
00:26:25,716 --> 00:26:27,646
to begin audio, re-enqueueing


894
00:26:27,646 --> 00:26:30,456
audio data at any media time, so


895
00:26:30,456 --> 00:26:32,786
you do it beginning at the seek


896
00:26:32,786 --> 00:26:33,256
to time.


897
00:26:33,796 --> 00:26:36,216
And after you've enqueued media


898
00:26:36,216 --> 00:26:38,246
data the seek to time, kick off


899
00:26:38,246 --> 00:26:39,846
playback once again and there


900
00:26:39,846 --> 00:26:40,146
you go.


901
00:26:40,216 --> 00:26:43,876
So, that's a seek.


902
00:26:45,096 --> 00:26:46,586
So, now that I've shown you how


903
00:26:46,586 --> 00:26:47,926
seek works, let's take a look at


904
00:26:47,926 --> 00:26:49,046
how you would implement that in


905
00:26:49,046 --> 00:26:51,266
code, by extending Adam's app


906
00:26:51,806 --> 00:26:52,686
with a method called


907
00:26:52,686 --> 00:26:53,516
seek(toMediaTime.


908
00:26:54,626 --> 00:26:55,846
So, how do we implement this?


909
00:26:56,116 --> 00:26:56,926
Well it's pretty easy.


910
00:26:57,336 --> 00:26:58,536
The first thing we're going to


911
00:26:58,536 --> 00:26:59,936
do is we're going to tell the


912
00:26:59,936 --> 00:27:01,246
renderSynchronizer to stop


913
00:27:01,246 --> 00:27:02,756
playback by setting the rate to


914
00:27:02,756 --> 00:27:03,156
0.


915
00:27:03,966 --> 00:27:04,766
And then we're going to tell the


916
00:27:04,766 --> 00:27:06,286
AudioRenderer to stop requesting


917
00:27:06,286 --> 00:27:07,456
media data.


918
00:27:08,406 --> 00:27:10,056
Next, we're going flush the


919
00:27:10,056 --> 00:27:11,556
AudioRenderer to clear out all


920
00:27:11,556 --> 00:27:12,666
the old audio data we've


921
00:27:12,666 --> 00:27:13,106
enqueued.


922
00:27:14,436 --> 00:27:15,516
And then we're going to call


923
00:27:15,516 --> 00:27:17,706
some app specific code, that


924
00:27:17,706 --> 00:27:20,016
tells your sample generating


925
00:27:20,016 --> 00:27:21,346
code that the next sample that


926
00:27:21,346 --> 00:27:23,056
it should prepare is at that


927
00:27:23,056 --> 00:27:23,906
seek to time.


928
00:27:24,346 --> 00:27:26,066
Remember your app is responsible


929
00:27:26,066 --> 00:27:27,556
for producing the audio data


930
00:27:28,176 --> 00:27:30,706
when using this API, so you need


931
00:27:30,706 --> 00:27:32,226
to instruct it where the next


932
00:27:32,226 --> 00:27:33,596
audio sample should be generated


933
00:27:33,596 --> 00:27:33,916
from.


934
00:27:34,446 --> 00:27:37,046
And after that happens, you can


935
00:27:37,046 --> 00:27:38,516
reinstall your closure on the


936
00:27:38,516 --> 00:27:40,006
AudioRenderer to tell it to call


937
00:27:40,006 --> 00:27:41,296
you back for more audio data.


938
00:27:41,986 --> 00:27:43,426
And you can set a rate to 1 on


939
00:27:43,426 --> 00:27:44,246
the RenderSynchronizer.


940
00:27:45,556 --> 00:27:46,216
Pretty simple.


941
00:27:46,526 --> 00:27:47,136
So, that's seek.


942
00:27:47,586 --> 00:27:49,976
Let's get into something more


943
00:27:49,976 --> 00:27:50,946
interesting which is play


944
00:27:50,946 --> 00:27:51,406
queues.


945
00:27:51,406 --> 00:27:53,576
And what is a play queue?


946
00:27:53,576 --> 00:27:54,716
Well here we have a screenshot


947
00:27:54,716 --> 00:27:55,286
from Adam's app.


948
00:27:55,856 --> 00:27:57,516
And a play queue is very simple


949
00:27:57,516 --> 00:27:59,686
is, you know, you order a set of


950
00:27:59,686 --> 00:28:00,086
items.


951
00:28:00,086 --> 00:28:01,396
I hit play on one and they all


952
00:28:01,396 --> 00:28:02,176
play out in order.


953
00:28:02,616 --> 00:28:05,756
So, if we take these, this play


954
00:28:05,756 --> 00:28:06,976
queue and we lay it out on that


955
00:28:07,046 --> 00:28:08,616
media timeline, what we're going


956
00:28:08,616 --> 00:28:09,896
to see is Item 1 followed by


957
00:28:09,896 --> 00:28:11,876
Item 2, followed by Item 3.


958
00:28:13,156 --> 00:28:14,956
That's just a very generic look


959
00:28:14,956 --> 00:28:16,896
of those items laid out on this


960
00:28:16,896 --> 00:28:17,826
timeline we've been showing.


961
00:28:18,606 --> 00:28:19,886
But if we dig into the timelines


962
00:28:19,886 --> 00:28:22,026
especially we're going to see


963
00:28:22,026 --> 00:28:23,196
the timelines of each item,


964
00:28:23,196 --> 00:28:24,076
we're going to see something


965
00:28:24,076 --> 00:28:24,806
slightly different.


966
00:28:25,116 --> 00:28:26,416
Let's suppose hypothetically


967
00:28:26,416 --> 00:28:27,996
that each item is 100 seconds in


968
00:28:27,996 --> 00:28:28,346
length.


969
00:28:29,426 --> 00:28:30,886
That means that each item will


970
00:28:30,886 --> 00:28:33,936
go from 0 to 100, 0 to 100, 0 to


971
00:28:33,936 --> 00:28:34,336
100.


972
00:28:34,896 --> 00:28:38,636
Of course the AudioRenderer


973
00:28:38,636 --> 00:28:39,986
knows nothing about these


974
00:28:39,986 --> 00:28:40,996
individual items.


975
00:28:41,266 --> 00:28:42,536
All the AudioRenderer has is a


976
00:28:42,536 --> 00:28:43,816
continuous media timeline.


977
00:28:45,146 --> 00:28:46,606
So, as you enqueue audio data


978
00:28:46,836 --> 00:28:48,596
into the AVSampleBufferAudio


979
00:28:48,596 --> 00:28:51,096
Renderer, you may need to offset


980
00:28:51,736 --> 00:28:53,946
from the item's natural timeline


981
00:28:54,056 --> 00:28:55,816
to the continuous timeline of


982
00:28:55,816 --> 00:28:57,086
the AVSampleBufferAudio


983
00:28:57,086 --> 00:28:57,336
Renderer.


984
00:28:57,896 --> 00:29:00,396
And again, let's take a look at


985
00:29:00,396 --> 00:29:01,976
this enqueuing animation that


986
00:29:01,976 --> 00:29:02,746
we've seen before.


987
00:29:03,356 --> 00:29:05,076
Here are I'm playing back


988
00:29:05,076 --> 00:29:06,226
locally and I've just enqueued a


989
00:29:06,226 --> 00:29:07,266
few seconds ahead of the


990
00:29:07,496 --> 00:29:09,406
playhead and as you can see I'm


991
00:29:09,406 --> 00:29:10,966
going to queue each item as I go


992
00:29:10,966 --> 00:29:11,486
through it.


993
00:29:12,346 --> 00:29:13,606
And if the user is routed to an


994
00:29:13,606 --> 00:29:14,806
AirPlay 2 speaker, and we're


995
00:29:14,806 --> 00:29:15,676
working well ahead of the


996
00:29:15,676 --> 00:29:16,846
playhead, same idea.


997
00:29:16,846 --> 00:29:18,156
You're just going to enqueue


998
00:29:18,156 --> 00:29:20,176
ever so slightly ahead of the


999
00:29:21,136 --> 00:29:21,336
playhead.


1000
00:29:21,366 --> 00:29:21,836
All right.


1001
00:29:22,646 --> 00:29:24,426
So, that's the simple idea of


1002
00:29:24,426 --> 00:29:24,896
play queues.


1003
00:29:24,896 --> 00:29:25,566
Let's get into something a


1004
00:29:25,566 --> 00:29:26,536
little more interesting,


1005
00:29:26,776 --> 00:29:27,316
editing.


1006
00:29:28,566 --> 00:29:29,566
And in this hypothetical


1007
00:29:29,566 --> 00:29:32,266
example, let's suppose the user


1008
00:29:32,266 --> 00:29:33,206
decides they don't want to


1009
00:29:33,206 --> 00:29:34,226
listen to Item 2 anymore.


1010
00:29:34,466 --> 00:29:35,746
So, they remove it from the


1011
00:29:35,746 --> 00:29:37,916
queue, and so items 3 and 4


1012
00:29:37,916 --> 00:29:41,486
shift over in place.


1013
00:29:41,606 --> 00:29:43,836
Now, when that happens, it's not


1014
00:29:43,836 --> 00:29:44,326
a big deal.


1015
00:29:44,536 --> 00:29:46,806
The user will expect Item 1 to


1016
00:29:46,806 --> 00:29:48,516
play, followed by Item 3, and


1017
00:29:48,516 --> 00:29:49,296
followed by Item 4.


1018
00:29:49,826 --> 00:29:52,166
But that's a really relatively


1019
00:29:52,166 --> 00:29:53,186
simple case of play queue


1020
00:29:53,186 --> 00:29:56,416
editing, where I, made the edit


1021
00:29:56,756 --> 00:29:58,106
before playback is engaged.


1022
00:30:00,086 --> 00:30:01,516
But since we're working well


1023
00:30:01,516 --> 00:30:03,776
ahead of the playhead oftentimes


1024
00:30:03,776 --> 00:30:04,206
when using the


1025
00:30:04,206 --> 00:30:05,586
AVSampleBufferAudio Renderer,


1026
00:30:06,176 --> 00:30:07,376
you could run into a situation


1027
00:30:07,376 --> 00:30:10,146
like this, where the user is


1028
00:30:10,146 --> 00:30:11,456
instantiating, or initiated


1029
00:30:11,456 --> 00:30:12,346
playback.


1030
00:30:12,946 --> 00:30:15,436
You're still playing Item 1, but


1031
00:30:15,436 --> 00:30:17,646
you've begin to enqueue media


1032
00:30:17,646 --> 00:30:18,886
data from Item 2.


1033
00:30:19,426 --> 00:30:22,136
And then the user decides they


1034
00:30:22,136 --> 00:30:22,996
don't want to listen to Item 2


1035
00:30:22,996 --> 00:30:23,276
anymore.


1036
00:30:23,276 --> 00:30:25,966
So, what the user expects is


1037
00:30:25,966 --> 00:30:28,496
Item 2 disappears, Items 3 and 4


1038
00:30:28,496 --> 00:30:29,046
shift over.


1039
00:30:29,616 --> 00:30:33,266
And once again, what the user


1040
00:30:33,266 --> 00:30:35,356
expects is that after Item 1,


1041
00:30:35,356 --> 00:30:36,226
the currently playing track


1042
00:30:36,226 --> 00:30:37,886
plays, Item 3 will begin to


1043
00:30:37,886 --> 00:30:38,166
play.


1044
00:30:39,576 --> 00:30:40,576
But if we do nothing at this


1045
00:30:40,576 --> 00:30:41,646
point, since we've already


1046
00:30:41,646 --> 00:30:43,586
enqueued audio data from Item 2


1047
00:30:43,586 --> 00:30:45,146
into the AudioRenderer, well


1048
00:30:45,146 --> 00:30:46,196
that's what we're going to hear,


1049
00:30:46,286 --> 00:30:48,916
a blip of that, because we have


1050
00:30:48,916 --> 00:30:50,356
the wrong audio data in the


1051
00:30:50,356 --> 00:30:50,906
AudioRenderer.


1052
00:30:51,506 --> 00:30:54,046
So, what do we do here?


1053
00:30:54,636 --> 00:30:55,706
Well it's actually relatively


1054
00:30:55,706 --> 00:30:57,336
simple to handle, because


1055
00:30:57,336 --> 00:30:58,666
there's a command called Flush


1056
00:30:58,666 --> 00:30:59,506
from Source Time.


1057
00:31:00,746 --> 00:31:01,866
And what this command Flush from


1058
00:31:01,866 --> 00:31:03,686
Source Time does, what it means,


1059
00:31:03,686 --> 00:31:05,436
is that given this time that I


1060
00:31:05,436 --> 00:31:07,046
specify to you, I want you to


1061
00:31:07,046 --> 00:31:08,776
throw away all of the media data


1062
00:31:08,776 --> 00:31:11,056
on the timeline after it.


1063
00:31:11,256 --> 00:31:12,596
So, here I'm going to call Flush


1064
00:31:12,596 --> 00:31:15,226
from Source Time with the source


1065
00:31:15,226 --> 00:31:18,146
time pointing at the time in the


1066
00:31:18,176 --> 00:31:20,016
continuous timeline that it's


1067
00:31:20,016 --> 00:31:22,176
the transition from Item 1 to


1068
00:31:22,176 --> 00:31:22,756
the next item.


1069
00:31:23,326 --> 00:31:24,296
So, I call that and it throws


1070
00:31:24,296 --> 00:31:25,056
away all the media data.


1071
00:31:25,056 --> 00:31:27,876
Next thing that it does, is it


1072
00:31:27,876 --> 00:31:29,286
resets the pointer to the last


1073
00:31:29,286 --> 00:31:31,206
enqueued sample buffer to that


1074
00:31:31,356 --> 00:31:33,226
source time, so then I'm free to


1075
00:31:33,226 --> 00:31:35,186
enqueue media data from Item 3.


1076
00:31:36,416 --> 00:31:37,436
And of course, playback


1077
00:31:37,436 --> 00:31:39,166
progresses and we're good to go.


1078
00:31:40,116 --> 00:31:42,076
The key point about this is, for


1079
00:31:42,076 --> 00:31:44,006
the animation purposes in this


1080
00:31:44,006 --> 00:31:46,796
talk, I showed it may look like


1081
00:31:46,796 --> 00:31:48,486
playback was paused, but you can


1082
00:31:48,486 --> 00:31:49,856
actually execute this command


1083
00:31:50,026 --> 00:31:51,716
while playback is engaged, so it


1084
00:31:51,716 --> 00:31:53,166
can be totally transparent to


1085
00:31:53,166 --> 00:31:53,736
your users.


1086
00:31:55,936 --> 00:31:56,826
All right.


1087
00:31:57,396 --> 00:31:58,876
So, let's talk about the steps


1088
00:31:58,876 --> 00:32:00,336
involved in executing the flush


1089
00:32:00,336 --> 00:32:01,036
from source time.


1090
00:32:01,036 --> 00:32:02,356
There's basically three of them.


1091
00:32:02,356 --> 00:32:04,026
First is, stop enqueueing audio


1092
00:32:04,026 --> 00:32:04,846
data in the renderer.


1093
00:32:05,166 --> 00:32:06,566
Note that I'm not saying stop


1094
00:32:06,706 --> 00:32:07,726
playback, because again you


1095
00:32:07,726 --> 00:32:08,386
don't have to do that.


1096
00:32:09,226 --> 00:32:09,876
Then you issue the


1097
00:32:09,876 --> 00:32:12,196
flushfromSourceTime and after


1098
00:32:12,196 --> 00:32:12,766
you've issued the


1099
00:32:12,766 --> 00:32:14,856
flushfromSourceTime, well that


1100
00:32:14,856 --> 00:32:15,896
flush from source is an


1101
00:32:15,896 --> 00:32:17,886
asynchronous option, operation


1102
00:32:18,066 --> 00:32:19,396
into which you pass a closure.


1103
00:32:19,986 --> 00:32:20,756
Well, you should wait for the


1104
00:32:20,756 --> 00:32:21,586
callback to be called.


1105
00:32:22,126 --> 00:32:25,946
Now there are few gotchas with


1106
00:32:25,946 --> 00:32:26,716
this, even though it's a


1107
00:32:26,716 --> 00:32:28,046
relatively simple operation, and


1108
00:32:28,046 --> 00:32:29,526
the first is that the flush may


1109
00:32:29,526 --> 00:32:29,866
fail.


1110
00:32:30,746 --> 00:32:31,736
Now, why may it fail?


1111
00:32:32,236 --> 00:32:34,096
Well, suppose the source time


1112
00:32:34,096 --> 00:32:35,536
the you specified is too close


1113
00:32:35,536 --> 00:32:36,896
to or behind the playhead.


1114
00:32:37,706 --> 00:32:39,426
Well in those situations, we may


1115
00:32:39,426 --> 00:32:40,486
not be able to get the audio


1116
00:32:40,486 --> 00:32:41,686
data back from the audio


1117
00:32:41,686 --> 00:32:42,076
hardware.


1118
00:32:43,336 --> 00:32:46,066
And in those cases well, rather


1119
00:32:46,066 --> 00:32:47,866
than leaving you in an unknown


1120
00:32:47,866 --> 00:32:49,806
state where you may play out


1121
00:32:49,806 --> 00:32:51,366
stale data, we're just going to


1122
00:32:51,366 --> 00:32:52,986
fail the operation and it's, and


1123
00:32:52,986 --> 00:32:54,946
it's going to be as if it was


1124
00:32:54,946 --> 00:32:57,536
never issued in the first place.


1125
00:32:57,716 --> 00:32:59,326
So, the second gotcha which is


1126
00:32:59,326 --> 00:33:00,536
really just the other side of


1127
00:33:00,536 --> 00:33:01,886
the same coin is that you need


1128
00:33:01,886 --> 00:33:02,826
to wait for the callback.


1129
00:33:03,296 --> 00:33:05,146
So, you should wait for the


1130
00:33:05,146 --> 00:33:06,346
callback which will tell you


1131
00:33:06,346 --> 00:33:07,246
whether or not that flush


1132
00:33:07,246 --> 00:33:10,326
failed, and if it did you need


1133
00:33:10,326 --> 00:33:11,346
to take the appropriate action.


1134
00:33:12,056 --> 00:33:12,986
So, let's take a look at this in


1135
00:33:12,986 --> 00:33:14,566
code, again by extended the


1136
00:33:14,566 --> 00:33:17,036
sample app by calling a method


1137
00:33:17,036 --> 00:33:18,246
called -- by implementing a


1138
00:33:18,246 --> 00:33:18,906
method called


1139
00:33:19,396 --> 00:33:21,726
FlushfromSourceTime, perform


1140
00:33:21,726 --> 00:33:22,506
FlushfromSourceTime.


1141
00:33:23,656 --> 00:33:26,836
So, once again, in this method


1142
00:33:26,966 --> 00:33:28,156
the first thing you're going to


1143
00:33:28,156 --> 00:33:28,906
do is you're going to tell the


1144
00:33:28,906 --> 00:33:30,366
AudioRenderer to stop requesting


1145
00:33:30,366 --> 00:33:30,866
media data.


1146
00:33:31,746 --> 00:33:33,046
And then you're likely to call


1147
00:33:33,046 --> 00:33:34,886
some app specific logic to


1148
00:33:34,886 --> 00:33:36,296
really ensure that no media,


1149
00:33:36,296 --> 00:33:37,956
additional media data is


1150
00:33:37,956 --> 00:33:38,776
enqueued.


1151
00:33:38,776 --> 00:33:40,646
This is really important,


1152
00:33:40,886 --> 00:33:42,056
because this is an asynchronous


1153
00:33:42,056 --> 00:33:43,316
operation so it's naturally


1154
00:33:43,316 --> 00:33:43,736
racy.


1155
00:33:44,166 --> 00:33:44,986
So, we want to make sure that


1156
00:33:44,986 --> 00:33:46,136
you're not enqueueing any


1157
00:33:46,136 --> 00:33:47,646
additional audio data while that


1158
00:33:47,646 --> 00:33:49,276
FlushfromSourceTime is flight.


1159
00:33:49,766 --> 00:33:52,466
So, after you've made sure that


1160
00:33:52,466 --> 00:33:53,586
you're no longer enqueueing


1161
00:33:53,586 --> 00:33:54,826
audio data in the AudioRenderer,


1162
00:33:55,196 --> 00:33:56,426
you can actually execute the


1163
00:33:56,426 --> 00:33:57,416
FlushfromSourceTime.


1164
00:33:57,876 --> 00:33:59,296
Again, you'll pass in a closure


1165
00:33:59,296 --> 00:34:00,506
because this is an asynchronous


1166
00:34:00,666 --> 00:34:01,306
operation.


1167
00:34:01,416 --> 00:34:02,846
And then closure will tell you


1168
00:34:02,846 --> 00:34:04,846
then whether or not the call to


1169
00:34:04,846 --> 00:34:05,616
the closure will tell you


1170
00:34:05,616 --> 00:34:06,296
whether or not the flush


1171
00:34:06,296 --> 00:34:06,886
succeeded.


1172
00:34:07,606 --> 00:34:09,196
If it succeeded as I'm showing


1173
00:34:09,196 --> 00:34:11,485
here, well, the first thing


1174
00:34:11,485 --> 00:34:12,545
you're going to do is again


1175
00:34:12,545 --> 00:34:13,505
you're going to tell your sample


1176
00:34:13,505 --> 00:34:15,266
generating app, your app's


1177
00:34:15,545 --> 00:34:17,856
sample generating code to begin


1178
00:34:17,985 --> 00:34:19,815
preparing samples at that source


1179
00:34:19,815 --> 00:34:20,036
time.


1180
00:34:20,735 --> 00:34:22,036
And then you can reinstall your


1181
00:34:22,036 --> 00:34:23,545
callback closure and begin


1182
00:34:23,545 --> 00:34:24,755
enqueueing again.


1183
00:34:25,696 --> 00:34:27,585
Of course, the flush may fail,


1184
00:34:27,996 --> 00:34:29,456
in here it's really app specific


1185
00:34:29,456 --> 00:34:30,886
logic to decide what you want to


1186
00:34:30,886 --> 00:34:32,056
do in these situations.


1187
00:34:32,786 --> 00:34:33,926
Maybe you want to just advance


1188
00:34:33,926 --> 00:34:34,735
to the next track.


1189
00:34:34,735 --> 00:34:35,946
Maybe you want to execute a full


1190
00:34:35,946 --> 00:34:37,656
flush and glitch playback, it's


1191
00:34:37,656 --> 00:34:38,216
really up to you.


1192
00:34:38,786 --> 00:34:43,286
So, that's play queues and Flush


1193
00:34:43,286 --> 00:34:44,085
from Source Time.


1194
00:34:44,856 --> 00:34:47,676
Let's move on to the supported


1195
00:34:47,676 --> 00:34:48,646
audio formats in


1196
00:34:48,646 --> 00:34:49,746
AVSampleBufferAudio Renderer.


1197
00:34:51,656 --> 00:34:53,156
So, what audio formats does the


1198
00:34:53,156 --> 00:34:55,085
AVSampleBufferAudio Renderer


1199
00:34:55,085 --> 00:34:55,516
support?


1200
00:34:55,516 --> 00:34:57,886
Well good news, basically all


1201
00:34:57,886 --> 00:34:59,376
platform supported audio formats


1202
00:34:59,376 --> 00:35:00,596
are also supported by the


1203
00:35:00,596 --> 00:35:01,816
AVSampleBufferAudio Renderer.


1204
00:35:01,906 --> 00:35:04,526
So, this includes LPCM, AAC,


1205
00:35:04,556 --> 00:35:07,376
mp3, Apple Lossless, lots of


1206
00:35:07,376 --> 00:35:09,506
sample rates, lots of bit


1207
00:35:10,556 --> 00:35:10,756
depths.


1208
00:35:10,866 --> 00:35:13,126
And additionally, mixed formats


1209
00:35:13,126 --> 00:35:13,886
may be enqueued.


1210
00:35:14,466 --> 00:35:17,346
If Item 1 is AAC at 44.1 you can


1211
00:35:17,346 --> 00:35:19,556
then enqueue that and then MP3


1212
00:35:19,556 --> 00:35:23,376
at 48k and 16bit Apple Lossless


1213
00:35:23,786 --> 00:35:24,976
at 48 kilohertz.


1214
00:35:25,156 --> 00:35:26,836
You can just enqueue the audio


1215
00:35:26,836 --> 00:35:28,036
data from each of these back to


1216
00:35:28,036 --> 00:35:29,196
back and we will take care of


1217
00:35:29,196 --> 00:35:30,406
the format transitions for you.


1218
00:35:31,466 --> 00:35:33,236
So, those are the supported


1219
00:35:33,236 --> 00:35:33,616
formats.


1220
00:35:33,826 --> 00:35:34,866
Let's talk about the preferred


1221
00:35:34,866 --> 00:35:35,386
formats.


1222
00:35:35,926 --> 00:35:38,706
So, because we can play anything


1223
00:35:38,706 --> 00:35:39,716
just give us what you got.


1224
00:35:39,766 --> 00:35:41,176
You have LPCM, give it to us,


1225
00:35:41,176 --> 00:35:42,166
encoded give it to us.


1226
00:35:42,656 --> 00:35:44,276
But one caveat there is all


1227
00:35:44,276 --> 00:35:46,186
things being equal, if you have


1228
00:35:46,186 --> 00:35:48,846
a decision between encoded or


1229
00:35:48,846 --> 00:35:50,126
PCM, we would prefer the


1230
00:35:50,126 --> 00:35:51,576
encoded, but don't do any


1231
00:35:51,576 --> 00:35:53,196
special logic to give us that.


1232
00:35:53,286 --> 00:35:54,126
Just give us what you have.


1233
00:35:55,416 --> 00:35:56,476
And we prefer interleaved


1234
00:35:56,476 --> 00:35:58,336
channel formats and 1 to 2


1235
00:35:58,336 --> 00:35:59,536
seconds of audio per


1236
00:35:59,536 --> 00:36:00,366
CMSampleBuffer.


1237
00:36:00,366 --> 00:36:03,946
All right, now I'd like to take


1238
00:36:03,946 --> 00:36:05,326
a slight detour and talk about


1239
00:36:05,326 --> 00:36:06,356
video synchronization.


1240
00:36:07,656 --> 00:36:08,606
Now, you may be wondering why


1241
00:36:08,606 --> 00:36:09,996
we're talking about video in a


1242
00:36:09,996 --> 00:36:11,776
AirPlay talk that's mainly about


1243
00:36:11,776 --> 00:36:12,226
audio.


1244
00:36:13,196 --> 00:36:15,196
And it's because the classes


1245
00:36:15,196 --> 00:36:16,466
that we're introducing today not


1246
00:36:16,466 --> 00:36:18,706
only are they good for AirPlay 2


1247
00:36:18,886 --> 00:36:19,796
but they're also just really


1248
00:36:19,796 --> 00:36:21,346
good playback APIs for general


1249
00:36:21,346 --> 00:36:22,056
use cases.


1250
00:36:22,666 --> 00:36:24,036
And you may want to use them to


1251
00:36:24,036 --> 00:36:24,656
play video.


1252
00:36:25,226 --> 00:36:26,836
Now, once again, if you can use


1253
00:36:26,836 --> 00:36:28,186
AVPlayer, please use that.


1254
00:36:28,186 --> 00:36:29,946
But if you can't use AVPlayer


1255
00:36:29,946 --> 00:36:31,636
and you want to play video, here


1256
00:36:32,236 --> 00:36:33,066
you go.


1257
00:36:33,416 --> 00:36:36,496
So, let's refer, jump back to a


1258
00:36:36,496 --> 00:36:37,686
block diagram that we showed


1259
00:36:37,686 --> 00:36:38,786
earlier in the presentation.


1260
00:36:38,936 --> 00:36:40,506
We've got the Client App, the


1261
00:36:40,506 --> 00:36:41,206
AudioRenderer and the


1262
00:36:41,206 --> 00:36:41,766
Synchronizer.


1263
00:36:42,586 --> 00:36:43,826
Well, if I simply move those out


1264
00:36:43,826 --> 00:36:45,206
of the way, I can make room for


1265
00:36:45,206 --> 00:36:47,426
a new renderer class and here


1266
00:36:47,426 --> 00:36:48,396
I'm going to add an


1267
00:36:48,396 --> 00:36:49,626
AVSampleBufferDisplayLayer.


1268
00:36:50,556 --> 00:36:51,766
Now for those of you who are


1269
00:36:51,766 --> 00:36:52,366
unfamiliar with


1270
00:36:52,366 --> 00:36:54,596
AVSampleBufferDisplayLayer, it's


1271
00:36:54,596 --> 00:36:56,176
a class similar to the new


1272
00:36:56,176 --> 00:36:57,546
AVSampleBufferAudio Renderer,


1273
00:36:57,756 --> 00:36:58,956
except this one is geared


1274
00:36:58,956 --> 00:36:59,716
towards video.


1275
00:37:00,136 --> 00:37:01,026
It's been around for quite a


1276
00:37:01,026 --> 00:37:01,546
while and there's some


1277
00:37:01,546 --> 00:37:04,256
documentation and sample code on


1278
00:37:04,256 --> 00:37:05,196
the developer website.


1279
00:37:06,166 --> 00:37:09,136
But new in these releases is the


1280
00:37:09,136 --> 00:37:10,266
idea that you can have its


1281
00:37:10,266 --> 00:37:12,576
timeline controlled by a


1282
00:37:12,576 --> 00:37:13,446
RenderSynchronizer.


1283
00:37:13,726 --> 00:37:14,636
You can add it to a


1284
00:37:14,636 --> 00:37:15,886
RenderSynchronizer just as you


1285
00:37:15,886 --> 00:37:16,896
add the AudioRenderer to your


1286
00:37:16,896 --> 00:37:17,626
RenderSynchronizer.


1287
00:37:18,046 --> 00:37:19,906
And this is really cool.


1288
00:37:19,956 --> 00:37:21,636
Because then you can add the


1289
00:37:21,636 --> 00:37:23,736
audio data to the AudioRenderer.


1290
00:37:24,166 --> 00:37:27,366
The video data to the video


1291
00:37:27,366 --> 00:37:30,326
renderer, set a rate of 1 on the


1292
00:37:30,326 --> 00:37:32,726
Synchronizer, and because


1293
00:37:32,726 --> 00:37:33,466
they're tied to the same


1294
00:37:33,466 --> 00:37:34,916
timeline, that will come out in


1295
00:37:34,916 --> 00:37:35,126
sync.


1296
00:37:35,676 --> 00:37:37,086
Now a couple words of caution,


1297
00:37:37,306 --> 00:37:39,026
the video will not be AirPlayed,


1298
00:37:39,466 --> 00:37:40,476
and additionally when you're


1299
00:37:40,476 --> 00:37:42,036
using long-form you can only add


1300
00:37:42,036 --> 00:37:43,286
one AudioRenderer to a


1301
00:37:43,286 --> 00:37:44,126
RenderSynchronizer.


1302
00:37:44,716 --> 00:37:46,386
But never the less, I think you


1303
00:37:46,386 --> 00:37:48,556
can see the power in this and


1304
00:37:48,556 --> 00:37:49,946
flexibility in this architecture


1305
00:37:50,176 --> 00:37:51,246
and I'm sure you'll come up with


1306
00:37:51,246 --> 00:37:52,676
some really cool use case,


1307
00:37:52,676 --> 00:37:54,336
applications and we're excited


1308
00:37:54,336 --> 00:37:56,546
to find out what you can do.


1309
00:37:57,676 --> 00:37:58,106
All right.


1310
00:37:58,106 --> 00:37:59,336
Lastly, let's talk about the


1311
00:37:59,336 --> 00:38:00,716
availability of AirPlay 2.


1312
00:38:02,086 --> 00:38:04,406
So, I'm happy to say that the


1313
00:38:04,406 --> 00:38:06,706
APIs that I've discussed today


1314
00:38:06,706 --> 00:38:08,456
plus the advanced buffering are


1315
00:38:08,456 --> 00:38:10,976
all available in the beta that


1316
00:38:10,976 --> 00:38:12,206
you guys have access to today.


1317
00:38:13,486 --> 00:38:16,566
And you simply set the AirPlay 2


1318
00:38:16,566 --> 00:38:18,286
toggle in the developer pane,


1319
00:38:18,476 --> 00:38:19,276
flip that on.


1320
00:38:19,716 --> 00:38:21,166
And then you can use an updated


1321
00:38:21,166 --> 00:38:22,766
Apple TV as your AirPlay 2


1322
00:38:22,766 --> 00:38:24,686
receiver and send to it.


1323
00:38:26,016 --> 00:38:27,376
In an upcoming beta, we're going


1324
00:38:27,376 --> 00:38:28,766
to enable multi-room audio.


1325
00:38:29,586 --> 00:38:30,606
And lastly, this will be


1326
00:38:30,606 --> 00:38:31,896
available to customers in an


1327
00:38:31,896 --> 00:38:32,726
upcoming release.


1328
00:38:33,616 --> 00:38:34,476
Let's go through the summary.


1329
00:38:34,476 --> 00:38:36,006
We've covered a lot today.


1330
00:38:36,396 --> 00:38:38,456
So, AirPlay 2 introduces many


1331
00:38:38,456 --> 00:38:39,456
new features for audio.


1332
00:38:41,056 --> 00:38:42,566
Long-form audio applications can


1333
00:38:42,566 --> 00:38:44,566
enable them with the steps that


1334
00:38:44,566 --> 00:38:45,366
I've outlined today.


1335
00:38:46,336 --> 00:38:48,206
And AirPlay 2 adoption can begin


1336
00:38:48,206 --> 00:38:49,356
with the beta you guys all have


1337
00:38:49,356 --> 00:38:51,286
in your hands.


1338
00:38:51,536 --> 00:38:54,096
Additional information, this


1339
00:38:54,096 --> 00:38:55,556
website will have the


1340
00:38:55,556 --> 00:38:56,496
presentation that we've gone


1341
00:38:56,496 --> 00:38:57,516
over today, and there will be


1342
00:38:57,516 --> 00:38:58,756
sample code available soon.


1343
00:38:59,346 --> 00:39:02,016
A couple sessions that we've


1344
00:39:02,016 --> 00:39:02,806
talked about earlier.


1345
00:39:02,806 --> 00:39:03,926
There's the What's New in Audio


1346
00:39:03,926 --> 00:39:04,996
session, that's the one that


1347
00:39:04,996 --> 00:39:06,356
will discuss long-form in more


1348
00:39:06,356 --> 00:39:07,676
depth and also Introducing


1349
00:39:07,676 --> 00:39:08,176
MusicKit.


1350
00:39:08,616 --> 00:39:09,516
That's one that you should check


1351
00:39:09,516 --> 00:39:09,726
out.


1352
00:39:10,026 --> 00:39:11,086
Thank you very much and I hope


1353
00:39:11,086 --> 00:39:11,856
you have a great rest of the


1354
00:39:11,856 --> 00:39:12,116
week.


1355
00:39:13,016 --> 00:39:15,000
[ Applause ]

