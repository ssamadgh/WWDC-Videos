1
00:00:12,056 --> 00:00:12,606
>> Good morning, everyone.


2
00:00:12,976 --> 00:00:15,786
My name is Kapil Krishnamurthy
and I work in Core Audio.


3
00:00:16,746 --> 00:00:17,956
I'm here today to talk to you


4
00:00:17,956 --> 00:00:20,506
about a new API called
AVAudioEngine


5
00:00:20,746 --> 00:00:25,106
that we are introducing for
Mac OS X Yosemite and iOS 8.


6
00:00:26,986 --> 00:00:29,706
As part of today's talk we'll
first look at an overview


7
00:00:29,706 --> 00:00:32,866
of Core Audio and then we'll
dive into AVAudioEngine,


8
00:00:33,536 --> 00:00:35,376
look at some of the
goals behind the project,


9
00:00:35,796 --> 00:00:36,986
features of the new API,


10
00:00:38,006 --> 00:00:39,716
the different building
blocks you'll be using


11
00:00:40,616 --> 00:00:43,816
and finally we'll do a
section on gaming and 3D audio.


12
00:00:45,276 --> 00:00:46,036
So let's get started.


13
00:00:46,036 --> 00:00:49,846
For those of you who aren't
familiar with Core Audio,


14
00:00:50,726 --> 00:00:53,896
Core Audio provides a
number of C APIs as part


15
00:00:53,896 --> 00:00:57,176
of a different frameworks
on both iOS and Mac OS X.


16
00:00:57,176 --> 00:00:59,766
And you can use these
different APIs


17
00:00:59,916 --> 00:01:02,286
to implement audio features
in your applications.


18
00:01:03,746 --> 00:01:07,026
So using these APIs you will be
able to play in the card sounds


19
00:01:07,126 --> 00:01:10,966
with low latency, convert
between different file


20
00:01:10,966 --> 00:01:15,716
and data formats, read and write
audio files, work with many data


21
00:01:15,856 --> 00:01:18,486
and also play sounds
that get spatialized.


22
00:01:20,996 --> 00:01:25,076
Several years ago we added
some simple objective C classes


23
00:01:25,146 --> 00:01:28,226
to AVFoundation and
they're called AVAudioPlayer


24
00:01:28,226 --> 00:01:29,366
and AVAudioRecorder.


25
00:01:29,996 --> 00:01:33,176
And using these classes you
can play sounds from files


26
00:01:33,306 --> 00:01:35,526
or record directly to a file.


27
00:01:35,986 --> 00:01:37,806
Now while these classes
worked really well


28
00:01:37,806 --> 00:01:39,046
for simple use cases,


29
00:01:39,386 --> 00:01:41,936
a more advanced user might
find themselves a bit limited.


30
00:01:42,586 --> 00:01:45,276
So this year we're adding
a whole new set of API


31
00:01:45,676 --> 00:01:48,746
to AVFoundation called
AVAudioEngine


32
00:01:49,376 --> 00:01:51,716
and my colleague Doug
also spoke about a number


33
00:01:51,716 --> 00:01:54,826
of AV audio utility
classes in session 501.


34
00:01:55,806 --> 00:01:58,706
So using this new
API you will be able


35
00:01:58,706 --> 00:02:02,206
to write powerful features with
just a fraction of the amount


36
00:02:02,206 --> 00:02:04,186
of code that you may have
had to previously write.


37
00:02:05,296 --> 00:02:06,106
So let's get started.


38
00:02:06,926 --> 00:02:08,556
What were the goals
behind this project?


39
00:02:09,436 --> 00:02:12,156
One of the biggest goals
was to provide a powerful


40
00:02:12,156 --> 00:02:13,476
and feature-rich API set.


41
00:02:13,476 --> 00:02:16,646
And we're able to do that
because we're building on top


42
00:02:16,646 --> 00:02:18,936
of our existing Core Audio APIs.


43
00:02:19,866 --> 00:02:22,246
Using this API we want
to developers to be able


44
00:02:22,246 --> 00:02:24,726
to achieve simple as
well as complex tasks.


45
00:02:25,316 --> 00:02:27,926
And a simple task could be
something like playing a sound


46
00:02:27,926 --> 00:02:29,356
and running it through
an effect.


47
00:02:29,776 --> 00:02:31,886
A complex task could
be something as big


48
00:02:31,886 --> 00:02:34,026
as writing an entire
audio engine for a game.


49
00:02:35,376 --> 00:02:37,666
We also wanted to
simplify real-time audio.


50
00:02:38,546 --> 00:02:40,036
For those of you
who are not familiar


51
00:02:40,036 --> 00:02:42,906
with real-time audio it
can be quite challenging.


52
00:02:43,856 --> 00:02:46,446
You have a number of audio
callbacks every second


53
00:02:47,056 --> 00:02:48,366
and for each callback you have


54
00:02:48,366 --> 00:02:50,136
to provide data in
a timely fashion.


55
00:02:50,916 --> 00:02:53,386
You can't do things like
take locks on the I/O thread


56
00:02:53,676 --> 00:02:55,806
or call functions that
could block indefinitely.


57
00:02:56,636 --> 00:02:58,896
So we make all of this
easier for you to work


58
00:02:58,896 --> 00:03:02,486
with by giving you a
real-time audio system but one


59
00:03:02,486 --> 00:03:05,446
that you interact with in
a non real-time context.


60
00:03:06,616 --> 00:03:10,686
Features of the new API: this
is a full-featured Objective-C


61
00:03:10,736 --> 00:03:11,976
API set.


62
00:03:12,446 --> 00:03:15,416
You get a real-time audio
system to work with meaning


63
00:03:15,416 --> 00:03:17,376
that any changes
that you make on any


64
00:03:17,376 --> 00:03:19,516
of the blocks take
effect immediately.


65
00:03:20,336 --> 00:03:24,006
Using this API you will be able
to read and write audio files,


66
00:03:24,646 --> 00:03:28,846
play and record audio, connect
different audio processing


67
00:03:28,846 --> 00:03:31,856
blocks together and then
while the engine is running


68
00:03:31,936 --> 00:03:35,096
and audio is flowing through
this system you can tap the


69
00:03:35,096 --> 00:03:37,276
output of each of these
processing blocks.


70
00:03:37,966 --> 00:03:40,426
You'll also be able to
implement 3D audio for games.


71
00:03:41,166 --> 00:03:44,436
Now before we actually jump


72
00:03:44,556 --> 00:03:46,676
into the engine's building
blocks I thought I would give


73
00:03:46,676 --> 00:03:49,656
you two sample use cases
to give you a little flavor


74
00:03:49,656 --> 00:03:51,636
of what you'll be able
to do using this API.


75
00:03:52,606 --> 00:03:55,986
So the first sample use case
is a karaoke application.


76
00:03:56,696 --> 00:03:58,316
You have a backing
track that's playing


77
00:03:58,316 --> 00:04:00,966
and the user is singing
along with it in real-time.


78
00:04:01,806 --> 00:04:04,916
The output of the microphone
is passed through a delay,


79
00:04:04,976 --> 00:04:07,636
which is just a musical
effect and both


80
00:04:07,636 --> 00:04:10,856
of these audio chains are mixed
and sent to the output hardware.


81
00:04:11,116 --> 00:04:12,916
This could be a speaker
or headphones.


82
00:04:14,206 --> 00:04:18,026
Let's also say that you tap
the output of the microphone


83
00:04:18,646 --> 00:04:23,066
and analyze that raw data
to see the user's on pitch,


84
00:04:23,066 --> 00:04:24,016
he's doing a great job.


85
00:04:24,666 --> 00:04:26,916
And if he is, play
some sound effects,


86
00:04:27,156 --> 00:04:29,626
so this stream also
gets mixed in and played


87
00:04:29,626 --> 00:04:30,746
out to the output hardware.


88
00:04:32,276 --> 00:04:33,556
Here's another use case.


89
00:04:33,976 --> 00:04:36,766
You have a streaming
application and you receive data


90
00:04:36,766 --> 00:04:37,866
from the remote location.


91
00:04:38,556 --> 00:04:41,606
You can now stuff this
data into different buffers


92
00:04:41,606 --> 00:04:43,066
and schedule them on a player.


93
00:04:44,286 --> 00:04:45,836
You can run the output
of the player


94
00:04:46,196 --> 00:04:49,406
through an EQ whose UI
you present to the user


95
00:04:49,546 --> 00:04:52,326
so that they can tweak the
EQ based on that preference.


96
00:04:53,116 --> 00:04:55,966
The output of the EQ then
goes to the output hardware.


97
00:04:56,876 --> 00:04:59,276
So these are just
two sample use cases.


98
00:04:59,276 --> 00:05:01,736
You'll be able to do a
whole lot more once we talk


99
00:05:01,736 --> 00:05:02,866
about AVAudioEngine.


100
00:05:03,276 --> 00:05:04,086
So let's get started.


101
00:05:04,716 --> 00:05:08,946
The two main objects
that we're going to start


102
00:05:08,946 --> 00:05:11,616
with are the engine
object and the node object.


103
00:05:11,616 --> 00:05:15,176
And there are three specific
types of nodes: the output node,


104
00:05:15,346 --> 00:05:16,736
mixer node and the player node.


105
00:05:17,826 --> 00:05:19,986
We have other nodes as
well that we will get to


106
00:05:20,136 --> 00:05:22,126
but these are the initial
building block nodes.


107
00:05:22,426 --> 00:05:24,726
So the engine is an object


108
00:05:25,006 --> 00:05:27,256
that maintains a
graph of audio nodes.


109
00:05:28,396 --> 00:05:31,026
You create nodes and you
attach them to the engine


110
00:05:31,026 --> 00:05:33,306
and then you use the
engine to make connections


111
00:05:33,346 --> 00:05:34,776
between these different
audio nodes.


112
00:05:35,616 --> 00:05:38,986
The engine will analyze these
connections and determine


113
00:05:38,986 --> 00:05:41,036
which ones add up
to an active chain.


114
00:05:41,926 --> 00:05:43,496
When you then start the engine,


115
00:05:44,226 --> 00:05:46,636
audio flows through all
of the active chains.


116
00:05:47,746 --> 00:05:51,816
A powerful feature that the
engine has is that it allows you


117
00:05:51,816 --> 00:05:54,056
to dynamically reconfigure
these nodes.


118
00:05:54,476 --> 00:05:58,476
This means that while the engine
is rendering you can add new


119
00:05:58,476 --> 00:06:00,796
nodes and then wire them up.


120
00:06:00,956 --> 00:06:04,066
And so essentially you're adding
or removing chains dynamically.


121
00:06:04,726 --> 00:06:07,936
So the typical workflow
of the engine is


122
00:06:07,936 --> 00:06:10,876
that you create an instance of
the engine, create instances


123
00:06:10,876 --> 00:06:12,636
of all the nodes you
want to work with,


124
00:06:13,076 --> 00:06:15,946
attach them to the engine so
the engine is now aware of them


125
00:06:15,946 --> 00:06:19,756
and then connect them
together, start the engine.


126
00:06:20,436 --> 00:06:22,166
This will create an
active render thread


127
00:06:22,166 --> 00:06:25,666
and audio will flow through
all of the active chains.


128
00:06:26,856 --> 00:06:29,236
So let's now talk about a node.


129
00:06:30,106 --> 00:06:33,486
A node is a basic audio
block and we have three types


130
00:06:33,486 --> 00:06:36,376
of nodes: there are source
nodes, which are nodes


131
00:06:36,376 --> 00:06:37,406
that generate an audio.


132
00:06:37,806 --> 00:06:40,726
And examples of this are the
player or the input node.


133
00:06:41,646 --> 00:06:43,596
You have nodes that
process audio.


134
00:06:44,066 --> 00:06:47,326
So they take some audio and do
something to it and push it up.


135
00:06:47,736 --> 00:06:50,626
And examples are a
mixer or an effect.


136
00:06:51,046 --> 00:06:53,886
You also have destination
nodes that receive audio


137
00:06:53,976 --> 00:06:55,966
and do something with it.


138
00:06:56,396 --> 00:06:58,796
Every one of these nodes
has a certain number


139
00:06:58,796 --> 00:07:01,766
of input and output buses.


140
00:07:01,766 --> 00:07:05,886
And typically you see that
most nodes have a single input


141
00:07:05,886 --> 00:07:06,976
and output bus.


142
00:07:07,366 --> 00:07:09,516
But an exception to
this is a mixer node


143
00:07:09,726 --> 00:07:12,796
that has multiple input busses
and a single output bus.


144
00:07:13,816 --> 00:07:17,946
Every bus now has an audio
data format associated with it.


145
00:07:18,756 --> 00:07:20,486
So let's talk about connections.


146
00:07:21,806 --> 00:07:24,276
If you have a connection
between a source node


147
00:07:24,516 --> 00:07:27,416
and a destination node,
that forms an active chain.


148
00:07:28,326 --> 00:07:30,636
You can insert any
number of processing nodes


149
00:07:30,636 --> 00:07:32,956
between the source node
and the destination node.


150
00:07:33,456 --> 00:07:35,576
But as long as you
wire every bit


151
00:07:35,576 --> 00:07:37,806
of this chain up,
it's an active chain.


152
00:07:38,546 --> 00:07:41,986
As soon as you break one of the
connections, all of the nodes


153
00:07:42,016 --> 00:07:44,766
that are upstream of the
point of disconnection go


154
00:07:44,766 --> 00:07:45,876
into an inactive state.


155
00:07:46,716 --> 00:07:48,636
In this case, I've
broken the connection


156
00:07:48,796 --> 00:07:51,236
between the processing node
and the destination node,


157
00:07:51,736 --> 00:07:52,806
so my processing node


158
00:07:52,806 --> 00:07:55,226
and my source node are
now in an inactive state.


159
00:07:56,316 --> 00:07:57,976
The same holds true
in this example.


160
00:07:57,976 --> 00:08:03,016
So let's now look at
the specific node types.


161
00:08:03,796 --> 00:08:05,256
The first node that
we're going to talk


162
00:08:05,256 --> 00:08:06,236
about is the output node.


163
00:08:06,916 --> 00:08:09,486
The engine has an
implicit destination node


164
00:08:09,666 --> 00:08:10,876
and it's called the output node.


165
00:08:10,876 --> 00:08:14,156
And the role of the output
node is to take the data


166
00:08:14,156 --> 00:08:16,756
that it receives and hand
it to the output hardware,


167
00:08:17,076 --> 00:08:18,136
so this could be the speaker.


168
00:08:19,406 --> 00:08:22,236
You cannot create a standalone
instance of the output node.


169
00:08:22,526 --> 00:08:24,196
You have to get it
from the instance


170
00:08:24,196 --> 00:08:27,466
of the engine that
you've created.


171
00:08:27,566 --> 00:08:29,386
Let's move on to the mixer node.


172
00:08:30,186 --> 00:08:33,726
Mixer nodes are processing
nodes and they receive data


173
00:08:34,116 --> 00:08:36,666
on different input
busses which they then mix


174
00:08:37,176 --> 00:08:40,096
to a single output, which
goes out on the output bus.


175
00:08:41,145 --> 00:08:43,346
When you use a mixer,
you get control


176
00:08:43,346 --> 00:08:45,236
of the volume of each input bus.


177
00:08:45,696 --> 00:08:47,946
And if you add an application
that was playing a number


178
00:08:47,946 --> 00:08:50,196
of sounds and you put
each of these sounds


179
00:08:50,256 --> 00:08:51,746
in on a separate input bus,


180
00:08:52,406 --> 00:08:55,376
using this volume control
you can essentially blend


181
00:08:55,376 --> 00:08:57,926
in the amount of each sound
that you want to hear.


182
00:08:58,236 --> 00:08:59,556
So you create a mix.


183
00:09:00,656 --> 00:09:03,806
You now have control
over the output volume


184
00:09:03,946 --> 00:09:05,086
as well using a mixer.


185
00:09:05,486 --> 00:09:06,956
So you are controlling
the volume


186
00:09:06,956 --> 00:09:08,576
of the mix that you've created.


187
00:09:09,456 --> 00:09:13,206
If your application has
several categories of sound,


188
00:09:13,496 --> 00:09:16,026
you can make use of a
concept called submixing


189
00:09:16,346 --> 00:09:17,546
to create submixers.


190
00:09:18,276 --> 00:09:20,516
So let's say that you
have some UI sounds


191
00:09:20,576 --> 00:09:21,586
and you have some music.


192
00:09:22,076 --> 00:09:24,166
And you put all of the UI
sounds through one mixer,


193
00:09:24,346 --> 00:09:25,996
all of the music
through another mixer.


194
00:09:26,726 --> 00:09:29,246
Using the output volumes
of each of these mixers,


195
00:09:29,246 --> 00:09:30,886
you can essentially
control the volume


196
00:09:30,886 --> 00:09:32,366
of each of these submixers.


197
00:09:33,196 --> 00:09:35,976
Let's take that concept a
step further and put all


198
00:09:35,976 --> 00:09:38,456
of the submixers
through a master mixer.


199
00:09:39,646 --> 00:09:43,136
The output volume of the master
mixer will essentially control


200
00:09:43,136 --> 00:09:45,676
the volume of the entire
mix in your application.


201
00:09:46,926 --> 00:09:49,776
Now the engine has an
implicit mixer node.


202
00:09:50,306 --> 00:09:52,666
And when you ask the
engine for its mixer node,


203
00:09:52,986 --> 00:09:54,576
it creates an instance
of a mixer.


204
00:09:54,576 --> 00:09:56,696
It creates an instance
of the output node


205
00:09:56,996 --> 00:09:59,196
and connects it together
for you by default.


206
00:10:00,326 --> 00:10:02,346
The difference here
between the mixer node


207
00:10:02,346 --> 00:10:05,396
and the output node is that you
can create additional instances


208
00:10:05,856 --> 00:10:07,496
and then attach them
to the engine


209
00:10:07,496 --> 00:10:08,846
and use them how you please.


210
00:10:10,396 --> 00:10:14,346
Mixers can also have
different audio data formats


211
00:10:14,456 --> 00:10:15,826
for each input bus.


212
00:10:16,166 --> 00:10:17,696
And the mixer will do the work


213
00:10:17,866 --> 00:10:21,456
of efficiently converting
the input data formats


214
00:10:21,896 --> 00:10:23,846
to the output data format.


215
00:10:24,836 --> 00:10:28,426
So now that we have looked
at these initial nodes,


216
00:10:28,806 --> 00:10:32,156
let's talk about how this works
in the context of the engine.


217
00:10:32,696 --> 00:10:33,676
So let's say that I have an app


218
00:10:33,996 --> 00:10:35,666
that creates an instance
of the engine.


219
00:10:35,666 --> 00:10:39,376
We can now ask the engine
for its main mixer node


220
00:10:39,376 --> 00:10:41,676
so it's going to create
the instance of a mixer,


221
00:10:41,886 --> 00:10:43,566
create a mixer of
the output node


222
00:10:43,886 --> 00:10:45,556
and connect the two together.


223
00:10:46,196 --> 00:10:49,666
I can now create a clear note
and attach it to the engine


224
00:10:50,166 --> 00:10:51,386
and connect it to the mixer.


225
00:10:52,166 --> 00:10:54,746
So at this point I have a
connection chain going all the


226
00:10:54,746 --> 00:10:56,636
way from a source
to a destination,


227
00:10:57,126 --> 00:10:58,686
so I have an active chain.


228
00:10:59,746 --> 00:11:01,486
When I then start up the engine,


229
00:11:01,846 --> 00:11:03,776
an active render
thread is created


230
00:11:03,776 --> 00:11:05,916
and data is pulled
by the destination.


231
00:11:06,676 --> 00:11:08,606
So I have an active
flow of data here.


232
00:11:09,626 --> 00:11:13,316
The app can now interact
with each one of these blocks


233
00:11:13,806 --> 00:11:16,106
and any change that
it makes on any


234
00:11:16,106 --> 00:11:18,246
of the nodes will take
effect immediately.


235
00:11:19,886 --> 00:11:21,006
So now that we've talked


236
00:11:21,156 --> 00:11:26,056
about an active render thread
how do you push your audio data


237
00:11:26,266 --> 00:11:27,186
on the render thread.


238
00:11:27,736 --> 00:11:29,486
You use a player to do that.


239
00:11:30,636 --> 00:11:31,716
Let's look at player nodes.


240
00:11:32,656 --> 00:11:34,556
Player nodes are nodes
that can play data


241
00:11:34,556 --> 00:11:36,876
from files and from buffers.


242
00:11:37,116 --> 00:11:40,346
And the way that it happens
or the way that it's done is


243
00:11:40,346 --> 00:11:41,506
by scheduling events,


244
00:11:42,086 --> 00:11:45,046
which simply means play
data at a specified time.


245
00:11:45,866 --> 00:11:49,466
That data, that time could be
now or sometime in the future.


246
00:11:51,266 --> 00:11:54,666
When you're scheduling buffers
you can schedule either multiple


247
00:11:54,666 --> 00:11:57,956
buffers and as each
buffer is consumed


248
00:11:57,956 --> 00:12:00,076
by the player you get
an individual callback,


249
00:12:00,626 --> 00:12:04,216
which you can then use a cue to
go ahead and schedule more data.


250
00:12:05,516 --> 00:12:07,806
You can also schedule
a single buffer


251
00:12:07,806 --> 00:12:09,256
that plays in a loop fashion.


252
00:12:10,056 --> 00:12:12,976
And this is useful in the case
when you may have a musical loop


253
00:12:13,016 --> 00:12:15,836
or a sound effect that you want
to play over and over again.


254
00:12:16,476 --> 00:12:19,436
So, you load the data and
then you play the buffer


255
00:12:19,436 --> 00:12:22,886
and it'll continue to play
until you stop the player


256
00:12:23,276 --> 00:12:25,776
or you interrupt it
with another buffer.


257
00:12:25,776 --> 00:12:27,076
We'll get into that.


258
00:12:27,916 --> 00:12:30,146
You can also schedule
a file or a portion


259
00:12:30,146 --> 00:12:31,486
of a file called a segment.


260
00:12:32,986 --> 00:12:35,966
So going back to our previous
diagram, we had an engine


261
00:12:35,966 --> 00:12:37,026
that was in a running state.


262
00:12:37,676 --> 00:12:41,356
So now I can create an instance
of a buffer and load my data


263
00:12:41,426 --> 00:12:43,856
into it, shown by the red arrow.


264
00:12:44,156 --> 00:12:48,216
Once I do that, I can schedule
this buffer on the player.


265
00:12:48,666 --> 00:12:52,326
And when the player is playing,
the player will consume the data


266
00:12:52,566 --> 00:12:55,516
in the buffer and push
it on the render thread.


267
00:12:55,516 --> 00:12:59,526
In a similar manner, I can
work with multiple buffers.


268
00:12:59,926 --> 00:13:02,086
So over here I have
multiple buffers.


269
00:13:02,176 --> 00:13:05,796
I load data into each one of
them and I schedule each one


270
00:13:05,796 --> 00:13:08,126
of them to play in
sequence on the player.


271
00:13:09,456 --> 00:13:12,026
As each buffer is
consumed by the player,


272
00:13:12,496 --> 00:13:14,916
I get individual
callbacks letting me know


273
00:13:14,916 --> 00:13:16,006
that that buffer is done.


274
00:13:16,006 --> 00:13:18,996
I can use that as a cue
and schedule more data.


275
00:13:19,656 --> 00:13:24,316
In a similar manner you
can work with a file.


276
00:13:24,316 --> 00:13:27,606
And the difference here is you
don't have to actually deal


277
00:13:27,606 --> 00:13:29,046
with the audio data yourself.


278
00:13:29,786 --> 00:13:32,996
All you need is a URL
to a physical audio file


279
00:13:33,316 --> 00:13:35,996
with which you can create
an AVAudioFile object


280
00:13:36,566 --> 00:13:38,516
and then schedule that
directly on the player.


281
00:13:39,146 --> 00:13:41,966
The player will do the work of
reading the data from the file


282
00:13:42,086 --> 00:13:45,716
and pushing it on
the render thread.


283
00:13:45,896 --> 00:13:48,426
So, let's now look
at a good example


284
00:13:48,456 --> 00:13:49,806
of how we can achieve this.


285
00:13:50,366 --> 00:13:54,966
I first create an instance of
the engine, create an instance


286
00:13:54,966 --> 00:13:57,816
of a player and attach
the player to the engine,


287
00:13:57,986 --> 00:13:59,516
so the engine is now
aware of the player.


288
00:13:59,816 --> 00:14:02,986
I'm now going to
split my example up


289
00:14:02,986 --> 00:14:05,066
and show how you can
first work with a file.


290
00:14:05,986 --> 00:14:08,636
So, given a URL to
an audio file,


291
00:14:08,726 --> 00:14:10,936
I can create the
AudioFile object.


292
00:14:12,366 --> 00:14:15,866
The next thing that I do is ask
the engine for its mainMixer.


293
00:14:16,676 --> 00:14:19,066
So the engine will create
an instance of a mixer node,


294
00:14:19,716 --> 00:14:22,226
create an output node and
connect the two together.


295
00:14:22,226 --> 00:14:26,726
I can now go ahead and
connect the player to the mixer


296
00:14:27,386 --> 00:14:28,936
with the files processing
format.


297
00:14:28,936 --> 00:14:31,526
So I have so I have a connection
chain going all the way


298
00:14:31,526 --> 00:14:33,026
from a player that's a source


299
00:14:33,586 --> 00:14:35,516
to the output node
that's a destination.


300
00:14:37,176 --> 00:14:40,776
Now I can schedule my
file to play atTime:nil,


301
00:14:41,056 --> 00:14:42,666
which is as soon as possible.


302
00:14:43,206 --> 00:14:45,776
And in this case I pass a nil
for the completion handler.


303
00:14:46,486 --> 00:14:48,326
If I had some work that
I needed to be done


304
00:14:48,426 --> 00:14:50,276
after the file is
consumed by the player,


305
00:14:50,486 --> 00:14:51,826
I can pass in a block over here.


306
00:14:53,146 --> 00:14:56,986
So in a similar manner I can
work with a buffer as well.


307
00:14:57,636 --> 00:15:01,066
Let's say that I create
an AVAudioPCMBuffer object


308
00:15:01,156 --> 00:15:02,996
and load some data into it.


309
00:15:03,546 --> 00:15:06,876
The specifics of that part
are covered in session 501.


310
00:15:07,016 --> 00:15:09,496
So if you missed that,
please refer to that session.


311
00:15:10,346 --> 00:15:13,966
Once I have my buffer object I
can go ahead and ask the engine


312
00:15:13,966 --> 00:15:17,116
for its mixer and make the
connection between the player


313
00:15:17,526 --> 00:15:20,216
to the mixer with
the buffer's format.


314
00:15:21,196 --> 00:15:25,226
Now I can go ahead and
schedule this buffer atTime:nil,


315
00:15:25,226 --> 00:15:27,036
which is as soon as possible.


316
00:15:27,726 --> 00:15:29,776
But note that we have
an additional argument


317
00:15:29,966 --> 00:15:32,856
when we are working with
buffers, the options argument.


318
00:15:33,536 --> 00:15:35,656
We're going to talk about
that right after this.


319
00:15:35,766 --> 00:15:37,246
But for now I'm going
to pass nil,


320
00:15:38,386 --> 00:15:40,296
and nil for the completion
handler as well.


321
00:15:41,256 --> 00:15:42,816
So now that I've
scheduled my data


322
00:15:43,206 --> 00:15:46,046
on the player I can go
ahead and start the engine.


323
00:15:46,506 --> 00:15:48,196
This creates an active
render thread


324
00:15:48,196 --> 00:15:50,376
and then call play
on the player.


325
00:15:50,376 --> 00:15:53,336
And the player will do the
work of creating the data


326
00:15:53,706 --> 00:15:56,666
from the file in the buffer and
pushing it on the render thread.


327
00:15:57,566 --> 00:15:59,946
So let's now talk about
the different buffer


328
00:15:59,946 --> 00:16:01,186
scheduling options.


329
00:16:03,436 --> 00:16:06,596
In all of the examples that
I'm going to talk about now,


330
00:16:06,596 --> 00:16:10,126
I'm going to specify nil
for the atTime argument


331
00:16:10,126 --> 00:16:12,976
and that just means that
in all of these examples,


332
00:16:13,156 --> 00:16:15,976
I'm going to schedule something
to play as soon as possible.


333
00:16:16,606 --> 00:16:20,076
So let's talk about the first
option and that's when you want


334
00:16:20,076 --> 00:16:22,746
to schedule a buffer to
play as soon as possible.


335
00:16:23,506 --> 00:16:27,826
In that case all you need
to do is schedule a buffer


336
00:16:27,826 --> 00:16:28,946
with the option set to nil.


337
00:16:29,566 --> 00:16:32,066
You call play on the player
and that buffer gets played.


338
00:16:33,676 --> 00:16:36,146
If you have a buffer that's
playing now and you want


339
00:16:36,146 --> 00:16:39,816
to append a new buffer,
it's the exact same call.


340
00:16:40,416 --> 00:16:43,506
You schedule the new buffer
with the option set to nil


341
00:16:44,036 --> 00:16:47,216
and so the new buffer
gets appended to the cue


342
00:16:47,216 --> 00:16:48,676
of currently playing buffers.


343
00:16:48,676 --> 00:16:51,536
On the other hand if I want


344
00:16:51,536 --> 00:16:54,686
to interrupt my currently
playing buffer


345
00:16:54,856 --> 00:16:58,156
with a new buffer I can
schedule the new buffer


346
00:16:58,346 --> 00:17:01,486
with the AVAudioPlayerNode
BufferInterrupts option.


347
00:17:02,056 --> 00:17:04,476
So that will interrupt the
currently playing buffer


348
00:17:04,596 --> 00:17:06,836
and start playing my
new buffer right away.


349
00:17:08,445 --> 00:17:11,616
Let's now look at the different
variants with a looping buffer.


350
00:17:12,346 --> 00:17:14,695
So like I said earlier,
if I have a buffer that's


351
00:17:14,695 --> 00:17:17,016
to be played in a looped
fashion, like a sound effect,


352
00:17:17,016 --> 00:17:21,066
for instance, I can load the
data in that buffer and schedule


353
00:17:21,066 --> 00:17:24,046
that buffer with the
AVAudioPlayerNodeBufferLoops


354
00:17:24,046 --> 00:17:24,435
option.


355
00:17:25,346 --> 00:17:27,976
When I call play on the
player, that buffer starts


356
00:17:27,976 --> 00:17:29,306
to play in a looped fashion.


357
00:17:30,946 --> 00:17:35,826
If I want to interrupt a looping
buffer it's the same option


358
00:17:35,876 --> 00:17:37,206
as what we've seen before.


359
00:17:37,206 --> 00:17:39,346
I have to schedule a new buffer


360
00:17:39,566 --> 00:17:42,646
with the AVAudioPlayerNode
BufferInterrupts option.


361
00:17:43,296 --> 00:17:46,106
So essentially it's the same
option for when you want


362
00:17:46,106 --> 00:17:49,706
to interrupt a regular
buffer or a looping buffer.


363
00:17:51,256 --> 00:17:53,786
The last case is
an interesting one.


364
00:17:54,626 --> 00:17:56,936
So if you have a looping
buffer but you want


365
00:17:56,936 --> 00:18:00,636
to let the current loop finish
before you start playing your


366
00:18:00,636 --> 00:18:04,516
new data you can
schedule your new buffer


367
00:18:04,646 --> 00:18:08,576
with the AVAudioPlayerNodeBuffer
InterruptsAtLoop option.


368
00:18:09,576 --> 00:18:13,276
So this will let the current
loop finish and as soon


369
00:18:13,276 --> 00:18:16,266
as that loop is done the
new buffer starts playing.


370
00:18:17,006 --> 00:18:20,666
Now that was a whole bunch
of options, so let's look


371
00:18:20,666 --> 00:18:23,736
at one practical example of
how we can use these options.


372
00:18:24,476 --> 00:18:26,616
So let's say that I have
a sound that's broken


373
00:18:26,616 --> 00:18:27,696
up into three parts.


374
00:18:27,786 --> 00:18:30,106
And the example that I'm
going to use here is a siren.


375
00:18:31,066 --> 00:18:33,996
So you have the initial buildup
of the sound which is the


376
00:18:33,996 --> 00:18:35,346
"attack" portion of the siren.


377
00:18:36,316 --> 00:18:38,706
You have the droning
portion of the siren,


378
00:18:38,776 --> 00:18:41,196
which can be modeled using
just a looping buffer.


379
00:18:41,896 --> 00:18:43,976
And this is the "sustain"
portion of the sound.


380
00:18:44,746 --> 00:18:46,976
And then you have the
dying down of the siren,


381
00:18:47,246 --> 00:18:48,966
which is the "release"
portion of the sound.


382
00:18:49,376 --> 00:18:52,776
So let's say that I load
up each of these sounds


383
00:18:52,826 --> 00:18:53,886
into different buffers.


384
00:18:54,186 --> 00:18:57,346
The way that I can
implement this in code is


385
00:18:57,346 --> 00:19:00,786
to first schedule the attack
buffer with my options set


386
00:19:00,786 --> 00:19:04,576
to nil and then schedule
the sustain buffer


387
00:19:04,996 --> 00:19:07,746
with the AVAudioPlayerNodeBuffer
loops option.


388
00:19:08,876 --> 00:19:10,586
So when I call play
on the player,


389
00:19:10,966 --> 00:19:13,996
what this will do is play the
attack portion of the sound


390
00:19:14,476 --> 00:19:17,036
and then immediately start
playing the sustain portion


391
00:19:17,036 --> 00:19:20,866
of the sound and continue
to loop that sustain buffer


392
00:19:21,356 --> 00:19:23,856
and that goes on until
I'm ready to interrupt it.


393
00:19:24,716 --> 00:19:27,746
So after some time has gone
by when I'm ready to interrupt


394
00:19:27,786 --> 00:19:30,866
that I can schedule
my release buffer


395
00:19:31,056 --> 00:19:34,486
with the AVAudioPlayerNodeBuffer
InterruptsAtLoop option.


396
00:19:34,966 --> 00:19:38,596
So this will let the last loop
of the sustained buffer finish


397
00:19:38,596 --> 00:19:42,486
up and then play the
release portion of my sound.


398
00:19:44,756 --> 00:19:47,526
Now remember that I said
in the beginning that all


399
00:19:47,526 --> 00:19:49,856
of my examples involve
scheduling events


400
00:19:49,856 --> 00:19:51,206
that play as soon as possible.


401
00:19:51,826 --> 00:19:54,346
I can also schedule events
to play in the future.


402
00:19:54,796 --> 00:19:56,336
So here's an example of that.


403
00:19:56,796 --> 00:19:59,276
In this case I'm just
going to schedule a buffer


404
00:19:59,276 --> 00:20:01,936
to play 10 seconds
in the future.


405
00:20:02,096 --> 00:20:04,186
So I create an AVAudioTime
object


406
00:20:04,836 --> 00:20:08,566
that has a relative sample
time 10 seconds in the future,


407
00:20:08,986 --> 00:20:11,716
and I use the buffer sampleRate
as my reference point.


408
00:20:12,846 --> 00:20:16,196
I can now schedule the buffer
with this AVAudioTime object


409
00:20:16,196 --> 00:20:18,536
and call play on the player


410
00:20:18,536 --> 00:20:24,706
and my buffer gets played
10 seconds in the future.


411
00:20:24,796 --> 00:20:27,856
So we've talked about
player nodes


412
00:20:28,076 --> 00:20:29,796
and how you can use a player


413
00:20:30,346 --> 00:20:33,016
to push your audio data
on the render thread.


414
00:20:33,966 --> 00:20:35,386
Well if you wanted to pull data


415
00:20:35,386 --> 00:20:38,016
from the render thread
how do you do that?


416
00:20:38,486 --> 00:20:40,546
You use a node tap.


417
00:20:40,546 --> 00:20:43,336
And here's some reasons for
why you may want to do that.


418
00:20:43,526 --> 00:20:45,796
Let's say you want to capture
the output of the microphone


419
00:20:46,626 --> 00:20:51,796
and save that data to disk, or
if you have a music application


420
00:20:51,946 --> 00:20:53,796
and you want to record
a live performance


421
00:20:54,576 --> 00:20:56,216
or if you have a
game and you want


422
00:20:56,216 --> 00:20:57,836
to capture the output
mix of the game.


423
00:20:58,456 --> 00:21:00,976
You can do all of
that using a node tap.


424
00:21:01,286 --> 00:21:05,096
And what that is, is essentially
a tap that you install


425
00:21:05,096 --> 00:21:06,736
on the output bus of a node.


426
00:21:07,726 --> 00:21:11,176
So the data that's captured
by the tap is returned back


427
00:21:11,176 --> 00:21:14,146
to your application
via the callback log.


428
00:21:14,716 --> 00:21:16,686
So going back to a
familiar diagram,


429
00:21:17,086 --> 00:21:18,706
I have two players
that's connected


430
00:21:18,706 --> 00:21:19,856
to the engines main mixer.


431
00:21:19,856 --> 00:21:24,946
And I want to tap the output of
the mixer so I can install a tap


432
00:21:24,946 --> 00:21:27,886
on the mixer and the tap
will start pulling data


433
00:21:27,886 --> 00:21:28,826
from the render thread.


434
00:21:29,726 --> 00:21:32,636
I can then go ahead, the
tap will then go ahead


435
00:21:32,806 --> 00:21:36,696
and create a buffer object,
stuff that data into the buffer


436
00:21:37,456 --> 00:21:38,846
and return that back


437
00:21:38,846 --> 00:21:41,016
to the application
via a callback block.


438
00:21:41,966 --> 00:21:45,186
In code it's just
one function call.


439
00:21:45,796 --> 00:21:53,426
You install a tap on the mixer's
output bus 0 with a buffer size


440
00:21:53,426 --> 00:21:59,766
of 4096 frames and the mixer's
output format for that bus.


441
00:22:00,106 --> 00:22:03,716
Within the block I have an
AVAudioPCMBuffer that contains


442
00:22:03,946 --> 00:22:05,036
that much amount of data.


443
00:22:05,636 --> 00:22:07,606
And I can do whatever I
need to do with that data.


444
00:22:08,546 --> 00:22:10,336
Alright, so to quickly
summarize,


445
00:22:10,726 --> 00:22:12,426
you have an active
render thread.


446
00:22:12,686 --> 00:22:14,986
You use player nodes
to push your audio data


447
00:22:15,246 --> 00:22:18,616
on the render thread and use
node taps to pull audio data


448
00:22:18,826 --> 00:22:20,916
from the render thread.


449
00:22:21,246 --> 00:22:23,216
Let's now switch gears and talk


450
00:22:23,216 --> 00:22:25,436
about a new node
called the input node.


451
00:22:26,496 --> 00:22:29,776
The input node receives
data from the input hardware


452
00:22:29,886 --> 00:22:31,896
and it's parallel
to the output node.


453
00:22:32,776 --> 00:22:35,836
With the input node you cannot
create a standalone instance.


454
00:22:36,056 --> 00:22:40,776
You have to get the
instance from the engine.


455
00:22:40,776 --> 00:22:44,396
When you've connected the
input node in an active chain


456
00:22:44,396 --> 00:22:49,206
and the engine is running, data
is pulled from the input node.


457
00:22:50,346 --> 00:22:52,076
So let's go back to
a familiar diagram.


458
00:22:52,586 --> 00:22:55,496
I've connected the input
node to the mixer nodes


459
00:22:55,946 --> 00:22:57,616
and that's connected
to the output node.


460
00:22:58,226 --> 00:23:01,126
So when I start the engine,
this is an active chain


461
00:23:01,516 --> 00:23:03,956
and data is pulled
from the input node.


462
00:23:04,806 --> 00:23:07,676
So, if I'm receiving
data from the input node


463
00:23:07,746 --> 00:23:11,736
and the engine is running and
I want to stop receiving data


464
00:23:11,736 --> 00:23:14,056
at a certain point,
how do I do that?


465
00:23:14,486 --> 00:23:15,186
It's very simple.


466
00:23:15,586 --> 00:23:18,236
All you have to do-oh I'm sorry.


467
00:23:18,236 --> 00:23:18,936
I raced ahead.


468
00:23:18,936 --> 00:23:20,786
Let's look at a code example


469
00:23:20,946 --> 00:23:22,446
of how you can connect
the input node.


470
00:23:23,416 --> 00:23:25,416
So I get the input
node from the engine.


471
00:23:26,126 --> 00:23:28,016
Just make a connection
to any other node


472
00:23:28,526 --> 00:23:30,076
with the input node's
hardware format


473
00:23:31,596 --> 00:23:32,846
and then start the engine.


474
00:23:33,396 --> 00:23:34,736
This creates an active
render thread


475
00:23:34,736 --> 00:23:36,456
and the input nodes
pull for data.


476
00:23:37,426 --> 00:23:38,836
So like I was saying earlier,


477
00:23:38,906 --> 00:23:42,646
if you have an input node that's
being pulled and you don't want


478
00:23:42,646 --> 00:23:45,856
to receive data anymore from
the input node, what do you do?


479
00:23:46,266 --> 00:23:47,856
Just disconnect the input node.


480
00:23:48,656 --> 00:23:51,496
So the input node will no
longer be in an active chain


481
00:23:52,416 --> 00:23:53,756
and it won't be pulled for data.


482
00:23:54,786 --> 00:23:57,276
In order to do that, it's
just one line of code.


483
00:23:57,896 --> 00:24:00,776
Using the engine, you
disconnect the node output


484
00:24:00,776 --> 00:24:02,396
of the input node.


485
00:24:02,396 --> 00:24:08,456
Now if you want to capture
data from the input node,


486
00:24:08,756 --> 00:24:11,296
you can install a node tap
and we've talked about that.


487
00:24:12,086 --> 00:24:15,216
But what's interesting about
this particular example is,


488
00:24:15,216 --> 00:24:18,186
if I wanted to work with
just the input node,


489
00:24:18,416 --> 00:24:22,346
say just capture data from the
microphone and maybe examine it,


490
00:24:22,606 --> 00:24:25,096
analyze it in real time or
maybe write it out to file,


491
00:24:25,136 --> 00:24:29,136
I can directly install
a tap on the input node.


492
00:24:29,786 --> 00:24:32,976
And the tap will do the work of
pulling the input node for data,


493
00:24:33,526 --> 00:24:35,866
stuffing it in buffers
and then returning


494
00:24:35,866 --> 00:24:37,076
that back to the application.


495
00:24:37,696 --> 00:24:39,796
Once you have that data you
can do whatever you need


496
00:24:39,796 --> 00:24:41,006
to do with it.


497
00:24:42,626 --> 00:24:46,696
And let's now talk about
the last type of nodes


498
00:24:46,816 --> 00:24:49,046
in this section, effect nodes.


499
00:24:50,156 --> 00:24:52,666
Effect nodes are nodes
that process data.


500
00:24:53,226 --> 00:24:56,326
So, depending on the type of
effect, they take some amount


501
00:24:56,326 --> 00:24:59,356
of data in, they process
it and push that data out.


502
00:25:00,736 --> 00:25:03,066
We have two main
categories of effects.


503
00:25:03,226 --> 00:25:07,666
You have AVAudioUnitEffects
and AVAudioUnitTimeEffects.


504
00:25:08,676 --> 00:25:10,586
So what's the difference
between the two?


505
00:25:11,456 --> 00:25:16,286
AVAudioUnitEffects require the
same amount of data on input


506
00:25:16,286 --> 00:25:19,396
as the amount of data they're
being asked to provide.


507
00:25:20,136 --> 00:25:23,296
So let's take the example
of a distortion effect.


508
00:25:24,016 --> 00:25:28,636
If a distortion node has
to provide 24ms of output,


509
00:25:29,206 --> 00:25:31,646
all it needs is 24ms of input


510
00:25:31,876 --> 00:25:33,776
that it then processes
and pushes out.


511
00:25:33,776 --> 00:25:38,696
As opposed to that, TimeEffects
don't have that constraint.


512
00:25:39,666 --> 00:25:42,146
So let's say that you have a
TimeEffect that's doing some


513
00:25:42,146 --> 00:25:43,246
amount of time stretching.


514
00:25:43,986 --> 00:25:47,366
If it is being asked to
provide 24ms of output,


515
00:25:47,646 --> 00:25:50,196
it may require 48ms of input.


516
00:25:51,346 --> 00:25:53,206
So that brings me
to my second point.


517
00:25:53,756 --> 00:25:57,386
It is for that reason why you
cannot connect a TimeEffect


518
00:25:57,716 --> 00:25:59,326
directly with the input node.


519
00:26:00,226 --> 00:26:02,556
Because, when you have
the input node running


520
00:26:02,556 --> 00:26:07,536
in real-time it cannot provide
data that it doesn't have.


521
00:26:07,756 --> 00:26:09,066
As opposed to that,


522
00:26:09,146 --> 00:26:12,186
with AVAudioUnitEffects you
can connect them anywhere


523
00:26:12,186 --> 00:26:12,666
in the chain.


524
00:26:13,006 --> 00:26:14,516
So you can use them with players


525
00:26:14,826 --> 00:26:17,526
or you can use them
with the input node.


526
00:26:19,256 --> 00:26:21,556
These are the list of effects


527
00:26:21,616 --> 00:26:22,976
that we currently
have available.


528
00:26:29,076 --> 00:26:32,276
So on the effects side
we have the Delay,


529
00:26:32,276 --> 00:26:34,026
Distortion, EQ and Reverb.


530
00:26:34,326 --> 00:26:36,736
If you're a musician you're
probably already familiar


531
00:26:36,736 --> 00:26:39,156
with these effects so you
can use them in real time


532
00:26:39,186 --> 00:26:40,076
or use them in the player.


533
00:26:40,816 --> 00:26:43,346
And on the TimeEffect side,


534
00:26:43,636 --> 00:26:45,676
we have the Varispeed
and the TimePitch.


535
00:26:46,146 --> 00:26:49,136
And these effects are useful
in cases where you want


536
00:26:49,136 --> 00:26:52,116
to manipulate the amount of time
stretching or maybe the pitch


537
00:26:52,116 --> 00:26:53,096
of the source content.


538
00:26:53,796 --> 00:26:56,476
So let's say that you have a
speech file that you're playing


539
00:26:56,606 --> 00:27:00,296
and you want to pitch the voice
up to sound like a chipmunk.


540
00:27:01,086 --> 00:27:05,386
Well you can do that using
one of the TimeEffects.


541
00:27:05,666 --> 00:27:07,416
So let's now look at an example


542
00:27:07,596 --> 00:27:09,226
of how you can use
one of these effects.


543
00:27:10,086 --> 00:27:13,316
In this example I'm going
to make use of the EQ.


544
00:27:13,726 --> 00:27:17,556
But note that over here I've
connected the EQ directly


545
00:27:18,046 --> 00:27:19,076
to the output node.


546
00:27:20,126 --> 00:27:24,226
In all of my prior examples
I was connecting nodes


547
00:27:24,226 --> 00:27:26,936
to the mixers, to the
engine's mixer node.


548
00:27:27,246 --> 00:27:28,936
But I don't always
have to do that.


549
00:27:29,546 --> 00:27:32,386
If I just have one chain of data


550
00:27:33,066 --> 00:27:35,536
in my application then I
can just directly connect it


551
00:27:35,536 --> 00:27:37,926
to the output node, which
is what I've done here.


552
00:27:40,356 --> 00:27:45,136
So this is a multiband EQ
and I specify the number


553
00:27:45,356 --> 00:27:48,106
of bands I'm going to use when
I create an instance of the EQ.


554
00:27:48,876 --> 00:27:51,126
So over here, I'm
going to use two bands


555
00:27:51,546 --> 00:27:53,046
so I create an EQ
with two bands.


556
00:27:53,916 --> 00:27:56,896
I can then go ahead and get
access to each of the bands


557
00:27:56,896 --> 00:27:59,136
and set up the different
filter parameters.


558
00:27:59,586 --> 00:28:04,466
Connecting the EQ
is no different


559
00:28:04,566 --> 00:28:06,376
than what we've already seen.


560
00:28:06,836 --> 00:28:09,106
I can connect the
player to the EQ


561
00:28:09,166 --> 00:28:12,296
with the file's processing
format and connect the EQ


562
00:28:12,296 --> 00:28:17,766
to the engine's output node with
the same format and that's it.


563
00:28:18,136 --> 00:28:20,696
So with all of this information,
let's look at a demo


564
00:28:20,786 --> 00:28:23,596
that makes use of some of the
nodes that we've talked about.


565
00:28:24,306 --> 00:28:26,806
Okay, so I'm going to
explain what I have here.


566
00:28:27,666 --> 00:28:30,396
Over here I have two
player nodes and each


567
00:28:30,396 --> 00:28:33,236
of these players is going
to be fed by a separate,


568
00:28:33,336 --> 00:28:34,816
by separate looping buffers.


569
00:28:36,496 --> 00:28:39,576
Each of the players are
connected to separate effects


570
00:28:40,396 --> 00:28:43,826
and each of these effects are
connected to separate inputs


571
00:28:43,886 --> 00:28:45,196
of the engine's main mixer.


572
00:28:45,816 --> 00:28:50,506
I have control over the output
volume of the main mixer


573
00:28:51,526 --> 00:28:54,826
and down here I have
a transport control,


574
00:28:55,316 --> 00:28:57,686
which essentially
controls a node tap


575
00:28:58,056 --> 00:28:59,906
that I've installed
on the main mixer.


576
00:29:00,846 --> 00:29:04,526
So when I hit Record, a tap
gets installed and I capture


577
00:29:04,526 --> 00:29:06,486
that data and save it to a file.


578
00:29:07,116 --> 00:29:08,896
And then when I hit
Play I'm just going


579
00:29:08,896 --> 00:29:09,856
to play that file back.


580
00:29:10,726 --> 00:29:12,286
So let's listen to what
this sounds like [music].


581
00:29:14,596 --> 00:29:18,336
So here I'm playing the drums.


582
00:29:18,806 --> 00:29:23,286
I can change the volume
and the pan of each player,


583
00:29:26,756 --> 00:29:27,816
so you can hear that effect.


584
00:29:28,776 --> 00:29:31,076
I'm now going to go ahead and
play the reverb a little bit.


585
00:29:31,276 --> 00:29:36,556
It sounds a little too wet, so
I'm going to keep it about here.


586
00:29:37,566 --> 00:29:38,726
Let me start the other player.


587
00:29:39,516 --> 00:29:44,546
[ Music ]


588
00:29:45,046 --> 00:29:48,686
Okay, so what I thought I'd
do now is use the node tap


589
00:29:49,126 --> 00:29:53,506
to maybe capture a little live
performance, and any changes


590
00:29:53,556 --> 00:29:56,126
that I make to any of the
nodes here should be captured


591
00:29:56,126 --> 00:29:57,146
in that performance.


592
00:29:57,146 --> 00:29:59,566
So when we go back and listen
to that we should hear that.


593
00:30:00,386 --> 00:30:01,846
So let me do that.


594
00:30:02,516 --> 00:30:21,636
[ Music ]


595
00:30:22,136 --> 00:30:26,296
Okay, so I'm going to stop my
recording, stop my delay player.


596
00:30:28,006 --> 00:30:31,276
And let's go back and
listen to the recording.


597
00:30:32,516 --> 00:30:48,516
[ Music ]


598
00:30:49,016 --> 00:30:52,000
[ Silence ]


599
00:30:52,046 --> 00:30:54,576
And that's a preview of
AVAudioEngine in action.


600
00:30:55,966 --> 00:30:57,116
Let's go back to slides.


601
00:30:57,996 --> 00:31:01,216
Alright, so two of the
settings that I was changing


602
00:31:02,966 --> 00:31:06,036
with the players were the volume
and the pan for each player.


603
00:31:06,616 --> 00:31:11,786
But these are actually
settings of the input mixer bus


604
00:31:12,116 --> 00:31:13,606
that the player is connected to.


605
00:31:14,636 --> 00:31:18,066
So the way we've exposed
mixer input bus settings


606
00:31:18,656 --> 00:31:21,926
in the audio engine is
through a protocol called the


607
00:31:21,926 --> 00:31:23,366
AVAudioMixing protocol.


608
00:31:24,286 --> 00:31:27,936
Source nodes conform to this
protocol so the player node


609
00:31:27,936 --> 00:31:29,816
and the input node do that.


610
00:31:30,196 --> 00:31:33,046
And settings, like
volume, you can change


611
00:31:33,046 --> 00:31:38,076
by just doing player.volume=.5
or player.band=minus 1.


612
00:31:39,776 --> 00:31:43,616
When a source node is in an
active connection with the mixer


613
00:31:43,876 --> 00:31:46,416
and you make changes
to the protocols,


614
00:31:46,416 --> 00:31:49,476
different properties they
take effect immediately.


615
00:31:50,316 --> 00:31:53,266
However, if a source node
is not connected to a mixer


616
00:31:53,666 --> 00:31:55,806
and you make changes to
the protocol's properties,


617
00:31:56,596 --> 00:32:00,546
those changes are cached in the
source node and then applied


618
00:32:00,616 --> 00:32:03,016
when you make a physical
connection to a mixer.


619
00:32:03,666 --> 00:32:06,516
So these are the
mixing properties


620
00:32:06,516 --> 00:32:07,546
that we have available.


621
00:32:08,216 --> 00:32:10,776
Under the common mixing
properties we just have volume


622
00:32:10,776 --> 00:32:11,156
right now.


623
00:32:11,926 --> 00:32:14,256
Under the stereo mixing
properties, we have pan


624
00:32:15,216 --> 00:32:17,556
and we have a number
of 3D mixing properties


625
00:32:17,746 --> 00:32:19,436
that we're going to look
at in the next section.


626
00:32:19,846 --> 00:32:22,626
So in the form of a diagram,


627
00:32:23,106 --> 00:32:26,326
let's say that I have Player
1 connected to Mixer 1


628
00:32:27,076 --> 00:32:30,686
and I go ahead and set player
to start pan to minus 1,


629
00:32:31,016 --> 00:32:34,366
hard pan it to the left and
player 1's volume to .5.


630
00:32:35,546 --> 00:32:39,676
So these mixing settings are
now associated with Player 1.


631
00:32:39,886 --> 00:32:41,576
And because Player
1 is connected


632
00:32:41,576 --> 00:32:44,966
to Mixer 1 they also get
applied on the mixer.


633
00:32:45,326 --> 00:32:49,236
If I were to disconnect Player
1 and connect it to Mixer 2,


634
00:32:49,706 --> 00:32:52,656
these mixing settings
travel along with Player 1


635
00:32:53,056 --> 00:32:54,896
and get applied to Mixer 2.


636
00:32:55,516 --> 00:32:59,576
So in this sense, we've
been able to carry settings


637
00:32:59,576 --> 00:33:02,906
that belong to the input
bus of a mixer along


638
00:33:02,906 --> 00:33:04,346
with the source node itself.


639
00:33:04,926 --> 00:33:10,236
Alright, so let's now
move onto the next section


640
00:33:10,436 --> 00:33:11,836
on gaming and 3D audio.


641
00:33:14,606 --> 00:33:17,226
So in games, typically
you have several types


642
00:33:17,226 --> 00:33:18,156
of sounds that you play.


643
00:33:18,156 --> 00:33:22,956
You have short sounds, and we've
seen AudioServices, which is one


644
00:33:22,956 --> 00:33:25,086
of our C-APIs get used for that.


645
00:33:25,836 --> 00:33:29,476
For playing music we see
AVAudioPlayer getting used


646
00:33:29,476 --> 00:33:30,036
a lot.


647
00:33:30,266 --> 00:33:32,196
And for sounds that
need to be spatialized,


648
00:33:32,516 --> 00:33:34,396
OpenAL is the API of choice.


649
00:33:35,796 --> 00:33:38,836
Now while each of these
APIs work really well


650
00:33:38,836 --> 00:33:42,086
for what they were designed
for, if your application has


651
00:33:42,116 --> 00:33:46,356
to make use of all of them, then
one of the biggest tradeoffs is


652
00:33:46,356 --> 00:33:48,156
that you have to
familiarize yourself


653
00:33:48,156 --> 00:33:51,156
with the nomenclature
associated with each API.


654
00:33:52,266 --> 00:33:55,406
In addition, with AudioServices
you don't have a latency


655
00:33:55,406 --> 00:33:57,156
guarantee of when
your sound will play.


656
00:33:57,866 --> 00:34:00,316
With AVAudioPlayer
you can't play sounds


657
00:34:00,316 --> 00:34:01,496
that you have in buffers.


658
00:34:01,976 --> 00:34:05,486
And with OpenAL, you can't play
sounds directly from a file


659
00:34:05,706 --> 00:34:07,536
or play compressed data.


660
00:34:08,016 --> 00:34:10,556
With our knowledge
of AVAudioEngine,


661
00:34:10,616 --> 00:34:12,716
we have to go back
and cover cases one


662
00:34:12,716 --> 00:34:15,146
and two; we can easily do so.


663
00:34:15,906 --> 00:34:17,775
For short sounds we
can just load them


664
00:34:17,775 --> 00:34:20,946
into AVAudioBuffer objects
and schedule them on a player.


665
00:34:22,005 --> 00:34:25,706
For music you can just create
an AVAudioFile log object


666
00:34:25,795 --> 00:34:27,876
and schedule that
directly on a player.


667
00:34:28,815 --> 00:34:31,386
So how do you play sounds
that need to be spatialized?


668
00:34:31,746 --> 00:34:32,626
We'll look at that now.


669
00:34:33,536 --> 00:34:36,946
I'd like to introduce a new
node called the environment node


670
00:34:37,406 --> 00:34:39,726
and this is essentially
a 3D mixer.


671
00:34:40,946 --> 00:34:44,775
So when you create an instance
of the environment node.


672
00:34:44,775 --> 00:34:48,706
You have a 3D space and you
get a listener that's implicit


673
00:34:48,746 --> 00:34:49,926
to that 3D space.


674
00:34:50,976 --> 00:34:53,025
All of the source
nodes that connect


675
00:34:53,656 --> 00:34:58,026
to the environment node act
as sources in this 3D space.


676
00:34:59,366 --> 00:35:01,416
So the environment
has some attributes


677
00:35:01,506 --> 00:35:03,856
that you can set directly
on the environment node.


678
00:35:04,496 --> 00:35:07,136
And then each of these
sources have some attributes


679
00:35:07,226 --> 00:35:10,536
and you can set that using
the AVAudioMixing protocol's


680
00:35:10,596 --> 00:35:11,636
3D properties.


681
00:35:13,086 --> 00:35:16,596
Now in terms of data formats,
I just wanted to point out that


682
00:35:16,596 --> 00:35:18,186
when you're working with
the environment node,


683
00:35:18,666 --> 00:35:23,116
all of the sources need to have
a mono data format in order


684
00:35:23,116 --> 00:35:24,726
for that audio to
be spatialized.


685
00:35:25,636 --> 00:35:27,846
If the sources have
a stereo data format,


686
00:35:28,236 --> 00:35:30,176
then that data is passed through


687
00:35:30,626 --> 00:35:33,526
and currently the environment
node doesn't support a data


688
00:35:33,526 --> 00:35:36,166
format greater than
two channels on input.


689
00:35:36,326 --> 00:35:40,216
So as a diagram, this
is what it looks like.


690
00:35:40,586 --> 00:35:43,086
I've created an instance
of an environment node


691
00:35:43,236 --> 00:35:45,226
which means I now
have a 3D space


692
00:35:45,996 --> 00:35:47,446
and I have an implicit listener.


693
00:35:48,866 --> 00:35:50,496
I now create two player nodes.


694
00:35:50,496 --> 00:35:54,106
Who are going to act as
sources in my 3D space


695
00:35:54,676 --> 00:35:57,846
and using the AVAudioMixing
protocol I can set all


696
00:35:57,846 --> 00:35:59,196
of the source attributes.


697
00:35:59,766 --> 00:36:04,326
So what makes things
sound 3D or virtual 3D?


698
00:36:04,786 --> 00:36:07,436
Well, we have a number of
attributes and some belong


699
00:36:07,436 --> 00:36:09,966
to the sources, others
belong to the environment.


700
00:36:10,486 --> 00:36:12,736
Let's walk through each of
the source attributes first.


701
00:36:13,826 --> 00:36:17,476
So every source has a
position in this 3D space.


702
00:36:17,896 --> 00:36:20,746
And right now it's specified
using the right-handed cartesian


703
00:36:20,746 --> 00:36:25,126
coordinate system that
right positive Y is up


704
00:36:25,126 --> 00:36:26,846
and positive Z is
towards the listener.


705
00:36:28,076 --> 00:36:30,006
Now with respect
to the listener,


706
00:36:30,706 --> 00:36:33,486
the listener uses
some spatial cues


707
00:36:33,596 --> 00:36:35,886
to localize the position
of the source.


708
00:36:36,646 --> 00:36:38,836
There's an inter-aural
time difference,


709
00:36:39,086 --> 00:36:43,396
just a slight time difference
for the sound made by the source


710
00:36:43,486 --> 00:36:45,466
to get to each one of the
listeners in those years.


711
00:36:45,826 --> 00:36:48,326
There's also an inter-aural
level difference.


712
00:36:49,066 --> 00:36:52,576
In addition, your head has the
effect of doing some filtering


713
00:36:53,096 --> 00:36:55,766
and you also have some
filtering here with the ears,


714
00:36:55,816 --> 00:36:56,786
depending on the ears.


715
00:36:58,156 --> 00:37:01,926
So we have several rendering
algorithms and each one


716
00:37:01,926 --> 00:37:04,216
of them model these
spatial cues differently.


717
00:37:05,396 --> 00:37:08,456
The thing is that we've exposed
this as a source property.


718
00:37:08,956 --> 00:37:11,806
So you can pick a rendering
algorithm per source


719
00:37:12,286 --> 00:37:15,636
and some algorithms may sound
better depending on the type


720
00:37:15,636 --> 00:37:17,736
of content your source
is playing


721
00:37:18,476 --> 00:37:21,686
and also they differ
in terms of CPU cost.


722
00:37:22,146 --> 00:37:25,816
So you may want to pick a
more expensive algorithm


723
00:37:26,036 --> 00:37:27,466
for an important source


724
00:37:27,866 --> 00:37:30,336
and a cheaper algorithm
for a regular source.


725
00:37:30,856 --> 00:37:36,686
The next two properties,
obstruction and occlusion,


726
00:37:36,936 --> 00:37:40,356
deal with the filtering of
sound if there are obstacles


727
00:37:40,356 --> 00:37:42,066
between the source and listener.


728
00:37:43,406 --> 00:37:47,216
So in this case, I have the
source, that's the monster,


729
00:37:48,086 --> 00:37:50,126
and the listener, that's
the handsome prince,


730
00:37:50,586 --> 00:37:53,236
and there is a column between
the source and the listener.


731
00:37:53,326 --> 00:37:58,116
So the direct path of sound is
muffled whereas the reflected


732
00:37:58,176 --> 00:38:00,746
paths across the walls are clear


733
00:38:01,006 --> 00:38:02,626
and this is modeled
by obstruction.


734
00:38:03,516 --> 00:38:08,096
On the other hand, if the
source and the listener are


735
00:38:08,096 --> 00:38:11,256
on different spaces, so
right now that's a wall


736
00:38:11,256 --> 00:38:12,556
between the source
and the listener.


737
00:38:13,276 --> 00:38:14,866
Both the direct part of sound


738
00:38:15,296 --> 00:38:17,576
and the reflective parts
of sound are muffled.


739
00:38:18,126 --> 00:38:22,226
Let's now move on
to the listener,


740
00:38:22,516 --> 00:38:23,866
the environment attributes.


741
00:38:25,276 --> 00:38:27,706
So every environment
has an implicit listener


742
00:38:27,706 --> 00:38:30,936
and the listener has a
position and an orientation.


743
00:38:31,526 --> 00:38:34,266
The position is specified using
the same coordinate system.


744
00:38:34,816 --> 00:38:38,226
And for the orientation, you
can specify using either two


745
00:38:38,226 --> 00:38:40,366
vectors, a front
and an up vector,


746
00:38:41,406 --> 00:38:45,876
or three angles yaw,
pitch and draw.


747
00:38:46,646 --> 00:38:49,446
You also have distance
attenuation in the environment,


748
00:38:49,916 --> 00:38:51,736
which is just the
attenuation of sound


749
00:38:52,276 --> 00:38:54,656
as a source moves away
from the listener.


750
00:38:55,446 --> 00:38:58,326
So in this graph there are
two points of interest.


751
00:38:58,796 --> 00:39:01,626
There's the reference
distance, which is the distance


752
00:39:01,626 --> 00:39:05,236
above which we start applying
some amount of attenuation.


753
00:39:05,886 --> 00:39:09,216
There's also the maximum
distance, which is the point


754
00:39:09,216 --> 00:39:10,836
above which the amount


755
00:39:10,836 --> 00:39:12,996
of attenuation being
applied is capped.


756
00:39:13,936 --> 00:39:15,706
So all of the exciting
stuff happens


757
00:39:15,706 --> 00:39:18,366
between the reference distance
and the maximum distance.


758
00:39:19,156 --> 00:39:22,126
And in that region we have three
curves that you can pick from.


759
00:39:22,746 --> 00:39:25,796
So, in the form of code,
this is what it looks like.


760
00:39:26,126 --> 00:39:28,786
All you need to do is get the
distance attenuation parameters


761
00:39:28,786 --> 00:39:32,426
object from the environment
and then you can go ahead


762
00:39:32,476 --> 00:39:33,826
and tweak all the settings.


763
00:39:34,386 --> 00:39:39,766
Now every environment
also has reverberation


764
00:39:39,986 --> 00:39:41,596
which is just a simulation


765
00:39:41,596 --> 00:39:43,856
of the sound reflections
within that space.


766
00:39:44,846 --> 00:39:49,666
The environment node has a
built-in reverb and you can pick


767
00:39:50,286 --> 00:39:52,246
from a selection
of factory presets.


768
00:39:53,166 --> 00:39:56,536
Now once you pick the type
of reverb you want to use,


769
00:39:57,296 --> 00:40:00,306
you can set a blend
amount for each source


770
00:40:01,186 --> 00:40:04,266
and that just affects
the amount of each source


771
00:40:04,266 --> 00:40:05,966
that you'll here
in the reverb mix.


772
00:40:06,496 --> 00:40:08,256
So for some sources,
you may want them


773
00:40:08,256 --> 00:40:11,226
to sound completely dry, so you
set the blend amount to zero.


774
00:40:11,636 --> 00:40:14,286
And other sources you may
want to sound more ambient


775
00:40:14,426 --> 00:40:16,026
so you can turn up
the blend amount.


776
00:40:17,396 --> 00:40:20,076
We also have a single
filter that applies


777
00:40:20,136 --> 00:40:21,346
to the output of the reverb.


778
00:40:21,786 --> 00:40:24,566
So let's say that you pick
one of the factory presets


779
00:40:24,906 --> 00:40:27,116
and you want it to sound
maybe a little brighter.


780
00:40:27,726 --> 00:40:29,466
You can do that using
the filter.


781
00:40:30,726 --> 00:40:32,316
In code, this is
what it looks like.


782
00:40:32,646 --> 00:40:35,376
I get the ReverbParameters
object from the environment.


783
00:40:35,986 --> 00:40:37,576
In this case, I'm enabling it


784
00:40:37,746 --> 00:40:41,796
and then I load a factory
preset, LargeHall preset.


785
00:40:42,276 --> 00:40:44,826
And using the AVAudioMixing
protocol,


786
00:40:44,826 --> 00:40:47,896
I set the source's
reverbBlend to 0.2.


787
00:40:48,816 --> 00:40:51,276
So now we've talked about
two types of mixers.


788
00:40:51,516 --> 00:40:54,816
You have the 2D mixer and
you have the 3D mixer.


789
00:40:55,636 --> 00:40:58,506
And source nodes, that is
the player or the input node,


790
00:40:59,096 --> 00:41:02,836
talk to these mixers using
the AVAudioMixing protocol.


791
00:41:03,896 --> 00:41:05,826
So I just wanted
to point out that


792
00:41:05,826 --> 00:41:08,566
when a source node is
connected to a 2D mixer,


793
00:41:08,996 --> 00:41:12,106
then all of the common and
the 2D mixing properties


794
00:41:12,166 --> 00:41:12,766
take effect.


795
00:41:13,716 --> 00:41:16,656
When a source node is
connected to a 3D mixer,


796
00:41:17,156 --> 00:41:20,696
then all of the common and
the 3D mixing properties


797
00:41:20,696 --> 00:41:21,256
take effect.


798
00:41:22,336 --> 00:41:24,646
Let's look at what
that looks like here.


799
00:41:26,126 --> 00:41:27,866
So let's say that
I have Player 1


800
00:41:28,316 --> 00:41:30,906
who is connected
to the 2D mixer.


801
00:41:31,776 --> 00:41:36,246
I set the pan to be -1
and volume to be .5.


802
00:41:36,996 --> 00:41:40,206
Note that pan is a
2D mixing property


803
00:41:40,466 --> 00:41:42,496
but volume is a common
mixing property.


804
00:41:43,136 --> 00:41:45,646
But in this case both of
them will take effect,


805
00:41:46,036 --> 00:41:47,846
because the mixer
node implements both


806
00:41:47,846 --> 00:41:48,716
of these properties.


807
00:41:49,556 --> 00:41:53,026
If I disconnect Player 1
from the mixer and connect it


808
00:41:53,026 --> 00:41:57,836
to the environment node, the
pan property will now be cached.


809
00:41:57,946 --> 00:42:00,956
It doesn't take effect because
it's a 2D mixing property.


810
00:42:01,136 --> 00:42:03,046
It doesn't apply to
the environment node.


811
00:42:04,036 --> 00:42:07,106
Volume, on the other hand,
will continue to take effect,


812
00:42:07,486 --> 00:42:10,276
because it's a common mixing
property and it's implemented


813
00:42:10,276 --> 00:42:11,216
by the environment node.


814
00:42:11,706 --> 00:42:13,806
So with all of that
information let's look


815
00:42:13,886 --> 00:42:15,986
at a sample gaming setup.


816
00:42:16,786 --> 00:42:20,016
This is just one of many
ways that you can do this


817
00:42:20,016 --> 00:42:21,336
and this is just a suggestion.


818
00:42:21,676 --> 00:42:23,476
It really all depends
on your application.


819
00:42:24,436 --> 00:42:28,316
But in this case, I
have two 3D sources.


820
00:42:28,646 --> 00:42:31,366
So, I'm going to use a
player to play some sounds


821
00:42:31,366 --> 00:42:34,336
that will be spatialized
and also live input.


822
00:42:34,946 --> 00:42:37,466
So let's say that the user
is chatting and then you want


823
00:42:37,466 --> 00:42:39,576
to spatialize that
in a 3D environment.


824
00:42:40,116 --> 00:42:42,186
I can connect the player
node and the input node


825
00:42:42,496 --> 00:42:43,456
to the environment node.


826
00:42:43,876 --> 00:42:46,136
And that's connected out
to the engine's main mixer.


827
00:42:47,066 --> 00:42:50,526
I can now have a second
player that I'm going


828
00:42:50,526 --> 00:42:52,456
to dedicate to playing music.


829
00:42:53,216 --> 00:42:56,776
So this player is going to play
music and I'm going to run it


830
00:42:56,816 --> 00:42:59,086
through an EQ and connect
that to the main mixer.


831
00:42:59,746 --> 00:43:02,346
Let's say that I present
some UI for the users


832
00:43:02,346 --> 00:43:04,166
so that he can tweak
the EQ settings,


833
00:43:04,826 --> 00:43:06,456
maybe to make the
music sound better.


834
00:43:06,456 --> 00:43:10,926
I have a third player
now that I'm going


835
00:43:10,926 --> 00:43:13,626
to dedicate only to
UI sound effects.


836
00:43:14,016 --> 00:43:17,336
So maybe the sounds that are
made as I navigate through menus


837
00:43:17,846 --> 00:43:20,726
or if my game avatar has
picked up a bonus item,


838
00:43:20,926 --> 00:43:25,406
etc. So the UI player
is connected directly


839
00:43:25,526 --> 00:43:26,666
to the engine's main mixer.


840
00:43:27,586 --> 00:43:30,746
This is what the overall
picture looks like.


841
00:43:33,256 --> 00:43:36,576
So given all of this information
let's now look at a demo


842
00:43:36,576 --> 00:43:38,016
of the environment node.


843
00:43:39,516 --> 00:43:50,586
[ Balls popping ]


844
00:43:51,086 --> 00:43:53,366
So I want to explain
what's happening over here.


845
00:43:53,366 --> 00:43:56,916
In this demo, I am using
SceneKit for the graphics


846
00:43:56,986 --> 00:43:59,146
and SceneKit also comes
with a physics engine.


847
00:43:59,276 --> 00:44:01,306
So this works nicely
with AVAudioEngine.


848
00:44:01,686 --> 00:44:05,226
So I basically have two types
of sounds that I'm playing;


849
00:44:05,656 --> 00:44:07,956
that's the "fffuh"
sound that plays


850
00:44:08,136 --> 00:44:10,916
and that's before
any ball is launched.


851
00:44:11,596 --> 00:44:13,906
So to do that I use
a player node


852
00:44:14,646 --> 00:44:19,196
and I have the long sound effect
in a buffer and I schedule


853
00:44:19,196 --> 00:44:20,576
that buffer on the player node.


854
00:44:21,076 --> 00:44:24,146
But I make use of the
completion handler to know


855
00:44:24,146 --> 00:44:26,066
when the player has
consumed the buffer.


856
00:44:26,786 --> 00:44:29,506
So, when the player lets me know
that it's done with the buffer,


857
00:44:29,506 --> 00:44:32,946
I go ahead and now
create a SceneKit node.


858
00:44:33,246 --> 00:44:37,496
That's a ball and I also
create an AVAudioPlayer node,


859
00:44:38,266 --> 00:44:41,756
attach it to the
engine and connect


860
00:44:41,876 --> 00:44:43,416
that to the environment node.


861
00:44:43,936 --> 00:44:48,176
So I'm tying a player, a
dedicated player to each ball.


862
00:44:49,456 --> 00:44:53,046
Now the ball is launched into
the world and as it goes about


863
00:44:53,046 --> 00:44:56,166
and collides with other
surfaces, for every collision


864
00:44:56,166 --> 00:44:59,946
that happens SceneKit's
physics engine lets me know


865
00:44:59,946 --> 00:45:02,326
that a collision has happened
with some other surface.


866
00:45:02,666 --> 00:45:07,406
And I get the point of
collision and also the impulse.


867
00:45:08,656 --> 00:45:11,836
So using that, I can go and dig


868
00:45:11,836 --> 00:45:16,036
up the player node that's
tied to the SceneKit node.


869
00:45:16,036 --> 00:45:19,816
I can set the position
on the player based


870
00:45:19,816 --> 00:45:23,486
on where the collision
happened, calculate a volume


871
00:45:23,706 --> 00:45:26,126
for the collision sound
based on the impulse


872
00:45:26,996 --> 00:45:28,246
and then just play the sound.


873
00:45:29,076 --> 00:45:33,166
But you can see now
how, in this setup,


874
00:45:33,346 --> 00:45:36,926
for every ball that's
born into this world,


875
00:45:37,016 --> 00:45:39,106
a new player node
is also created.


876
00:45:39,516 --> 00:45:41,296
So the number of
players is growing


877
00:45:41,296 --> 00:45:44,866
and I'm dynamically attaching it
to the engine and connecting it


878
00:45:44,866 --> 00:45:45,836
to the environment node.


879
00:45:46,366 --> 00:45:47,976
So this setup is very flexible.


880
00:45:48,516 --> 00:45:52,546
[ Balls popping ]


881
00:45:53,046 --> 00:45:54,296
Alright, so let's
get back to slides.


882
00:45:55,126 --> 00:45:56,666
That brings us to
the end of our talk.


883
00:45:56,936 --> 00:45:59,376
So let's quickly summarize all
the things we've seen today.


884
00:46:00,586 --> 00:46:02,516
We started off with
talking about an engine


885
00:46:03,076 --> 00:46:06,016
and how you can create different
nodes, attach them to the engine


886
00:46:06,116 --> 00:46:08,016
and then use the engine
to make connections


887
00:46:08,556 --> 00:46:09,806
between each of these nodes.


888
00:46:10,126 --> 00:46:13,396
We then looked at the
different types of nodes:


889
00:46:13,656 --> 00:46:16,536
the destination node,
which is the output node.


890
00:46:17,536 --> 00:46:19,656
And we talked about
two source nodes,


891
00:46:20,126 --> 00:46:21,976
the player node and
the input node.


892
00:46:22,936 --> 00:46:25,576
The player is the node you use


893
00:46:25,576 --> 00:46:27,796
to push audio data
on the render thread.


894
00:46:28,546 --> 00:46:32,316
We looked at two types of
mixer nodes, the 2D Mixer


895
00:46:32,676 --> 00:46:37,596
and the 3D Mixer and
how source nodes talk


896
00:46:37,596 --> 00:46:40,806
to these mixers using the
AVAudioMixing protocol.


897
00:46:41,886 --> 00:46:45,506
We then looked at effect nodes
and two types of effect nodes:


898
00:46:45,586 --> 00:46:48,486
the AVAudioEffects and
AVAudioUnitTime effects.


899
00:46:49,416 --> 00:46:53,596
Finally we talked
about node taps


900
00:46:53,596 --> 00:46:55,906
and that's how you pull
data from the render thread.


901
00:46:56,486 --> 00:46:58,206
So I just wanted to point


902
00:46:58,206 --> 00:47:01,026
out that node taps are also
a useful debugging tool.


903
00:47:01,666 --> 00:47:03,776
Let's say that you have
a number of connections


904
00:47:03,896 --> 00:47:06,966
in your application and things
don't sound the way you expect


905
00:47:06,966 --> 00:47:07,566
them to sound.


906
00:47:08,326 --> 00:47:11,876
What you can do is install
node taps at different points


907
00:47:11,876 --> 00:47:15,016
in your chain on different nodes
and just examine the output


908
00:47:15,016 --> 00:47:16,106
of each of these nodes.


909
00:47:16,596 --> 00:47:19,336
And using that you can drill
down and where the problem is.


910
00:47:19,476 --> 00:47:22,246
So in that sense node taps
are a useful debugging tool.


911
00:47:22,716 --> 00:47:25,856
So that brings us to
the end of our session.


912
00:47:26,506 --> 00:47:29,126
I just want to say that
this is the first version


913
00:47:29,126 --> 00:47:31,836
of AVAudioEngine and we
are very excited about it.


914
00:47:32,536 --> 00:47:34,016
So, we'd love to
hear what you think.


915
00:47:34,506 --> 00:47:36,626
Please try it out and
give us your feedback.


916
00:47:37,736 --> 00:47:40,256
If you have any further
questions at a later point,


917
00:47:40,476 --> 00:47:42,916
you can contact Filip,
who's our Graphics


918
00:47:42,916 --> 00:47:45,666
and Game Technologies
Evangelist.

