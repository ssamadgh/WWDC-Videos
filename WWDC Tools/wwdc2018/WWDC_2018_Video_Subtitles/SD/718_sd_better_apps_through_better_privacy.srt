1
00:00:07,016 --> 00:00:15,500
[ Music ]


2
00:00:20,516 --> 00:00:29,216
[ Applause ]


3
00:00:29,716 --> 00:00:32,516
>> Privacy is about people.


4
00:00:34,166 --> 00:00:35,686
My name is Joey Tyson.


5
00:00:35,726 --> 00:00:37,286
I'm a privacy engineer at Apple,


6
00:00:37,386 --> 00:00:39,336
and I know you've heard a lot of


7
00:00:39,336 --> 00:00:40,876
great information this week on


8
00:00:40,876 --> 00:00:43,066
exciting new features and you're


9
00:00:43,066 --> 00:00:44,506
ready to get out there and build


10
00:00:44,506 --> 00:00:47,066
some new apps, but I know you


11
00:00:47,066 --> 00:00:49,206
also care deeply about your


12
00:00:49,206 --> 00:00:50,246
user's privacy.


13
00:00:51,416 --> 00:00:53,806
And this is the first of three


14
00:00:53,806 --> 00:00:56,366
big ideas I want to explore with


15
00:00:56,426 --> 00:00:58,676
you about privacy today before


16
00:00:58,676 --> 00:01:00,216
we get into this year's updates.


17
00:01:01,446 --> 00:01:02,236
Because we're going to talk a


18
00:01:02,236 --> 00:01:05,536
lot about data privacy, but we


19
00:01:05,536 --> 00:01:07,716
can never forget that data is


20
00:01:07,716 --> 00:01:08,996
about people.


21
00:01:09,686 --> 00:01:11,196
It belongs to them.


22
00:01:12,456 --> 00:01:14,616
So when I say that privacy is


23
00:01:14,616 --> 00:01:16,896
about people, I mean it's about


24
00:01:16,896 --> 00:01:19,166
building a relationship of trust


25
00:01:19,556 --> 00:01:20,586
with your users.


26
00:01:22,176 --> 00:01:24,096
This lays a foundation for


27
00:01:24,096 --> 00:01:26,206
better engagement, which leads


28
00:01:26,206 --> 00:01:27,046
to better apps.


29
00:01:28,506 --> 00:01:29,826
Think about other relationships.


30
00:01:29,826 --> 00:01:31,596
It's the people that you trust


31
00:01:32,086 --> 00:01:33,676
that you're more likely to work


32
00:01:33,676 --> 00:01:35,056
with and spend time with.


33
00:01:36,496 --> 00:01:40,076
And as your users understand why


34
00:01:40,076 --> 00:01:41,636
you're collecting data, how it's


35
00:01:41,696 --> 00:01:44,256
being used, as you handle that


36
00:01:44,256 --> 00:01:45,716
data respectfully and


37
00:01:45,806 --> 00:01:47,946
thoughtfully, you're going to


38
00:01:47,946 --> 00:01:50,416
get better data, because they're


39
00:01:50,416 --> 00:01:51,506
going to be more comfortable


40
00:01:51,506 --> 00:01:54,076
using your apps and sharing


41
00:01:54,076 --> 00:01:56,006
information, and this builds


42
00:01:56,006 --> 00:01:57,316
loyalty over time.


43
00:01:59,396 --> 00:02:02,446
And I start here because I want


44
00:02:02,446 --> 00:02:04,676
you to apply this context to


45
00:02:04,676 --> 00:02:06,596
your development process.


46
00:02:08,026 --> 00:02:09,816
None of us do engineering in


47
00:02:09,886 --> 00:02:12,746
isolation, so whether you're


48
00:02:12,746 --> 00:02:15,056
working with health records or


49
00:02:15,056 --> 00:02:16,266
just building a simple puzzle


50
00:02:16,266 --> 00:02:19,136
game, the information you gather


51
00:02:19,336 --> 00:02:21,096
and the ways that you use it


52
00:02:21,736 --> 00:02:23,896
could have a very real impact on


53
00:02:23,896 --> 00:02:24,956
people's lives.


54
00:02:26,256 --> 00:02:28,186
So it's critical for each of us


55
00:02:28,486 --> 00:02:29,846
to think carefully about the


56
00:02:29,846 --> 00:02:30,856
technologies that we're


57
00:02:30,856 --> 00:02:31,486
building.


58
00:02:31,896 --> 00:02:34,956
In a recent commencement address


59
00:02:35,156 --> 00:02:37,556
at Duke University, Tim Cook


60
00:02:37,556 --> 00:02:39,136
talked about Apple's approach to


61
00:02:39,136 --> 00:02:41,846
privacy, and he said that "In


62
00:02:41,846 --> 00:02:44,196
every way, at every turn, the


63
00:02:44,196 --> 00:02:46,646
question we ask ourselves is not


64
00:02:46,646 --> 00:02:49,906
what can we do, but what should


65
00:02:49,906 --> 00:02:50,396
we do."


66
00:02:50,896 --> 00:02:54,566
And that leads me to the second


67
00:02:54,896 --> 00:02:58,606
big idea, ask the "should"


68
00:02:58,826 --> 00:02:59,696
questions.


69
00:03:00,336 --> 00:03:03,306
No matter what your role,


70
00:03:03,606 --> 00:03:05,006
whether you're a solo app


71
00:03:05,066 --> 00:03:06,656
developer or part of a large


72
00:03:06,656 --> 00:03:09,606
organization, you can be the one


73
00:03:10,136 --> 00:03:12,016
to stand up for your users.


74
00:03:12,476 --> 00:03:14,016
Remember your responsibility


75
00:03:14,016 --> 00:03:16,536
towards them, and ask questions


76
00:03:16,536 --> 00:03:18,286
about the data flows in your


77
00:03:19,076 --> 00:03:19,346
app.


78
00:03:20,186 --> 00:03:23,646
For example, why do we actually


79
00:03:23,646 --> 00:03:24,296
need this data?


80
00:03:25,776 --> 00:03:27,256
This isn't merely an accusation.


81
00:03:27,256 --> 00:03:28,606
It's to think about is this


82
00:03:28,606 --> 00:03:31,236
necessary for our use case?


83
00:03:31,766 --> 00:03:32,836
Should we collect it?


84
00:03:34,176 --> 00:03:36,196
Would this surprise our users?


85
00:03:37,256 --> 00:03:38,876
If people understood this and it


86
00:03:38,876 --> 00:03:41,076
scared them, why should we be


87
00:03:41,076 --> 00:03:41,916
doing it at all?


88
00:03:43,116 --> 00:03:45,226
Could we use less granular data,


89
00:03:45,226 --> 00:03:46,476
less precise?


90
00:03:47,416 --> 00:03:48,656
Are there other approaches we


91
00:03:48,656 --> 00:03:49,466
should consider?


92
00:03:50,616 --> 00:03:52,316
And should we delete or


93
00:03:52,316 --> 00:03:53,876
aggregate this data after a


94
00:03:53,876 --> 00:03:54,766
period of time?


95
00:03:55,166 --> 00:03:58,046
Part of the reason for asking


96
00:03:58,046 --> 00:03:59,646
these kinds of questions is that


97
00:03:59,646 --> 00:04:01,436
we can all fall prey to


98
00:04:01,436 --> 00:04:03,326
assumptions about our data.


99
00:04:03,906 --> 00:04:06,946
So just thinking, we should just


100
00:04:06,946 --> 00:04:07,976
log this for everyone.


101
00:04:09,036 --> 00:04:10,156
Maybe that's the way we've


102
00:04:10,156 --> 00:04:12,526
always done it in the past or


103
00:04:12,526 --> 00:04:14,306
the way others do it, but again,


104
00:04:14,306 --> 00:04:15,726
is it really necessary for what


105
00:04:15,726 --> 00:04:16,966
we're trying to accomplish?


106
00:04:19,516 --> 00:04:21,065
Or you might think that data


107
00:04:21,065 --> 00:04:22,696
couldn't possibly be sensitive


108
00:04:22,696 --> 00:04:24,046
in the context you're working,


109
00:04:24,046 --> 00:04:26,656
but maybe that data is very


110
00:04:26,656 --> 00:04:28,506
sensitive in a different context


111
00:04:28,806 --> 00:04:30,316
or for users in a vulnerable


112
00:04:30,316 --> 00:04:30,976
population.


113
00:04:33,026 --> 00:04:34,446
If you're taking data that was


114
00:04:34,446 --> 00:04:35,956
gathered for one purpose and


115
00:04:35,956 --> 00:04:38,196
apply it in a new way, would


116
00:04:38,196 --> 00:04:41,126
users understand or expect that?


117
00:04:42,396 --> 00:04:44,096
You also hear people talk about


118
00:04:44,096 --> 00:04:45,376
personally identifiable


119
00:04:45,376 --> 00:04:48,436
information, or PII, but even


120
00:04:48,436 --> 00:04:49,996
data that falls outside that


121
00:04:49,996 --> 00:04:51,306
definition can still have a


122
00:04:51,306 --> 00:04:52,416
privacy impact.


123
00:04:53,896 --> 00:04:55,926
Just like if you're protecting


124
00:04:55,926 --> 00:04:57,226
data with encryption and good


125
00:04:57,226 --> 00:04:59,876
security, that's wonderful, but


126
00:04:59,876 --> 00:05:01,796
privacy is so much more than


127
00:05:01,796 --> 00:05:02,106
that.


128
00:05:03,096 --> 00:05:04,276
Because this again doesn't get


129
00:05:04,276 --> 00:05:06,306
back to the should questions of


130
00:05:06,306 --> 00:05:08,116
should we even have this data at


131
00:05:08,116 --> 00:05:08,596
all.


132
00:05:09,876 --> 00:05:11,926
Now as you're asking questions


133
00:05:11,926 --> 00:05:13,146
about the data flows in your


134
00:05:13,146 --> 00:05:14,576
app, if you want to even go one


135
00:05:14,576 --> 00:05:16,726
step further, you can create


136
00:05:16,726 --> 00:05:18,376
privacy guarantees.


137
00:05:19,436 --> 00:05:20,936
These are high-level statements


138
00:05:20,936 --> 00:05:22,646
about the privacy expectations


139
00:05:22,646 --> 00:05:24,226
in your app that you want to be


140
00:05:24,226 --> 00:05:24,766
able to make.


141
00:05:25,946 --> 00:05:27,986
And by establishing these early


142
00:05:27,986 --> 00:05:29,886
on in the development process,


143
00:05:30,676 --> 00:05:32,396
it provides a framework to guide


144
00:05:32,396 --> 00:05:33,316
you as you're building your


145
00:05:33,316 --> 00:05:35,216
features and something to test


146
00:05:35,216 --> 00:05:36,926
against once you're done.


147
00:05:38,416 --> 00:05:39,966
There's some examples on the


148
00:05:39,966 --> 00:05:41,256
screen here of these kinds of


149
00:05:41,256 --> 00:05:42,326
statements that are similar to


150
00:05:42,326 --> 00:05:44,416
statements Apple has made about


151
00:05:44,416 --> 00:05:46,436
some of our features, and


152
00:05:46,846 --> 00:05:48,866
there's many options for


153
00:05:48,946 --> 00:05:50,436
implementing each of these,


154
00:05:51,586 --> 00:05:54,516
which brings me to the third big


155
00:05:54,516 --> 00:05:54,946
idea.


156
00:05:56,166 --> 00:05:58,956
Align your data practices with


157
00:05:59,036 --> 00:06:00,216
your use cases.


158
00:06:00,986 --> 00:06:04,166
To illustrate this, let's think


159
00:06:04,166 --> 00:06:05,886
of data collection, the amount


160
00:06:05,886 --> 00:06:06,986
and types of data that you


161
00:06:06,986 --> 00:06:07,536
gather.


162
00:06:07,996 --> 00:06:09,846
We mentioned earlier those


163
00:06:09,846 --> 00:06:10,576
assumptions.


164
00:06:10,576 --> 00:06:11,856
You know, you may think


165
00:06:11,856 --> 00:06:13,366
sometimes well shouldn't I just


166
00:06:13,466 --> 00:06:15,376
gather as much data as possible?


167
00:06:15,376 --> 00:06:17,706
Well, you know, in the past you


168
00:06:17,706 --> 00:06:18,856
may have hear people call data


169
00:06:18,856 --> 00:06:20,526
the fuel of the new economy.


170
00:06:21,066 --> 00:06:24,416
Like actual fuel, data should be


171
00:06:24,416 --> 00:06:25,726
handled with caution.


172
00:06:27,316 --> 00:06:29,606
Because data is very powerful,


173
00:06:29,606 --> 00:06:30,796
and that unlocks a lot of great


174
00:06:30,796 --> 00:06:32,866
use cases, but because it's so


175
00:06:32,866 --> 00:06:34,576
powerful, it can also be


176
00:06:34,576 --> 00:06:36,596
dangerous if not handled


177
00:06:36,596 --> 00:06:37,166
carefully.


178
00:06:38,756 --> 00:06:40,926
Gathering data creates overhead


179
00:06:40,926 --> 00:06:41,636
for you as an engineer.


180
00:06:41,636 --> 00:06:43,806
You're going to need to spend


181
00:06:43,966 --> 00:06:46,596
time and resources managing that


182
00:06:46,596 --> 00:06:47,696
data, keeping up with it,


183
00:06:47,696 --> 00:06:48,376
filtering it.


184
00:06:48,476 --> 00:06:49,746
It's time you could be spending


185
00:06:50,136 --> 00:06:51,506
working on new features for your


186
00:06:51,506 --> 00:06:52,126
users.


187
00:06:53,156 --> 00:06:55,316
It also creates liabilities.


188
00:06:56,096 --> 00:06:57,376
We've all heard about companies


189
00:06:57,376 --> 00:06:58,766
suffering data breaches, which


190
00:06:58,766 --> 00:06:59,966
is a bad situation.


191
00:07:00,836 --> 00:07:02,166
But if the data that gets leaked


192
00:07:02,296 --> 00:07:03,876
includes information that's not


193
00:07:03,926 --> 00:07:06,626
relevant to the use case, that's


194
00:07:06,626 --> 00:07:08,456
an even worse situation.


195
00:07:09,396 --> 00:07:11,286
Unexpected data collection


196
00:07:11,626 --> 00:07:13,536
creates all sorts of risks.


197
00:07:13,716 --> 00:07:15,846
And it destroys that foundation


198
00:07:15,846 --> 00:07:17,666
of trust with your users.


199
00:07:18,606 --> 00:07:20,086
The next time you think about


200
00:07:20,286 --> 00:07:21,386
gathering as much data as


201
00:07:21,386 --> 00:07:23,926
possible, I want you to picture


202
00:07:23,926 --> 00:07:26,236
these tanks of chemicals and


203
00:07:26,236 --> 00:07:27,816
remember your responsibility to


204
00:07:27,816 --> 00:07:30,036
your users to handle their data


205
00:07:30,586 --> 00:07:32,306
carefully and thoughtfully.


206
00:07:32,306 --> 00:07:34,726
Instead, you want to practice


207
00:07:35,346 --> 00:07:38,206
what we call proportional data


208
00:07:38,206 --> 00:07:38,686
collection.


209
00:07:39,236 --> 00:07:41,396
This is the idea of collecting


210
00:07:41,396 --> 00:07:43,246
only what's necessary to achieve


211
00:07:43,246 --> 00:07:45,196
your goal, and again sometimes


212
00:07:45,196 --> 00:07:46,326
you might start off thinking


213
00:07:46,326 --> 00:07:47,186
that you need a lot of


214
00:07:47,186 --> 00:07:49,006
information when a different


215
00:07:49,006 --> 00:07:50,106
dataset may suffice.


216
00:07:50,106 --> 00:07:52,366
You can even start with the


217
00:07:52,366 --> 00:07:53,946
assumption of no data and figure


218
00:07:53,946 --> 00:07:55,726
out what's actually necessary


219
00:07:55,726 --> 00:07:56,936
for what you're trying to solve.


220
00:07:58,666 --> 00:08:00,006
This gets back to user


221
00:08:00,006 --> 00:08:01,116
expectations.


222
00:08:02,126 --> 00:08:04,286
People should understand why


223
00:08:04,286 --> 00:08:05,676
you're collecting data and how


224
00:08:05,676 --> 00:08:06,446
you're using it.


225
00:08:06,896 --> 00:08:07,986
It should be in line with what


226
00:08:07,986 --> 00:08:08,566
they expect.


227
00:08:09,716 --> 00:08:10,656
You should always be able to


228
00:08:10,656 --> 00:08:13,206
provide a clear rationale for


229
00:08:13,266 --> 00:08:14,296
the use cases that you're


230
00:08:14,296 --> 00:08:14,816
building.


231
00:08:15,226 --> 00:08:18,356
But of course, this is about


232
00:08:18,356 --> 00:08:19,486
data collection, but when we


233
00:08:19,486 --> 00:08:20,746
talk about aligning data


234
00:08:20,746 --> 00:08:22,866
practices with use cases, that


235
00:08:22,866 --> 00:08:24,596
extends to the entire data life


236
00:08:24,596 --> 00:08:27,036
cycle and being good stewards of


237
00:08:27,036 --> 00:08:28,376
the information that's been


238
00:08:28,376 --> 00:08:29,296
entrusted to you.


239
00:08:30,286 --> 00:08:32,426
So even beyond just proportional


240
00:08:32,426 --> 00:08:33,796
data collection, you want to


241
00:08:33,796 --> 00:08:35,265
develop and use privacy


242
00:08:35,265 --> 00:08:38,226
techniques throughout your app's


243
00:08:38,226 --> 00:08:38,806
workflow.


244
00:08:39,285 --> 00:08:41,626
You could develop a whole


245
00:08:41,626 --> 00:08:43,686
toolbox or repertoire of


246
00:08:43,686 --> 00:08:45,466
techniques that will help you


247
00:08:45,466 --> 00:08:47,026
build privacy into your app.


248
00:08:47,796 --> 00:08:49,546
Things like aggregation,


249
00:08:50,386 --> 00:08:52,426
providing transparency to users,


250
00:08:53,126 --> 00:08:54,806
using a scoped identifier


251
00:08:54,806 --> 00:08:56,256
instead of a real identity,


252
00:08:56,756 --> 00:08:58,336
automatically rotating those


253
00:08:58,336 --> 00:08:58,926
with time.


254
00:08:59,966 --> 00:09:01,366
Even more advanced techniques


255
00:09:01,366 --> 00:09:02,646
like differential privacy.


256
00:09:03,396 --> 00:09:05,166
I don't have time today to go


257
00:09:05,166 --> 00:09:06,396
into the entire list of


258
00:09:06,396 --> 00:09:08,476
techniques available, but what I


259
00:09:08,476 --> 00:09:10,116
want to focus on now is the idea


260
00:09:10,116 --> 00:09:12,396
of adjusting these to match your


261
00:09:12,396 --> 00:09:13,216
use case.


262
00:09:14,636 --> 00:09:15,986
You can think of a mixing board


263
00:09:15,986 --> 00:09:16,676
for music.


264
00:09:17,436 --> 00:09:18,556
If you have one track that's


265
00:09:18,556 --> 00:09:20,696
particularly loud or soft, you


266
00:09:20,696 --> 00:09:22,366
may need to adjust others to


267
00:09:22,366 --> 00:09:23,956
balance things out and achieve a


268
00:09:23,956 --> 00:09:24,686
good mix.


269
00:09:25,776 --> 00:09:27,686
And again, there will be times


270
00:09:27,686 --> 00:09:29,026
when you do need to collect a


271
00:09:29,056 --> 00:09:30,626
lot of data for a particular


272
00:09:30,626 --> 00:09:32,986
feature, but in those cases, you


273
00:09:33,236 --> 00:09:34,636
want to make sure you adjust


274
00:09:34,636 --> 00:09:36,946
those privacy techniques to


275
00:09:36,946 --> 00:09:38,396
create a great experience for


276
00:09:38,396 --> 00:09:39,156
your users.


277
00:09:40,416 --> 00:09:42,456
Ideally, these apply across all


278
00:09:42,456 --> 00:09:44,086
systems where the data lives so


279
00:09:44,086 --> 00:09:45,626
those privacy guarantees stay


280
00:09:45,626 --> 00:09:47,796
consistent and are billed as


281
00:09:47,866 --> 00:09:49,276
technical enforcement rather


282
00:09:49,276 --> 00:09:50,526
than just policy statements


283
00:09:50,526 --> 00:09:52,396
about what you plan to do.


284
00:09:53,676 --> 00:09:55,316
But I know this can all be a


285
00:09:55,316 --> 00:09:56,996
little abstract, so to


286
00:09:56,996 --> 00:09:58,766
illustrate further, I want to


287
00:09:58,766 --> 00:10:00,096
share some features that Apple


288
00:10:00,096 --> 00:10:01,366
has built where we've this kind


289
00:10:01,366 --> 00:10:01,976
of thinking.


290
00:10:03,516 --> 00:10:05,146
First is activity sharing where


291
00:10:05,146 --> 00:10:06,466
you can share fitness data with


292
00:10:06,466 --> 00:10:07,176
your friends.


293
00:10:08,206 --> 00:10:09,376
Now for me as a privacy


294
00:10:09,376 --> 00:10:11,246
engineer, I like to turn all of


295
00:10:11,246 --> 00:10:12,856
these sliders up to 100 percent


296
00:10:12,856 --> 00:10:15,106
as much as possible, but that's


297
00:10:15,106 --> 00:10:16,426
not always feasible for a given


298
00:10:16,426 --> 00:10:16,946
feature.


299
00:10:17,406 --> 00:10:18,956
In this case, you're sharing


300
00:10:18,956 --> 00:10:21,186
data with friends, so they know


301
00:10:21,186 --> 00:10:21,626
your name.


302
00:10:21,626 --> 00:10:22,646
They know whose data it is.


303
00:10:23,676 --> 00:10:24,906
So you can't just make this data


304
00:10:24,906 --> 00:10:25,846
de-identified.


305
00:10:25,906 --> 00:10:27,636
It's already very identifiable


306
00:10:27,636 --> 00:10:28,706
as part of the use case.


307
00:10:29,646 --> 00:10:31,556
Consequently, we turn up other


308
00:10:31,556 --> 00:10:32,976
privacy techniques like only


309
00:10:32,976 --> 00:10:34,326
showing a summary of the data,


310
00:10:34,746 --> 00:10:36,236
not minute-by-minute statistics


311
00:10:36,296 --> 00:10:37,756
or the exact location of your


312
00:10:38,396 --> 00:10:38,716
run.


313
00:10:38,716 --> 00:10:40,176
We also provide a lot of control


314
00:10:40,176 --> 00:10:41,536
over who you share with and


315
00:10:41,536 --> 00:10:41,906
when.


316
00:10:43,166 --> 00:10:45,716
Now, in the Apple News app, we


317
00:10:45,716 --> 00:10:47,496
collect analytics data using a


318
00:10:47,496 --> 00:10:49,126
scoped identifier that's not


319
00:10:49,126 --> 00:10:50,816
connected to your Apple ID.


320
00:10:51,766 --> 00:10:53,246
That gives us more flexibility


321
00:10:53,246 --> 00:10:54,486
around the precision of data we


322
00:10:54,486 --> 00:10:56,666
collect, but since it's still


323
00:10:56,666 --> 00:10:58,006
sensitive information, we still


324
00:10:58,006 --> 00:10:59,416
provide control through things


325
00:10:59,416 --> 00:11:00,896
like being able to reset that


326
00:11:00,896 --> 00:11:02,296
identifier at any time.


327
00:11:03,516 --> 00:11:05,456
Finally, there's photo memories.


328
00:11:05,456 --> 00:11:06,516
You may have seen these on your


329
00:11:06,516 --> 00:11:07,126
device.


330
00:11:08,086 --> 00:11:10,346
These use facial recognition


331
00:11:10,346 --> 00:11:11,946
data to identify people in


332
00:11:11,946 --> 00:11:12,616
pictures.


333
00:11:13,696 --> 00:11:15,266
It also uses precise location


334
00:11:15,266 --> 00:11:16,816
information to connect similar


335
00:11:16,816 --> 00:11:17,596
photos together.


336
00:11:18,686 --> 00:11:20,776
Now that's very sensitive data.


337
00:11:21,856 --> 00:11:23,466
So consequently, there's another


338
00:11:23,466 --> 00:11:24,846
privacy technique we turn way


339
00:11:24,846 --> 00:11:25,056
up.


340
00:11:26,146 --> 00:11:27,366
All the processing to build


341
00:11:27,366 --> 00:11:29,416
these memories happens on your


342
00:11:29,416 --> 00:11:30,036
device.


343
00:11:30,806 --> 00:11:31,976
And by the way, that's a great


344
00:11:31,976 --> 00:11:33,776
tool for your toolbox to do


345
00:11:33,776 --> 00:11:34,866
processing locally.


346
00:11:35,396 --> 00:11:39,196
So to recap, three big ideas.


347
00:11:39,856 --> 00:11:43,706
Privacy is about people, ask the


348
00:11:43,706 --> 00:11:46,386
"should" questions, and align


349
00:11:46,386 --> 00:11:48,596
your data practices with your


350
00:11:48,596 --> 00:11:49,496
use cases.


351
00:11:51,256 --> 00:11:52,816
In the time we have remaining, I


352
00:11:52,816 --> 00:11:53,996
want to talk through some


353
00:11:53,996 --> 00:11:55,136
features and tools that are


354
00:11:55,136 --> 00:11:56,856
available to you as developers


355
00:11:56,856 --> 00:11:58,746
to help you build privacy in


356
00:11:58,746 --> 00:12:00,046
your app.


357
00:12:00,546 --> 00:12:02,216
And these fall into two general


358
00:12:02,216 --> 00:12:03,866
categories of accessing user


359
00:12:03,866 --> 00:12:05,986
data and more broadly, data


360
00:12:05,986 --> 00:12:06,656
stewardship.


361
00:12:07,896 --> 00:12:09,776
So for data access, let's start


362
00:12:09,826 --> 00:12:12,336
by talking about iOS, and much


363
00:12:12,336 --> 00:12:13,656
of this guidance will apply to


364
00:12:13,656 --> 00:12:15,666
tvOS and watchOS as well.


365
00:12:16,186 --> 00:12:18,576
Let's imagine you're building a


366
00:12:18,576 --> 00:12:20,266
game for iOS where players can


367
00:12:20,266 --> 00:12:21,916
compete against each other, and


368
00:12:21,916 --> 00:12:22,746
you want them to be able to


369
00:12:22,746 --> 00:12:24,256
upload a photo to identify


370
00:12:24,256 --> 00:12:24,896
themselves.


371
00:12:25,816 --> 00:12:26,716
Now we've all seen those


372
00:12:26,756 --> 00:12:28,276
permission prompts for access to


373
00:12:28,276 --> 00:12:30,386
photos, but wouldn't it be great


374
00:12:30,386 --> 00:12:31,416
if we could just have the user


375
00:12:31,416 --> 00:12:32,906
click a button, select a


376
00:12:32,906 --> 00:12:34,436
picture, and have it appear


377
00:12:34,436 --> 00:12:36,646
immediately in their app?


378
00:12:37,276 --> 00:12:39,006
Well, you can already do this


379
00:12:39,006 --> 00:12:39,396
today.


380
00:12:40,786 --> 00:12:41,796
Because we have a feature


381
00:12:41,796 --> 00:12:43,366
available called out-of-process


382
00:12:43,366 --> 00:12:45,056
pickers for contacts, camera,


383
00:12:45,056 --> 00:12:47,646
and photos data, where the


384
00:12:47,646 --> 00:12:49,356
picker that appears runs outside


385
00:12:49,356 --> 00:12:51,476
your apps process so the only


386
00:12:51,476 --> 00:12:52,786
information that's shared back


387
00:12:52,786 --> 00:12:54,396
with the app is what the user


388
00:12:54,396 --> 00:12:55,096
selects.


389
00:12:56,196 --> 00:12:57,916
We talked about those privacy


390
00:12:57,916 --> 00:12:58,516
techniques.


391
00:12:58,516 --> 00:13:00,256
This is a case where because


392
00:13:00,256 --> 00:13:01,856
this doesn't involve ongoing


393
00:13:01,856 --> 00:13:03,846
access to the entire library,


394
00:13:04,416 --> 00:13:06,376
the user has control by picking


395
00:13:06,376 --> 00:13:08,266
what they share, so we don't


396
00:13:08,266 --> 00:13:09,596
need to show a permission prompt


397
00:13:09,916 --> 00:13:11,366
and ask them to make a decision


398
00:13:11,556 --> 00:13:13,066
about future access.


399
00:13:14,476 --> 00:13:16,976
This is the default method for


400
00:13:16,976 --> 00:13:18,696
accessing contacts, camera, and


401
00:13:18,696 --> 00:13:19,586
photos data.


402
00:13:20,856 --> 00:13:22,176
There are going to be times


403
00:13:22,176 --> 00:13:23,446
where you may need access to the


404
00:13:23,446 --> 00:13:24,706
broader library, but in most


405
00:13:24,706 --> 00:13:26,296
situations, you'll find that


406
00:13:26,296 --> 00:13:27,936
this works great for a whole lot


407
00:13:27,936 --> 00:13:29,186
of apps.


408
00:13:31,876 --> 00:13:33,356
This is a case where it does


409
00:13:33,356 --> 00:13:34,776
just work and only requires a


410
00:13:34,776 --> 00:13:35,816
few lines of code.


411
00:13:36,206 --> 00:13:37,326
You can see some snippets here


412
00:13:37,326 --> 00:13:39,116
for how to call these pickers in


413
00:13:39,116 --> 00:13:39,796
your app.


414
00:13:40,656 --> 00:13:42,076
Now as I said, there are going


415
00:13:42,076 --> 00:13:43,176
to be some times where you do


416
00:13:43,176 --> 00:13:44,766
need access to the broader data,


417
00:13:44,856 --> 00:13:46,026
and as you know, there's a


418
00:13:46,026 --> 00:13:47,726
variety of protected resources


419
00:13:47,726 --> 00:13:50,686
available on the device, but if


420
00:13:50,686 --> 00:13:52,426
you're using one of these APIs,


421
00:13:53,176 --> 00:13:54,256
you need to keep in mind three


422
00:13:54,256 --> 00:13:55,336
things before you start


423
00:13:55,336 --> 00:13:56,646
requesting access.


424
00:13:57,986 --> 00:13:59,646
You should only request access


425
00:14:00,226 --> 00:14:01,836
to the data that's necessary for


426
00:14:01,836 --> 00:14:04,326
your use case in your app.


427
00:14:05,056 --> 00:14:06,556
If you don't actually need the


428
00:14:06,556 --> 00:14:08,716
entire library of information,


429
00:14:09,496 --> 00:14:10,586
rather than requesting it, you


430
00:14:10,586 --> 00:14:11,806
should look for an alternative


431
00:14:11,806 --> 00:14:12,886
solution, like those


432
00:14:12,886 --> 00:14:13,946
out-of-process pickers.


433
00:14:15,096 --> 00:14:16,296
You should only make these


434
00:14:16,296 --> 00:14:17,526
requests when it's needed.


435
00:14:17,526 --> 00:14:19,556
You want it to be in the moment


436
00:14:19,656 --> 00:14:21,936
when a user makes a decision not


437
00:14:22,366 --> 00:14:23,636
when they first open the app and


438
00:14:23,636 --> 00:14:25,126
they're bombarded with questions


439
00:14:25,126 --> 00:14:26,586
that they don't even understand


440
00:14:26,586 --> 00:14:26,886
yet.


441
00:14:27,886 --> 00:14:29,016
You want the prompt to be in


442
00:14:29,016 --> 00:14:29,876
context.


443
00:14:30,606 --> 00:14:32,356
But also you want to rely only


444
00:14:32,356 --> 00:14:33,946
on the API for status.


445
00:14:34,956 --> 00:14:36,406
Remember, a user can revoke


446
00:14:36,406 --> 00:14:38,046
their decision at any time.


447
00:14:38,106 --> 00:14:39,546
You just want make sure your app


448
00:14:39,546 --> 00:14:41,356
still functions regardless of


449
00:14:41,356 --> 00:14:42,586
what the user had decided.


450
00:14:44,326 --> 00:14:46,676
Now when you request access, as


451
00:14:46,676 --> 00:14:47,996
you know, you need to include a


452
00:14:47,996 --> 00:14:49,816
purpose string or a usage


453
00:14:49,816 --> 00:14:50,366
description.


454
00:14:51,446 --> 00:14:52,526
And when I say you need to,


455
00:14:52,946 --> 00:14:54,466
these are required.


456
00:14:55,596 --> 00:14:57,196
You'll find your app rejected in


457
00:14:57,196 --> 00:14:58,406
app review without these, and in


458
00:14:58,406 --> 00:14:59,716
fact, you'll find your apps


459
00:14:59,716 --> 00:15:00,946
start crashing if you try to


460
00:15:00,946 --> 00:15:02,306
access this data without a


461
00:15:02,306 --> 00:15:02,916
purpose string.


462
00:15:04,356 --> 00:15:06,216
Now this is one way of providing


463
00:15:06,216 --> 00:15:07,306
transparency to users.


464
00:15:07,306 --> 00:15:08,686
It's certainly not the only way.


465
00:15:09,546 --> 00:15:10,746
By that, I don't mean showing a


466
00:15:10,746 --> 00:15:12,256
fig prompt before the real one


467
00:15:12,256 --> 00:15:13,156
to get them to click.


468
00:15:13,816 --> 00:15:14,596
I mean this should be part of


469
00:15:14,596 --> 00:15:16,236
the overall program of informing


470
00:15:16,236 --> 00:15:17,796
your users about data flows in


471
00:15:17,796 --> 00:15:18,846
your app.


472
00:15:19,496 --> 00:15:21,876
The goal here is to explain the


473
00:15:21,876 --> 00:15:24,906
reason for a request so that a


474
00:15:24,906 --> 00:15:26,526
user can make an informed,


475
00:15:26,526 --> 00:15:28,716
effective decision based on


476
00:15:28,716 --> 00:15:29,716
their priorities.


477
00:15:30,226 --> 00:15:32,946
When I say explain the reason,


478
00:15:33,676 --> 00:15:35,176
this is not what I'm talking


479
00:15:35,176 --> 00:15:35,546
about.


480
00:15:36,566 --> 00:15:37,996
And we've seen purpose strings


481
00:15:37,996 --> 00:15:38,936
like this in the past, but


482
00:15:38,936 --> 00:15:40,526
again, this is going to lead to


483
00:15:40,526 --> 00:15:41,996
rejections in app review.


484
00:15:43,246 --> 00:15:44,876
We're increasingly enforcing


485
00:15:45,156 --> 00:15:47,086
quality purpose strings both


486
00:15:47,086 --> 00:15:48,596
through automated validation and


487
00:15:48,596 --> 00:15:49,396
manual review.


488
00:15:50,766 --> 00:15:53,026
Placeholders or blank strings


489
00:15:53,026 --> 00:15:54,286
are not going to be sufficient.


490
00:15:54,676 --> 00:15:56,316
Just saying advertising doesn't


491
00:15:56,316 --> 00:15:57,016
tell a user much.


492
00:15:57,056 --> 00:15:58,796
Requires location doesn't


493
00:15:58,796 --> 00:15:59,696
explain the why.


494
00:16:00,126 --> 00:16:01,776
Even this last one about more


495
00:16:01,776 --> 00:16:03,286
relevant content, that's nice,


496
00:16:03,316 --> 00:16:05,066
but it's pretty vague.


497
00:16:06,276 --> 00:16:07,586
And when you look at our own


498
00:16:07,646 --> 00:16:09,376
maps app, when it requests


499
00:16:09,376 --> 00:16:11,156
location, this is what you see.


500
00:16:11,586 --> 00:16:12,806
It will be displayed on the map


501
00:16:12,966 --> 00:16:14,676
and used for directions, nearby


502
00:16:14,676 --> 00:16:15,816
search results, and travel


503
00:16:15,816 --> 00:16:16,406
times.


504
00:16:17,356 --> 00:16:19,046
So this is explaining the reason


505
00:16:19,046 --> 00:16:21,116
the use case for this request.


506
00:16:21,816 --> 00:16:24,666
It's specific, includes examples


507
00:16:24,846 --> 00:16:26,206
of how the data will be used.


508
00:16:26,686 --> 00:16:29,696
The TV app also, this is another


509
00:16:29,696 --> 00:16:31,446
one that Apple wrote using your


510
00:16:31,446 --> 00:16:32,566
location to determine what's


511
00:16:32,566 --> 00:16:34,136
available to you and show you


512
00:16:34,136 --> 00:16:35,716
live games, events, and news


513
00:16:35,716 --> 00:16:37,066
from your area.


514
00:16:37,636 --> 00:16:39,396
Remember, if users understand


515
00:16:39,396 --> 00:16:41,136
why they're being asked for this


516
00:16:41,136 --> 00:16:42,206
data, they're going to be more


517
00:16:42,206 --> 00:16:44,596
likely to allow it.


518
00:16:44,726 --> 00:16:45,946
If you were building a transit


519
00:16:45,946 --> 00:16:47,296
app for a subway system, you


520
00:16:47,596 --> 00:16:48,876
might write something like this.


521
00:16:49,146 --> 00:16:50,856
This app uses your location to


522
00:16:50,856 --> 00:16:52,486
show nearby stops and stations


523
00:16:52,526 --> 00:16:54,106
and allows you to plan trips


524
00:16:54,106 --> 00:16:55,206
from your current location.


525
00:16:56,596 --> 00:16:59,056
Again, explain the use case, be


526
00:16:59,056 --> 00:17:01,396
specific, provide an example.


527
00:17:01,916 --> 00:17:04,326
Some additional guidelines to


528
00:17:04,326 --> 00:17:05,276
remember when you're working


529
00:17:05,276 --> 00:17:06,736
with protected resources on a


530
00:17:06,736 --> 00:17:07,266
device.


531
00:17:08,076 --> 00:17:09,596
Access should not be required


532
00:17:09,596 --> 00:17:10,915
for your app to function.


533
00:17:11,596 --> 00:17:12,715
Again, this can lead to


534
00:17:12,715 --> 00:17:14,556
rejections in the app review


535
00:17:14,556 --> 00:17:15,326
process.


536
00:17:16,435 --> 00:17:17,856
Your app should have graceful


537
00:17:17,856 --> 00:17:19,925
fallback mechanisms so that even


538
00:17:19,925 --> 00:17:21,465
if a user declines access, it


539
00:17:21,465 --> 00:17:22,465
still functions.


540
00:17:23,056 --> 00:17:24,356
For example, with that transit


541
00:17:24,356 --> 00:17:26,266
app, if the user denies location


542
00:17:26,266 --> 00:17:27,906
access, you can have a field


543
00:17:27,906 --> 00:17:29,026
where they can enter location


544
00:17:29,026 --> 00:17:30,786
manually and use that instead.


545
00:17:31,986 --> 00:17:33,586
Again, you want to verify the


546
00:17:33,586 --> 00:17:35,176
authorization status of your app


547
00:17:35,316 --> 00:17:37,046
whenever it needs this data to


548
00:17:37,046 --> 00:17:38,776
make sure that the user still is


549
00:17:38,776 --> 00:17:41,106
getting access, and stay aware


550
00:17:41,106 --> 00:17:43,006
of third-part STKs.


551
00:17:43,566 --> 00:17:45,136
Again, requesting access should


552
00:17:45,136 --> 00:17:46,546
only be for the use cases of


553
00:17:46,546 --> 00:17:48,336
your app, and if you're


554
00:17:48,336 --> 00:17:49,656
including libraries that change


555
00:17:49,656 --> 00:17:50,726
that or tell you to set purpose


556
00:17:50,726 --> 00:17:52,226
strings, you should probably


557
00:17:52,226 --> 00:17:53,686
look for a different solution or


558
00:17:53,686 --> 00:17:55,026
update your code.


559
00:17:55,566 --> 00:17:56,886
Finally, you want to provide


560
00:17:56,886 --> 00:17:58,106
ongoing transparency.


561
00:17:58,106 --> 00:17:59,326
Again, this should not be the


562
00:17:59,326 --> 00:18:00,736
only time that a user gets to


563
00:18:00,736 --> 00:18:02,846
understand how their data is


564
00:18:02,846 --> 00:18:03,666
being used.


565
00:18:04,386 --> 00:18:05,616
You can do this in a privacy


566
00:18:05,616 --> 00:18:07,086
policy, which is required for


567
00:18:07,086 --> 00:18:08,746
apps in the App Store, or


568
00:18:08,746 --> 00:18:09,876
throughout your app you can


569
00:18:09,876 --> 00:18:11,506
provide links and documentation


570
00:18:11,676 --> 00:18:13,606
to help users understand how


571
00:18:13,606 --> 00:18:14,676
their data is being used.


572
00:18:16,296 --> 00:18:18,356
Now this is all broader guidance


573
00:18:18,616 --> 00:18:21,026
around these kinds of APIs, but


574
00:18:21,026 --> 00:18:22,206
there's a few specific ones I


575
00:18:22,206 --> 00:18:23,786
want to highlight with changes


576
00:18:23,786 --> 00:18:24,276
this year.


577
00:18:24,766 --> 00:18:26,706
The first is for WiFi network


578
00:18:26,706 --> 00:18:27,406
information.


579
00:18:27,926 --> 00:18:29,546
If your app uses see and copy


580
00:18:29,546 --> 00:18:30,946
current network info, you're


581
00:18:31,276 --> 00:18:32,236
going to need to add an


582
00:18:32,236 --> 00:18:33,046
entitlement,


583
00:18:33,046 --> 00:18:35,566
AccessWiFiInformation in Xcode.


584
00:18:36,616 --> 00:18:37,996
This is a capability that you


585
00:18:37,996 --> 00:18:39,076
can enable for your app.


586
00:18:39,736 --> 00:18:41,666
For example, if your app is


587
00:18:41,666 --> 00:18:42,746
communicating with a hardware


588
00:18:42,746 --> 00:18:44,236
accessory and needs to verify if


589
00:18:44,236 --> 00:18:46,066
they're on the same network, you


590
00:18:46,066 --> 00:18:46,746
would need this.


591
00:18:47,296 --> 00:18:48,246
If you're not doing that use


592
00:18:48,246 --> 00:18:49,576
case, you don't need to worry


593
00:18:49,576 --> 00:18:50,196
about this.


594
00:18:50,196 --> 00:18:51,536
This is only if it's necessary


595
00:18:51,536 --> 00:18:52,606
for the functionality of your


596
00:18:52,606 --> 00:18:52,856
app.


597
00:18:53,296 --> 00:18:55,386
You may have also heard about


598
00:18:55,386 --> 00:18:56,746
our new Health Records API.


599
00:18:57,726 --> 00:18:59,096
Because we know that developers


600
00:18:59,096 --> 00:19:00,686
have a lot of great ideas around


601
00:19:00,686 --> 00:19:02,156
building apps using health data,


602
00:19:02,626 --> 00:19:03,916
but we also recognize that's


603
00:19:04,106 --> 00:19:05,626
very sensitive information.


604
00:19:06,616 --> 00:19:08,046
So again, adjusting those


605
00:19:08,046 --> 00:19:09,846
privacy techniques rather than


606
00:19:09,846 --> 00:19:11,156
just a simple permission prompt,


607
00:19:11,536 --> 00:19:12,386
we provide a greater


608
00:19:12,386 --> 00:19:13,866
transparency and control for the


609
00:19:13,866 --> 00:19:14,316
user.


610
00:19:14,936 --> 00:19:16,166
You can see there's a purpose


611
00:19:16,166 --> 00:19:17,726
string, a link to a privacy


612
00:19:17,726 --> 00:19:18,706
policy, which should be a


613
00:19:18,706 --> 00:19:20,226
website or if it's in your app


614
00:19:20,346 --> 00:19:21,486
available while signed out.


615
00:19:22,256 --> 00:19:23,676
There's also controls over what


616
00:19:23,676 --> 00:19:25,076
categories of data to share,


617
00:19:25,076 --> 00:19:26,266
whether it's current or current


618
00:19:26,266 --> 00:19:28,156
and future data, and there's a


619
00:19:28,156 --> 00:19:29,256
session from Tuesday you can


620
00:19:29,256 --> 00:19:30,536
refer to that provides a lot


621
00:19:30,536 --> 00:19:31,926
more information about how to


622
00:19:31,926 --> 00:19:34,026
use this API in your app.


623
00:19:35,516 --> 00:19:37,226
Finally, for iOS, I wanted to


624
00:19:37,226 --> 00:19:38,476
share a quick update on how


625
00:19:38,476 --> 00:19:39,816
Apple's been using the technique


626
00:19:39,816 --> 00:19:41,526
of differential privacy to


627
00:19:41,526 --> 00:19:43,136
improve our apps and devices.


628
00:19:44,016 --> 00:19:45,686
The first new use case this year


629
00:19:45,686 --> 00:19:46,796
is for commonly misspelled


630
00:19:46,796 --> 00:19:47,446
words.


631
00:19:47,776 --> 00:19:49,056
If you're typing and correct


632
00:19:49,056 --> 00:19:51,166
yourself, devices that are opted


633
00:19:51,166 --> 00:19:52,666
in to device analytics will now


634
00:19:52,666 --> 00:19:54,566
donate data on those words to


635
00:19:54,566 --> 00:19:55,706
help us improve our keyboard


636
00:19:55,706 --> 00:19:57,386
algorithms even further while


637
00:19:57,386 --> 00:19:58,616
protecting user privacy.


638
00:19:59,376 --> 00:20:01,936
Also, for Safari, since last


639
00:20:01,936 --> 00:20:03,126
year, we've added the ability


640
00:20:03,126 --> 00:20:04,716
for those devices to donate data


641
00:20:04,956 --> 00:20:06,946
around websites that typically


642
00:20:06,946 --> 00:20:08,816
cause crashes to improve the


643
00:20:08,816 --> 00:20:09,956
stability of the browser.


644
00:20:10,456 --> 00:20:12,226
So that's iOS.


645
00:20:12,226 --> 00:20:14,176
Let's talk for a moment about


646
00:20:14,176 --> 00:20:15,256
macOS as well.


647
00:20:15,606 --> 00:20:17,086
Because as you may have heard,


648
00:20:17,436 --> 00:20:18,766
we've made some changes to how


649
00:20:18,766 --> 00:20:20,356
protected resources are handled


650
00:20:20,356 --> 00:20:21,236
on macOS.


651
00:20:22,136 --> 00:20:23,206
It's an expanded list of


652
00:20:23,206 --> 00:20:24,546
categories of data where there


653
00:20:24,886 --> 00:20:26,576
are protections in place, and


654
00:20:26,576 --> 00:20:27,546
these can now trigger a


655
00:20:27,546 --> 00:20:29,746
permission prompt or for some of


656
00:20:29,746 --> 00:20:31,056
these an opt-in through system


657
00:20:31,056 --> 00:20:31,776
preferences.


658
00:20:32,816 --> 00:20:34,076
I just want to highlight this so


659
00:20:34,076 --> 00:20:35,136
that if you're developing for


660
00:20:35,136 --> 00:20:37,246
the Mac, I want you to be aware


661
00:20:37,246 --> 00:20:38,716
of some of these changes because


662
00:20:38,716 --> 00:20:39,706
you need to know how they're


663
00:20:39,706 --> 00:20:41,176
going to affect your app if


664
00:20:41,176 --> 00:20:42,056
you're accessing these


665
00:20:42,056 --> 00:20:42,816
resources.


666
00:20:43,896 --> 00:20:45,416
Again, since these can trigger a


667
00:20:45,416 --> 00:20:46,486
permission prompt, you want to


668
00:20:46,486 --> 00:20:47,906
know when that's going to happen


669
00:20:47,906 --> 00:20:49,446
so your users aren't surprised,


670
00:20:50,136 --> 00:20:51,786
and please note, this applies to


671
00:20:51,976 --> 00:20:54,006
all third-party app processes


672
00:20:54,346 --> 00:20:56,666
including those outside of the


673
00:20:57,236 --> 00:20:58,276
app store.


674
00:20:58,496 --> 00:20:59,746
Just like with iOS, you'll need


675
00:20:59,746 --> 00:21:00,886
to set a purpose string for


676
00:21:00,886 --> 00:21:02,026
these permission prompts as


677
00:21:02,026 --> 00:21:03,456
well, and again there's a


678
00:21:03,456 --> 00:21:04,676
session from Tuesday that goes


679
00:21:04,676 --> 00:21:06,556
into a lot more detail on how


680
00:21:06,556 --> 00:21:07,696
this works for your app.


681
00:21:08,076 --> 00:21:11,306
Now to talk about accessing data


682
00:21:11,306 --> 00:21:13,066
on the web, I'm going to turn it


683
00:21:13,066 --> 00:21:14,316
over to my fellow privacy


684
00:21:14,316 --> 00:21:15,976
engineer, Brandon Van Ryswyk.


685
00:21:16,516 --> 00:21:22,816
[ Applause ]


686
00:21:23,316 --> 00:21:23,826
>> Thanks Joey.


687
00:21:25,376 --> 00:21:27,046
The web is one of the largest


688
00:21:27,046 --> 00:21:28,996
venues for data access today.


689
00:21:29,856 --> 00:21:31,546
If your business depends on


690
00:21:31,546 --> 00:21:33,286
providing content on third-party


691
00:21:33,286 --> 00:21:35,226
websites, this section is for


692
00:21:35,226 --> 00:21:35,406
you.


693
00:21:37,336 --> 00:21:38,886
This year we introduced the


694
00:21:38,886 --> 00:21:40,186
Storage Access API.


695
00:21:41,106 --> 00:21:43,086
The Storage Access API allows


696
00:21:43,086 --> 00:21:45,146
users to engage with logged-in


697
00:21:45,146 --> 00:21:47,186
content from embedded third


698
00:21:47,186 --> 00:21:49,926
parties across the web including


699
00:21:50,126 --> 00:21:51,226
from domains that have been


700
00:21:51,226 --> 00:21:53,236
classified as a tracker by


701
00:21:53,236 --> 00:21:54,676
intelligent tracking prevention.


702
00:21:55,806 --> 00:21:57,556
Now the Storage Access API does


703
00:21:57,616 --> 00:21:59,386
this only with the user's


704
00:21:59,426 --> 00:22:00,486
explicit consent.


705
00:22:01,016 --> 00:22:02,646
Let's go through an example.


706
00:22:03,156 --> 00:22:05,696
Here the user is browsing a news


707
00:22:05,696 --> 00:22:08,576
site, news.example, and the news


708
00:22:08,576 --> 00:22:10,036
site has an embedded video


709
00:22:10,036 --> 00:22:11,786
player from video.example.


710
00:22:12,686 --> 00:22:14,676
Now the user has a paid account


711
00:22:14,676 --> 00:22:16,406
on the video site and would like


712
00:22:16,406 --> 00:22:17,986
to grant the embedded video


713
00:22:17,986 --> 00:22:20,666
access to its cookies so that


714
00:22:20,666 --> 00:22:22,226
they can enjoy the benefits of


715
00:22:22,226 --> 00:22:24,066
their subscription while reading


716
00:22:24,066 --> 00:22:24,566
the news.


717
00:22:25,746 --> 00:22:26,676
To accomplish this,


718
00:22:26,776 --> 00:22:29,146
video.example needs to implement


719
00:22:29,206 --> 00:22:30,576
the Storage Access API.


720
00:22:32,866 --> 00:22:35,356
Video.example should add a call


721
00:22:35,446 --> 00:22:37,636
to the Storage Access API when


722
00:22:37,636 --> 00:22:39,336
the user clicks the play button


723
00:22:39,366 --> 00:22:40,486
in their app.


724
00:22:41,026 --> 00:22:42,976
Now this is an asynchronous API


725
00:22:43,086 --> 00:22:44,976
that will return a promise, so


726
00:22:44,976 --> 00:22:46,206
you should be prepared to handle


727
00:22:46,206 --> 00:22:47,856
successes as well as failures.


728
00:22:48,396 --> 00:22:50,946
So when a user clicks the play


729
00:22:50,946 --> 00:22:52,286
button, this will kick off a


730
00:22:52,286 --> 00:22:54,726
request, which will result in a


731
00:22:54,876 --> 00:22:57,296
prompt asking the user if they


732
00:22:57,296 --> 00:22:58,146
would like to grant


733
00:22:58,496 --> 00:23:00,776
video.example access to its


734
00:23:00,776 --> 00:23:02,676
cookies while embedded in


735
00:23:02,676 --> 00:23:03,486
news.example.


736
00:23:04,036 --> 00:23:06,896
If the user clicks allow, this


737
00:23:06,896 --> 00:23:07,916
choice will be sticky, and the


738
00:23:07,916 --> 00:23:09,566
user won't be prompted again on


739
00:23:09,566 --> 00:23:11,006
this combination of domains.


740
00:23:12,156 --> 00:23:13,516
But if the user clicks deny, the


741
00:23:13,516 --> 00:23:14,776
site can always reprompt.


742
00:23:15,186 --> 00:23:17,446
Let's assume the user clicked


743
00:23:17,446 --> 00:23:17,836
allow.


744
00:23:18,766 --> 00:23:20,776
The request will go through, and


745
00:23:20,776 --> 00:23:22,396
cookies will be returned to the


746
00:23:22,396 --> 00:23:22,996
embedded site.


747
00:23:24,496 --> 00:23:25,376
Now, this could create a


748
00:23:25,376 --> 00:23:27,106
tracking risk as now


749
00:23:27,106 --> 00:23:29,466
video.example has their users


750
00:23:29,566 --> 00:23:31,956
logged in identity associated


751
00:23:31,956 --> 00:23:33,796
with their presence on this news


752
00:23:33,796 --> 00:23:34,026
site.


753
00:23:35,446 --> 00:23:36,926
Now this is especially important


754
00:23:37,016 --> 00:23:38,616
given changes to intelligent


755
00:23:38,616 --> 00:23:40,046
tracking prevention this year.


756
00:23:41,016 --> 00:23:44,076
Now outside of the user consent


757
00:23:44,076 --> 00:23:45,516
provided via the Storage Access


758
00:23:45,516 --> 00:23:48,106
API, cookies from domains that


759
00:23:48,106 --> 00:23:49,616
are classified as trackers will


760
00:23:49,616 --> 00:23:52,156
be partitioned immediately and


761
00:23:52,156 --> 00:23:53,316
can never be used in a


762
00:23:53,316 --> 00:23:54,526
third-party context.


763
00:23:55,916 --> 00:23:58,106
Additionally, after 30 days


764
00:23:58,106 --> 00:23:59,566
without user involvement, these


765
00:23:59,566 --> 00:24:01,236
cookies will be purged entirely.


766
00:24:01,746 --> 00:24:05,346
Now importantly here, access via


767
00:24:05,346 --> 00:24:07,466
the Storage Access API will


768
00:24:07,466 --> 00:24:09,296
count towards this 30-day


769
00:24:09,296 --> 00:24:10,176
interaction timer.


770
00:24:11,196 --> 00:24:12,316
That means that users who


771
00:24:12,346 --> 00:24:13,626
interact frequently with your


772
00:24:13,626 --> 00:24:15,786
site in a third-party context


773
00:24:16,016 --> 00:24:17,026
will stay logged in.


774
00:24:17,476 --> 00:24:20,576
So in this sequence, the user


775
00:24:20,776 --> 00:24:23,166
will visit video.example both in


776
00:24:23,166 --> 00:24:24,956
a first-party context, logged


777
00:24:24,956 --> 00:24:26,806
into the home page, and in a


778
00:24:26,806 --> 00:24:28,456
third-party context, where it's


779
00:24:28,456 --> 00:24:30,206
embedded across the web.


780
00:24:31,056 --> 00:24:32,946
So first, the user visits the


781
00:24:32,946 --> 00:24:34,736
site in a first-party context.


782
00:24:35,606 --> 00:24:37,416
Now notice that the days since


783
00:24:37,416 --> 00:24:39,206
interaction timer will read zero


784
00:24:39,296 --> 00:24:40,856
as the user is currently


785
00:24:40,856 --> 00:24:41,796
interacting with the site.


786
00:24:43,476 --> 00:24:45,566
But as the user interacts with


787
00:24:45,566 --> 00:24:47,166
the embedded content throughout


788
00:24:47,166 --> 00:24:50,406
their web browsing, the timer


789
00:24:50,406 --> 00:24:51,046
will update.


790
00:24:52,466 --> 00:24:53,886
This means that when the user


791
00:24:53,886 --> 00:24:56,146
returns in a first-party


792
00:24:56,146 --> 00:24:58,316
context, the days since


793
00:24:58,316 --> 00:25:00,426
interaction timer will read 5


794
00:25:00,426 --> 00:25:04,026
days despite there being 45 days


795
00:25:04,726 --> 00:25:06,136
since the user was last on


796
00:25:06,136 --> 00:25:07,976
video.example in a first-party


797
00:25:07,976 --> 00:25:08,686
context.


798
00:25:10,096 --> 00:25:11,926
Adopting the Storage Access API


799
00:25:11,926 --> 00:25:13,876
will allow your users to stay


800
00:25:13,876 --> 00:25:16,446
logged in and prevent unwanted


801
00:25:16,446 --> 00:25:16,816
tracking.


802
00:25:17,306 --> 00:25:21,366
Now privacy does not end with


803
00:25:21,366 --> 00:25:23,006
gaining access to a user's data.


804
00:25:23,986 --> 00:25:25,336
Privacy is a continued


805
00:25:25,336 --> 00:25:27,256
obligation to your users to


806
00:25:27,256 --> 00:25:29,246
maintain their trust throughout


807
00:25:29,246 --> 00:25:30,146
the data's lifetime.


808
00:25:31,046 --> 00:25:32,636
This is where data stewardship


809
00:25:32,766 --> 00:25:33,266
begins.


810
00:25:33,866 --> 00:25:36,646
Now I want to give you examples


811
00:25:36,646 --> 00:25:38,206
from four areas of data


812
00:25:38,206 --> 00:25:39,466
stewardship for you to think


813
00:25:39,466 --> 00:25:42,476
about when developing your apps.


814
00:25:42,546 --> 00:25:43,406
First is deletion.


815
00:25:44,356 --> 00:25:45,706
Part of being a good data


816
00:25:45,706 --> 00:25:47,176
steward is respecting your


817
00:25:47,176 --> 00:25:49,106
user's intent to delete


818
00:25:49,106 --> 00:25:51,166
something from your app.


819
00:25:52,696 --> 00:25:54,476
So you should recognize that


820
00:25:54,476 --> 00:25:55,756
there are data flows that go


821
00:25:55,846 --> 00:25:58,066
outside your app, and you should


822
00:25:58,066 --> 00:26:00,236
ensure consistency between these


823
00:26:00,236 --> 00:26:02,136
systems when the user deletes


824
00:26:02,136 --> 00:26:03,686
something in your app.


825
00:26:04,346 --> 00:26:06,686
Now the operating system doesn't


826
00:26:06,686 --> 00:26:08,066
know what's happening inside


827
00:26:08,066 --> 00:26:09,776
your app, so if you've donated


828
00:26:09,776 --> 00:26:11,586
information to Siri Shortcuts or


829
00:26:11,586 --> 00:26:13,666
posted a notification, you


830
00:26:13,666 --> 00:26:14,966
should make sure that you delete


831
00:26:14,996 --> 00:26:16,786
that content when the user


832
00:26:16,786 --> 00:26:18,486
removes it from your app.


833
00:26:20,516 --> 00:26:22,936
For example, if a user deletes


834
00:26:22,936 --> 00:26:25,186
someone in your app's contact


835
00:26:25,186 --> 00:26:27,166
list, Siri Suggestions should


836
00:26:27,166 --> 00:26:28,686
not suggest them to message


837
00:26:28,686 --> 00:26:30,646
using your app.


838
00:26:30,786 --> 00:26:32,826
Or if a user deletes a thread in


839
00:26:32,826 --> 00:26:34,756
your messaging application, you


840
00:26:34,756 --> 00:26:36,166
should delete the notifications


841
00:26:36,166 --> 00:26:37,836
for that content as well as it


842
00:26:37,836 --> 00:26:39,476
would be unexpected for a user


843
00:26:39,476 --> 00:26:40,906
to see notifications still on


844
00:26:40,906 --> 00:26:42,556
their device from a thread they


845
00:26:42,556 --> 00:26:43,436
thought they'd completely


846
00:26:43,436 --> 00:26:43,806
deleted.


847
00:26:44,316 --> 00:26:46,696
And finally, if you're a


848
00:26:46,696 --> 00:26:48,316
passwords manager, and you've


849
00:26:48,316 --> 00:26:50,216
donated passwords to the New


850
00:26:50,216 --> 00:26:52,346
System Passwords API, you should


851
00:26:52,346 --> 00:26:53,326
make sure to delete this


852
00:26:53,326 --> 00:26:55,336
information if the user removes


853
00:26:55,336 --> 00:26:57,566
the site from your password


854
00:26:58,376 --> 00:26:58,756
manager.


855
00:26:59,116 --> 00:27:00,786
Now data stewardship continues


856
00:27:00,786 --> 00:27:02,156
through to device tracking,


857
00:27:02,156 --> 00:27:03,576
which means something very


858
00:27:03,576 --> 00:27:05,216
specific in the context I'll


859
00:27:05,216 --> 00:27:07,736
talk about today.


860
00:27:07,896 --> 00:27:09,546
You might have questions about


861
00:27:09,546 --> 00:27:11,666
the devices that use your apps.


862
00:27:12,406 --> 00:27:14,476
For example, did this device


863
00:27:14,626 --> 00:27:17,806
already consume a free trial, or


864
00:27:17,966 --> 00:27:19,896
was this device previously used


865
00:27:19,896 --> 00:27:21,926
by an abusive user or for


866
00:27:21,926 --> 00:27:23,106
fraudulent activities.


867
00:27:24,596 --> 00:27:26,026
We offer an API called


868
00:27:26,026 --> 00:27:27,826
DeviceCheck that allows you to


869
00:27:27,826 --> 00:27:28,986
answer these questions.


870
00:27:30,716 --> 00:27:32,426
DeviceCheck lets you set two


871
00:27:32,426 --> 00:27:34,536
bits of data per device, which


872
00:27:34,536 --> 00:27:36,616
are stored by Apple and can be


873
00:27:36,616 --> 00:27:38,126
returned to you with a


874
00:27:38,126 --> 00:27:38,666
signature.


875
00:27:40,036 --> 00:27:42,076
These bits persist across a


876
00:27:42,076 --> 00:27:44,546
device reset or a device erase


877
00:27:44,546 --> 00:27:44,966
install.


878
00:27:45,466 --> 00:27:47,626
These bits provide


879
00:27:47,736 --> 00:27:49,816
high-integrity answers to your


880
00:27:49,816 --> 00:27:51,246
questions about a device's


881
00:27:51,286 --> 00:27:54,116
history without exposing unique


882
00:27:54,116 --> 00:27:55,446
device identifiers.


883
00:27:58,016 --> 00:27:59,356
Now, you should adopt


884
00:27:59,356 --> 00:28:01,526
DeviceCheck and not rely on


885
00:28:01,646 --> 00:28:03,346
unsupported device tracking


886
00:28:03,346 --> 00:28:05,126
mechanisms like finger printing.


887
00:28:05,636 --> 00:28:08,406
As Craig said in the keynote, we


888
00:28:08,406 --> 00:28:10,546
continue to remove entropy from


889
00:28:10,546 --> 00:28:12,836
our platform and to remove


890
00:28:12,906 --> 00:28:14,166
functionality that is being


891
00:28:14,166 --> 00:28:16,176
abused to uniquely identify


892
00:28:16,176 --> 00:28:16,676
users.


893
00:28:17,896 --> 00:28:19,556
So you should adopt DeviceCheck


894
00:28:19,606 --> 00:28:20,966
to answer your questions while


895
00:28:20,966 --> 00:28:22,476
being a good data steward.


896
00:28:23,896 --> 00:28:25,166
Now in addition to being a good


897
00:28:25,166 --> 00:28:26,686
data steward in your own apps,


898
00:28:27,786 --> 00:28:28,736
you should consider your


899
00:28:28,936 --> 00:28:30,876
third-party partners as well.


900
00:28:33,276 --> 00:28:35,426
Now, you as developers are


901
00:28:35,426 --> 00:28:37,566
responsible for all of the code


902
00:28:37,696 --> 00:28:38,486
that chips in your app.


903
00:28:39,516 --> 00:28:40,756
This includes code that you've


904
00:28:40,756 --> 00:28:42,246
written but also code that


905
00:28:42,246 --> 00:28:43,566
you've imported in the form of a


906
00:28:43,566 --> 00:28:44,126
library.


907
00:28:45,206 --> 00:28:46,906
So you should understand how


908
00:28:46,906 --> 00:28:48,576
these libraries that you import


909
00:28:49,176 --> 00:28:51,386
access or transfer your user's


910
00:28:51,386 --> 00:28:52,636
data off the device.


911
00:28:53,736 --> 00:28:54,976
This way you can be complete


912
00:28:54,976 --> 00:28:56,186
when giving transparency.


913
00:28:56,636 --> 00:28:57,746
Don't just talk about the code


914
00:28:57,746 --> 00:28:58,306
that you wrote.


915
00:28:58,666 --> 00:28:59,936
You should describe the full


916
00:28:59,936 --> 00:29:01,426
impact on a user's privacy.


917
00:29:01,906 --> 00:29:04,226
And as Joey mentioned earlier,


918
00:29:05,116 --> 00:29:06,806
you should avoid unnecessary


919
00:29:06,806 --> 00:29:08,226
requests for resources.


920
00:29:09,266 --> 00:29:12,036
So, for example, if a library


921
00:29:12,036 --> 00:29:14,076
that you want to use requires an


922
00:29:14,076 --> 00:29:15,926
entitlement or access to some


923
00:29:16,066 --> 00:29:17,876
other sensitive resource that


924
00:29:17,876 --> 00:29:19,646
isn't required from the


925
00:29:19,646 --> 00:29:20,776
functionality you're trying to


926
00:29:20,776 --> 00:29:22,896
get out of this library, you


927
00:29:22,896 --> 00:29:23,956
should either find a different


928
00:29:23,956 --> 00:29:26,016
library or reach out to the


929
00:29:26,016 --> 00:29:27,606
developer of that library and


930
00:29:27,606 --> 00:29:29,056
ask them to remove that


931
00:29:29,056 --> 00:29:30,246
sensitive resource request.


932
00:29:30,926 --> 00:29:34,076
Now thinking about third-parties


933
00:29:34,146 --> 00:29:35,946
extends to your server side as


934
00:29:35,946 --> 00:29:36,246
well.


935
00:29:37,556 --> 00:29:39,356
You should understand how data


936
00:29:39,356 --> 00:29:41,756
flows to all third parties your


937
00:29:41,756 --> 00:29:42,476
server's touch.


938
00:29:43,676 --> 00:29:45,796
Now this includes the full


939
00:29:45,796 --> 00:29:47,516
breadth of systems that support


940
00:29:47,516 --> 00:29:49,976
your app, not just analytics or


941
00:29:49,976 --> 00:29:51,976
advertising networks but the


942
00:29:51,976 --> 00:29:54,016
network security systems, the


943
00:29:54,116 --> 00:29:55,726
email provider that sends


944
00:29:55,796 --> 00:29:58,096
customers password reset emails


945
00:29:58,686 --> 00:30:00,766
or third-party customer support


946
00:30:00,766 --> 00:30:01,446
integrations.


947
00:30:01,916 --> 00:30:04,776
Being a good data steward means


948
00:30:04,836 --> 00:30:06,956
taking responsibility for the


949
00:30:06,956 --> 00:30:09,226
full picture and considering


950
00:30:09,226 --> 00:30:10,536
privacy when considering


951
00:30:10,616 --> 00:30:11,556
partners to work with.


952
00:30:12,146 --> 00:30:15,466
Now for a topic you may have


953
00:30:15,466 --> 00:30:16,266
heard a little bit about,


954
00:30:16,506 --> 00:30:17,056
machine learning.


955
00:30:18,186 --> 00:30:20,216
So across the industry, much of


956
00:30:20,216 --> 00:30:21,476
the talk about machine learning


957
00:30:21,856 --> 00:30:23,596
centers around the performance


958
00:30:23,596 --> 00:30:24,846
characteristics of a new


959
00:30:24,846 --> 00:30:26,976
algorithm or the power of a


960
00:30:26,976 --> 00:30:27,936
cloud-based solution.


961
00:30:28,866 --> 00:30:30,246
And while these are important


962
00:30:30,246 --> 00:30:32,516
technical developments, as Tim


963
00:30:32,516 --> 00:30:33,416
said in his commencement


964
00:30:33,416 --> 00:30:35,776
address, the question that we


965
00:30:35,776 --> 00:30:37,706
ask is not what can we do but


966
00:30:38,366 --> 00:30:38,976
what should we do.


967
00:30:40,206 --> 00:30:42,346
Now this applies particularly to


968
00:30:42,346 --> 00:30:44,256
machine learning, and we've been


969
00:30:44,256 --> 00:30:45,646
working on it for years.


970
00:30:47,496 --> 00:30:49,236
Face ID was built with


971
00:30:49,366 --> 00:30:50,936
privacy-friendly machine


972
00:30:50,936 --> 00:30:51,976
learning at its core.


973
00:30:53,416 --> 00:30:54,896
And we've made it easy for you


974
00:30:54,896 --> 00:30:57,336
to add Face ID authentication to


975
00:30:57,336 --> 00:30:58,496
your apps using the


976
00:30:58,496 --> 00:31:00,156
LocalAuthentication API.


977
00:31:01,606 --> 00:31:03,126
You can take advantage of the


978
00:31:03,126 --> 00:31:05,566
work Apple has done to build


979
00:31:05,566 --> 00:31:07,806
strong biometric authentication


980
00:31:08,206 --> 00:31:09,976
using privacy friendly machine


981
00:31:09,976 --> 00:31:10,756
learning techniques.


982
00:31:12,676 --> 00:31:15,936
Similarly, ARKit uses machine


983
00:31:15,936 --> 00:31:16,996
learning to model the


984
00:31:16,996 --> 00:31:18,486
environment around a user's


985
00:31:18,486 --> 00:31:18,966
device.


986
00:31:19,846 --> 00:31:22,686
And new in ARKit 2, you can


987
00:31:22,686 --> 00:31:25,116
create, persist, and store this


988
00:31:25,116 --> 00:31:26,436
map of the environment in your


989
00:31:26,436 --> 00:31:29,396
own apps, but you should collect


990
00:31:29,396 --> 00:31:31,176
this map only if it's needed for


991
00:31:31,176 --> 00:31:32,836
your feature as this data might


992
00:31:32,836 --> 00:31:33,686
be quite sensitive.


993
00:31:34,166 --> 00:31:36,226
It comprises a representation of


994
00:31:36,226 --> 00:31:38,456
what's around a user.


995
00:31:38,556 --> 00:31:39,786
So if you send it off the


996
00:31:39,786 --> 00:31:41,126
device, it should be expected.


997
00:31:41,646 --> 00:31:42,816
Like if you're playing a game


998
00:31:42,816 --> 00:31:44,186
collaboratively with a shared


999
00:31:44,186 --> 00:31:44,656
object.


1000
00:31:45,766 --> 00:31:47,556
And if you use Game Center, you


1001
00:31:47,556 --> 00:31:48,596
can take advantage of the


1002
00:31:48,596 --> 00:31:51,156
MultipeerConnectivity API, which


1003
00:31:51,156 --> 00:31:52,896
supports end-to-end encryption


1004
00:31:53,636 --> 00:31:55,206
to transfer these models between


1005
00:31:55,206 --> 00:31:55,756
devices.


1006
00:31:57,016 --> 00:32:00,256
Now Face ID and ARKit are good


1007
00:32:00,256 --> 00:32:02,226
examples of features that Apple


1008
00:32:02,226 --> 00:32:04,036
has developed that depend on


1009
00:32:04,116 --> 00:32:05,286
privacy-friendly machine


1010
00:32:05,286 --> 00:32:07,136
learning that you can use in


1011
00:32:07,136 --> 00:32:07,706
your apps.


1012
00:32:09,026 --> 00:32:10,096
But many of you want more


1013
00:32:10,096 --> 00:32:10,766
flexibility.


1014
00:32:12,156 --> 00:32:15,086
Create ML and Core ML allow you


1015
00:32:15,436 --> 00:32:17,156
to build your own features on


1016
00:32:17,156 --> 00:32:17,996
top of machine learning.


1017
00:32:20,356 --> 00:32:23,016
With Create ML and Core ML, it


1018
00:32:23,016 --> 00:32:24,676
is easier than ever to add


1019
00:32:24,916 --> 00:32:26,816
on-device machine learning to


1020
00:32:26,816 --> 00:32:27,406
your apps.


1021
00:32:28,816 --> 00:32:30,686
Create ML allows you to train a


1022
00:32:30,686 --> 00:32:32,286
machine learning model directly


1023
00:32:32,286 --> 00:32:35,016
on your Mac, and Core ML lets


1024
00:32:35,016 --> 00:32:36,586
you then take this model and


1025
00:32:36,586 --> 00:32:38,446
evaluate it directly on a user's


1026
00:32:38,446 --> 00:32:38,936
device.


1027
00:32:40,466 --> 00:32:42,206
This avoids collecting sensitive


1028
00:32:42,206 --> 00:32:44,376
user data to evaluate the model,


1029
00:32:45,136 --> 00:32:46,926
and protecting sensitive data on


1030
00:32:46,926 --> 00:32:48,156
your servers requires a lot of


1031
00:32:48,156 --> 00:32:48,886
engineering work.


1032
00:32:49,496 --> 00:32:51,516
Evaluating a model on a user's


1033
00:32:51,516 --> 00:32:53,546
device can lower your server's


1034
00:32:53,546 --> 00:32:55,296
security requirements and will


1035
00:32:55,296 --> 00:32:56,706
lower your breach risk as you


1036
00:32:56,706 --> 00:32:58,256
don't hold this sensitive


1037
00:32:58,256 --> 00:33:00,036
information in the first place.


1038
00:33:01,096 --> 00:33:03,566
Now these two APIs make adoption


1039
00:33:03,566 --> 00:33:05,276
of on-device machine learning


1040
00:33:05,506 --> 00:33:05,816
easy.


1041
00:33:08,896 --> 00:33:10,486
And you should already be asking


1042
00:33:10,486 --> 00:33:12,086
privacy questions when


1043
00:33:12,086 --> 00:33:13,256
developing features like the


1044
00:33:13,256 --> 00:33:15,926
ones Joey went over earlier, and


1045
00:33:15,926 --> 00:33:17,656
these questions are great, and


1046
00:33:17,656 --> 00:33:19,106
Joey and I use them every day


1047
00:33:19,106 --> 00:33:21,776
doing feature reviews at Apple.


1048
00:33:21,976 --> 00:33:23,116
However, machine learning


1049
00:33:23,116 --> 00:33:24,856
requires you to ask a new set of


1050
00:33:24,916 --> 00:33:26,986
questions to address the same


1051
00:33:27,096 --> 00:33:29,006
underlying privacy goals.


1052
00:33:30,256 --> 00:33:32,836
For example, you should ask does


1053
00:33:32,836 --> 00:33:35,296
my model reveal anything about


1054
00:33:35,296 --> 00:33:36,656
the data it was trained on?


1055
00:33:38,386 --> 00:33:40,456
It's actually possible to invert


1056
00:33:40,536 --> 00:33:42,446
a machine learned model and


1057
00:33:42,446 --> 00:33:44,026
recover much of the data it was


1058
00:33:44,026 --> 00:33:44,516
trained on.


1059
00:33:45,766 --> 00:33:47,446
This could result in unexpected


1060
00:33:47,446 --> 00:33:49,276
disclosure if you ship a model


1061
00:33:49,276 --> 00:33:51,076
with your app and it's inverted


1062
00:33:51,076 --> 00:33:53,096
to expose information about


1063
00:33:53,096 --> 00:33:54,126
people it was trained on.


1064
00:33:54,606 --> 00:33:58,006
Now this is an area of active


1065
00:33:58,076 --> 00:33:59,866
academic research that you can


1066
00:33:59,866 --> 00:34:01,066
learn more about in the paper


1067
00:34:01,066 --> 00:34:03,936
that I've put on this slide.


1068
00:34:04,446 --> 00:34:07,076
Similarly, you should ask, could


1069
00:34:07,076 --> 00:34:09,576
I infer more about my users than


1070
00:34:09,576 --> 00:34:10,196
they expected?


1071
00:34:11,306 --> 00:34:13,456
So users might expect that you'd


1072
00:34:13,456 --> 00:34:15,616
classify activity type via


1073
00:34:15,616 --> 00:34:16,146
sensor data.


1074
00:34:17,326 --> 00:34:19,356
But you should ask, did I


1075
00:34:19,356 --> 00:34:21,136
accidentally encode the fact


1076
00:34:21,626 --> 00:34:23,235
that this specific user uses a


1077
00:34:23,235 --> 00:34:23,746
wheelchair?


1078
00:34:24,255 --> 00:34:26,206
It could be great to offer a


1079
00:34:26,206 --> 00:34:27,406
feature for wheelchair users,


1080
00:34:28,436 --> 00:34:30,366
but this should be clear and


1081
00:34:30,366 --> 00:34:31,156
sold as a feature.


1082
00:34:32,596 --> 00:34:34,126
As with general data collection,


1083
00:34:34,126 --> 00:34:36,606
you should obtain new consent if


1084
00:34:36,606 --> 00:34:38,396
you have a new use case enabled


1085
00:34:38,396 --> 00:34:39,246
by machine learning.


1086
00:34:39,735 --> 00:34:42,746
Now it turns out that two small


1087
00:34:42,746 --> 00:34:44,516
modifications can help mitigate


1088
00:34:44,516 --> 00:34:45,556
both of these issues.


1089
00:34:47,116 --> 00:34:48,866
The first is to ensure that you


1090
00:34:48,866 --> 00:34:50,565
train on the right data.


1091
00:34:51,985 --> 00:34:53,146
This means training on a


1092
00:34:53,146 --> 00:34:55,516
sufficient quantity of diverse


1093
00:34:55,516 --> 00:34:57,646
inputs that were collected with


1094
00:34:57,646 --> 00:34:58,546
the proper consent.


1095
00:34:59,026 --> 00:35:02,226
The second is to keep your model


1096
00:35:02,226 --> 00:35:04,296
complexity proportional to the


1097
00:35:04,296 --> 00:35:05,266
goal that you are trying to


1098
00:35:05,266 --> 00:35:05,636
solve.


1099
00:35:07,146 --> 00:35:08,436
Both of these techniques can


1100
00:35:08,436 --> 00:35:11,146
prevent model overfitting, which


1101
00:35:11,146 --> 00:35:12,916
makes a model inversion or


1102
00:35:12,916 --> 00:35:14,316
unexpected inference more


1103
00:35:14,316 --> 00:35:14,706
likely.


1104
00:35:15,296 --> 00:35:17,936
Now at Apple, we believe that


1105
00:35:17,936 --> 00:35:19,426
considering questions like these


1106
00:35:19,856 --> 00:35:21,326
are an important part of


1107
00:35:21,326 --> 00:35:22,736
building products that users can


1108
00:35:22,736 --> 00:35:23,116
trust.


1109
00:35:24,436 --> 00:35:27,286
Because fundamentally privacy is


1110
00:35:27,286 --> 00:35:27,986
about people.


1111
00:35:29,386 --> 00:35:31,226
It's about building trust with


1112
00:35:31,266 --> 00:35:33,586
your users and respecting your


1113
00:35:33,586 --> 00:35:35,286
users in handling their data.


1114
00:35:36,636 --> 00:35:38,276
By applying the techniques that


1115
00:35:38,276 --> 00:35:39,086
we've gone over in this


1116
00:35:39,086 --> 00:35:41,656
presentation, you too can build


1117
00:35:41,656 --> 00:35:44,516
products with great features and


1118
00:35:44,786 --> 00:35:45,776
great privacy.


1119
00:35:46,286 --> 00:35:49,406
Now in summary, I hope you take


1120
00:35:49,406 --> 00:35:51,046
away three big ideas about


1121
00:35:51,046 --> 00:35:51,546
privacy.


1122
00:35:52,416 --> 00:35:53,886
That privacy is about people,


1123
00:35:54,546 --> 00:35:56,456
that you have to ask the should


1124
00:35:56,456 --> 00:35:58,406
questions, and that you should


1125
00:35:58,406 --> 00:36:00,196
align your data practices with


1126
00:36:00,196 --> 00:36:01,436
the use cases you're trying to


1127
00:36:01,436 --> 00:36:01,796
solve.


1128
00:36:02,386 --> 00:36:05,236
For more information, please


1129
00:36:05,236 --> 00:36:07,116
check out these sessions, and I


1130
00:36:07,116 --> 00:36:08,276
look forward to seeing you at


1131
00:36:08,276 --> 00:36:10,056
our lab after this session so


1132
00:36:10,146 --> 00:36:11,616
that we can help you build


1133
00:36:11,736 --> 00:36:13,276
better apps through better


1134
00:36:13,276 --> 00:36:13,846
privacy.


1135
00:36:14,396 --> 00:36:14,756
Thank you.


1136
00:36:15,516 --> 00:36:19,500
[ Applause ]

