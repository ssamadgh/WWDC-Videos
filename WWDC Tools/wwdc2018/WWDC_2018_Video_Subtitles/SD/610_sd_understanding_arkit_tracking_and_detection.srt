1
00:00:07,516 --> 00:00:16,500
[ Music ]


2
00:00:19,516 --> 00:00:25,500
[ Applause ]


3
00:00:26,456 --> 00:00:27,606
>> Hello, everybody.


4
00:00:27,986 --> 00:00:29,526
I'm very excited to be here


5
00:00:29,526 --> 00:00:31,446
today to talk about


6
00:00:31,446 --> 00:00:34,326
understanding ARKit Tracking and


7
00:00:34,326 --> 00:00:36,496
Detection to empower you to


8
00:00:36,496 --> 00:00:38,796
create great augmented reality


9
00:00:38,796 --> 00:00:39,576
experiences.


10
00:00:40,596 --> 00:00:42,316
My name is Marion and I'm from


11
00:00:42,346 --> 00:00:43,266
the ARKit Team.


12
00:00:43,266 --> 00:00:44,756
And what about you?


13
00:00:46,126 --> 00:00:47,786
Are you an experienced ARKit


14
00:00:47,786 --> 00:00:49,206
developer, already, but you are


15
00:00:49,206 --> 00:00:50,736
interested in what's going on


16
00:00:50,736 --> 00:00:51,276
under the hood?


17
00:00:51,896 --> 00:00:53,316
Then, this talk is for you.


18
00:00:53,316 --> 00:00:56,596
Or you may be new to ARKit.


19
00:00:57,546 --> 00:00:59,606
Then, you'll learn different


20
00:00:59,606 --> 00:01:01,046
kind of tracking technologies,


21
00:01:01,206 --> 00:01:02,876
as well as some basics and


22
00:01:02,966 --> 00:01:04,676
terminology used in augmented


23
00:01:04,676 --> 00:01:06,756
reality, which will then help


24
00:01:06,756 --> 00:01:08,176
you to create your very own


25
00:01:08,176 --> 00:01:09,416
first augmented reality


26
00:01:09,416 --> 00:01:10,016
experience.


27
00:01:10,726 --> 00:01:14,586
So, let's get started.


28
00:01:15,106 --> 00:01:17,926
What's tracking?


29
00:01:18,066 --> 00:01:19,536
Tracking provides your camera


30
00:01:19,836 --> 00:01:22,906
viewing position and orientation


31
00:01:23,156 --> 00:01:24,956
into your physical environment,


32
00:01:25,376 --> 00:01:26,806
which will then allow you to


33
00:01:26,806 --> 00:01:29,036
augment virtual content into


34
00:01:29,036 --> 00:01:30,096
your camera's view.


35
00:01:31,146 --> 00:01:33,656
In this video, for example, the


36
00:01:33,696 --> 00:01:35,836
front table and the chairs is


37
00:01:35,836 --> 00:01:39,206
virtual content augmented on top


38
00:01:39,926 --> 00:01:41,416
of the real physical terrace.


39
00:01:42,756 --> 00:01:44,006
This, by the way, is Ikea.


40
00:01:45,296 --> 00:01:47,186
And the virtual content will


41
00:01:47,186 --> 00:01:48,926
appear always virtually correct.


42
00:01:49,746 --> 00:01:52,086
Correct placement, correct size,


43
00:01:52,686 --> 00:01:53,816
and correct perspective


44
00:01:53,816 --> 00:01:54,516
appearance.


45
00:01:55,066 --> 00:01:56,836
So, different tracking


46
00:01:56,836 --> 00:01:58,906
technologies are just providing


47
00:01:59,076 --> 00:02:00,606
a difference reference system


48
00:02:00,906 --> 00:02:01,586
for the camera.


49
00:02:01,816 --> 00:02:03,556
Meaning the camera with respect


50
00:02:03,556 --> 00:02:05,246
to your world, the camera with


51
00:02:05,246 --> 00:02:07,106
respect to an image, or maybe, a


52
00:02:07,106 --> 00:02:07,916
3D object.


53
00:02:09,086 --> 00:02:11,186
And we'll talk about those


54
00:02:11,296 --> 00:02:12,396
different kind of tracking


55
00:02:12,396 --> 00:02:14,116
technologies in the next hour,


56
00:02:14,936 --> 00:02:16,476
such that you'll be able to make


57
00:02:16,476 --> 00:02:17,626
the right choice for your


58
00:02:17,626 --> 00:02:18,796
specific use case.


59
00:02:19,216 --> 00:02:22,296
We'll talk about the already


60
00:02:22,296 --> 00:02:24,196
existing AR technologies'


61
00:02:24,486 --> 00:02:26,726
Orientation Tracking, World


62
00:02:26,866 --> 00:02:28,876
Tracking, and Plane Detection.


63
00:02:29,616 --> 00:02:31,376
Before we then have a close look


64
00:02:31,496 --> 00:02:33,466
at our new tracking and


65
00:02:33,466 --> 00:02:36,536
detection technologies which


66
00:02:36,536 --> 00:02:38,076
came out now with ARKit 2.


67
00:02:38,696 --> 00:02:40,316
Which are saving and loading


68
00:02:40,316 --> 00:02:43,126
maps, image tracking, and object


69
00:02:43,186 --> 00:02:43,776
detection.


70
00:02:45,266 --> 00:02:46,926
But before diving deep into


71
00:02:46,926 --> 00:02:48,886
those technologies, let's start


72
00:02:48,946 --> 00:02:51,416
with a very short recap of ARKit


73
00:02:51,416 --> 00:02:52,646
like on a high level.


74
00:02:53,176 --> 00:02:54,166
This is, specifically,


75
00:02:54,166 --> 00:02:55,936
interesting if you are new to


76
00:02:55,936 --> 00:02:56,276
ARKit.


77
00:02:56,786 --> 00:03:00,556
So, the first thing you'll do is


78
00:03:00,556 --> 00:03:01,826
create an ARSession.


79
00:03:02,496 --> 00:03:04,576
An ARSession is the object that


80
00:03:04,576 --> 00:03:06,946
handles everything from


81
00:03:06,946 --> 00:03:09,686
configuring to running the AR


82
00:03:09,686 --> 00:03:11,226
technologies.


83
00:03:11,476 --> 00:03:14,226
And also, returning the results


84
00:03:14,226 --> 00:03:15,696
of the AR technologies.


85
00:03:16,196 --> 00:03:19,406
You then, have to describe what


86
00:03:19,496 --> 00:03:20,796
kind of technologies you


87
00:03:20,856 --> 00:03:21,866
actually want to run.


88
00:03:22,266 --> 00:03:23,256
Like, what kind of tracking


89
00:03:23,256 --> 00:03:25,136
technologies and what kind of


90
00:03:25,136 --> 00:03:26,376
features should be enabled, like


91
00:03:26,436 --> 00:03:28,096
Plane Detection, for example.


92
00:03:28,966 --> 00:03:32,506
You'll then, take this specific


93
00:03:32,506 --> 00:03:35,406
ARConfiguration and call run


94
00:03:36,256 --> 00:03:39,476
method on your instance of the


95
00:03:39,476 --> 00:03:40,116
ARSession.


96
00:03:41,576 --> 00:03:43,206
Then, the ARSession, internally,


97
00:03:44,306 --> 00:03:47,336
will start configuring an


98
00:03:47,336 --> 00:03:49,596
AVCaptureSession to start


99
00:03:50,096 --> 00:03:52,716
receiving the images, as well as


100
00:03:52,716 --> 00:03:55,096
a call motion manager to begin


101
00:03:55,096 --> 00:03:56,816
receiving the motion sensor, so,


102
00:03:57,326 --> 00:03:57,616
data.


103
00:03:57,616 --> 00:03:58,626
So, this is, basically, the


104
00:03:58,626 --> 00:04:01,496
built-in input system from your


105
00:04:01,496 --> 00:04:02,666
device for ARKit.


106
00:04:04,066 --> 00:04:07,066
Now, after processing the


107
00:04:07,066 --> 00:04:09,306
results are returned in ARFrames


108
00:04:09,306 --> 00:04:10,926
at 60 frames per second.


109
00:04:12,126 --> 00:04:14,226
An ARFrame is a snapshot in time


110
00:04:14,226 --> 00:04:15,396
which gives you everything you


111
00:04:15,396 --> 00:04:17,055
need to render your augmented


112
00:04:17,055 --> 00:04:17,875
reality scene.


113
00:04:18,366 --> 00:04:20,776
Like, the captured camera image,


114
00:04:20,776 --> 00:04:22,326
which would then be, which will


115
00:04:22,326 --> 00:04:23,576
be rendered in the background of


116
00:04:23,576 --> 00:04:25,176
your augmented reality scenario.


117
00:04:25,826 --> 00:04:27,136
As well as a track camera


118
00:04:27,136 --> 00:04:29,456
motion, which will then be


119
00:04:29,456 --> 00:04:31,256
applied to your virtual camera


120
00:04:31,706 --> 00:04:34,406
to render the virtual content


121
00:04:34,446 --> 00:04:36,746
from the same perspective as the


122
00:04:36,746 --> 00:04:37,586
physical camera.


123
00:04:38,766 --> 00:04:40,426
It also contains information


124
00:04:40,876 --> 00:04:42,086
about the environment.


125
00:04:42,086 --> 00:04:43,146
Like, for example, detected


126
00:04:43,146 --> 00:04:43,536
plates.


127
00:04:43,536 --> 00:04:46,806
So, let's now start with our


128
00:04:46,806 --> 00:04:48,436
first tracking technology and


129
00:04:48,436 --> 00:04:49,216
build up from there.


130
00:04:51,586 --> 00:04:53,176
Orientation Tracking.


131
00:04:54,386 --> 00:04:56,396
Orientation Tracking tracks,


132
00:04:56,536 --> 00:04:56,976
guess what?


133
00:04:57,176 --> 00:04:58,156
Orientation.


134
00:04:58,676 --> 00:05:00,746
Meaning it tracks the rotation,


135
00:05:00,746 --> 00:05:01,226
only.


136
00:05:02,136 --> 00:05:03,206
You can think about it as you


137
00:05:03,206 --> 00:05:05,456
can only use your hat to view


138
00:05:05,456 --> 00:05:06,916
virtual content, which also,


139
00:05:07,156 --> 00:05:09,046
only allows rotation.


140
00:05:09,966 --> 00:05:11,716
Meaning you can experience the


141
00:05:11,866 --> 00:05:13,406
virtual content from the same


142
00:05:13,406 --> 00:05:15,656
positional point of view, but no


143
00:05:15,656 --> 00:05:17,276
change in the position is going


144
00:05:17,276 --> 00:05:17,856
to be tracked.


145
00:05:19,596 --> 00:05:21,546
The rotation data is tracked


146
00:05:21,546 --> 00:05:22,836
around three axles.


147
00:05:22,946 --> 00:05:24,276
That's why it's also, sometimes,


148
00:05:24,316 --> 00:05:25,676
called the three degrees of


149
00:05:25,676 --> 00:05:26,416
freedom tracking.


150
00:05:26,416 --> 00:05:28,926
You can use it, for example, in


151
00:05:28,926 --> 00:05:30,846
a spherical virtual environment.


152
00:05:30,846 --> 00:05:32,256
Like, for example, experience a


153
00:05:32,256 --> 00:05:35,066
360-degree video, in which the


154
00:05:35,066 --> 00:05:36,496
virtual content can be viewed


155
00:05:36,496 --> 00:05:38,116
from the same positional point.


156
00:05:39,346 --> 00:05:41,226
You can also, use it to augment


157
00:05:41,226 --> 00:05:43,196
objects that are very far away.


158
00:05:44,336 --> 00:05:46,296
Orientation Tracking is not


159
00:05:46,396 --> 00:05:48,236
suited for physical world


160
00:05:48,236 --> 00:05:49,746
augmentation, in which you want


161
00:05:49,746 --> 00:05:50,886
to view the content from


162
00:05:50,886 --> 00:05:52,666
different points of views.


163
00:05:54,226 --> 00:05:56,416
So, let's now have a look at


164
00:05:56,556 --> 00:05:58,016
what happens under the hood when


165
00:05:58,016 --> 00:05:59,486
Orientation Tracking is running.


166
00:06:00,036 --> 00:06:02,676
It is, actually, quite simple.


167
00:06:03,106 --> 00:06:05,016
It only uses the rotation data


168
00:06:05,016 --> 00:06:07,166
from core motion, which applies


169
00:06:07,326 --> 00:06:09,146
sensor fusion to the motion


170
00:06:09,146 --> 00:06:09,986
sensors data.


171
00:06:11,426 --> 00:06:13,626
As motion data is provided at a


172
00:06:13,756 --> 00:06:15,696
higher frequency than the camera


173
00:06:15,696 --> 00:06:17,936
image, Orientation Tracking


174
00:06:18,206 --> 00:06:20,736
takes the latest motion data


175
00:06:20,736 --> 00:06:22,616
from commotion, once the camera


176
00:06:22,616 --> 00:06:23,636
image is available.


177
00:06:23,776 --> 00:06:25,766
And then, returns both results


178
00:06:26,086 --> 00:06:27,206
in an ARFrame.


179
00:06:27,526 --> 00:06:28,066
So, that's it.


180
00:06:28,126 --> 00:06:28,956
Very simple.


181
00:06:29,686 --> 00:06:31,726
So, please note that the camera


182
00:06:31,726 --> 00:06:33,266
feed is not processed in


183
00:06:33,266 --> 00:06:34,526
Orientation Tracking.


184
00:06:34,856 --> 00:06:35,956
Meaning there's no computer


185
00:06:35,956 --> 00:06:37,036
version under the hood here.


186
00:06:38,286 --> 00:06:40,376
Now, to run Orientation Tracking


187
00:06:40,986 --> 00:06:43,096
you only need to configure your


188
00:06:43,286 --> 00:06:45,876
ARSession with an AROrientation


189
00:06:45,876 --> 00:06:47,256
TrackingConfiguration.


190
00:06:48,266 --> 00:06:49,686
The results will then be


191
00:06:49,686 --> 00:06:52,216
returned in an ARCamera object


192
00:06:52,996 --> 00:06:55,116
provided by the ARFrames.


193
00:06:55,116 --> 00:06:57,926
Now, an ARCamera object always


194
00:06:57,926 --> 00:06:59,826
contains the transform, which in


195
00:06:59,826 --> 00:07:01,106
this case of Orientation


196
00:07:01,106 --> 00:07:02,936
Tracking, only contains the


197
00:07:02,936 --> 00:07:05,006
rotation data of your tracked


198
00:07:05,116 --> 00:07:06,016
physical camera.


199
00:07:07,096 --> 00:07:09,616
Alternatively, the rotation is


200
00:07:09,666 --> 00:07:11,596
also represented in eulerAngles.


201
00:07:12,276 --> 00:07:13,956
You can use whatever fits best


202
00:07:14,066 --> 00:07:14,306
to you.


203
00:07:16,866 --> 00:07:18,686
Let's now move over to more


204
00:07:18,686 --> 00:07:20,326
advanced tracking technologies.


205
00:07:21,156 --> 00:07:22,546
We'll start with World Tracking.


206
00:07:23,076 --> 00:07:25,406
World Tracking tracks your


207
00:07:25,676 --> 00:07:28,186
camera viewing orientation, and


208
00:07:28,186 --> 00:07:30,096
also, the change in position


209
00:07:30,316 --> 00:07:32,056
into your physical environment


210
00:07:32,326 --> 00:07:34,196
without any prior information


211
00:07:34,196 --> 00:07:35,226
about your environment.


212
00:07:36,266 --> 00:07:37,656
Here, you can see on the left


213
00:07:37,656 --> 00:07:41,316
side the real camera's view into


214
00:07:41,316 --> 00:07:42,766
the environment, while on the


215
00:07:42,766 --> 00:07:45,416
right side you see the tracked


216
00:07:45,546 --> 00:07:47,466
camera motion while exploring


217
00:07:47,466 --> 00:07:50,506
the world represented in the


218
00:07:50,506 --> 00:07:51,756
coordinate system.


219
00:07:52,986 --> 00:07:54,406
Let's now explain better, what


220
00:07:54,506 --> 00:07:55,466
happens here, when World


221
00:07:55,536 --> 00:07:56,376
Tracking is running.


222
00:07:56,896 --> 00:08:00,406
World Tracking uses a motion


223
00:08:00,406 --> 00:08:03,236
sensor, the motion data of your


224
00:08:03,236 --> 00:08:05,436
device's accelerometer and


225
00:08:05,436 --> 00:08:08,376
gyroscope to compute its change


226
00:08:08,376 --> 00:08:10,796
in orientation and translation


227
00:08:11,116 --> 00:08:12,436
on a high frequency.


228
00:08:14,706 --> 00:08:16,926
It also provides its information


229
00:08:17,116 --> 00:08:19,276
in correct scale in Metal.


230
00:08:20,656 --> 00:08:23,056
In literature, just this part of


231
00:08:23,106 --> 00:08:24,306
the tracking system is also


232
00:08:24,306 --> 00:08:25,976
called Inertial Odometry.


233
00:08:27,076 --> 00:08:29,046
While this motion data provides


234
00:08:29,106 --> 00:08:31,026
good motion information for


235
00:08:31,026 --> 00:08:32,826
movement across small time


236
00:08:32,826 --> 00:08:34,226
intervals and whenever there's


237
00:08:34,275 --> 00:08:36,566
like, sudden movement, it does


238
00:08:36,645 --> 00:08:38,635
drift over larger time intervals


239
00:08:39,015 --> 00:08:40,456
as the data is not ideally


240
00:08:40,616 --> 00:08:42,126
precise and subject to


241
00:08:42,126 --> 00:08:43,296
cumulative errors.


242
00:08:44,496 --> 00:08:45,616
That's why it cannot be used


243
00:08:45,906 --> 00:08:47,446
just by its own for tracking.


244
00:08:48,036 --> 00:08:51,456
Now, to compensate this drift,


245
00:08:51,696 --> 00:08:53,416
World Tracking, additionally,


246
00:08:53,516 --> 00:08:55,506
applies a computer version


247
00:08:55,506 --> 00:08:58,656
process in which it uses the


248
00:08:58,656 --> 00:08:59,606
camera frames.


249
00:09:00,706 --> 00:09:02,426
This technology provides a


250
00:09:02,506 --> 00:09:05,276
higher accuracy, but at the cost


251
00:09:05,276 --> 00:09:06,546
of computation time.


252
00:09:08,056 --> 00:09:10,176
Also, this technology is


253
00:09:10,226 --> 00:09:12,596
sensitive to fast camera motions


254
00:09:12,666 --> 00:09:14,396
and this results in motion blur


255
00:09:14,396 --> 00:09:15,296
in the camera frames.


256
00:09:16,516 --> 00:09:18,626
Now, this version only part of


257
00:09:18,626 --> 00:09:20,926
the system is also called Visual


258
00:09:20,926 --> 00:09:21,666
Odometry.


259
00:09:22,016 --> 00:09:24,276
Now, by fusing those both


260
00:09:24,276 --> 00:09:26,576
systems, computer vision and


261
00:09:26,576 --> 00:09:28,916
motion, ARKit takes the best of


262
00:09:28,986 --> 00:09:30,016
those both systems.


263
00:09:30,646 --> 00:09:32,086
From computer version, it takes


264
00:09:32,086 --> 00:09:34,116
a high accuracy over the larger


265
00:09:34,116 --> 00:09:34,946
time intervals.


266
00:09:35,566 --> 00:09:37,576
And from the motion data it


267
00:09:37,576 --> 00:09:39,586
takes the high update rates and


268
00:09:39,586 --> 00:09:41,356
good precision for the smaller


269
00:09:41,356 --> 00:09:43,446
time intervals, as well as the


270
00:09:43,446 --> 00:09:44,276
metric scale.


271
00:09:44,856 --> 00:09:47,456
Now, by combining those both


272
00:09:47,456 --> 00:09:49,446
systems World Tracking can skip


273
00:09:49,586 --> 00:09:51,036
the computer version processing


274
00:09:51,246 --> 00:09:52,886
for some of those frames, while


275
00:09:52,886 --> 00:09:54,736
still keeping an efficient and


276
00:09:54,776 --> 00:09:55,966
responsive tracking.


277
00:09:56,956 --> 00:09:58,856
This frees CPU resources, which


278
00:09:58,856 --> 00:10:00,356
you can then, additionally, use


279
00:10:00,356 --> 00:10:00,956
for your apps.


280
00:10:02,876 --> 00:10:04,376
In Literature, this combined


281
00:10:04,376 --> 00:10:06,456
technology is also called Visual


282
00:10:06,506 --> 00:10:07,696
Inertial Odometry.


283
00:10:08,916 --> 00:10:11,456
Let's have a closer look at the


284
00:10:11,456 --> 00:10:13,306
visual part of it.


285
00:10:14,116 --> 00:10:15,926
So, within the computer version


286
00:10:15,926 --> 00:10:19,286
process interesting regions in


287
00:10:19,286 --> 00:10:21,096
the camera images I extracted,


288
00:10:21,356 --> 00:10:22,696
like here, the blue and the


289
00:10:22,696 --> 00:10:23,526
orange dot.


290
00:10:24,406 --> 00:10:26,006
And they are extracted such that


291
00:10:26,006 --> 00:10:27,496
they can robustly all to be


292
00:10:27,946 --> 00:10:30,246
extracted and other images of


293
00:10:30,286 --> 00:10:31,516
the same environment.


294
00:10:33,016 --> 00:10:34,096
Those interesting regions are


295
00:10:34,096 --> 00:10:35,166
also called features.


296
00:10:36,496 --> 00:10:37,616
Now, those features are then


297
00:10:37,616 --> 00:10:39,996
matched between multiple images


298
00:10:40,196 --> 00:10:42,326
over the camera stream based on


299
00:10:42,326 --> 00:10:43,686
their similarity and their


300
00:10:43,686 --> 00:10:44,326
appearance.


301
00:10:45,176 --> 00:10:46,716
And what then happens is pretty


302
00:10:46,716 --> 00:10:48,566
much how you are able to see 3D


303
00:10:48,566 --> 00:10:49,286
with your eyes.


304
00:10:50,176 --> 00:10:51,576
You have two of them and they


305
00:10:51,576 --> 00:10:53,516
are within the sidewise small


306
00:10:53,516 --> 00:10:54,226
distance.


307
00:10:55,056 --> 00:10:56,976
And this parallax between the


308
00:10:56,976 --> 00:10:58,796
eyes is important as this


309
00:10:58,796 --> 00:11:00,636
results in slightly different


310
00:11:00,636 --> 00:11:02,236
views into the environment,


311
00:11:02,606 --> 00:11:04,706
which allows you to see stereo


312
00:11:04,816 --> 00:11:06,056
and perceive the depth.


313
00:11:07,106 --> 00:11:08,406
And this is what ARKit now,


314
00:11:08,406 --> 00:11:10,176
also, does with the different


315
00:11:10,176 --> 00:11:12,076
views of the same camera stream


316
00:11:12,076 --> 00:11:12,956
during the process of


317
00:11:13,026 --> 00:11:14,156
triangulation.


318
00:11:14,736 --> 00:11:16,206
And it does it once there's


319
00:11:16,256 --> 00:11:18,046
enough parallax present.


320
00:11:18,896 --> 00:11:20,786
It computes the missing depth


321
00:11:20,786 --> 00:11:22,736
information for those matched


322
00:11:22,816 --> 00:11:23,306
features.


323
00:11:23,706 --> 00:11:26,826
Meaning those 2D features from


324
00:11:26,826 --> 00:11:28,556
the image are now reconstructed


325
00:11:28,556 --> 00:11:29,316
in 3D.


326
00:11:30,806 --> 00:11:32,066
Please, note that this


327
00:11:32,066 --> 00:11:34,476
reconstruction to be successful,


328
00:11:35,706 --> 00:11:37,536
the camera position must have


329
00:11:37,676 --> 00:11:39,776
changed by a translation to


330
00:11:39,776 --> 00:11:41,316
provide enough parallax.


331
00:11:42,356 --> 00:11:44,086
For example, with the sidewise


332
00:11:44,086 --> 00:11:44,626
movement.


333
00:11:44,966 --> 00:11:47,746
The pure rotation does not give


334
00:11:47,746 --> 00:11:48,856
enough information here.


335
00:11:50,536 --> 00:11:52,606
So, this is your first small map


336
00:11:52,606 --> 00:11:53,646
of your environment.


337
00:11:53,646 --> 00:11:55,826
In ARKit we call this a World


338
00:11:55,906 --> 00:11:56,136
map.


339
00:11:57,396 --> 00:11:59,826
In this same moment, also, the


340
00:11:59,826 --> 00:12:01,626
camera's positions and


341
00:12:01,626 --> 00:12:04,226
orientations of your sequences


342
00:12:04,226 --> 00:12:06,546
are computed, denoted with a C


343
00:12:06,546 --> 00:12:06,766
here.


344
00:12:07,476 --> 00:12:08,546
Meaning, your World Tracking


345
00:12:08,546 --> 00:12:09,396
just initialized.


346
00:12:09,396 --> 00:12:10,496
This is the moment of


347
00:12:10,526 --> 00:12:12,496
initialization of the tracking


348
00:12:12,496 --> 00:12:12,746
system.


349
00:12:12,746 --> 00:12:15,886
Please note that also in this


350
00:12:15,886 --> 00:12:17,326
moment of this initial


351
00:12:17,326 --> 00:12:19,216
reconstruction of the World map,


352
00:12:19,406 --> 00:12:21,246
the world origin was defined.


353
00:12:21,986 --> 00:12:23,676
And it is set to the first


354
00:12:23,886 --> 00:12:25,846
camera's origin of the


355
00:12:25,846 --> 00:12:27,326
triangulated frames.


356
00:12:28,046 --> 00:12:29,896
And it is also set to be gravity


357
00:12:29,896 --> 00:12:30,296
aligned.


358
00:12:31,056 --> 00:12:33,446
It's denoted with a W in the


359
00:12:34,096 --> 00:12:34,286
slides.


360
00:12:34,906 --> 00:12:35,786
So, you now have a small


361
00:12:35,786 --> 00:12:37,246
representation of your real


362
00:12:37,246 --> 00:12:39,366
environment reconstructed as a


363
00:12:39,366 --> 00:12:40,896
World map in its own world


364
00:12:40,896 --> 00:12:42,046
coordinates system.


365
00:12:42,836 --> 00:12:44,446
And you have your current camera


366
00:12:44,596 --> 00:12:46,646
tracked with respect to the same


367
00:12:46,646 --> 00:12:48,646
world coordinate system.


368
00:12:50,896 --> 00:12:53,186
You can now start adding virtual


369
00:12:53,186 --> 00:12:56,436
content to augment them into the


370
00:12:56,436 --> 00:12:57,056
camera's view.


371
00:12:58,656 --> 00:13:01,016
Now, to place virtual content


372
00:13:01,066 --> 00:13:03,416
correctly to an ARSession, you


373
00:13:03,416 --> 00:13:06,256
should use ARAnchors from ARKit,


374
00:13:06,636 --> 00:13:07,826
which are denoted with an A


375
00:13:07,826 --> 00:13:08,096
here.


376
00:13:09,536 --> 00:13:12,326
ARAnchors are reference points


377
00:13:12,536 --> 00:13:14,076
within this World map, within


378
00:13:14,076 --> 00:13:15,816
this world coordinates system.


379
00:13:16,486 --> 00:13:18,386
And you should use them because


380
00:13:18,386 --> 00:13:20,686
the World Tracking might update


381
00:13:20,686 --> 00:13:22,206
them during the tracking.


382
00:13:22,206 --> 00:13:23,636
Meaning that, also, all the


383
00:13:23,636 --> 00:13:25,336
virtual content that is assigned


384
00:13:25,336 --> 00:13:27,776
to it will be updated and


385
00:13:27,886 --> 00:13:31,626
correctly augmented into the


386
00:13:32,456 --> 00:13:32,746
camera's view.


387
00:13:32,746 --> 00:13:34,446
So, now that you've used the


388
00:13:34,446 --> 00:13:36,436
ARAnchors you can add virtual


389
00:13:36,466 --> 00:13:38,356
content to the anchor, which


390
00:13:38,386 --> 00:13:40,496
will them be augmented correctly


391
00:13:40,886 --> 00:13:44,116
into the current camera's view.


392
00:13:45,576 --> 00:13:48,676
From now on, this created 3D


393
00:13:48,676 --> 00:13:51,026
World map of your environment is


394
00:13:51,026 --> 00:13:52,526
your reference system for the


395
00:13:52,526 --> 00:13:53,236
World Tracking.


396
00:13:54,046 --> 00:13:55,506
It is used to reference new


397
00:13:55,506 --> 00:13:56,456
images against.


398
00:13:57,076 --> 00:13:58,796
And features are matched from


399
00:13:58,796 --> 00:14:01,806
image to image and triangulated.


400
00:14:02,666 --> 00:14:04,216
And at the same time, also, new


401
00:14:04,216 --> 00:14:05,776
robust features are extracted,


402
00:14:06,106 --> 00:14:08,246
matched, and triangulated, which


403
00:14:08,246 --> 00:14:10,496
are then extending your World


404
00:14:10,556 --> 00:14:10,786
map.


405
00:14:11,286 --> 00:14:13,346
Meaning ARKit is learning your


406
00:14:13,346 --> 00:14:14,036
environment.


407
00:14:15,926 --> 00:14:17,006
This then allows, again, the


408
00:14:17,006 --> 00:14:18,686
computation of tracking updates


409
00:14:18,946 --> 00:14:20,936
of the current camera's position


410
00:14:20,936 --> 00:14:21,956
and orientation.


411
00:14:23,086 --> 00:14:24,986
And finally, the correct


412
00:14:24,986 --> 00:14:26,796
augmentation into the current


413
00:14:26,796 --> 00:14:27,426
camera's view.


414
00:14:27,946 --> 00:14:31,966
While you continue to explore


415
00:14:31,966 --> 00:14:33,786
the world, World Tracking will


416
00:14:33,786 --> 00:14:35,706
continue to track your physical


417
00:14:35,706 --> 00:14:37,826
camera and continue to learn


418
00:14:38,076 --> 00:14:39,426
your physical environment.


419
00:14:40,506 --> 00:14:42,866
But over time, the augmentation


420
00:14:42,866 --> 00:14:45,506
might drift slightly, which can


421
00:14:45,506 --> 00:14:47,366
be noticed like you can see in


422
00:14:47,366 --> 00:14:49,096
the left image, in a small


423
00:14:49,096 --> 00:14:51,086
offset of the augmentation.


424
00:14:52,366 --> 00:14:54,736
This is because even small


425
00:14:55,216 --> 00:14:58,016
offsets, even small errors will


426
00:14:58,016 --> 00:14:59,786
become noticeable when


427
00:14:59,786 --> 00:15:01,496
accumulated over time.


428
00:15:03,486 --> 00:15:05,286
Now, when the device comes back


429
00:15:05,426 --> 00:15:07,216
to a similar view, which was


430
00:15:07,216 --> 00:15:09,036
already explored before, like


431
00:15:09,086 --> 00:15:10,956
for example, the starting point


432
00:15:10,956 --> 00:15:11,756
where we started the


433
00:15:11,756 --> 00:15:14,116
exploration, ARKit can perform


434
00:15:14,116 --> 00:15:15,846
another optimization step.


435
00:15:16,546 --> 00:15:18,086
And this addition makes, a


436
00:15:18,086 --> 00:15:19,496
Visual Intertial Odometry


437
00:15:19,496 --> 00:15:21,986
system, makes the system that


438
00:15:21,986 --> 00:15:24,176
ARKit supplies to a Visual


439
00:15:24,286 --> 00:15:26,206
Inertial SLAM System.


440
00:15:27,376 --> 00:15:28,826
So, let's bring back this first


441
00:15:28,916 --> 00:15:30,726
image where the World Tracking


442
00:15:30,726 --> 00:15:32,476
started the exploration.


443
00:15:33,956 --> 00:15:35,136
So, what happens now is that


444
00:15:35,136 --> 00:15:37,166
World Tracking will check how


445
00:15:37,166 --> 00:15:39,246
well the tracking information


446
00:15:39,446 --> 00:15:41,076
and the World map of the current


447
00:15:41,076 --> 00:15:43,666
view aligns with the past views,


448
00:15:43,926 --> 00:15:45,276
like the one from the beginning.


449
00:15:45,276 --> 00:15:48,586
And will then perform the


450
00:15:48,586 --> 00:15:51,886
optimization step and align the


451
00:15:52,066 --> 00:15:54,316
current information and the


452
00:15:54,316 --> 00:15:56,306
current World map with your real


453
00:15:56,306 --> 00:15:57,666
physical environment.


454
00:15:58,816 --> 00:16:00,256
Have you noticed that during


455
00:16:00,256 --> 00:16:01,946
this step, also the ARAnchor was


456
00:16:01,946 --> 00:16:02,536
updated?


457
00:16:03,006 --> 00:16:04,476
And that is the reason why you


458
00:16:04,476 --> 00:16:07,036
should use ARAnchors when adding


459
00:16:07,036 --> 00:16:08,426
virtual content to your


460
00:16:08,826 --> 00:16:09,436
scenario.


461
00:16:09,956 --> 00:16:14,396
In this video, you can see the


462
00:16:14,396 --> 00:16:16,956
same step again with a real


463
00:16:16,956 --> 00:16:17,716
camera feed.


464
00:16:18,116 --> 00:16:19,396
On the left side you see the


465
00:16:19,396 --> 00:16:20,736
camera's view into the


466
00:16:20,736 --> 00:16:22,596
environment, and also, features


467
00:16:22,596 --> 00:16:25,026
which are tracked in the images.


468
00:16:25,406 --> 00:16:26,986
And on the right side, you see a


469
00:16:26,986 --> 00:16:28,356
bird eye's view onto the


470
00:16:28,356 --> 00:16:30,596
scenario, showing what ARKit


471
00:16:30,596 --> 00:16:33,166
knows about it and showing the


472
00:16:34,016 --> 00:16:35,726
3D reconstruction of the


473
00:16:35,726 --> 00:16:36,386
environment.


474
00:16:36,996 --> 00:16:39,506
The colors of the points are


475
00:16:39,506 --> 00:16:41,346
just encoding the height of the


476
00:16:41,346 --> 00:16:43,306
reconstructed points with blue


477
00:16:43,306 --> 00:16:45,036
being the ground floor and red


478
00:16:45,096 --> 00:16:46,746
being the table and the chairs.


479
00:16:47,376 --> 00:16:51,216
Once the camera returns back to


480
00:16:51,216 --> 00:16:52,576
a similar view it has seen


481
00:16:52,576 --> 00:16:54,546
before, like here the starting


482
00:16:54,546 --> 00:16:56,636
point, ARKit will now apply this


483
00:16:56,636 --> 00:16:57,896
optimization step.


484
00:16:57,996 --> 00:16:59,436
So, just pay attention to the


485
00:16:59,436 --> 00:17:00,716
point cloud and the camera


486
00:17:00,796 --> 00:17:01,416
trajectory.


487
00:17:02,826 --> 00:17:04,106
Have you noticed the update?


488
00:17:04,556 --> 00:17:05,506
Let me show you, once more.


489
00:17:05,996 --> 00:17:10,866
This update aligns the ARKit


490
00:17:10,866 --> 00:17:12,175
knowledge with your real


491
00:17:12,286 --> 00:17:15,016
physical world, and also, the


492
00:17:15,016 --> 00:17:17,536
camera movement and results in


493
00:17:17,536 --> 00:17:19,425
the better augmentation for the


494
00:17:19,425 --> 00:17:20,626
coming camera frames.


495
00:17:21,955 --> 00:17:23,306
By the way, all those


496
00:17:23,306 --> 00:17:24,935
computations of World Tracking,


497
00:17:25,435 --> 00:17:27,425
and also, all this information


498
00:17:27,425 --> 00:17:28,886
about your learned environment,


499
00:17:29,346 --> 00:17:31,076
everything is done on your


500
00:17:31,076 --> 00:17:31,996
device only.


501
00:17:32,136 --> 00:17:33,496
And all this information, also,


502
00:17:33,496 --> 00:17:35,096
stays on your device only.


503
00:17:35,096 --> 00:17:37,836
So, how can you use this complex


504
00:17:37,836 --> 00:17:40,566
technology, now, in your app?


505
00:17:41,926 --> 00:17:45,606
It is actually quite simple.


506
00:17:45,716 --> 00:17:47,486
To run World Tracking you just


507
00:17:47,526 --> 00:17:49,766
configure your ARSession with an


508
00:17:49,816 --> 00:17:51,896
ARWorldTrackingConfiguration.


509
00:17:52,976 --> 00:17:55,376
Again, its results are retuned


510
00:17:55,376 --> 00:17:57,336
in an ARCamera object of the


511
00:17:57,336 --> 00:17:57,956
ARFrame.


512
00:18:00,216 --> 00:18:03,636
An ARCamera object, again,


513
00:18:03,636 --> 00:18:05,626
contains the transform, which in


514
00:18:05,626 --> 00:18:06,946
this case of World Tracking


515
00:18:07,496 --> 00:18:08,806
contains, additionally, to the


516
00:18:08,806 --> 00:18:11,016
rotation, also, the translation


517
00:18:11,016 --> 00:18:12,116
of the track camera.


518
00:18:13,286 --> 00:18:15,106
Additionally, the ARCamera also


519
00:18:15,106 --> 00:18:16,886
contains information about the


520
00:18:16,886 --> 00:18:18,556
tracking state and


521
00:18:18,556 --> 00:18:19,746
trackingStateReason.


522
00:18:20,166 --> 00:18:22,366
This will provide some


523
00:18:22,366 --> 00:18:24,206
information about the current


524
00:18:24,306 --> 00:18:25,216
tracking quality.


525
00:18:26,736 --> 00:18:27,976
So, tracking quality.


526
00:18:28,516 --> 00:18:29,906
Have you ever experienced


527
00:18:30,016 --> 00:18:32,376
opening an AR app and the


528
00:18:32,376 --> 00:18:34,476
tracking worked very poorly or


529
00:18:34,476 --> 00:18:35,646
maybe it didn't work at all?


530
00:18:36,276 --> 00:18:37,346
How did that feel?


531
00:18:38,446 --> 00:18:39,846
Maybe frustrating?


532
00:18:39,846 --> 00:18:40,796
You might not open the app,


533
00:18:40,796 --> 00:18:41,166
again.


534
00:18:42,026 --> 00:18:43,566
So, how can you get a higher


535
00:18:43,566 --> 00:18:45,326
tracking quality for your app?


536
00:18:46,726 --> 00:18:48,626
For this, we need to understand


537
00:18:48,626 --> 00:18:49,846
the main factors that are


538
00:18:49,846 --> 00:18:51,016
influencing the tracking


539
00:18:51,016 --> 00:18:51,646
quality.


540
00:18:52,156 --> 00:18:53,436
And I want to highlight three of


541
00:18:53,476 --> 00:18:53,856
them here.


542
00:18:55,056 --> 00:18:56,756
First of all, World Tracking


543
00:18:56,756 --> 00:18:58,996
relies on a constant stream of


544
00:18:59,056 --> 00:19:01,136
camera images and sensor data.


545
00:19:01,556 --> 00:19:02,876
If this is interrupted for too


546
00:19:02,876 --> 00:19:04,996
long, tracking will become


547
00:19:04,996 --> 00:19:05,456
limited.


548
00:19:06,926 --> 00:19:08,676
Second, World Tracking also


549
00:19:08,676 --> 00:19:10,326
works best in textured and


550
00:19:10,326 --> 00:19:12,306
well-lit environments because


551
00:19:12,646 --> 00:19:14,656
World Tracking uses those


552
00:19:14,656 --> 00:19:16,726
visually robust points to map


553
00:19:16,726 --> 00:19:18,256
and finally triangulate its


554
00:19:18,316 --> 00:19:18,916
location.


555
00:19:18,916 --> 00:19:20,976
It is important that there is


556
00:19:20,976 --> 00:19:23,136
enough visual complexity in the


557
00:19:23,136 --> 00:19:23,686
environment.


558
00:19:24,706 --> 00:19:26,216
If this is not the case because


559
00:19:26,466 --> 00:19:28,046
it's, for example, too dark or


560
00:19:28,046 --> 00:19:29,666
you're looking against a white


561
00:19:29,666 --> 00:19:31,606
wall, then also, the tracking


562
00:19:31,606 --> 00:19:32,656
will perform poorly.


563
00:19:33,166 --> 00:19:36,936
And third, also, World Tracking


564
00:19:36,936 --> 00:19:38,646
works best in static


565
00:19:38,646 --> 00:19:39,366
environments.


566
00:19:40,326 --> 00:19:42,026
If too much of what your camera


567
00:19:42,106 --> 00:19:45,166
sees is moving, then the visual


568
00:19:45,166 --> 00:19:47,036
data won't correspond with the


569
00:19:47,036 --> 00:19:50,526
motion data, which might result


570
00:19:50,526 --> 00:19:51,776
in the potential drift.


571
00:19:52,846 --> 00:19:54,706
Also, device itself should not


572
00:19:54,706 --> 00:19:55,936
be on a moving platform like a


573
00:19:55,936 --> 00:19:57,456
bus or an elevator.


574
00:19:58,326 --> 00:19:59,726
Because in those moments the


575
00:19:59,726 --> 00:20:01,176
motion sensor would actually


576
00:20:01,176 --> 00:20:02,706
sense a motion like going up or


577
00:20:02,706 --> 00:20:04,476
down in the elevator while,


578
00:20:04,476 --> 00:20:06,766
visually, your environment had


579
00:20:06,816 --> 00:20:07,566
not changed.


580
00:20:08,066 --> 00:20:11,846
So, how can you get notified


581
00:20:11,846 --> 00:20:13,966
about the tracking quality that


582
00:20:14,036 --> 00:20:15,036
the user is currently


583
00:20:15,036 --> 00:20:16,926
experiencing with your app?


584
00:20:18,236 --> 00:20:19,916
ARKit monitors its tracking


585
00:20:19,916 --> 00:20:20,586
performance.


586
00:20:21,186 --> 00:20:22,776
We applied machine learning,


587
00:20:23,076 --> 00:20:24,536
which was trained on thousands


588
00:20:24,536 --> 00:20:26,546
of data sets to which we had the


589
00:20:26,546 --> 00:20:28,706
information how well tracking


590
00:20:28,706 --> 00:20:30,266
performed in those situations.


591
00:20:31,776 --> 00:20:33,276
To train a classifier, which


592
00:20:33,276 --> 00:20:35,036
tells you how tracking performs,


593
00:20:35,036 --> 00:20:36,846
we used annotations like the


594
00:20:36,846 --> 00:20:39,486
number of visual-- visible


595
00:20:39,566 --> 00:20:41,136
features tracked in the image


596
00:20:41,936 --> 00:20:43,576
and also, the current velocity


597
00:20:43,576 --> 00:20:44,346
of the device.


598
00:20:45,496 --> 00:20:47,866
Now, during runtime, the health


599
00:20:47,866 --> 00:20:50,636
of tracking is determined based


600
00:20:50,636 --> 00:20:51,746
on those parameters.


601
00:20:52,616 --> 00:20:55,116
In this video, we can see how


602
00:20:55,116 --> 00:20:57,026
the health estimate, which can


603
00:20:57,026 --> 00:20:58,406
be seen-- which, is reported in


604
00:20:58,406 --> 00:21:00,846
the lower left, gets worse when


605
00:21:00,846 --> 00:21:03,926
the camera is covered while we


606
00:21:03,926 --> 00:21:05,576
are still moving and exploring


607
00:21:05,576 --> 00:21:06,296
the environment.


608
00:21:07,796 --> 00:21:09,596
It also shows how it returns


609
00:21:09,596 --> 00:21:11,336
back to normal after the camera


610
00:21:11,336 --> 00:21:12,496
view is uncovered.


611
00:21:14,036 --> 00:21:16,066
Now, ARKit simplifies its


612
00:21:16,066 --> 00:21:18,586
information for you by providing


613
00:21:18,586 --> 00:21:19,596
a tracking state.


614
00:21:20,466 --> 00:21:22,186
And the tracking state can have


615
00:21:22,316 --> 00:21:23,426
three different values.


616
00:21:23,426 --> 00:21:26,776
It can be normal, which is the


617
00:21:26,856 --> 00:21:29,676
healthy state and is the case in


618
00:21:29,676 --> 00:21:30,546
most of the time.


619
00:21:30,546 --> 00:21:31,566
It's the case in most of the


620
00:21:31,566 --> 00:21:31,956
times.


621
00:21:32,506 --> 00:21:34,096
And it can also be limited,


622
00:21:34,236 --> 00:21:35,396
which is whenever tracking


623
00:21:35,396 --> 00:21:36,426
performs poorly.


624
00:21:37,486 --> 00:21:40,116
If that's the case, then the


625
00:21:40,116 --> 00:21:42,056
limited state will also come


626
00:21:42,056 --> 00:21:43,786
along with the reason, like


627
00:21:43,946 --> 00:21:45,406
insufficient features or


628
00:21:45,406 --> 00:21:47,766
excessive motion or being


629
00:21:47,806 --> 00:21:49,606
currently in the initialization


630
00:21:49,606 --> 00:21:49,966
phase.


631
00:21:50,716 --> 00:21:53,496
It can also be not available,


632
00:21:53,496 --> 00:21:55,256
which means that tracking did


633
00:21:55,256 --> 00:21:56,086
not start yet.


634
00:21:57,116 --> 00:21:58,826
Now, whenever the tracking state


635
00:21:58,916 --> 00:22:01,146
changes, a delegate is called.


636
00:22:01,416 --> 00:22:03,636
The camera did change tracking


637
00:22:03,636 --> 00:22:04,076
state.


638
00:22:05,216 --> 00:22:06,116
And this gives you the


639
00:22:06,116 --> 00:22:08,666
opportunity to notify the user


640
00:22:08,896 --> 00:22:10,346
when a limited state has been


641
00:22:10,346 --> 00:22:11,096
encountered.


642
00:22:12,116 --> 00:22:13,226
You should, then, give


643
00:22:13,426 --> 00:22:15,426
informative and actionable


644
00:22:15,776 --> 00:22:17,936
feedback what the user can do to


645
00:22:17,936 --> 00:22:19,626
improve his tracking situation,


646
00:22:20,136 --> 00:22:22,566
as most of it is actually in the


647
00:22:22,566 --> 00:22:23,396
user's hand.


648
00:22:23,776 --> 00:22:25,706
Like for example, as we learned


649
00:22:25,706 --> 00:22:27,996
before, like a sidewise movement


650
00:22:28,096 --> 00:22:30,656
to allow initialization or


651
00:22:31,206 --> 00:22:32,236
making sure there's enough


652
00:22:32,236 --> 00:22:34,316
adequate lightning for enough


653
00:22:34,416 --> 00:22:35,646
visual complexity.


654
00:22:36,236 --> 00:22:39,646
So, let me wrap up the World


655
00:22:39,646 --> 00:22:40,446
Tracking for you.


656
00:22:42,136 --> 00:22:46,156
World Tracking tracks your


657
00:22:46,156 --> 00:22:48,046
camera 6 degree of freedom


658
00:22:48,276 --> 00:22:51,556
orientation and position with


659
00:22:51,556 --> 00:22:53,356
respect to your surrounding


660
00:22:53,356 --> 00:22:55,286
environment and without any


661
00:22:55,286 --> 00:22:56,916
prior information about your


662
00:22:56,916 --> 00:22:59,066
environment, which then allows


663
00:22:59,606 --> 00:23:01,866
the physical world augmentation


664
00:23:01,866 --> 00:23:02,836
in which the content can


665
00:23:02,836 --> 00:23:04,776
actually be viewed from any kind


666
00:23:04,776 --> 00:23:05,166
of view.


667
00:23:06,636 --> 00:23:08,676
Also, World Tracking creates a


668
00:23:08,676 --> 00:23:11,196
World map, which becomes the


669
00:23:11,196 --> 00:23:13,206
tracking's reference system to


670
00:23:13,206 --> 00:23:14,876
localize new camera images


671
00:23:14,916 --> 00:23:15,416
against.


672
00:23:17,266 --> 00:23:18,386
To create a great user


673
00:23:18,386 --> 00:23:20,466
experience, the tracking quality


674
00:23:20,466 --> 00:23:22,696
should be monitored and feedback


675
00:23:22,786 --> 00:23:24,476
and guidance should be provided


676
00:23:24,476 --> 00:23:25,196
to your user.


677
00:23:25,736 --> 00:23:28,676
And World Tracking runs on your


678
00:23:28,676 --> 00:23:29,606
device only.


679
00:23:30,056 --> 00:23:31,516
And all results stay on your


680
00:23:31,516 --> 00:23:31,926
device.


681
00:23:33,446 --> 00:23:34,716
If you have not done it already,


682
00:23:35,576 --> 00:23:37,206
try out one of our developer


683
00:23:37,206 --> 00:23:37,836
examples.


684
00:23:37,936 --> 00:23:39,116
For example, the Build Your


685
00:23:39,116 --> 00:23:41,546
First AR Experience, and play a


686
00:23:41,546 --> 00:23:43,726
bit around, just 15 minutes with


687
00:23:43,726 --> 00:23:44,866
the tracking quality in


688
00:23:44,866 --> 00:23:46,536
different situations; light


689
00:23:46,616 --> 00:23:48,036
situations or movements.


690
00:23:48,536 --> 00:23:50,476
And always remember to guide the


691
00:23:50,476 --> 00:23:53,596
user whenever he encounters a


692
00:23:53,826 --> 00:23:56,696
limited tracking situation to


693
00:23:56,696 --> 00:23:58,646
guarantee that he has a great


694
00:23:58,886 --> 00:24:00,036
tracking experience.


695
00:24:01,456 --> 00:24:04,586
So, World Tracking is about the


696
00:24:04,586 --> 00:24:06,566
camera-- where your camera is


697
00:24:06,566 --> 00:24:08,526
with respect to your physical


698
00:24:08,526 --> 00:24:09,186
environment.


699
00:24:10,156 --> 00:24:13,026
Let's now talk about how the


700
00:24:13,026 --> 00:24:14,536
virtual content can interact


701
00:24:14,836 --> 00:24:16,776
with the physical environment.


702
00:24:17,136 --> 00:24:19,156
And this is possible with Plane


703
00:24:19,156 --> 00:24:19,726
Detection.


704
00:24:22,916 --> 00:24:24,876
The following video, again, from


705
00:24:24,876 --> 00:24:26,576
the Ikea app, shows a great use


706
00:24:26,576 --> 00:24:28,146
case for the Plane Detection,


707
00:24:28,466 --> 00:24:30,626
placing virtual objects into


708
00:24:30,626 --> 00:24:32,566
your physical environment, and


709
00:24:32,566 --> 00:24:34,426
then interacting with it.


710
00:24:35,296 --> 00:24:37,786
So first, please note how, also,


711
00:24:37,786 --> 00:24:39,256
in the Ikea app the user is


712
00:24:39,256 --> 00:24:41,156
guided to make some movement.


713
00:24:42,296 --> 00:24:44,326
Then, once a horizontal plane is


714
00:24:44,376 --> 00:24:46,796
detected, the virtual table set


715
00:24:46,796 --> 00:24:48,836
is displayed and is waiting to


716
00:24:48,836 --> 00:24:49,916
be placed by you.


717
00:24:51,256 --> 00:24:52,856
Once you position it, rotate it


718
00:24:52,856 --> 00:24:54,646
as you want it, you can lock the


719
00:24:54,646 --> 00:24:55,886
object in its environment.


720
00:24:56,126 --> 00:24:56,976
And did you notice the


721
00:24:56,976 --> 00:24:59,206
interaction between the detected


722
00:24:59,256 --> 00:25:01,156
ground plane and the table set


723
00:25:01,376 --> 00:25:02,196
in the moment of locking?


724
00:25:02,196 --> 00:25:04,166
It kind of bounces shortly on


725
00:25:04,166 --> 00:25:04,916
the ground plane.


726
00:25:05,276 --> 00:25:06,986
And this is possible because we


727
00:25:06,986 --> 00:25:08,416
know where the ground plane is.


728
00:25:09,556 --> 00:25:10,996
So, let's have a look at what


729
00:25:10,996 --> 00:25:12,696
happened under the hood here.


730
00:25:14,016 --> 00:25:16,086
Plane Detection uses the World


731
00:25:16,166 --> 00:25:18,726
map provided by the world I just


732
00:25:18,726 --> 00:25:20,656
talked about, just talked about


733
00:25:20,656 --> 00:25:22,726
a moment ago, which is


734
00:25:22,726 --> 00:25:24,246
represented here in those yellow


735
00:25:24,246 --> 00:25:24,716
points.


736
00:25:25,176 --> 00:25:28,446
And then, it uses them to detect


737
00:25:28,446 --> 00:25:30,966
surfaces that are horizontal or


738
00:25:30,966 --> 00:25:32,936
vertical, like the ground, the


739
00:25:32,936 --> 00:25:34,586
bench, and the small wall.


740
00:25:35,386 --> 00:25:36,946
It does this by accumulating


741
00:25:36,946 --> 00:25:38,746
information over multiple


742
00:25:38,746 --> 00:25:39,266
ARFrames.


743
00:25:40,096 --> 00:25:42,496
So, as the user moves around the


744
00:25:42,596 --> 00:25:44,316
scene, more and more information


745
00:25:44,316 --> 00:25:45,416
about the real surface is


746
00:25:45,416 --> 00:25:46,026
acquired.


747
00:25:46,896 --> 00:25:48,086
It also allows the Plane


748
00:25:48,086 --> 00:25:49,966
Detection to provide and like


749
00:25:50,296 --> 00:25:52,056
extent the surface, like a


750
00:25:52,056 --> 00:25:52,916
convex hull.


751
00:25:53,386 --> 00:25:58,006
If multiple planes belonging to


752
00:25:58,006 --> 00:26:00,676
the same physical surface are


753
00:26:00,676 --> 00:26:02,876
detected, like in this part now,


754
00:26:02,876 --> 00:26:04,636
the green and the purple one,


755
00:26:05,226 --> 00:26:06,726
then they will be merged once


756
00:26:06,726 --> 00:26:07,926
they start overlapping.


757
00:26:08,596 --> 00:26:11,356
If horizontal and vertical


758
00:26:11,356 --> 00:26:13,026
planes intersect they are


759
00:26:13,026 --> 00:26:14,366
clipped at the line of


760
00:26:14,366 --> 00:26:15,666
intersection, which is actually


761
00:26:15,666 --> 00:26:18,156
a new feature in ARKit 2.


762
00:26:19,876 --> 00:26:21,776
Plane Detection is designed to


763
00:26:21,776 --> 00:26:24,686
have very little overhead as it


764
00:26:24,766 --> 00:26:27,166
repurposes the mapped 3D points


765
00:26:27,166 --> 00:26:28,296
from the World Tracking.


766
00:26:29,086 --> 00:26:31,166
And then it fits planes into


767
00:26:31,166 --> 00:26:33,386
those point clouds and over time


768
00:26:33,696 --> 00:26:36,346
continuously aggregates more and


769
00:26:36,346 --> 00:26:38,326
more points and merge the planes


770
00:26:38,606 --> 00:26:40,286
that start to overlap.


771
00:26:40,936 --> 00:26:42,516
Therefore, it takes some time


772
00:26:42,516 --> 00:26:44,156
until the first planes are


773
00:26:44,156 --> 00:26:44,806
detected.


774
00:26:46,096 --> 00:26:47,276
What does that mean for you?


775
00:26:48,856 --> 00:26:50,726
If your app is started, there


776
00:26:50,726 --> 00:26:53,036
might not directly be planes to


777
00:26:53,036 --> 00:26:55,516
place objects on or to interact


778
00:26:55,626 --> 00:26:55,846
with.


779
00:26:57,266 --> 00:26:58,786
If the detection of a plane is


780
00:26:58,876 --> 00:27:01,496
mandatory for your experience,


781
00:27:01,786 --> 00:27:04,006
you should again guide the user


782
00:27:04,426 --> 00:27:06,006
to move the camera with enough


783
00:27:06,106 --> 00:27:08,956
translation to ensure a dense


784
00:27:08,956 --> 00:27:11,616
reconstruction based on the


785
00:27:11,616 --> 00:27:13,396
parallax, and also, enough


786
00:27:13,396 --> 00:27:15,086
visual complexity in the scene.


787
00:27:15,976 --> 00:27:18,096
Again, for the reconstruction, a


788
00:27:18,096 --> 00:27:20,416
rotation only is not enough.


789
00:27:20,946 --> 00:27:23,856
Now, how can you enable the


790
00:27:23,856 --> 00:27:24,796
Plane Detection?


791
00:27:25,786 --> 00:27:26,966
It's, again, very simple.


792
00:27:27,116 --> 00:27:28,676
As the Plane Detection reuses


793
00:27:28,676 --> 00:27:30,286
the 3D map from the World


794
00:27:30,286 --> 00:27:32,486
Tracking, it can be configured


795
00:27:32,646 --> 00:27:33,316
by using the


796
00:27:33,316 --> 00:27:35,126
ARWorldTrackingConfiguration.


797
00:27:35,646 --> 00:27:38,346
Then, the property


798
00:27:38,346 --> 00:27:40,706
planeDetection just needs to be


799
00:27:40,706 --> 00:27:41,986
set to either horizontal,


800
00:27:41,986 --> 00:27:43,626
vertical, or like in this case,


801
00:27:43,706 --> 00:27:43,926
both.


802
00:27:45,116 --> 00:27:47,446
And then, just call your


803
00:27:47,446 --> 00:27:49,046
ARSession with this


804
00:27:49,106 --> 00:27:50,226
configuration.


805
00:27:50,476 --> 00:27:52,146
And the detection of the planes


806
00:27:52,146 --> 00:27:52,856
will be started.


807
00:27:53,526 --> 00:27:56,086
Now, how are those, the results


808
00:27:56,086 --> 00:27:57,686
of the detected planes returned


809
00:27:57,686 --> 00:27:58,576
to you?


810
00:28:01,496 --> 00:28:03,216
The detected planes are returned


811
00:28:03,216 --> 00:28:05,006
as an ARPlaneAnchor.


812
00:28:05,936 --> 00:28:07,996
An ARPlaneAnchor is a subclass


813
00:28:07,996 --> 00:28:08,946
of an ARAnchor.


814
00:28:10,106 --> 00:28:11,696
Each ARAnchor provides a


815
00:28:11,876 --> 00:28:14,476
transform containing the


816
00:28:14,476 --> 00:28:16,326
information where the anchor is


817
00:28:16,606 --> 00:28:17,566
in your World map.


818
00:28:18,126 --> 00:28:20,216
Now, a plane anchor,


819
00:28:20,216 --> 00:28:21,436
specifically, also has


820
00:28:21,436 --> 00:28:24,026
information about the geometry


821
00:28:24,026 --> 00:28:25,886
of the surface of the plane,


822
00:28:26,816 --> 00:28:27,856
which is represented in two


823
00:28:27,856 --> 00:28:28,976
alternative ways.


824
00:28:29,316 --> 00:28:31,256
Either like a bounding box with


825
00:28:31,256 --> 00:28:35,096
a center and an extent, or as a


826
00:28:35,096 --> 00:28:37,306
3D mesh describing the shape of


827
00:28:37,356 --> 00:28:39,336
the convex hull of the detected


828
00:28:39,386 --> 00:28:41,306
plane and its geometry property.


829
00:28:42,636 --> 00:28:44,646
To get notified about detected


830
00:28:44,736 --> 00:28:48,266
planes, delegates are going to


831
00:28:48,266 --> 00:28:51,266
be called whenever planes are


832
00:28:51,266 --> 00:28:53,676
added, updated, or removed.


833
00:28:54,656 --> 00:28:57,086
This will then allow you to use


834
00:28:57,086 --> 00:28:59,566
those planes, as well as react


835
00:28:59,686 --> 00:29:01,486
to any updates.


836
00:29:01,666 --> 00:29:02,926
Now, what can you do with


837
00:29:03,006 --> 00:29:03,366
planes?


838
00:29:04,866 --> 00:29:05,956
Like what we've seen before on


839
00:29:05,956 --> 00:29:07,296
the Ikea app, these are great


840
00:29:07,296 --> 00:29:07,796
examples.


841
00:29:08,026 --> 00:29:09,496
Place virtual objects, for


842
00:29:09,496 --> 00:29:10,916
example, with hit testing.


843
00:29:12,046 --> 00:29:13,606
Or you can interact with some,


844
00:29:13,606 --> 00:29:15,186
for example, physically.


845
00:29:15,286 --> 00:29:17,846
Like we've seen bouncing is a


846
00:29:17,846 --> 00:29:18,706
possibility.


847
00:29:19,816 --> 00:29:21,966
Or you can also use it by adding


848
00:29:21,966 --> 00:29:23,226
an occlusion plane into the


849
00:29:23,226 --> 00:29:25,346
detected plane, which will then


850
00:29:25,496 --> 00:29:27,286
hide all the virtual content


851
00:29:27,336 --> 00:29:30,036
below or behind the added


852
00:29:30,036 --> 00:29:30,916
occlusion plane.


853
00:29:32,616 --> 00:29:34,436
So, let me summarize what we've


854
00:29:34,436 --> 00:29:35,386
already gone through.


855
00:29:36,536 --> 00:29:37,756
We've had a look at the


856
00:29:37,816 --> 00:29:41,176
Orientation Tracking, the World


857
00:29:41,286 --> 00:29:43,816
Tracking, and the Plane


858
00:29:43,816 --> 00:29:44,386
Detection.


859
00:29:45,306 --> 00:29:47,666
Next, Michele will explain, in


860
00:29:47,666 --> 00:29:48,986
depth, our new tracking


861
00:29:48,986 --> 00:29:50,316
technologies, which were


862
00:29:50,386 --> 00:29:52,976
introduced in ARKit 2.


863
00:29:53,046 --> 00:29:54,246
So, welcome Michele.


864
00:29:55,176 --> 00:29:57,500
[ Applause ]


865
00:29:58,396 --> 00:29:59,186
>> Thank you, Marion.


866
00:30:00,506 --> 00:30:01,766
My name is Michele, and it's a


867
00:30:01,766 --> 00:30:02,766
pleasure to continue with the


868
00:30:02,766 --> 00:30:03,636
remaining topics of this


869
00:30:03,636 --> 00:30:03,886
session.


870
00:30:05,136 --> 00:30:07,886
Next up is saving and loading


871
00:30:07,886 --> 00:30:08,326
maps.


872
00:30:08,936 --> 00:30:10,486
This is a feature that allows to


873
00:30:10,486 --> 00:30:11,816
store all the information that


874
00:30:11,816 --> 00:30:12,906
are required in a session.


875
00:30:13,336 --> 00:30:14,256
So, that it can literally be


876
00:30:14,256 --> 00:30:16,216
restored in another session at a


877
00:30:16,216 --> 00:30:18,116
later point in time to create


878
00:30:18,336 --> 00:30:19,706
augmented reality experiences


879
00:30:19,706 --> 00:30:21,146
that persist to a particular


880
00:30:21,146 --> 00:30:21,486
place.


881
00:30:22,406 --> 00:30:23,566
Or that could, also, be stored


882
00:30:23,566 --> 00:30:25,416
by another device to create


883
00:30:25,606 --> 00:30:27,296
multiple user augmented reality


884
00:30:27,296 --> 00:30:27,906
experiences.


885
00:30:28,646 --> 00:30:30,000
Let's take a look at an example.


886
00:30:37,076 --> 00:30:38,876
What you see here is a guy;


887
00:30:38,876 --> 00:30:40,906
let's name him Andre, that's


888
00:30:40,906 --> 00:30:41,956
walking around the table with


889
00:30:41,956 --> 00:30:43,506
his device having an augmented


890
00:30:43,506 --> 00:30:44,416
reality experience.


891
00:30:45,366 --> 00:30:47,596
And you can see his device now


892
00:30:47,986 --> 00:30:48,856
is making this seem more


893
00:30:48,856 --> 00:30:50,766
interesting by adding a virtual


894
00:30:50,766 --> 00:30:52,506
vase on the table.


895
00:30:54,556 --> 00:30:56,746
A few minutes later his friends


896
00:30:56,986 --> 00:30:58,126
arrive at the same scene.


897
00:30:58,366 --> 00:31:00,116
And now, they're both looking at


898
00:31:00,116 --> 00:31:00,576
the scene.


899
00:31:00,656 --> 00:31:01,926
You're going to see Andre's


900
00:31:02,446 --> 00:31:04,196
device on the left and his


901
00:31:04,196 --> 00:31:05,126
friend on the right now.


902
00:31:06,546 --> 00:31:07,446
So, you can see that they're


903
00:31:07,706 --> 00:31:08,836
looking at the same space.


904
00:31:08,926 --> 00:31:09,886
They can see each other.


905
00:31:10,266 --> 00:31:11,806
But most importantly, they see


906
00:31:11,806 --> 00:31:13,386
the same virtual content.


907
00:31:14,286 --> 00:31:15,446
They're having a shared


908
00:31:15,446 --> 00:31:19,246
augmented reality experience.


909
00:31:19,246 --> 00:31:21,456
So, what we have seen in these


910
00:31:21,456 --> 00:31:23,126
examples can be discovered in


911
00:31:23,186 --> 00:31:23,916
three stages.


912
00:31:24,266 --> 00:31:25,816
First, Andre went around the


913
00:31:25,816 --> 00:31:27,416
table and acquired the World


914
00:31:27,416 --> 00:31:27,616
map.


915
00:31:28,886 --> 00:31:30,246
Then, the World map was shared


916
00:31:30,536 --> 00:31:31,506
across devices.


917
00:31:32,276 --> 00:31:34,346
And then, his friend's device


918
00:31:34,496 --> 00:31:36,056
re-localized to the World map.


919
00:31:37,496 --> 00:31:38,986
This means that the object was


920
00:31:38,986 --> 00:31:40,536
able to understand in the new


921
00:31:40,536 --> 00:31:41,646
device that this was the same


922
00:31:41,646 --> 00:31:42,936
place as the other device,


923
00:31:43,586 --> 00:31:45,406
computed the precise position of


924
00:31:45,406 --> 00:31:46,556
the device with respect to the


925
00:31:46,556 --> 00:31:48,356
map, and then, started tracking


926
00:31:48,356 --> 00:31:50,106
from there just like the new


927
00:31:50,106 --> 00:31:51,346
device acquired the World map


928
00:31:51,346 --> 00:31:51,746
itself.


929
00:31:52,376 --> 00:31:54,956
We're going to go into more


930
00:31:54,956 --> 00:31:56,196
detail about these three phases.


931
00:31:56,866 --> 00:31:59,246
But first, let's review what's


932
00:31:59,246 --> 00:32:00,426
in the World map.


933
00:32:01,156 --> 00:32:02,866
The World map includes all the


934
00:32:02,866 --> 00:32:04,966
tracking data that are needed


935
00:32:04,966 --> 00:32:06,476
for the system to be localized,


936
00:32:06,986 --> 00:32:08,546
which includes the feature


937
00:32:08,546 --> 00:32:09,866
points as Marion greatly


938
00:32:09,866 --> 00:32:10,506
explained before.


939
00:32:10,876 --> 00:32:12,596
As well as local appearance for


940
00:32:12,596 --> 00:32:13,000
this point.


941
00:32:17,046 --> 00:32:18,496
They also contain all the


942
00:32:18,496 --> 00:32:19,826
anchors that were added to the


943
00:32:19,826 --> 00:32:21,806
session, either by the users,


944
00:32:21,916 --> 00:32:23,636
like planes, for example.


945
00:32:24,896 --> 00:32:26,126
I mean by the system-- like


946
00:32:26,126 --> 00:32:26,456
planes.


947
00:32:26,456 --> 00:32:28,126
Or by the users, like the vase,


948
00:32:28,446 --> 00:32:29,646
as we have seen in the example.


949
00:32:30,746 --> 00:32:33,786
This data is serializable and


950
00:32:33,786 --> 00:32:35,536
available to you so that you can


951
00:32:35,536 --> 00:32:37,346
create compelling persistent or


952
00:32:37,826 --> 00:32:39,326
multiple user augmented reality


953
00:32:39,326 --> 00:32:39,966
experiences.


954
00:32:40,326 --> 00:32:41,486
So, now let's take a look at the


955
00:32:41,486 --> 00:32:43,526
first stage, which is acquiring


956
00:32:43,526 --> 00:32:44,006
the World map.


957
00:32:44,756 --> 00:32:47,576
We can play back the first video


958
00:32:48,056 --> 00:32:49,396
where Andre went around the


959
00:32:49,396 --> 00:32:51,026
table that you can see his


960
00:32:51,026 --> 00:32:52,596
device on the left, here.


961
00:32:53,256 --> 00:32:57,216
And on the right, you see the


962
00:32:57,216 --> 00:32:59,126
World map from a top view as


963
00:32:59,126 --> 00:33:00,426
acquired by the tracking system.


964
00:33:00,776 --> 00:33:02,516
You can [inaudible] is the table


965
00:33:03,096 --> 00:33:04,566
and the chair around it.


966
00:33:05,136 --> 00:33:06,956
There's a few things to pay


967
00:33:06,956 --> 00:33:08,106
attention to during this


968
00:33:08,186 --> 00:33:09,276
acquisition process.


969
00:33:10,076 --> 00:33:11,986
First, everything that Marion


970
00:33:11,986 --> 00:33:13,396
said during tracking also


971
00:33:13,396 --> 00:33:13,946
applies here.


972
00:33:14,456 --> 00:33:15,606
So, we want enough visual


973
00:33:15,606 --> 00:33:17,356
complexity on the scene to get


974
00:33:17,446 --> 00:33:18,626
dense feature points on the map.


975
00:33:19,546 --> 00:33:21,276
And the scene must be static.


976
00:33:22,086 --> 00:33:22,856
Of course, we can deal with


977
00:33:22,856 --> 00:33:24,276
minor changes, as you have seen


978
00:33:24,276 --> 00:33:25,466
the tablecloth moving by the


979
00:33:25,466 --> 00:33:25,716
wind.


980
00:33:26,026 --> 00:33:27,216
But the scene must be mostly


981
00:33:27,566 --> 00:33:27,956
static.


982
00:33:29,036 --> 00:33:30,606
In addition, when we are


983
00:33:30,606 --> 00:33:31,936
specifically acquiring a World


984
00:33:31,936 --> 00:33:33,856
map for sharing we want to go


985
00:33:33,856 --> 00:33:35,316
around the environment from


986
00:33:35,316 --> 00:33:36,636
multiple points of view.


987
00:33:37,516 --> 00:33:38,746
In particular, we want to cover


988
00:33:38,746 --> 00:33:41,046
all the direction from which we


989
00:33:41,106 --> 00:33:42,786
want to later be localized from.


990
00:33:45,366 --> 00:33:46,926
To make this easy, we also made


991
00:33:47,356 --> 00:33:50,296
available a world mapping status


992
00:33:50,296 --> 00:33:51,736
which gives you information


993
00:33:51,736 --> 00:33:52,566
about the World map.


994
00:33:53,446 --> 00:33:54,816
If you guys have been to the


995
00:33:54,816 --> 00:33:56,066
What's New in ARKit talk,


996
00:33:56,636 --> 00:33:57,506
[inaudible] greatly expand this


997
00:33:57,546 --> 00:33:58,936
to quickly recap.


998
00:33:59,576 --> 00:34:00,766
When you start the session the


999
00:34:00,766 --> 00:34:02,196
World map status will start


1000
00:34:02,336 --> 00:34:02,776
limited.


1001
00:34:02,776 --> 00:34:03,936
And then, will switch to a


1002
00:34:04,056 --> 00:34:06,746
standing as more of the scene is


1003
00:34:06,746 --> 00:34:07,616
learned by the device.


1004
00:34:08,036 --> 00:34:09,176
And then, finally, we go to


1005
00:34:09,176 --> 00:34:10,856
mapped when the system is


1006
00:34:10,856 --> 00:34:12,156
confident you're staying in the


1007
00:34:12,156 --> 00:34:12,766
same place.


1008
00:34:13,726 --> 00:34:15,206
And that's what you want to save


1009
00:34:15,206 --> 00:34:16,976
the map in the mapped state.


1010
00:34:17,626 --> 00:34:20,315
So, that's good information.


1011
00:34:20,315 --> 00:34:22,056
But this is mostly on the user


1012
00:34:22,056 --> 00:34:24,326
side applied to acquire the


1013
00:34:24,326 --> 00:34:24,746
session.


1014
00:34:24,966 --> 00:34:26,076
So, what does this mean to you


1015
00:34:26,156 --> 00:34:26,926
as a developer?


1016
00:34:27,505 --> 00:34:29,085
That you need to guide the user.


1017
00:34:30,275 --> 00:34:32,255
So, we can indicate the mapping


1018
00:34:32,326 --> 00:34:33,985
status and even disabling the


1019
00:34:33,985 --> 00:34:35,835
saving or sharing of the World


1020
00:34:35,835 --> 00:34:37,886
map until the mapping status


1021
00:34:37,926 --> 00:34:39,476
goes to the mapped state.


1022
00:34:39,996 --> 00:34:44,255
We can also, monitor the


1023
00:34:44,326 --> 00:34:45,576
tracking quality during the


1024
00:34:45,576 --> 00:34:48,696
acquisition session and report


1025
00:34:48,926 --> 00:34:50,045
to the user if the tracking


1026
00:34:50,045 --> 00:34:51,356
state has been limited for more


1027
00:34:51,356 --> 00:34:52,146
than a few seconds.


1028
00:34:53,056 --> 00:34:54,356
And maybe even give an option to


1029
00:34:54,356 --> 00:34:56,036
restart the acquisition session.


1030
00:34:56,476 --> 00:34:58,496
On the receiving end of the


1031
00:34:58,496 --> 00:35:00,726
device, we can also guide the


1032
00:35:00,726 --> 00:35:01,956
user to better localization


1033
00:35:01,956 --> 00:35:02,346
process.


1034
00:35:03,116 --> 00:35:04,856
So, when we are, again, in the


1035
00:35:04,856 --> 00:35:06,776
acquisition device, when we are


1036
00:35:06,776 --> 00:35:08,196
in the map state we can take a


1037
00:35:08,196 --> 00:35:10,296
picture of the scene and then,


1038
00:35:10,396 --> 00:35:11,496
ship that together with the


1039
00:35:11,496 --> 00:35:11,946
World map.


1040
00:35:11,986 --> 00:35:13,596
And on the receiving end we can


1041
00:35:13,596 --> 00:35:16,026
ask the user find this view to


1042
00:35:16,026 --> 00:35:17,766
start your shared experience.


1043
00:35:18,386 --> 00:35:20,946
That was how to acquire the


1044
00:35:20,946 --> 00:35:21,336
World map.


1045
00:35:21,506 --> 00:35:22,876
Now, let's see how you can share


1046
00:35:22,876 --> 00:35:24,666
the World map.


1047
00:35:24,966 --> 00:35:26,366
First, you can get the World map


1048
00:35:26,366 --> 00:35:27,386
by simply calling the


1049
00:35:27,496 --> 00:35:29,926
getCurrentWorldMap method in the


1050
00:35:29,926 --> 00:35:30,536
ARSession.


1051
00:35:30,896 --> 00:35:32,596
And this will give you the World


1052
00:35:33,186 --> 00:35:33,286
map.


1053
00:35:34,376 --> 00:35:37,156
The World map is a serializable


1054
00:35:37,156 --> 00:35:37,476
class.


1055
00:35:37,566 --> 00:35:38,856
So, then we can simply use


1056
00:35:38,956 --> 00:35:40,576
NSKeyedArchiver utility to


1057
00:35:40,576 --> 00:35:42,106
serialize it to a binary stream


1058
00:35:42,106 --> 00:35:44,446
of data, which then, you can


1059
00:35:44,446 --> 00:35:46,276
either save to disk in case of a


1060
00:35:46,276 --> 00:35:48,086
single user persistent


1061
00:35:48,166 --> 00:35:48,946
application.


1062
00:35:49,706 --> 00:35:52,126
Or you can share it across


1063
00:35:52,156 --> 00:35:52,636
devices.


1064
00:35:52,906 --> 00:35:54,626
And for that, you can use the


1065
00:35:54,626 --> 00:35:56,176
MultiPeerConnectivity framework,


1066
00:35:56,936 --> 00:35:58,196
which has great feature like


1067
00:35:58,196 --> 00:36:00,186
automatic device, nearby device


1068
00:36:00,186 --> 00:36:01,856
discovery, and allows efficient


1069
00:36:01,856 --> 00:36:04,356
communication of data between


1070
00:36:04,356 --> 00:36:04,876
devices.


1071
00:36:05,626 --> 00:36:06,996
We also, have an example of how


1072
00:36:06,996 --> 00:36:09,396
to use that in ARKit called


1073
00:36:09,396 --> 00:36:11,036
Creating a Multiuser AR


1074
00:36:11,036 --> 00:36:12,126
Experience that you can check


1075
00:36:12,126 --> 00:36:13,516
out on our developer website.


1076
00:36:14,076 --> 00:36:16,836
On the receiving end of the


1077
00:36:16,836 --> 00:36:18,086
device, once you've got the


1078
00:36:18,086 --> 00:36:20,096
World map let's see how you can


1079
00:36:20,096 --> 00:36:21,046
set up the World Tracking


1080
00:36:21,046 --> 00:36:22,126
configuration to use it.


1081
00:36:22,426 --> 00:36:22,986
Very simple.


1082
00:36:22,986 --> 00:36:24,916
You just set the initial World


1083
00:36:24,916 --> 00:36:27,366
map property to that World map.


1084
00:36:28,266 --> 00:36:29,986
When you run the session, the


1085
00:36:29,986 --> 00:36:31,426
system will try to find that


1086
00:36:31,526 --> 00:36:32,376
previous World map.


1087
00:36:33,636 --> 00:36:35,336
But it may take some time, even


1088
00:36:35,336 --> 00:36:36,366
because the user may not be


1089
00:36:36,366 --> 00:36:37,546
pointing at the same scene as


1090
00:36:37,546 --> 00:36:37,906
before.


1091
00:36:38,756 --> 00:36:39,656
So, how do we know when


1092
00:36:39,656 --> 00:36:40,606
localization happen?


1093
00:36:41,686 --> 00:36:43,776
That information is available in


1094
00:36:43,776 --> 00:36:44,666
the tracking state.


1095
00:36:44,666 --> 00:36:45,896
So, as soon as you start the


1096
00:36:45,896 --> 00:36:47,596
session with the initial World


1097
00:36:47,596 --> 00:36:49,256
map, the tracking state will be


1098
00:36:49,256 --> 00:36:50,326
limited with reason


1099
00:36:50,536 --> 00:36:51,366
Relocalizing.


1100
00:36:51,906 --> 00:36:54,226
Note that you will still get the


1101
00:36:54,226 --> 00:36:55,816
tracking data available here,


1102
00:36:56,176 --> 00:36:58,726
but the world origin will be the


1103
00:36:58,726 --> 00:37:00,376
first camera, just like a new


1104
00:37:00,376 --> 00:37:00,826
session.


1105
00:37:01,336 --> 00:37:04,156
As soon as the user points the


1106
00:37:04,156 --> 00:37:05,606
device to the same scene, the


1107
00:37:05,606 --> 00:37:06,616
system will localize.


1108
00:37:07,076 --> 00:37:08,136
The tracking state will go to


1109
00:37:08,136 --> 00:37:09,866
normal and the world origin will


1110
00:37:09,866 --> 00:37:11,576
be the same as the recorded


1111
00:37:11,616 --> 00:37:12,036
World map.


1112
00:37:13,046 --> 00:37:15,496
At this point, all your previous


1113
00:37:15,496 --> 00:37:16,856
anchors are also available in


1114
00:37:16,856 --> 00:37:17,916
your session, so you can put


1115
00:37:17,976 --> 00:37:19,226
back the virtual content.


1116
00:37:21,816 --> 00:37:23,506
Note here that because of what's


1117
00:37:23,506 --> 00:37:24,986
happening behind the hood,


1118
00:37:25,206 --> 00:37:26,806
behind the scenes, is that we're


1119
00:37:26,806 --> 00:37:28,206
matching those feature points,


1120
00:37:28,616 --> 00:37:29,996
there needs to be enough visual


1121
00:37:29,996 --> 00:37:32,306
similarity between the scenes


1122
00:37:32,306 --> 00:37:33,326
where you acquired the World map


1123
00:37:33,536 --> 00:37:34,666
and the scene where you want to


1124
00:37:34,666 --> 00:37:35,146
relocalize.


1125
00:37:36,116 --> 00:37:37,406
So, if you go back to this table


1126
00:37:37,406 --> 00:37:38,696
at night, chances are it's not


1127
00:37:38,696 --> 00:37:41,946
going to work very well.


1128
00:37:42,206 --> 00:37:44,116
And that was how you can create


1129
00:37:44,506 --> 00:37:46,516
multiple user experiences or


1130
00:37:46,516 --> 00:37:47,686
persistent experiences using the


1131
00:37:47,686 --> 00:37:49,526
saving and loading map.


1132
00:37:50,536 --> 00:37:54,846
Next, image tracking.


1133
00:37:54,846 --> 00:37:56,306
So, augmented reality is all


1134
00:37:56,306 --> 00:37:59,186
about adding visual content on


1135
00:37:59,186 --> 00:38:00,306
top of the physical world.


1136
00:38:00,536 --> 00:38:01,686
And on the physical world,


1137
00:38:01,686 --> 00:38:02,866
images are found everywhere.


1138
00:38:02,866 --> 00:38:05,226
Think about [inaudible] the


1139
00:38:05,226 --> 00:38:07,086
world, magazine covers,


1140
00:38:07,426 --> 00:38:08,336
advertisements.


1141
00:38:08,816 --> 00:38:10,136
Image tracking is a tool that


1142
00:38:10,136 --> 00:38:11,506
allows you to recognize those


1143
00:38:11,506 --> 00:38:13,976
physical images and build


1144
00:38:14,216 --> 00:38:15,366
augmented reality experiences


1145
00:38:15,366 --> 00:38:15,876
around them.


1146
00:38:17,876 --> 00:38:18,746
Let's see an example.


1147
00:38:20,006 --> 00:38:21,896
You can see here; two images


1148
00:38:21,896 --> 00:38:23,356
being tracked simultaneously.


1149
00:38:24,426 --> 00:38:26,766
On the left, a beautiful


1150
00:38:27,206 --> 00:38:29,376
elephant is put on top of the


1151
00:38:29,376 --> 00:38:30,716
physical image of the elephant.


1152
00:38:31,606 --> 00:38:32,896
On the right, the physical image


1153
00:38:32,896 --> 00:38:35,116
turned into a virtual screen.


1154
00:38:36,186 --> 00:38:37,626
Note also, that the images can


1155
00:38:37,626 --> 00:38:39,016
freely move around the


1156
00:38:39,016 --> 00:38:40,966
environment as tracking around


1157
00:38:40,966 --> 00:38:42,146
at 60 frames per second.


1158
00:38:43,306 --> 00:38:45,146
Let's talk about looking at


1159
00:38:45,146 --> 00:38:46,866
what's happening behind the


1160
00:38:47,156 --> 00:38:47,476
scenes.


1161
00:38:47,476 --> 00:38:48,996
So, let's say you have an image


1162
00:38:48,996 --> 00:38:50,526
like this one of the elephant


1163
00:38:50,896 --> 00:38:52,186
and you want to find it in a


1164
00:38:52,236 --> 00:38:52,876
scene like this.


1165
00:38:54,266 --> 00:38:55,556
We're using grayscale for this.


1166
00:38:55,556 --> 00:38:56,956
And the first type is pretty


1167
00:38:56,956 --> 00:38:58,426
similar to what we do in


1168
00:38:58,426 --> 00:38:58,836
tracking.


1169
00:38:58,936 --> 00:38:59,956
So, we'll track those


1170
00:39:00,046 --> 00:39:01,526
interesting points from both the


1171
00:39:01,526 --> 00:39:03,256
reference image and the current


1172
00:39:03,316 --> 00:39:03,566
scene.


1173
00:39:04,676 --> 00:39:05,996
And then, we try to go in the


1174
00:39:05,996 --> 00:39:07,356
current scene and match those


1175
00:39:07,356 --> 00:39:08,616
features to the one on the


1176
00:39:08,616 --> 00:39:09,346
reference image.


1177
00:39:10,436 --> 00:39:11,696
By applying some projected


1178
00:39:11,696 --> 00:39:12,866
geometry and linear algebra,


1179
00:39:13,186 --> 00:39:14,366
this is enough to give an


1180
00:39:14,456 --> 00:39:16,126
initial estimation of the


1181
00:39:16,126 --> 00:39:17,286
position orientation of the


1182
00:39:17,286 --> 00:39:18,556
image with respect to the


1183
00:39:18,556 --> 00:39:19,156
current scene.


1184
00:39:20,516 --> 00:39:21,386
But we don't stop here.


1185
00:39:22,566 --> 00:39:23,486
In order to give you a really


1186
00:39:23,486 --> 00:39:25,836
precise pose and track at 60


1187
00:39:25,836 --> 00:39:27,846
frames per second, we then do a


1188
00:39:27,846 --> 00:39:29,276
dense tracking stage.


1189
00:39:29,986 --> 00:39:31,316
So, with that initial estimate


1190
00:39:31,976 --> 00:39:33,136
we take the pixels from the


1191
00:39:33,136 --> 00:39:36,486
current scene and warp them back


1192
00:39:36,566 --> 00:39:38,266
to a rectangular shape like you


1193
00:39:38,266 --> 00:39:40,616
see on the right-- top right


1194
00:39:40,696 --> 00:39:40,916
there.


1195
00:39:41,306 --> 00:39:42,776
So, that's a reconstructed image


1196
00:39:43,536 --> 00:39:45,296
by warping the pixels of the


1197
00:39:45,296 --> 00:39:46,576
current image into the


1198
00:39:46,576 --> 00:39:46,986
rectangle.


1199
00:39:47,916 --> 00:39:48,766
We can then compare the


1200
00:39:48,766 --> 00:39:50,456
reconstructed image with a


1201
00:39:50,506 --> 00:39:51,726
reference image that we have


1202
00:39:51,726 --> 00:39:54,056
available to create an error


1203
00:39:54,056 --> 00:39:55,106
image like the one you see


1204
00:39:55,106 --> 00:39:55,436
below.


1205
00:39:56,636 --> 00:39:58,296
We then optimize the position


1206
00:39:58,296 --> 00:39:59,836
orientation of the image, such


1207
00:39:59,906 --> 00:40:00,976
that that error is minimized.


1208
00:40:03,366 --> 00:40:04,786
So, what this means to you that


1209
00:40:04,786 --> 00:40:06,796
the post would be really


1210
00:40:06,796 --> 00:40:07,266
accurate.


1211
00:40:08,146 --> 00:40:09,706
Thank you.


1212
00:40:10,526 --> 00:40:12,076
And will still track at 60


1213
00:40:12,076 --> 00:40:12,826
frames per second.


1214
00:40:15,296 --> 00:40:16,906
So, let's see how we can do all


1215
00:40:16,906 --> 00:40:18,126
of this in ARKit.


1216
00:40:18,916 --> 00:40:21,856
As usual, the ARKit API is


1217
00:40:21,856 --> 00:40:22,506
really simple.


1218
00:40:22,566 --> 00:40:24,566
We have three simple steps.


1219
00:40:24,606 --> 00:40:26,686
First, we want to collect all


1220
00:40:26,686 --> 00:40:27,596
the reference images.


1221
00:40:28,486 --> 00:40:30,616
Then, we set up the AR Session


1222
00:40:30,616 --> 00:40:31,346
Configuration.


1223
00:40:31,606 --> 00:40:32,646
There are two options here.


1224
00:40:33,166 --> 00:40:34,306
One is the World Tracking


1225
00:40:34,306 --> 00:40:35,756
configuration that gives, also,


1226
00:40:35,756 --> 00:40:37,426
the device position.


1227
00:40:37,426 --> 00:40:38,316
And this is the one we have


1228
00:40:38,406 --> 00:40:39,296
talked, so far.


1229
00:40:39,856 --> 00:40:41,506
And in iOS12, introduced a new


1230
00:40:41,506 --> 00:40:42,576
configuration, which is a


1231
00:40:42,576 --> 00:40:43,766
standalone image tracking


1232
00:40:43,766 --> 00:40:44,476
configuration.


1233
00:40:44,936 --> 00:40:47,906
Once you start the session you


1234
00:40:47,906 --> 00:40:49,896
will start receiving the results


1235
00:40:49,896 --> 00:40:52,326
in the form of an ARImageAnchor.


1236
00:40:53,296 --> 00:40:54,306
We're now going into more


1237
00:40:54,306 --> 00:40:55,626
details of these three steps,


1238
00:40:56,036 --> 00:40:57,456
starting from the reference


1239
00:40:57,456 --> 00:40:57,886
images.


1240
00:40:58,426 --> 00:41:01,286
The easiest way to add reference


1241
00:41:01,286 --> 00:41:02,836
images to your application is


1242
00:41:02,896 --> 00:41:04,746
through the, called asset


1243
00:41:04,746 --> 00:41:05,106
catalog.


1244
00:41:06,146 --> 00:41:08,066
You simply create an AR Resource


1245
00:41:08,066 --> 00:41:09,646
Groups and drag and drop your


1246
00:41:09,646 --> 00:41:10,386
images in there.


1247
00:41:11,566 --> 00:41:13,026
Next, you have to set the


1248
00:41:13,026 --> 00:41:14,446
physical dimension of the image,


1249
00:41:14,556 --> 00:41:16,196
which you can do on the property


1250
00:41:16,196 --> 00:41:17,436
window on the top right.


1251
00:41:17,946 --> 00:41:20,656
Setting the physical dimension


1252
00:41:21,096 --> 00:41:22,336
is a requirement and there's a


1253
00:41:22,336 --> 00:41:23,126
few reason for that.


1254
00:41:24,466 --> 00:41:26,126
First, it allows the pose of the


1255
00:41:26,126 --> 00:41:27,816
image to be in physical scale.


1256
00:41:28,386 --> 00:41:29,776
Which means, also, your content


1257
00:41:29,816 --> 00:41:31,076
will be in physical scale.


1258
00:41:31,186 --> 00:41:32,666
In ARKit, everything is in


1259
00:41:32,666 --> 00:41:33,986
meters, so also, your visual


1260
00:41:33,986 --> 00:41:36,226
content will be in meters.


1261
00:41:37,056 --> 00:41:38,626
In addition, it's especially


1262
00:41:38,626 --> 00:41:40,266
important to set the correct


1263
00:41:40,266 --> 00:41:41,586
physical dimension of the image


1264
00:41:41,946 --> 00:41:43,166
in case we combine the image


1265
00:41:43,216 --> 00:41:44,046
tracking with the World


1266
00:41:44,046 --> 00:41:44,476
Tracking.


1267
00:41:44,936 --> 00:41:46,206
As this will give immediately


1268
00:41:46,526 --> 00:41:48,616
consistent pose between the


1269
00:41:48,616 --> 00:41:51,196
image and the world.


1270
00:41:51,196 --> 00:41:52,826
Let's see some example of this


1271
00:41:53,256 --> 00:41:54,166
reference images.


1272
00:41:54,816 --> 00:41:57,366
You can see here, two beautiful


1273
00:41:57,366 --> 00:41:57,806
images.


1274
00:41:58,476 --> 00:41:59,776
These images will work really


1275
00:41:59,776 --> 00:42:01,066
great with image tracking.


1276
00:42:01,156 --> 00:42:03,426
They have high texture, high


1277
00:42:03,426 --> 00:42:04,956
level of contrast, well


1278
00:42:04,956 --> 00:42:06,376
distributed histograms, as well


1279
00:42:06,376 --> 00:42:07,356
as they do not contain


1280
00:42:07,356 --> 00:42:08,416
repetitive structures.


1281
00:42:08,536 --> 00:42:10,486
There are, also, other kinds of


1282
00:42:10,486 --> 00:42:12,316
images that will work less good


1283
00:42:12,316 --> 00:42:12,936
with the system.


1284
00:42:13,436 --> 00:42:14,726
You can see an example of this


1285
00:42:15,456 --> 00:42:16,176
on the right.


1286
00:42:17,116 --> 00:42:19,606
And if we take a look at these


1287
00:42:19,696 --> 00:42:21,596
top two examples, you can see


1288
00:42:21,596 --> 00:42:23,466
that the good image we have a


1289
00:42:23,466 --> 00:42:25,036
lot of those interesting points.


1290
00:42:25,536 --> 00:42:26,306
And you can see that the


1291
00:42:26,306 --> 00:42:27,716
histogram is well distributed


1292
00:42:27,716 --> 00:42:28,686
across the whole range.


1293
00:42:29,316 --> 00:42:30,126
While, [inaudible] image,


1294
00:42:30,126 --> 00:42:33,096
there's only a few of those


1295
00:42:33,096 --> 00:42:34,286
interesting points and the


1296
00:42:34,286 --> 00:42:36,116
histogram is all skewed toward


1297
00:42:36,116 --> 00:42:36,536
the whites.


1298
00:42:37,236 --> 00:42:40,536
You can get an estimation of how


1299
00:42:40,536 --> 00:42:42,066
good an image will be directly


1300
00:42:42,066 --> 00:42:42,916
in Xcode.


1301
00:42:44,076 --> 00:42:46,066
As soon as you drag an image in


1302
00:42:46,066 --> 00:42:48,396
there, the image is analyzed and


1303
00:42:48,576 --> 00:42:50,316
[inaudible] to you in the form


1304
00:42:50,316 --> 00:42:52,206
of warnings to give you early


1305
00:42:52,276 --> 00:42:53,826
feedback, even before you run


1306
00:42:53,826 --> 00:42:54,516
your application.


1307
00:42:55,646 --> 00:42:56,666
For example, if you click on


1308
00:42:56,666 --> 00:42:59,086
this bottom image that could be


1309
00:42:59,086 --> 00:43:01,836
a magazine page, for example, we


1310
00:43:01,836 --> 00:43:04,396
can see that the Xcode says that


1311
00:43:04,556 --> 00:43:05,776
the histogram is not well


1312
00:43:05,776 --> 00:43:06,436
distributed.


1313
00:43:06,676 --> 00:43:07,626
In fact, you can see there's a


1314
00:43:07,626 --> 00:43:09,326
lot of whites in the image.


1315
00:43:09,476 --> 00:43:10,626
And it would also say that this


1316
00:43:10,626 --> 00:43:11,716
image contains repetitive


1317
00:43:11,716 --> 00:43:14,486
structures, mainly caused by the


1318
00:43:15,476 --> 00:43:15,636
text.


1319
00:43:15,776 --> 00:43:18,076
Another example, if you have two


1320
00:43:18,076 --> 00:43:20,126
images which are too similar and


1321
00:43:20,126 --> 00:43:22,176
are at risk of being confused at


1322
00:43:22,506 --> 00:43:24,366
detection time, also, Xcode


1323
00:43:24,366 --> 00:43:25,166
warns you about that.


1324
00:43:25,906 --> 00:43:26,986
You can see an example of these


1325
00:43:26,986 --> 00:43:28,906
two images of the same mountain


1326
00:43:28,906 --> 00:43:29,756
range, the Sierra.


1327
00:43:30,156 --> 00:43:32,446
There's a few things that we can


1328
00:43:32,446 --> 00:43:33,506
do to deal with this warning.


1329
00:43:33,936 --> 00:43:34,956
For example, let's go back to


1330
00:43:34,956 --> 00:43:38,236
this image that had repetitive


1331
00:43:38,236 --> 00:43:40,746
structures and not well


1332
00:43:40,746 --> 00:43:41,746
distributed histograms.


1333
00:43:42,516 --> 00:43:44,186
You can try to identify a region


1334
00:43:44,186 --> 00:43:45,126
of this image which is


1335
00:43:45,126 --> 00:43:46,596
distinctive enough, like in this


1336
00:43:46,596 --> 00:43:48,736
case, for example, the actual


1337
00:43:48,736 --> 00:43:49,536
image of the page.


1338
00:43:50,106 --> 00:43:51,276
And then, you can crop that out


1339
00:43:51,276 --> 00:43:52,756
and use this as the reference


1340
00:43:52,756 --> 00:43:53,556
image, instead.


1341
00:43:53,876 --> 00:43:55,126
Which will give you, of course,


1342
00:43:55,476 --> 00:43:56,756
all the warnings are going to be


1343
00:43:56,756 --> 00:43:58,296
removed and will give you better


1344
00:43:58,946 --> 00:43:59,716
tracking quality.


1345
00:44:00,026 --> 00:44:03,106
Another thing that we can do is


1346
00:44:03,436 --> 00:44:06,376
use multiple AR Resource Groups.


1347
00:44:07,496 --> 00:44:09,176
This allow many more images to


1348
00:44:09,396 --> 00:44:10,146
be detected.


1349
00:44:10,416 --> 00:44:11,726
As with the commands to have a


1350
00:44:11,726 --> 00:44:13,816
maximum of 25 images per group


1351
00:44:14,236 --> 00:44:15,296
to keep your experience


1352
00:44:15,296 --> 00:44:16,736
efficient and responsive.


1353
00:44:17,936 --> 00:44:19,016
But you can have as many groups


1354
00:44:19,246 --> 00:44:19,826
as you want.


1355
00:44:20,036 --> 00:44:21,896
And then, you can switch between


1356
00:44:21,896 --> 00:44:23,156
groups programmatically.


1357
00:44:23,376 --> 00:44:26,076
For example, if you are want to


1358
00:44:26,076 --> 00:44:27,076
create an augmented reality


1359
00:44:27,076 --> 00:44:28,846
experience in a museum that may


1360
00:44:28,846 --> 00:44:30,646
have hundreds of images.


1361
00:44:31,816 --> 00:44:33,396
Usually though, those images are


1362
00:44:33,396 --> 00:44:34,586
actually physically located in


1363
00:44:34,586 --> 00:44:35,336
different rooms.


1364
00:44:35,646 --> 00:44:37,646
So, what you can do is put the


1365
00:44:38,246 --> 00:44:39,756
images that physically will be


1366
00:44:39,756 --> 00:44:41,146
present in the room into a


1367
00:44:41,146 --> 00:44:41,476
group.


1368
00:44:41,686 --> 00:44:43,036
And images of another room into


1369
00:44:43,036 --> 00:44:43,596
another group.


1370
00:44:44,246 --> 00:44:45,806
And then use, for example, core


1371
00:44:45,806 --> 00:44:47,586
location to switch between


1372
00:44:48,306 --> 00:44:48,686
rooms.


1373
00:44:49,186 --> 00:44:52,116
Note also, that you can have


1374
00:44:52,446 --> 00:44:53,906
similar images, now, as long as


1375
00:44:53,946 --> 00:44:55,126
they are in different groups.


1376
00:44:55,896 --> 00:44:57,716
So, that was all about reference


1377
00:44:57,746 --> 00:44:58,266
images.


1378
00:44:58,266 --> 00:45:00,606
Let's now, see our two


1379
00:45:01,166 --> 00:45:01,916
configurations.


1380
00:45:03,006 --> 00:45:05,286
The ARImageTrackingConfiguration


1381
00:45:05,416 --> 00:45:06,876
is a new standalone image


1382
00:45:06,876 --> 00:45:07,836
tracking configuration, which


1383
00:45:07,866 --> 00:45:09,256
means it doesn't run the World


1384
00:45:09,256 --> 00:45:09,646
Tracking.


1385
00:45:10,636 --> 00:45:11,846
Which also, means there is no


1386
00:45:12,136 --> 00:45:12,916
world origin.


1387
00:45:13,216 --> 00:45:14,676
So, every image will be given to


1388
00:45:14,676 --> 00:45:15,976
you with respect to the current


1389
00:45:15,976 --> 00:45:16,526
camera view.


1390
00:45:18,246 --> 00:45:19,346
You can also combine image


1391
00:45:19,406 --> 00:45:20,906
tracking with a World Tracking


1392
00:45:20,906 --> 00:45:21,646
configuration.


1393
00:45:22,466 --> 00:45:24,536
And in this case, you will have


1394
00:45:24,536 --> 00:45:25,636
all the scene understanding


1395
00:45:25,636 --> 00:45:27,506
capability available like Plane


1396
00:45:27,506 --> 00:45:29,356
Detection, light estimation,


1397
00:45:29,356 --> 00:45:30,026
everything else.


1398
00:45:31,066 --> 00:45:32,796
So, what is more appropriate to


1399
00:45:32,796 --> 00:45:34,376
use which configurations?


1400
00:45:35,066 --> 00:45:35,536
Let's see.


1401
00:45:35,626 --> 00:45:37,106
So, in the


1402
00:45:37,516 --> 00:45:39,916
ARImageTrackingConfigurations is


1403
00:45:39,916 --> 00:45:41,226
really tailored for use cases


1404
00:45:41,226 --> 00:45:42,906
which are built around images.


1405
00:45:43,196 --> 00:45:44,286
We can see an example on the


1406
00:45:44,286 --> 00:45:44,826
left here.


1407
00:45:46,606 --> 00:45:48,516
We can have an image that could


1408
00:45:48,516 --> 00:45:49,666
be a page of a textbook.


1409
00:45:50,506 --> 00:45:51,576
And to make the experience more


1410
00:45:51,576 --> 00:45:53,976
engaging, we are overlaying


1411
00:45:54,286 --> 00:45:54,746
[inaudible] graph.


1412
00:45:54,746 --> 00:45:55,846
In this case, on how to build an


1413
00:45:55,846 --> 00:45:56,796
equilateral triangle.


1414
00:45:57,866 --> 00:45:58,746
So, you can see that this


1415
00:45:58,746 --> 00:46:00,256
experience is really tailored


1416
00:46:00,256 --> 00:46:00,966
around an image.


1417
00:46:01,116 --> 00:46:03,046
If you have, let's see this


1418
00:46:03,046 --> 00:46:03,826
other example.


1419
00:46:04,546 --> 00:46:05,516
Image tracking is used to


1420
00:46:05,606 --> 00:46:07,266
trigger some content that then


1421
00:46:07,266 --> 00:46:09,326
goes beyond the extent of the


1422
00:46:09,326 --> 00:46:09,496
image.


1423
00:46:09,496 --> 00:46:12,086
In this case, you want to use


1424
00:46:12,086 --> 00:46:13,726
the ARWorldTrackingConfiguration


1425
00:46:13,806 --> 00:46:14,906
as you will need the device


1426
00:46:14,906 --> 00:46:16,506
position to keep track of that


1427
00:46:16,506 --> 00:46:19,856
content outside the image.


1428
00:46:20,076 --> 00:46:21,146
Also, note that the image


1429
00:46:21,246 --> 00:46:23,576
tracking doesn't use the motion


1430
00:46:23,576 --> 00:46:24,676
data, which means it can also be


1431
00:46:24,676 --> 00:46:26,506
used on a bus or an elevator,


1432
00:46:27,076 --> 00:46:28,146
where the motion data don't


1433
00:46:28,146 --> 00:46:29,896
agree with the visual data.


1434
00:46:30,916 --> 00:46:33,036
So, let's see now, how we can do


1435
00:46:33,036 --> 00:46:33,756
this in code.


1436
00:46:35,236 --> 00:46:36,916
You can easily recognize those


1437
00:46:36,946 --> 00:46:37,716
three steps here.


1438
00:46:37,936 --> 00:46:39,836
The first one is to gather all


1439
00:46:39,836 --> 00:46:40,476
the images.


1440
00:46:40,876 --> 00:46:41,796
And there's a convenience


1441
00:46:41,796 --> 00:46:43,396
function for that in the


1442
00:46:43,566 --> 00:46:45,436
ARReferenceImage class that


1443
00:46:45,436 --> 00:46:47,436
gathers all the images that are


1444
00:46:47,436 --> 00:46:48,366
in a particular group.


1445
00:46:48,556 --> 00:46:50,226
In this case, it's named Room1.


1446
00:46:51,896 --> 00:46:53,256
We can then simply set the


1447
00:46:53,256 --> 00:46:55,626
trackingImages property to those


1448
00:46:55,626 --> 00:46:56,306
images in the


1449
00:46:56,306 --> 00:46:58,156
ARImageTrackingConfigurations.


1450
00:46:58,586 --> 00:46:59,516
And run the session.


1451
00:47:00,866 --> 00:47:02,766
You will then start receiving


1452
00:47:02,766 --> 00:47:04,306
the results, for example, to the


1453
00:47:04,306 --> 00:47:06,346
session, didUpdate anchors


1454
00:47:06,466 --> 00:47:08,266
delegate method, where you can


1455
00:47:08,266 --> 00:47:10,096
check if the anchors is of type


1456
00:47:10,146 --> 00:47:11,156
ARImageAnchor.


1457
00:47:12,606 --> 00:47:14,186
In the anchor, you will find, of


1458
00:47:14,246 --> 00:47:15,386
course, the position and


1459
00:47:15,386 --> 00:47:17,336
orientation of the image, as


1460
00:47:17,336 --> 00:47:18,216
well as the reference image


1461
00:47:18,216 --> 00:47:18,656
itself.


1462
00:47:18,716 --> 00:47:19,856
Where you can find, for example,


1463
00:47:19,856 --> 00:47:21,836
the name of the image as you


1464
00:47:21,836 --> 00:47:23,156
named it in the actual title so


1465
00:47:23,156 --> 00:47:24,126
that you know which image has


1466
00:47:24,126 --> 00:47:24,746
been detected.


1467
00:47:25,986 --> 00:47:26,966
There's also another Boolean


1468
00:47:26,966 --> 00:47:28,936
property, which tells you if


1469
00:47:28,936 --> 00:47:30,086
this image is currently being


1470
00:47:30,086 --> 00:47:33,666
tracked in the frame.


1471
00:47:33,866 --> 00:47:35,306
Note here that other than these


1472
00:47:35,306 --> 00:47:36,496
use cases that we have seen so


1473
00:47:36,496 --> 00:47:38,576
far when you build [inaudible]


1474
00:47:38,576 --> 00:47:40,546
around images, image detection


1475
00:47:40,546 --> 00:47:43,196
and tracking allows a few more


1476
00:47:43,976 --> 00:47:44,126
things.


1477
00:47:45,226 --> 00:47:47,496
For example, if two devices are


1478
00:47:47,496 --> 00:47:48,866
looking at the same physical


1479
00:47:48,866 --> 00:47:51,446
image, you can detect this image


1480
00:47:51,446 --> 00:47:52,416
from both devices.


1481
00:47:52,846 --> 00:47:54,586
And this will give you a shared


1482
00:47:54,586 --> 00:47:56,066
coordinate system that you can


1483
00:47:56,066 --> 00:47:57,626
then use as an alternative way


1484
00:47:58,106 --> 00:47:59,386
to have a shared experience.


1485
00:48:01,476 --> 00:48:03,846
Another example, if you happen


1486
00:48:03,846 --> 00:48:05,226
to know where an image is


1487
00:48:05,346 --> 00:48:06,916
physically located in the world,


1488
00:48:08,296 --> 00:48:09,826
like for example, you know that


1489
00:48:09,826 --> 00:48:11,446
the map of this park is in the


1490
00:48:11,446 --> 00:48:12,076
physical world.


1491
00:48:12,456 --> 00:48:14,446
You can use image tracking to


1492
00:48:14,446 --> 00:48:15,956
get the position of the device


1493
00:48:15,956 --> 00:48:17,576
with respect to the image and,


1494
00:48:17,576 --> 00:48:19,636
therefore, also the position of


1495
00:48:19,636 --> 00:48:20,656
the device with respect to the


1496
00:48:20,656 --> 00:48:21,946
world, which, you can then use,


1497
00:48:21,946 --> 00:48:23,956
for example, to overlay


1498
00:48:24,196 --> 00:48:26,396
directions really attached to


1499
00:48:26,396 --> 00:48:27,086
the physical world.


1500
00:48:27,646 --> 00:48:31,376
So, that concludes the image


1501
00:48:31,436 --> 00:48:31,836
tracking.


1502
00:48:31,836 --> 00:48:34,006
Let's now go and look at the


1503
00:48:34,066 --> 00:48:35,266
Object Detection.


1504
00:48:38,016 --> 00:48:39,546
So, with image tracking we have


1505
00:48:39,546 --> 00:48:41,716
seen how we can detect images,


1506
00:48:41,716 --> 00:48:43,676
which are planar objects in the


1507
00:48:43,676 --> 00:48:44,406
physical world.


1508
00:48:45,376 --> 00:48:46,786
Object detection extends this


1509
00:48:46,786 --> 00:48:48,256
concept to the third dimension


1510
00:48:48,346 --> 00:48:49,806
allowing the detection of more


1511
00:48:49,806 --> 00:48:50,586
generic objects.


1512
00:48:51,046 --> 00:48:53,626
Note, though, that this object


1513
00:48:53,816 --> 00:48:55,826
will be assumed to be static in


1514
00:48:55,826 --> 00:48:57,086
the scene, unlike images that


1515
00:48:57,086 --> 00:48:57,676
can move around.


1516
00:48:58,866 --> 00:49:00,046
We can see an example here.


1517
00:49:00,046 --> 00:49:02,566
That's the Nefertiti bust.


1518
00:49:02,716 --> 00:49:04,266
It's a statue that could be


1519
00:49:04,266 --> 00:49:05,326
present in a museum.


1520
00:49:05,636 --> 00:49:07,166
And now, you can detect it with


1521
00:49:07,166 --> 00:49:07,576
ARKit.


1522
00:49:08,216 --> 00:49:10,376
And then, for example, display


1523
00:49:10,376 --> 00:49:12,296
some information on top of the


1524
00:49:12,296 --> 00:49:14,476
physical object.


1525
00:49:15,466 --> 00:49:17,536
Note also that in the object


1526
00:49:17,736 --> 00:49:18,936
detection in ARKit, we are


1527
00:49:18,936 --> 00:49:20,516
talking about specific instances


1528
00:49:20,516 --> 00:49:21,116
of an object.


1529
00:49:21,646 --> 00:49:22,506
So, we're not talking about


1530
00:49:22,506 --> 00:49:23,986
detecting statues in general,


1531
00:49:24,346 --> 00:49:26,406
but this particular instance of


1532
00:49:26,406 --> 00:49:27,456
the Nefertiti statue.


1533
00:49:28,836 --> 00:49:29,986
So, how do we represent these


1534
00:49:29,986 --> 00:49:31,006
objects in ARKit?


1535
00:49:31,626 --> 00:49:33,366
You first need to scan the


1536
00:49:33,366 --> 00:49:33,736
object.


1537
00:49:33,806 --> 00:49:35,086
So, really, there's two steps to


1538
00:49:35,086 --> 00:49:35,196
it.


1539
00:49:35,316 --> 00:49:36,936
First, you scan the object and


1540
00:49:36,936 --> 00:49:38,336
then you can detect it.


1541
00:49:39,096 --> 00:49:40,546
Let's talk about the scanning


1542
00:49:40,546 --> 00:49:42,446
part, which mostly is going to


1543
00:49:42,446 --> 00:49:44,166
be on your side as a developer,


1544
00:49:44,886 --> 00:49:46,016
to basically, create that


1545
00:49:46,016 --> 00:49:47,296
representation of the object


1546
00:49:47,406 --> 00:49:49,176
that can be used for detection.


1547
00:49:51,276 --> 00:49:53,716
Internally, an object is


1548
00:49:53,716 --> 00:49:55,616
represented in a similar way as


1549
00:49:55,616 --> 00:49:56,196
the world map.


1550
00:49:56,776 --> 00:49:58,576
You can see an example of the 3D


1551
00:49:58,916 --> 00:50:00,456
feature points of the Nefertiti


1552
00:50:00,456 --> 00:50:01,886
statue there on the left.


1553
00:50:03,056 --> 00:50:04,876
And to scan the object, you can


1554
00:50:05,006 --> 00:50:06,516
use the Scanning and Detecting


1555
00:50:06,516 --> 00:50:08,496
3D Objects developer sample


1556
00:50:08,496 --> 00:50:10,416
that's available on the website.


1557
00:50:11,706 --> 00:50:13,136
And note here, that the


1558
00:50:13,136 --> 00:50:14,766
detection quality that you will


1559
00:50:14,766 --> 00:50:17,306
get at runtime, later, is highly


1560
00:50:17,306 --> 00:50:18,616
affected by the quality of the


1561
00:50:18,616 --> 00:50:18,986
scan.


1562
00:50:19,776 --> 00:50:21,826
So, let's spend a few moments to


1563
00:50:21,826 --> 00:50:23,266
see how we can get the best


1564
00:50:23,336 --> 00:50:27,386
quality during the scanning.


1565
00:50:27,546 --> 00:50:29,066
Once you build and run this


1566
00:50:29,066 --> 00:50:30,666
developer sample you will see


1567
00:50:30,786 --> 00:50:31,916
something like this on your


1568
00:50:31,916 --> 00:50:32,406
device.


1569
00:50:33,286 --> 00:50:35,976
The first step is to find the


1570
00:50:35,976 --> 00:50:37,566
region of space around your


1571
00:50:37,566 --> 00:50:37,926
object.


1572
00:50:39,036 --> 00:50:40,066
The application will try to


1573
00:50:40,066 --> 00:50:41,556
automatically estimate this


1574
00:50:41,586 --> 00:50:42,946
bounding box, exploring


1575
00:50:42,946 --> 00:50:43,946
different feature points.


1576
00:50:44,896 --> 00:50:46,206
But you can always adjust this


1577
00:50:46,256 --> 00:50:48,856
box by dragging on a side to


1578
00:50:49,066 --> 00:50:50,726
shrink it or make it larger.


1579
00:50:52,876 --> 00:50:54,506
Note here, that what is really


1580
00:50:54,506 --> 00:50:55,766
important that when you go


1581
00:50:55,766 --> 00:50:57,266
around the object you make sure


1582
00:50:57,266 --> 00:50:58,796
that you don't cut any of the


1583
00:50:58,796 --> 00:51:00,806
interesting points of the


1584
00:51:00,806 --> 00:51:01,166
object.


1585
00:51:01,856 --> 00:51:03,606
You can also, rotate the box


1586
00:51:03,606 --> 00:51:04,956
with a two-finger gesture from


1587
00:51:04,996 --> 00:51:05,146
top.


1588
00:51:05,856 --> 00:51:08,406
So, make sure that this box is


1589
00:51:08,606 --> 00:51:09,666
around the object and not


1590
00:51:09,696 --> 00:51:11,286
cutting any interesting part of


1591
00:51:11,916 --> 00:51:11,986
it.


1592
00:51:13,016 --> 00:51:14,746
The next part is the actual


1593
00:51:14,746 --> 00:51:15,246
scanning.


1594
00:51:16,376 --> 00:51:19,356
In this phase what we want to do


1595
00:51:19,356 --> 00:51:21,426
is really go around the objects


1596
00:51:21,736 --> 00:51:23,486
from all the points of view that


1597
00:51:23,486 --> 00:51:24,846
you think your users will want


1598
00:51:24,846 --> 00:51:25,806
to detect it later.


1599
00:51:27,056 --> 00:51:28,386
In order to make it easy for you


1600
00:51:28,386 --> 00:51:30,096
to understand which part of the


1601
00:51:30,096 --> 00:51:31,656
objects have been, already,


1602
00:51:31,656 --> 00:51:33,376
acquired like this beautiful


1603
00:51:33,376 --> 00:51:34,356
tile representation.


1604
00:51:34,726 --> 00:51:36,046
And you also can see a


1605
00:51:36,456 --> 00:51:37,676
percentage on top which tells


1606
00:51:37,676 --> 00:51:38,786
you how many tiles have already


1607
00:51:38,786 --> 00:51:39,366
been acquired.


1608
00:51:40,406 --> 00:51:41,786
And it's really important in


1609
00:51:41,786 --> 00:51:43,716
this phase that you spend time


1610
00:51:43,716 --> 00:51:45,126
on the regions of the object


1611
00:51:45,126 --> 00:51:46,546
which have a lot of features


1612
00:51:46,726 --> 00:51:47,726
that are distinctive enough.


1613
00:51:47,726 --> 00:51:49,446
And you go close enough to


1614
00:51:49,446 --> 00:51:50,346
capture all the details.


1615
00:51:50,686 --> 00:51:52,046
And again, that you really go


1616
00:51:52,046 --> 00:51:56,246
around from all the sides.


1617
00:51:56,436 --> 00:51:59,456
Like you see here.


1618
00:51:59,686 --> 00:52:01,186
Once you're happy with the


1619
00:52:01,186 --> 00:52:02,996
coverage of your objects, you


1620
00:52:02,996 --> 00:52:04,476
can go to the next step, which


1621
00:52:04,476 --> 00:52:06,656
is allows you to adjust the


1622
00:52:06,656 --> 00:52:09,166
origin by simply dragging on the


1623
00:52:09,166 --> 00:52:10,026
coloring system.


1624
00:52:10,536 --> 00:52:12,496
And this will be the coloring


1625
00:52:12,496 --> 00:52:13,886
system that will be later given


1626
00:52:13,886 --> 00:52:16,206
to you at detection time in the


1627
00:52:16,206 --> 00:52:16,506
anchor.


1628
00:52:16,506 --> 00:52:17,766
So, make sure that you put it in


1629
00:52:17,766 --> 00:52:19,316
a place which makes sense for


1630
00:52:19,316 --> 00:52:20,606
your virtual content.


1631
00:52:20,676 --> 00:52:25,526
So, at this point, you have a


1632
00:52:25,676 --> 00:52:27,286
full representation of your


1633
00:52:27,286 --> 00:52:30,176
object, which you can use for


1634
00:52:30,176 --> 00:52:30,686
detection.


1635
00:52:30,686 --> 00:52:33,106
And the application will now


1636
00:52:33,106 --> 00:52:34,976
switch to a detection mode.


1637
00:52:36,106 --> 00:52:37,386
We encourage you to use this


1638
00:52:37,386 --> 00:52:39,966
mode to get early feedback about


1639
00:52:39,966 --> 00:52:41,046
the detection quality.


1640
00:52:41,866 --> 00:52:44,776
So, you may want to go around


1641
00:52:44,776 --> 00:52:46,126
the object from different points


1642
00:52:46,126 --> 00:52:47,796
of view and verify that the


1643
00:52:47,796 --> 00:52:49,776
object is detected from all


1644
00:52:50,336 --> 00:52:51,696
these different point of view.


1645
00:52:51,696 --> 00:52:53,816
You can point your device away,


1646
00:52:53,816 --> 00:52:55,086
come back from another angle,


1647
00:52:55,836 --> 00:52:58,386
and make sure that the scan was


1648
00:52:58,386 --> 00:53:00,006
good to detect the object.


1649
00:53:00,526 --> 00:53:03,106
You can also, move these objects


1650
00:53:03,276 --> 00:53:04,506
around so that the light


1651
00:53:04,506 --> 00:53:05,866
condition will be different.


1652
00:53:06,836 --> 00:53:08,216
And you want to make sure that


1653
00:53:08,216 --> 00:53:09,196
those are still detected.


1654
00:53:09,196 --> 00:53:10,316
This is particularly important


1655
00:53:10,366 --> 00:53:12,536
for objects like toys that you


1656
00:53:12,536 --> 00:53:13,646
don't know where they're


1657
00:53:13,646 --> 00:53:14,986
actually going to be physically


1658
00:53:15,106 --> 00:53:15,546
located.


1659
00:53:17,096 --> 00:53:18,776
We, also, suggest that you take


1660
00:53:18,776 --> 00:53:20,176
the object and put it in a


1661
00:53:20,176 --> 00:53:21,556
completely different environment


1662
00:53:22,016 --> 00:53:24,036
and still make sure that it is


1663
00:53:24,076 --> 00:53:24,576
detected.


1664
00:53:25,666 --> 00:53:27,756
In case this is not detected you


1665
00:53:27,756 --> 00:53:28,806
may want to go back to the


1666
00:53:28,806 --> 00:53:31,406
scanning and make sure that your


1667
00:53:31,406 --> 00:53:32,506
environment is well lit.


1668
00:53:33,786 --> 00:53:35,956
We really like, well lit


1669
00:53:35,956 --> 00:53:37,156
environment during the scanning


1670
00:53:37,266 --> 00:53:37,916
is very important.


1671
00:53:38,506 --> 00:53:39,776
If your Verilux meter, it will


1672
00:53:39,776 --> 00:53:41,916
be about 500 lux will be best.


1673
00:53:43,046 --> 00:53:45,006
And if that is still not enough,


1674
00:53:45,166 --> 00:53:46,556
you may want to keep different


1675
00:53:46,556 --> 00:53:50,446
versions of the scans.


1676
00:53:50,576 --> 00:53:51,826
So, at this point, once you're


1677
00:53:51,826 --> 00:53:53,266
happy with the detection quality


1678
00:53:53,266 --> 00:53:55,136
you can simply drop the model to


1679
00:53:55,136 --> 00:53:57,846
your Mac and add it to the AR


1680
00:53:57,846 --> 00:53:59,656
Resource Groups, just like you


1681
00:53:59,746 --> 00:54:03,016
did for the images.


1682
00:54:03,016 --> 00:54:04,526
Also note that there are some


1683
00:54:04,526 --> 00:54:05,886
objects that will work really


1684
00:54:05,886 --> 00:54:07,366
great with this system.


1685
00:54:07,586 --> 00:54:08,706
Object like you can see on the


1686
00:54:08,706 --> 00:54:08,986
left.


1687
00:54:09,586 --> 00:54:10,956
First of all, they are rigid


1688
00:54:10,956 --> 00:54:13,206
objects and they are, also, rich


1689
00:54:13,206 --> 00:54:14,696
of texture, distinctive enough.


1690
00:54:15,436 --> 00:54:16,416
But there are also certain kinds


1691
00:54:16,416 --> 00:54:17,526
of object that will not work


1692
00:54:17,686 --> 00:54:18,506
well with the system.


1693
00:54:19,066 --> 00:54:21,256
You can see an example of this


1694
00:54:21,256 --> 00:54:22,136
on the right.


1695
00:54:22,686 --> 00:54:24,756
And for example, metallic,


1696
00:54:24,826 --> 00:54:26,606
transparent, or metallic or


1697
00:54:26,606 --> 00:54:27,796
reflective objects will not


1698
00:54:27,856 --> 00:54:28,096
work.


1699
00:54:29,206 --> 00:54:31,496
Or transparent objects like


1700
00:54:31,576 --> 00:54:32,796
glass material object will also


1701
00:54:32,796 --> 00:54:34,376
not work because the appearance


1702
00:54:34,376 --> 00:54:35,236
of these objects will really


1703
00:54:35,236 --> 00:54:37,766
depend on where they are in the


1704
00:54:38,816 --> 00:54:38,936
scene.


1705
00:54:39,756 --> 00:54:41,076
So, that was how to scan the


1706
00:54:41,076 --> 00:54:41,506
objects.


1707
00:54:41,706 --> 00:54:43,126
Again, make sure that you have


1708
00:54:43,126 --> 00:54:44,026
well-lit environment.


1709
00:54:44,996 --> 00:54:46,596
Let's now see how we can detect


1710
00:54:46,646 --> 00:54:48,056
this in ARKit.


1711
00:54:50,046 --> 00:54:51,956
If this looks familiar to you,


1712
00:54:51,956 --> 00:54:53,606
it's because the API is pretty


1713
00:54:53,606 --> 00:54:54,536
similar to the one of the


1714
00:54:54,536 --> 00:54:55,006
images.


1715
00:54:55,586 --> 00:54:56,716
We'll have convenience metered


1716
00:54:56,816 --> 00:54:58,586
to gather all the objects in a


1717
00:54:58,586 --> 00:54:58,896
group.


1718
00:54:59,506 --> 00:55:00,656
This time is in the


1719
00:55:00,656 --> 00:55:01,916
ARReferenceObjects class.


1720
00:55:02,806 --> 00:55:05,136
And to configure your


1721
00:55:05,136 --> 00:55:07,056
ARWorldTracking configuration,


1722
00:55:07,056 --> 00:55:08,516
you simply pass this object to


1723
00:55:08,516 --> 00:55:10,876
the detectionObjects property.


1724
00:55:13,206 --> 00:55:15,566
Once you run the session, again,


1725
00:55:15,566 --> 00:55:17,236
you will find your results.


1726
00:55:18,306 --> 00:55:19,356
And in this case, you want to


1727
00:55:19,356 --> 00:55:21,656
check for the ARObjectAnchor,


1728
00:55:22,386 --> 00:55:23,656
which will give you the position


1729
00:55:23,786 --> 00:55:25,436
and orientation of the object


1730
00:55:25,436 --> 00:55:27,866
with respect to the world.


1731
00:55:28,716 --> 00:55:30,346
And also, the name of the object


1732
00:55:30,346 --> 00:55:33,086
as was given in the asset


1733
00:55:35,016 --> 00:55:35,246
catalog.


1734
00:55:35,246 --> 00:55:36,516
So, you guys may have noticed


1735
00:55:36,706 --> 00:55:38,316
some similarities between the


1736
00:55:38,316 --> 00:55:40,506
object detection and the world


1737
00:55:40,506 --> 00:55:42,726
mapping relocalization.


1738
00:55:43,426 --> 00:55:44,206
But there's also few


1739
00:55:44,206 --> 00:55:44,806
differences.


1740
00:55:44,856 --> 00:55:46,316
So, in the case of the object


1741
00:55:46,316 --> 00:55:48,406
detection we are always giving


1742
00:55:48,746 --> 00:55:50,446
the object position with respect


1743
00:55:50,446 --> 00:55:50,946
to the world.


1744
00:55:51,506 --> 00:55:52,426
While in the world map


1745
00:55:52,426 --> 00:55:53,966
relocalization is the camera


1746
00:55:53,966 --> 00:55:56,066
itself that adjusts to the


1747
00:55:56,066 --> 00:55:56,816
previous world map.


1748
00:55:58,286 --> 00:56:00,306
In addition, you can detect


1749
00:56:00,536 --> 00:56:01,336
multiple objects.


1750
00:56:01,966 --> 00:56:03,626
And object detection works best


1751
00:56:03,736 --> 00:56:05,306
for objects which are tabletop,


1752
00:56:05,496 --> 00:56:06,376
furniture sized.


1753
00:56:07,146 --> 00:56:08,446
While, the world map is really


1754
00:56:08,446 --> 00:56:09,536
the whole scene that's been


1755
00:56:09,776 --> 00:56:10,266
acquired.


1756
00:56:10,776 --> 00:56:13,756
With this side, we conclude the


1757
00:56:13,756 --> 00:56:14,576
object detection.


1758
00:56:14,686 --> 00:56:16,266
Let's summarize what you have


1759
00:56:17,796 --> 00:56:19,336
seen, today.


1760
00:56:19,516 --> 00:56:21,876
Orientation tracking tracks only


1761
00:56:21,876 --> 00:56:23,696
the rotation of the device and


1762
00:56:23,696 --> 00:56:25,756
can be used to explore statical


1763
00:56:25,756 --> 00:56:26,416
environments.


1764
00:56:27,946 --> 00:56:29,266
World Tracking is the fully


1765
00:56:29,266 --> 00:56:30,776
featured position and


1766
00:56:30,776 --> 00:56:32,246
orientation tracking, which will


1767
00:56:32,246 --> 00:56:33,926
give you the device position


1768
00:56:33,926 --> 00:56:36,286
with respect to a world origin.


1769
00:56:37,016 --> 00:56:38,776
And enables all the scene


1770
00:56:38,776 --> 00:56:40,486
understanding capabilities like


1771
00:56:41,166 --> 00:56:43,366
the Plane Detection, which will


1772
00:56:44,476 --> 00:56:46,256
make you able to interact with


1773
00:56:46,256 --> 00:56:48,546
the physical, horizontal, and


1774
00:56:48,546 --> 00:56:50,226
vertical planes where you can


1775
00:56:50,226 --> 00:56:51,636
then put virtual objects.


1776
00:56:51,996 --> 00:56:55,306
We have seen how you can create


1777
00:56:55,656 --> 00:56:57,036
persistent or multiuser


1778
00:56:57,036 --> 00:56:58,926
experiences with the saving and


1779
00:56:58,926 --> 00:57:00,116
loading map features in the


1780
00:57:00,116 --> 00:57:00,566
architecture.


1781
00:57:01,516 --> 00:57:03,036
And how you can detect physical


1782
00:57:03,036 --> 00:57:04,816
images and track them at 60


1783
00:57:04,816 --> 00:57:06,096
frames per second with the image


1784
00:57:06,096 --> 00:57:08,596
tracking and how you can detect


1785
00:57:08,596 --> 00:57:09,916
more generic objects with the


1786
00:57:09,916 --> 00:57:11,176
object detections.


1787
00:57:12,576 --> 00:57:15,216
And with this, I really hope you


1788
00:57:15,216 --> 00:57:15,946
guys have a better


1789
00:57:15,946 --> 00:57:17,496
understanding, now, of all the


1790
00:57:17,496 --> 00:57:19,126
different tracking technology


1791
00:57:19,586 --> 00:57:20,816
that are present in ARKit and


1792
00:57:20,816 --> 00:57:22,016
how they work behind the scenes.


1793
00:57:23,006 --> 00:57:24,666
And how you can get the best


1794
00:57:24,806 --> 00:57:25,976
quality out of it.


1795
00:57:26,436 --> 00:57:28,106
And we're really looking forward


1796
00:57:28,106 --> 00:57:29,226
to see what you guys are going


1797
00:57:29,226 --> 00:57:29,656
to do with that.


1798
00:57:30,976 --> 00:57:32,416
More information can be found at


1799
00:57:32,486 --> 00:57:33,636
the session link in the


1800
00:57:33,636 --> 00:57:34,496
developer website.


1801
00:57:34,496 --> 00:57:36,226
And we have an ARKit Lab


1802
00:57:36,536 --> 00:57:37,586
tomorrow, 9 a.m.


1803
00:57:38,356 --> 00:57:39,806
We will both, me and Marion will


1804
00:57:39,806 --> 00:57:41,786
be there answering any question


1805
00:57:41,786 --> 00:57:42,666
on ARKit you may have.


1806
00:57:43,756 --> 00:57:44,976
And with that, thank you, very


1807
00:57:44,976 --> 00:57:46,976
much and enjoy the bash.


1808
00:57:47,516 --> 00:57:53,506
[ Applause ]

