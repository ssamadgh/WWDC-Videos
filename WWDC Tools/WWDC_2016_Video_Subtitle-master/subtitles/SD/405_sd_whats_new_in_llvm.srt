1
00:00:06,516 --> 00:00:17,500
[ Music ]


2
00:00:21,516 --> 00:00:29,226
[ Applause ]


3
00:00:29,726 --> 00:00:31,046
>> Hi, I'm Alex Rosenberg.


4
00:00:31,356 --> 00:00:32,426
I'm incredibly excited to share


5
00:00:32,426 --> 00:00:34,666
with you some amazing new
features we have in store


6
00:00:34,666 --> 00:00:36,056
for the Apple LLVM Compiler.


7
00:00:36,876 --> 00:00:39,266
But first, I want to talk
a little bit about LLVM,


8
00:00:40,216 --> 00:00:42,786
the project upon which we
build the Apple LLVM Compiler.


9
00:00:44,186 --> 00:00:45,686
LLVM is a modular framework


10
00:00:46,076 --> 00:00:48,486
for building compilers
and related tools.


11
00:00:49,236 --> 00:00:51,946
However, it need not be used
solely in this traditional way.


12
00:00:53,266 --> 00:00:56,656
We all know and love
Xcode, which uses many


13
00:00:56,656 --> 00:00:57,996
of the LLVM frameworks
internally.


14
00:00:58,746 --> 00:01:03,566
And now the amazing, new Swift
Playgrounds app, has joined it


15
00:01:04,135 --> 00:01:07,786
with similar internal
uses of LLVM.


16
00:01:08,026 --> 00:01:10,726
LLVM is also an integral part
of the inner workings of Metal


17
00:01:10,726 --> 00:01:14,626
and our other graphics APIs.


18
00:01:14,826 --> 00:01:16,256
LVVM is open source.


19
00:01:16,846 --> 00:01:18,136
Apple has a long history


20
00:01:18,166 --> 00:01:20,116
with open source
compilers and languages.


21
00:01:20,926 --> 00:01:24,036
The Swift language was born
out of the LLVM project.


22
00:01:24,936 --> 00:01:27,916
We're very happy with how you,
the community of contributors,


23
00:01:28,436 --> 00:01:30,936
have move Swift forward via
the open evolution process


24
00:01:30,936 --> 00:01:31,766
on Swift.org.


25
00:01:32,816 --> 00:01:36,066
That process, and the Swift
development processes were


26
00:01:36,066 --> 00:01:39,286
informed by the existing
LLVM project processes.


27
00:01:40,436 --> 00:01:42,736
LLVM has a thriving
open source community,


28
00:01:43,036 --> 00:01:45,226
backed by the nonprofit
LLVM Foundation.


29
00:01:45,876 --> 00:01:50,226
LLVM is highly customizable
and composable.


30
00:01:51,176 --> 00:01:53,816
By leveraging the same robust
and mature infrastructure,


31
00:01:53,916 --> 00:01:55,226
underpinning the
Clang front end,


32
00:01:55,866 --> 00:01:57,346
we power the Swift
compiler as well.


33
00:01:58,896 --> 00:02:01,966
For more info about Swift,
review the talk from earlier.


34
00:02:03,286 --> 00:02:04,726
Let's take a brief
look into some


35
00:02:04,726 --> 00:02:06,736
of these frameworks
available from open source


36
00:02:06,796 --> 00:02:07,986
and how they might be used.


37
00:02:09,776 --> 00:02:13,226
Clang is the front end for
C, Objective-C, and C++.


38
00:02:14,376 --> 00:02:16,546
Its libraries provide
for advanced features,


39
00:02:16,676 --> 00:02:19,626
like static analysis, and
in place for your writing,


40
00:02:19,866 --> 00:02:22,136
for code mitigation,
and modification.


41
00:02:23,426 --> 00:02:25,356
It also powers development
environment integration


42
00:02:25,356 --> 00:02:27,616
features, like source
code indexing


43
00:02:27,616 --> 00:02:31,146
and intelligent code completion
that we all love in Xcode.


44
00:02:32,896 --> 00:02:36,586
The open source tooling library
helps harness the power of Clang


45
00:02:36,586 --> 00:02:40,186
for your own custom command-line
tools to process source code.


46
00:02:40,986 --> 00:02:44,256
Imagine the amazing things
you could do with the power


47
00:02:44,256 --> 00:02:47,686
of a full parser at your
command, running on your code.


48
00:02:51,176 --> 00:02:54,836
The LLVM optimizer,
which we'll speak more


49
00:02:54,836 --> 00:02:57,096
about in a little
bit, has a full suite


50
00:02:57,096 --> 00:02:59,036
of modern compiler
optimizations.


51
00:02:59,786 --> 00:03:02,316
And this is where features like
Link-Time Optimization happen.


52
00:03:02,896 --> 00:03:05,596
Stay tuned for some
exciting developments in LTO.


53
00:03:06,096 --> 00:03:10,856
And then we have the back end,


54
00:03:10,976 --> 00:03:12,736
where final code
generation happens.


55
00:03:13,646 --> 00:03:15,776
There are libraries for
object file manipulation,


56
00:03:16,086 --> 00:03:18,016
features like assembly
and disassembly,


57
00:03:18,016 --> 00:03:19,636
and even advanced features


58
00:03:19,636 --> 00:03:25,006
like just-in-time compilation
can be simply plugged together.


59
00:03:25,006 --> 00:03:28,046
LLVM has many other tools
composed from its frameworks.


60
00:03:28,536 --> 00:03:29,186
Let's take a look.


61
00:03:30,376 --> 00:03:33,236
As part of a complete
LLVM based toolchain,


62
00:03:33,896 --> 00:03:36,296
there are the usual suite
of binary file utilities,


63
00:03:36,576 --> 00:03:39,246
including a burgeoning open
source effort on a framework


64
00:03:39,546 --> 00:03:41,386
for making linkers
and related tools.


65
00:03:42,396 --> 00:03:45,376
But perhaps the most prominent
of these projects is LLDB.


66
00:03:46,176 --> 00:03:49,666
LLDB combines many of the
libraries from Clang, Swift,


67
00:03:50,036 --> 00:03:52,996
the Code Generator, and its own
suite of debugger frameworks.


68
00:03:53,606 --> 00:03:56,426
There will be a great session of
LLDB tips and tricks on Friday.


69
00:03:57,696 --> 00:04:03,016
All of the amazing features
in Xcode, Swift Playgrounds,


70
00:04:03,246 --> 00:04:05,786
and the Apple LLVM Compiler
wouldn't be possible


71
00:04:06,096 --> 00:04:09,436
without the contributions from
the LLVM open source community.


72
00:04:10,186 --> 00:04:13,136
The community is made up of
many developers like you,


73
00:04:13,636 --> 00:04:16,526
inspired to bring their own
ideas into the LLVM frameworks.


74
00:04:18,435 --> 00:04:22,176
The LLVM open source project
is growing at a stunning pace


75
00:04:22,436 --> 00:04:25,236
that would be extremely
challenging for any one team,


76
00:04:25,346 --> 00:04:29,046
at any one company, to match.


77
00:04:29,236 --> 00:04:32,446
We'd like to invite you to check
out the project, at LLVM.org,


78
00:04:32,936 --> 00:04:34,666
and see how you can
fit into the community


79
00:04:34,666 --> 00:04:36,466
and contribute your
unique ideas.


80
00:04:36,466 --> 00:04:41,156
And now I'd like to introduce
Duncan, who will go over some


81
00:04:41,156 --> 00:04:43,016
of the exciting new
language features we have


82
00:04:43,266 --> 00:04:44,486
in the Apple LLVM Compiler.


83
00:04:45,516 --> 00:04:52,766
[ Applause ]


84
00:04:53,266 --> 00:04:54,686
>> Let's talk about
language support.


85
00:04:55,966 --> 00:04:59,176
First, we'll talk about new
language features, then updates


86
00:04:59,176 --> 00:05:02,746
to the C++ library, and
finally new errors and warnings


87
00:05:02,746 --> 00:05:04,506
to help improve your code.


88
00:05:05,046 --> 00:05:07,656
Let's start with new
language features.


89
00:05:09,256 --> 00:05:11,676
Objective-C now supports
class properties.


90
00:05:12,406 --> 00:05:15,076
This feature started in
Swift as type properties,


91
00:05:15,206 --> 00:05:16,686
and we've brought
it to Objective-C.


92
00:05:17,476 --> 00:05:18,956
Interoperation works great.


93
00:05:20,886 --> 00:05:24,436
In this example, the class
property someString is declared


94
00:05:24,746 --> 00:05:27,806
using property syntax,
by adding a class flag.


95
00:05:28,896 --> 00:05:32,296
Later, this someString property
is accessed using dot syntax.


96
00:05:34,046 --> 00:05:35,976
Class properties are
never synthesized.


97
00:05:36,346 --> 00:05:39,546
You can provide storage,
a getter, and a setter,


98
00:05:39,676 --> 00:05:40,626
in the implementation.


99
00:05:42,006 --> 00:05:44,936
Or you can use @dynamic to
defer resolution to runtime.


100
00:05:44,936 --> 00:05:48,716
Over to C++.


101
00:05:50,276 --> 00:05:54,056
LLVM has had great support
for C++11 for years.


102
00:05:54,596 --> 00:05:56,216
The only holdout
has been support


103
00:05:56,216 --> 00:05:57,426
for the thread-local keyword.


104
00:05:58,436 --> 00:05:59,856
This year, we added support.


105
00:06:00,606 --> 00:06:02,666
Let me tell you a
little about it.


106
00:06:03,806 --> 00:06:06,576
If a variable is declared
with a thread-local keyword,


107
00:06:07,026 --> 00:06:09,646
LLVM creates a separate
variable per thread.


108
00:06:11,056 --> 00:06:13,586
Initializers are called
before the first use


109
00:06:13,586 --> 00:06:14,456
after thread entry,


110
00:06:14,866 --> 00:06:16,846
and destructors are
called on thread exit.


111
00:06:18,466 --> 00:06:24,026
C++ style thread-local
storage, supports any C++ type.


112
00:06:26,386 --> 00:06:29,956
And its syntax is portable
with other C++ compilers.


113
00:06:31,456 --> 00:06:35,016
The Apple LLVM Compiler,
already has support


114
00:06:35,016 --> 00:06:36,716
for C-style thread-local
storage,


115
00:06:37,096 --> 00:06:39,316
even when compiling C++ code.


116
00:06:39,866 --> 00:06:41,326
There are two syntaxes
available:


117
00:06:41,886 --> 00:06:43,646
One with a GCC style keyword,


118
00:06:43,946 --> 00:06:45,576
and another from
the C11 standard.


119
00:06:46,976 --> 00:06:49,756
C-style thread-local
storage has lower overhead


120
00:06:49,806 --> 00:06:52,856
than C++ thread local,
but it has restrictions.


121
00:06:53,026 --> 00:06:55,266
It requires constant
initializers


122
00:06:55,266 --> 00:06:56,556
and plain old data types.


123
00:06:58,076 --> 00:06:59,976
If your code meets
these restrictions,


124
00:07:00,126 --> 00:07:03,226
you should continue to use
C-style thread-local storage


125
00:07:03,226 --> 00:07:04,526
for maximum performance.


126
00:07:05,276 --> 00:07:07,866
Otherwise, use the C++
thread-local keyword.


127
00:07:08,366 --> 00:07:09,146
Both work great.


128
00:07:10,616 --> 00:07:13,196
Thread-local variables
can help fix bugs


129
00:07:13,196 --> 00:07:14,666
in code that uses threads.


130
00:07:15,376 --> 00:07:19,336
For more on thread related
bugs, watch the Thread Sanitizer


131
00:07:19,336 --> 00:07:21,346
and Static Analysis talk.


132
00:07:22,436 --> 00:07:24,236
That's it for new
language features.


133
00:07:24,486 --> 00:07:28,786
Let's move on to the
C++ standard library.


134
00:07:29,396 --> 00:07:34,726
Libc++ has been the default
C++ standard library for years.


135
00:07:35,276 --> 00:07:38,426
We've been encouraging you
to move off of Libstandardc++


136
00:07:38,426 --> 00:07:41,996
and in Xcode 8, we've deprecated
it on all our platforms.


137
00:07:42,596 --> 00:07:44,536
Please move on as
soon as you can.


138
00:07:45,856 --> 00:07:49,186
If your Xcode project
still uses Libstandardc++,


139
00:07:49,586 --> 00:07:51,486
you should upgrade to Libc++


140
00:07:51,586 --> 00:07:54,646
by changing the C++ standard
library build setting.


141
00:07:55,296 --> 00:07:57,506
Xcode's project modernization
will offer


142
00:07:57,506 --> 00:07:58,706
to do this automatically.


143
00:08:00,296 --> 00:08:03,526
The Libc++.dylib has a
great update this year


144
00:08:03,526 --> 00:08:04,876
for all our platforms.


145
00:08:05,016 --> 00:08:08,096
It has complete library
support for C++14,


146
00:08:08,466 --> 00:08:10,246
as well as many other
improvements.


147
00:08:11,416 --> 00:08:13,746
Standard library features
that require the dylib,


148
00:08:14,166 --> 00:08:16,086
now have availability
attributes.


149
00:08:16,856 --> 00:08:20,206
While Apple frameworks encourage
deploying to old targets


150
00:08:20,206 --> 00:08:21,646
through the use of
runtime checks,


151
00:08:22,186 --> 00:08:25,286
the C++ standard library
availability checks are done


152
00:08:25,286 --> 00:08:26,096
at compile time.


153
00:08:27,306 --> 00:08:30,556
To use C++ features that
require the newest dylib,


154
00:08:30,966 --> 00:08:33,496
you need to target a
platform that supports them.


155
00:08:34,395 --> 00:08:37,616
That's the C++ library.


156
00:08:38,616 --> 00:08:42,196
Between Xcode 7 and
Xcode 8, we've added more


157
00:08:42,196 --> 00:08:43,966
than 100 new errors and warnings


158
00:08:43,966 --> 00:08:45,406
to help you find
bugs in your code.


159
00:08:46,256 --> 00:08:49,716
Let's talk about
just a few of them.


160
00:08:49,866 --> 00:08:53,086
In Xcode 7, we added
a great feature


161
00:08:53,086 --> 00:08:55,426
to Objective-C called
Lightweight Generics.


162
00:08:56,196 --> 00:08:58,966
The kind of type modifier
plays an important role,


163
00:08:59,306 --> 00:09:01,906
allowing implicit downcast
from the kind of type


164
00:09:02,066 --> 00:09:03,396
to any of its subclasses.


165
00:09:04,416 --> 00:09:07,136
In Xcode 8, we've
improved the diagnostics


166
00:09:07,136 --> 00:09:09,196
for method lookup
on kind of types.


167
00:09:10,176 --> 00:09:11,326
In this example,


168
00:09:11,586 --> 00:09:14,676
getAwesomeNumber is declared
inside My Custom Type,


169
00:09:15,126 --> 00:09:16,616
which inherits from NSObject.


170
00:09:17,546 --> 00:09:21,086
Later, getAwesomeNumber is
called on a kindof UIView.


171
00:09:21,916 --> 00:09:25,196
This code is broken since
My Custom Type is unrelated


172
00:09:25,196 --> 00:09:25,856
to UIView.


173
00:09:27,246 --> 00:09:28,866
Xcode 8 gives an error here.


174
00:09:29,536 --> 00:09:32,736
Type checking of methods called
on kind of types, is restricted


175
00:09:32,736 --> 00:09:33,986
to the same class hierarchy.


176
00:09:34,696 --> 00:09:37,896
This improved type checking
also avoids misleading warnings


177
00:09:37,966 --> 00:09:41,196
when an unrelated type declares
a method with the same name.


178
00:09:41,916 --> 00:09:48,666
In Xcode 8, kind of types
are way easier to use.


179
00:09:48,846 --> 00:09:51,966
Next, containers in
Objective-C, such NSArray


180
00:09:51,966 --> 00:09:54,646
or NSMutableSet can
hold arbitrary objects.


181
00:09:55,096 --> 00:09:57,786
However, the NSMutableSet
called s


182
00:09:57,846 --> 00:09:59,866
in this example is
added to itself.


183
00:10:01,476 --> 00:10:03,556
This creates a circular
dependency,


184
00:10:03,756 --> 00:10:05,526
which triggers a
warning in Xcode 8.


185
00:10:06,586 --> 00:10:08,536
Besides creating a
strong reference cycle,


186
00:10:08,826 --> 00:10:11,166
a circular dependency
can prevent some methods


187
00:10:11,246 --> 00:10:14,966
from being well-defined.


188
00:10:15,636 --> 00:10:16,816
Infinite recursion.


189
00:10:18,256 --> 00:10:20,726
This example implements
the factorial function.


190
00:10:21,696 --> 00:10:25,156
If n is positive, it returns n
times factorial of n minus 1,


191
00:10:25,206 --> 00:10:26,856
recursing to calculate
the answer.


192
00:10:27,916 --> 00:10:31,616
If n is zero, it returns
factorial of 1, also recursing.


193
00:10:33,006 --> 00:10:34,546
The compiler now has a warning.


194
00:10:34,946 --> 00:10:37,196
When all pass through a
function, call itself.


195
00:10:38,066 --> 00:10:40,656
This catches common cases
of infinite recursion.


196
00:10:42,356 --> 00:10:50,266
Here, one possible fix is
to return 1 when n is zero.


197
00:10:50,426 --> 00:10:53,526
Standard Move is a great
C++ language feature.


198
00:10:53,526 --> 00:10:56,936
It allows you to specify that
ownership should be transferred


199
00:10:56,936 --> 00:10:58,486
from one container to another.


200
00:10:59,366 --> 00:11:02,176
Moving a resource is
faster than a deep copy.


201
00:11:04,116 --> 00:11:07,386
However, using standard
move on a return value,


202
00:11:07,576 --> 00:11:09,986
blocks named return
value optimizations.


203
00:11:10,866 --> 00:11:13,596
Typically, when a local
variable is returned by value,


204
00:11:14,056 --> 00:11:16,706
the compiler can avoid
making a copy at all.


205
00:11:17,296 --> 00:11:20,586
In generateBars, calling
standard move forces the


206
00:11:20,586 --> 00:11:22,446
compiler to move bars.


207
00:11:24,206 --> 00:11:27,516
Although moving is fast,
doing nothing is even faster.


208
00:11:28,156 --> 00:11:32,386
LLVM now warns about
this pessimizing move.


209
00:11:32,666 --> 00:11:35,576
The fix is to avoid standard
move on return values,


210
00:11:35,576 --> 00:11:37,266
getting back the
lost performance.


211
00:11:38,756 --> 00:11:42,026
Similarly, when a function
takes an argument by value,


212
00:11:42,176 --> 00:11:43,706
and returns it by value,


213
00:11:44,126 --> 00:11:47,186
the compiler automatically uses
standard move on the return.


214
00:11:47,636 --> 00:11:49,216
There's no need to
call it explicitly.


215
00:11:50,536 --> 00:11:52,936
For example, the standard
move on the return value


216
00:11:52,936 --> 00:11:54,696
in rewriteText is redundant.


217
00:11:55,546 --> 00:11:57,726
Although this doesn't
actively hurt performance,


218
00:11:57,966 --> 00:11:59,206
it makes the code less readable.


219
00:12:00,306 --> 00:12:02,076
It's better to return
text directly.


220
00:12:02,526 --> 00:12:04,956
This is easier to
maintain and consistent


221
00:12:04,956 --> 00:12:06,406
with returning local variables.


222
00:12:06,776 --> 00:12:14,036
Finally, there are new warnings
for references to temporaries


223
00:12:14,036 --> 00:12:15,956
in C++ range-based for loops.


224
00:12:17,316 --> 00:12:20,496
Here, the loop is iterating
through a vector of shorts,


225
00:12:20,736 --> 00:12:24,096
but the iteration variable i,
is a const reference to an int.


226
00:12:25,226 --> 00:12:28,266
Because of the implicit
conversion between short


227
00:12:28,266 --> 00:12:30,836
and int, i is a temporary.


228
00:12:32,706 --> 00:12:35,226
This can lead to subtle
bugs, since it looks


229
00:12:35,226 --> 00:12:38,326
as if i is pointing inside
the range, but it isn't.


230
00:12:39,146 --> 00:12:42,076
The compiler now warns about
this unexpected conversion.


231
00:12:42,856 --> 00:12:45,586
One fix is to change i to
a const reference of short.


232
00:12:46,776 --> 00:12:49,766
Another is to make it
clear that i is a temporary


233
00:12:50,046 --> 00:12:51,306
by removing the reference.


234
00:12:52,456 --> 00:12:53,946
There is a similar warning


235
00:12:54,426 --> 00:12:56,426
when a range doesn't
return a reference at all.


236
00:12:56,426 --> 00:12:59,826
An iterator to standard
vector of bool,


237
00:12:59,826 --> 00:13:01,586
does not return a reference.


238
00:13:01,686 --> 00:13:05,286
So the iteration variable b in
this example is a temporary.


239
00:13:06,606 --> 00:13:10,026
It's surprising that b does
not point inside the vector.


240
00:13:10,346 --> 00:13:12,806
The compiler now warns
about this unexpected copy.


241
00:13:13,476 --> 00:13:18,116
The fix is to remove the
reference making it clear


242
00:13:18,456 --> 00:13:19,516
that b is a temporary.


243
00:13:19,976 --> 00:13:24,086
The new warnings for infinite
recursion, standard move,


244
00:13:24,226 --> 00:13:27,516
and C++ range-based for
loops are enabled by default.


245
00:13:28,086 --> 00:13:30,226
To try them out in
your expo project,


246
00:13:30,676 --> 00:13:32,866
add them to the Other
Warning Flags Build setting.


247
00:13:34,046 --> 00:13:35,576
That's it for new diagnostics.


248
00:13:36,046 --> 00:13:40,216
Let's move on to advancements
in compiler optimization.


249
00:13:40,716 --> 00:13:46,816
We've made improvements
throughout the LLVM compiler


250
00:13:46,996 --> 00:13:49,536
to optimize the runtime
performance of your code.


251
00:13:50,606 --> 00:13:52,256
We've picked just a
few to highlight today.


252
00:13:53,456 --> 00:13:56,236
We'll talk about improvements
to Link-Time Optimization,


253
00:13:56,646 --> 00:13:58,976
highlight new code
generation optimizations,


254
00:13:59,266 --> 00:14:02,376
and describe techniques
for arm64 cache tuning.


255
00:14:06,996 --> 00:14:09,406
It has been a couple of
years since we've talked


256
00:14:09,406 --> 00:14:10,736
about Link-Time Optimization


257
00:14:11,156 --> 00:14:12,756
and we have major
improvements to share.


258
00:14:13,896 --> 00:14:17,756
Link-Time Optimization or
LTO, optimizes the executable


259
00:14:17,786 --> 00:14:19,396
as a single monolithic unit.


260
00:14:20,106 --> 00:14:23,686
It inlines functions across
source files, removes dead code,


261
00:14:23,796 --> 00:14:27,026
and performs other powerful,
whole program optimizations.


262
00:14:28,016 --> 00:14:31,566
LTO blurs the line between
the compiler and the linker.


263
00:14:32,946 --> 00:14:35,936
To understand how LTO
works, let's first look


264
00:14:35,936 --> 00:14:37,686
at the traditional
compilation model.


265
00:14:38,746 --> 00:14:40,446
Let's say we have
four source files.


266
00:14:41,336 --> 00:14:44,886
The first step is to compile,
producing four object files.


267
00:14:46,446 --> 00:14:52,126
The object files are linked with
frameworks, to produce an app.


268
00:14:52,386 --> 00:14:54,406
An LTO build starts
the same way,


269
00:14:55,396 --> 00:14:57,806
by compiling the sources
into object files.


270
00:14:58,876 --> 00:15:02,586
In LTO, these object files
contain extra optimization


271
00:15:02,586 --> 00:15:04,896
information which
enables the linker


272
00:15:04,896 --> 00:15:06,936
to perform Link-Time
Optimizations,


273
00:15:07,196 --> 00:15:10,276
and produce a single,
monolithic object file.


274
00:15:11,236 --> 00:15:13,726
The output of LTO is
linked with frameworks


275
00:15:13,726 --> 00:15:18,726
such as Foundation,
to produce the app.


276
00:15:19,506 --> 00:15:21,756
LTO maximizes performance.


277
00:15:23,656 --> 00:15:26,796
Apple uses LTO extensively
on our own software,


278
00:15:27,356 --> 00:15:29,066
typically speeding
up executables


279
00:15:29,116 --> 00:15:31,866
by 10 percent compared to
regular release builds.


280
00:15:33,096 --> 00:15:34,836
Its effects multiply
when combined


281
00:15:34,836 --> 00:15:36,606
with profile guided
optimization.


282
00:15:37,296 --> 00:15:40,536
It can also reduce
code size dramatically,


283
00:15:40,976 --> 00:15:42,986
when optimizing for size.


284
00:15:44,336 --> 00:15:47,526
However, it has cost
at compile time.


285
00:15:48,276 --> 00:15:50,966
The monolithic optimization
step can have large memory


286
00:15:50,966 --> 00:15:54,016
requirements, doesn't take
advantage of all your cores,


287
00:15:54,536 --> 00:15:56,536
and has to be repeated
in incremental builds.


288
00:15:57,896 --> 00:16:00,546
Large C++ programs
with debug info,


289
00:16:00,626 --> 00:16:02,326
have been the most
expensive to compile.


290
00:16:03,966 --> 00:16:06,096
Over the last two
years, we've worked hard


291
00:16:06,096 --> 00:16:07,036
to reduce the overhead.


292
00:16:08,416 --> 00:16:10,326
For example, let's look
at the memory usage


293
00:16:10,326 --> 00:16:13,226
for linking the Apple
LLVM compiler itself


294
00:16:13,486 --> 00:16:14,986
with LTO and debug info.


295
00:16:15,446 --> 00:16:16,466
Smaller bars are better.


296
00:16:17,166 --> 00:16:20,236
In Xcode 6, this took
over 40 gigabytes.


297
00:16:21,206 --> 00:16:24,006
Since then, we've reduced
the memory usage by 4x.


298
00:16:24,576 --> 00:16:27,116
We've also reduced
compile time by 33 percent.


299
00:16:29,166 --> 00:16:32,796
The Line Tables Only debug
information level uses even less


300
00:16:32,796 --> 00:16:33,616
memory in LTO.


301
00:16:34,426 --> 00:16:39,786
Linking LLVM itself,
now takes only 7 gigs.


302
00:16:39,786 --> 00:16:42,076
LTO has never been better.


303
00:16:43,546 --> 00:16:46,436
But there is still a compile
time trade-off, particularly


304
00:16:46,436 --> 00:16:47,446
for incremental builds.


305
00:16:48,096 --> 00:16:52,086
We have an exciting
new technology


306
00:16:52,086 --> 00:16:53,496
that designs away these costs.


307
00:16:54,656 --> 00:16:57,696
Incremental LTO scales
with your system.


308
00:16:58,876 --> 00:17:01,146
It performs global
analysis and inlining


309
00:17:01,286 --> 00:17:02,836
without combining object files.


310
00:17:04,185 --> 00:17:07,886
This speeds up the build because
it can optimize each object file


311
00:17:07,886 --> 00:17:08,465
in parallel.


312
00:17:09,256 --> 00:17:12,836
Moreover, since the
object files stay separate,


313
00:17:13,296 --> 00:17:14,306
and you build cache


314
00:17:14,306 --> 00:17:17,146
in the linker keeps
incremental builds superfast.


315
00:17:17,146 --> 00:17:22,366
Let's look at how an
incremental LTO build works.


316
00:17:23,806 --> 00:17:26,266
The compile step is the
same as monolithic LTO,


317
00:17:26,945 --> 00:17:29,296
producing one object file
for each source file.


318
00:17:31,136 --> 00:17:35,246
Without combining object files,
the linker runs an LTO analysis


319
00:17:35,246 --> 00:17:36,426
of the entire program.


320
00:17:37,756 --> 00:17:39,656
This analysis feeds
optimizations


321
00:17:39,656 --> 00:17:43,296
for each object file, enabling
the objects to inline functions


322
00:17:43,296 --> 00:17:44,406
from each other, as well


323
00:17:44,406 --> 00:17:46,876
as other powerful whole
program optimizations.


324
00:17:48,576 --> 00:17:53,026
The LTO optimized object files
are stored in a linker cache,


325
00:17:54,456 --> 00:17:57,566
before being linked with
frameworks to produce the app.


326
00:17:59,256 --> 00:18:02,526
The resulting runtime
performance of programs built


327
00:18:02,526 --> 00:18:05,906
with incremental LTO, is
similar to monolithic LTO,


328
00:18:06,696 --> 00:18:08,236
with a few benchmarks
running slower,


329
00:18:08,236 --> 00:18:09,616
and a few running faster.


330
00:18:10,206 --> 00:18:12,676
But it's fast at
compile time too.


331
00:18:13,886 --> 00:18:16,856
Let's look at build times
with my favorite C++ project:


332
00:18:17,396 --> 00:18:19,036
The Apple LLVM compiler itself.


333
00:18:20,366 --> 00:18:22,006
In this graph, smaller
bars are better.


334
00:18:22,636 --> 00:18:24,796
The time at the top is
for building without LTO.


335
00:18:26,106 --> 00:18:29,066
Monolithic LTO adds
significant build time overhead,


336
00:18:29,286 --> 00:18:31,516
taking almost 20
minutes, instead of 6.


337
00:18:32,676 --> 00:18:35,586
Incremental LTO is much
faster at under 8 minutes,


338
00:18:35,646 --> 00:18:37,476
adding only 25 percent overhead.


339
00:18:38,146 --> 00:18:41,646
Let's zoom in on the link step
where LTO is doing its work.


340
00:18:43,066 --> 00:18:45,846
Without LTO, linking the
Apple LLVM compiler itself,


341
00:18:45,846 --> 00:18:47,366
takes less than two seconds.


342
00:18:47,956 --> 00:18:49,586
You can't see the
bar in the graph,


343
00:18:49,666 --> 00:18:51,366
because the linker
isn't performing any


344
00:18:51,366 --> 00:18:52,766
compiler optimizations.


345
00:18:54,036 --> 00:18:56,456
Monolithic LTO takes
almost 14 minutes,


346
00:18:57,846 --> 00:18:59,576
when it can use all your cores.


347
00:18:59,936 --> 00:19:02,116
Incremental LTO takes two
minutes and a quarter,


348
00:19:02,416 --> 00:19:05,016
more than 6x faster
than monolithic LTO.


349
00:19:06,506 --> 00:19:08,016
Memory usage is great too.


350
00:19:08,786 --> 00:19:12,266
Without LTO, linking the Apple
LLVM compiler uses a little more


351
00:19:12,266 --> 00:19:13,546
than 200 megabytes.


352
00:19:14,336 --> 00:19:17,746
As we saw earlier, monolithic
LTO takes 7 gigabytes.


353
00:19:19,156 --> 00:19:21,886
Incremental LTO uses
less than 800 megabytes.


354
00:19:22,226 --> 00:19:23,616
The scaling is incredible.


355
00:19:26,516 --> 00:19:32,436
[ Applause ]


356
00:19:32,936 --> 00:19:34,496
All these results are
for a fresh build.


357
00:19:35,026 --> 00:19:35,786
It gets even better.


358
00:19:36,846 --> 00:19:38,616
With incremental LTO,


359
00:19:38,666 --> 00:19:41,296
incremental builds don't
repeat unnecessary work.


360
00:19:42,376 --> 00:19:44,886
Let's look at an example
where the controller changes,


361
00:19:45,176 --> 00:19:48,666
and you start an
incremental build of the app.


362
00:19:48,926 --> 00:19:51,616
Changing the controller
invalidates the link itself,


363
00:19:52,096 --> 00:19:55,346
but the other LTO object files
are still in the linker cache.


364
00:19:56,146 --> 00:19:59,706
However, if a function from the
controller is inline into main,


365
00:20:00,206 --> 00:20:03,336
then main needs to be
reoptimized at LTO time as well.


366
00:20:03,946 --> 00:20:08,326
So we start the build, and
only the controller needs


367
00:20:08,326 --> 00:20:10,856
to be recompiled.


368
00:20:10,856 --> 00:20:13,846
After rerunning LTO
analysis, both controller.O


369
00:20:13,846 --> 00:20:17,486
and main.O are optimized and
new LTO object files are stored


370
00:20:17,486 --> 00:20:18,376
in the linker cache.


371
00:20:19,516 --> 00:20:25,186
Then the LTO objects are linked
as before to produce the app.


372
00:20:25,456 --> 00:20:28,456
Incremental LTO gives you
the performance you expect


373
00:20:28,676 --> 00:20:29,766
from an incremental build.


374
00:20:30,896 --> 00:20:33,806
When a source file with small
helper functions is changed,


375
00:20:34,306 --> 00:20:36,616
object files that use
them get reoptimized.


376
00:20:37,426 --> 00:20:39,476
But for a typical
incremental build,


377
00:20:39,826 --> 00:20:43,476
most LTO object files are linked
directly from the linker cache.


378
00:20:45,146 --> 00:20:46,866
Let's take a final
look at the time


379
00:20:46,866 --> 00:20:49,236
to link the Apple
LLVM compiler itself.


380
00:20:49,946 --> 00:20:51,536
The top three bars
show the times


381
00:20:51,536 --> 00:20:52,926
for a fresh build from before.


382
00:20:54,496 --> 00:20:57,206
If we change the implementation
of an optimization pass,


383
00:20:57,926 --> 00:21:00,596
monolithic LTO takes
the same amount of time.


384
00:21:00,596 --> 00:21:01,696
It's the same as a fresh build.


385
00:21:02,376 --> 00:21:06,026
But incremental LTO
takes only 8 seconds.


386
00:21:06,916 --> 00:21:09,196
This is 16x faster
than the initial build,


387
00:21:09,196 --> 00:21:11,886
and 100x faster than
monolithic LTO.


388
00:21:13,516 --> 00:21:16,446
[ Applause ]


389
00:21:16,946 --> 00:21:17,556
This is stunning.


390
00:21:19,466 --> 00:21:21,486
State of the art, link
time optimization,


391
00:21:21,836 --> 00:21:25,896
with low memory requirements
and fast incremental builds.


392
00:21:26,556 --> 00:21:28,736
Try incremental LTO today.


393
00:21:30,056 --> 00:21:32,836
The improvements to
LTO are fantastic,


394
00:21:33,616 --> 00:21:36,426
but if you're using LTO
with a large C++ project,


395
00:21:36,606 --> 00:21:39,426
you can minimize compile time
with the line tables only,


396
00:21:39,656 --> 00:21:41,046
debug information level.


397
00:21:41,666 --> 00:21:43,106
This gives you rich back traces


398
00:21:43,106 --> 00:21:45,076
in the debugger,
at the lowest cost.


399
00:21:45,686 --> 00:21:48,796
That's it for Link-Link
optimization.


400
00:21:49,726 --> 00:21:51,366
I invite Gerolf on stage to talk


401
00:21:51,366 --> 00:21:53,496
about new code generation
optimizations.


402
00:21:54,516 --> 00:21:59,256
[ Applause ]


403
00:21:59,756 --> 00:22:02,326
>> So, let's move on
with optimizations


404
00:22:02,326 --> 00:22:03,276
in the code generator.


405
00:22:03,686 --> 00:22:08,176
We put a lot of effort into
the Xcode 8 Apple LLVM compiler


406
00:22:08,176 --> 00:22:11,166
to improve the performance
of all apps.


407
00:22:12,276 --> 00:22:14,746
In this section, I will
talk about three of them:


408
00:22:15,216 --> 00:22:17,116
Stack packing, shrink wrapping,


409
00:22:18,336 --> 00:22:20,426
and selective fused
multiply-add.


410
00:22:21,056 --> 00:22:22,476
Let's start with stack packing.


411
00:22:23,156 --> 00:22:27,626
This is about local variables
and runtime stack memory.


412
00:22:27,836 --> 00:22:30,766
The Apple LLVM compiler
always had optimizations,


413
00:22:31,116 --> 00:22:34,826
that try to reduce the
stack memory space.


414
00:22:35,246 --> 00:22:38,326
In Xcode 8, the compiler
got better at doing


415
00:22:38,326 --> 00:22:39,406
that than ever before.


416
00:22:40,376 --> 00:22:41,056
And here is why.


417
00:22:41,236 --> 00:22:42,466
Let's look at that example.


418
00:22:42,936 --> 00:22:44,416
Pay attention to the definition


419
00:22:44,416 --> 00:22:48,386
of x inside the scope
of the if statement.


420
00:22:48,836 --> 00:22:52,066
Then look at the definition
of y after the if statement.


421
00:22:52,656 --> 00:22:55,246
If the compiler does not
optimize this code snippet,


422
00:22:55,246 --> 00:22:57,976
you would expect that the
two variables x and y,


423
00:22:58,316 --> 00:23:00,296
live at two different
stack locations


424
00:23:00,296 --> 00:23:01,726
on the runtime memory stack.


425
00:23:03,306 --> 00:23:07,886
However, according to C-style
language rules, the lifetime


426
00:23:07,886 --> 00:23:10,916
of a variable ends at the
scope where it is defined.


427
00:23:11,576 --> 00:23:14,156
And this is what the
compiler exploits.


428
00:23:14,726 --> 00:23:17,836
Let's take a look what this
does to our two variables.


429
00:23:19,196 --> 00:23:22,936
x is defined inside
the if statement.


430
00:23:23,476 --> 00:23:26,896
The scope of that if statement
ends just before y is defined.


431
00:23:28,426 --> 00:23:32,436
When y is defined, it starts a
lifetime of y and what you see


432
00:23:32,436 --> 00:23:35,936
in this picture is that x
and y, the lifetimes for x


433
00:23:35,936 --> 00:23:37,216
and y, do not overlap.


434
00:23:37,826 --> 00:23:40,856
So the compiler can assign
them the same stack location.


435
00:23:43,896 --> 00:23:48,646
This reduces the total memory
stack required for your program,


436
00:23:48,986 --> 00:23:52,826
and that may provide better
performance for your app.


437
00:23:54,106 --> 00:23:58,586
Now this is all great, but
there is a little caveat


438
00:23:58,706 --> 00:24:00,366
that comes with it.


439
00:24:01,516 --> 00:24:05,466
Programming language rules are
sometimes really hard to check.


440
00:24:06,776 --> 00:24:08,076
And when they are violated,


441
00:24:09,226 --> 00:24:12,876
your program may give
unpredictable results,


442
00:24:12,936 --> 00:24:16,076
or in a technical term, it
may have undefined behavior.


443
00:24:16,736 --> 00:24:18,166
Let's take a look
at the example.


444
00:24:19,626 --> 00:24:21,796
Pay attention to the
pointer variable.


445
00:24:22,066 --> 00:24:25,746
Inside the if block, it gets
assigned the address of x.


446
00:24:25,986 --> 00:24:27,386
Now look at the print statement.


447
00:24:27,886 --> 00:24:29,126
At that point, the address


448
00:24:29,126 --> 00:24:31,376
of x is used via the
pointer variable.


449
00:24:31,836 --> 00:24:34,486
So what is happening
here is that the address


450
00:24:34,486 --> 00:24:37,636
of x escapes the scope
of the if statement,


451
00:24:37,826 --> 00:24:39,696
and that [inaudible]
our language rules


452
00:24:39,956 --> 00:24:41,296
as undefined behavior.


453
00:24:43,016 --> 00:24:45,226
Luckily, it is easy to fix.


454
00:24:45,226 --> 00:24:46,856
It's just something
to be aware of.


455
00:24:47,266 --> 00:24:50,586
The way you fix this is by
simply extending the scope


456
00:24:50,586 --> 00:24:54,946
of the lifetime for x by moving
the definition of x outside the


457
00:24:54,946 --> 00:24:59,566
if statement, just before
the condition is checked.


458
00:24:59,936 --> 00:25:04,996
And now the definition of x is
in the same scope as the address


459
00:25:05,316 --> 00:25:07,256
that is used in the
print statement.


460
00:25:08,746 --> 00:25:13,296
The takeaway from this is simply
please make any effort you can


461
00:25:13,296 --> 00:25:15,996
think of to make
your program adhere


462
00:25:15,996 --> 00:25:17,566
to programming language rules,


463
00:25:17,696 --> 00:25:20,306
that will give a much
better experience for you,


464
00:25:20,466 --> 00:25:23,626
for the optimizing
compiler, and for our users.


465
00:25:24,536 --> 00:25:26,016
That is stack packing.


466
00:25:28,036 --> 00:25:29,306
Let's move on to
shrink wrapping.


467
00:25:30,706 --> 00:25:34,936
Shrink wrapping is about the
code the compiler generates


468
00:25:35,256 --> 00:25:37,366
in the entry and exit
of your function.


469
00:25:37,846 --> 00:25:39,306
It's resource management code


470
00:25:39,646 --> 00:25:42,286
that manages the runtime
stack and the registers.


471
00:25:43,156 --> 00:25:46,656
The observation here is
that this code is not needed


472
00:25:46,656 --> 00:25:48,786
for all the paths
through your function,


473
00:25:49,096 --> 00:25:53,216
and shrink wrapping will
place this resource managing


474
00:25:53,216 --> 00:25:56,016
instruction only to places
where they are actually needed.


475
00:25:57,036 --> 00:25:59,526
So let's take a look
at a simple example.


476
00:25:59,626 --> 00:26:05,396
So here's a simple function that
takes two parameters, a and b,


477
00:26:05,596 --> 00:26:06,866
simple compares the two.


478
00:26:07,726 --> 00:26:10,496
And if a is smaller than b,
then it calls a function foo


479
00:26:10,926 --> 00:26:14,756
which has the address of a
local variable as a parameter,


480
00:26:14,876 --> 00:26:16,296
and eventually it returns.


481
00:26:16,776 --> 00:26:18,106
To understand shrink wrapping,


482
00:26:18,446 --> 00:26:21,196
let's look at the
pseudo assembly code akin


483
00:26:21,196 --> 00:26:23,556
to the code the compiler
actually generates.


484
00:26:23,946 --> 00:26:27,976
So here you'll see an entry code
that allocates stack memory,


485
00:26:27,976 --> 00:26:30,626
saves registers, does
the comparison, branches.


486
00:26:31,006 --> 00:26:33,396
And the exit block that
restores the registers,


487
00:26:33,426 --> 00:26:35,766
deallocates the stack,
and returns.


488
00:26:36,116 --> 00:26:37,526
And then if the condition
is right,


489
00:26:37,846 --> 00:26:39,076
the function foo is called.


490
00:26:39,136 --> 00:26:42,716
So what we see here is we have
two paths in that program.


491
00:26:42,916 --> 00:26:46,626
One from the entry to the
exit block, and another path


492
00:26:46,626 --> 00:26:49,946
from the entry block to the
call -- to the exit block.


493
00:26:50,286 --> 00:26:52,226
The key observation here is


494
00:26:52,316 --> 00:26:54,446
that the resource
managing instructions


495
00:26:54,856 --> 00:26:58,656
for the runtime stack memory


496
00:26:58,656 --> 00:27:00,956
and for the registers
are only needed


497
00:27:01,406 --> 00:27:04,906
because we have a call
instruction in our function.


498
00:27:05,846 --> 00:27:08,806
So what shrink wrapping does,
it recognizes this condition


499
00:27:09,206 --> 00:27:14,196
and moves these instructions
out of the entry block and out


500
00:27:14,196 --> 00:27:17,166
of the exit block to the place
where they actually need it.


501
00:27:17,166 --> 00:27:18,946
So it shrinks the lifetimes


502
00:27:19,396 --> 00:27:22,046
of the resource managing
instructions and wraps them


503
00:27:22,046 --> 00:27:23,956
around the region, where
they are actually needed.


504
00:27:23,956 --> 00:27:26,216
In this case, the
region is just the call.


505
00:27:26,796 --> 00:27:29,516
But now imagine,
that the hot code


506
00:27:29,706 --> 00:27:34,076
in your function is the path
from the entry to the exit.


507
00:27:34,266 --> 00:27:37,216
No longer do we have
to execute the resource


508
00:27:37,216 --> 00:27:38,596
allocating instructions.


509
00:27:39,256 --> 00:27:40,706
And if this is hot functions,


510
00:27:40,706 --> 00:27:42,646
you have many functions
like this.


511
00:27:42,846 --> 00:27:45,206
You can imagine that this
way we can save millions


512
00:27:45,206 --> 00:27:47,876
of instructions, providing
nice performance gains,


513
00:27:48,066 --> 00:27:50,036
and also power savings
for your app.


514
00:27:51,056 --> 00:27:52,586
And that is shrink wrapping.


515
00:27:54,516 --> 00:27:59,516
[ Applause ]


516
00:28:00,016 --> 00:28:02,256
Let's move on with
fused multiply-adds.


517
00:28:03,116 --> 00:28:06,386
It takes us back to very
simple arithmetical operations,


518
00:28:06,386 --> 00:28:07,976
additions and multiplications.


519
00:28:08,456 --> 00:28:12,936
The arm64 processor
has one instructions --


520
00:28:12,936 --> 00:28:14,986
a fused multiply-add
instructions --


521
00:28:15,496 --> 00:28:17,696
instruction, that
computes an expression


522
00:28:17,896 --> 00:28:21,426
like a plus b times c, in
one single instruction.


523
00:28:21,966 --> 00:28:26,346
You would naively assume that
whenever you see an expression


524
00:28:26,346 --> 00:28:30,386
like this, it's always the best
to generate this instruction.


525
00:28:30,816 --> 00:28:33,366
And that's exactly what the
compiler has done so far.


526
00:28:33,946 --> 00:28:36,186
But it may surprise you.


527
00:28:36,476 --> 00:28:38,746
In some cases, it's
actually faster


528
00:28:39,056 --> 00:28:42,006
to generate two instructions,
an add instruction,


529
00:28:42,326 --> 00:28:43,826
and a multiply instruction,


530
00:28:43,996 --> 00:28:45,776
to get faster performance
for your app.


531
00:28:46,896 --> 00:28:50,746
Why? I've prepared a simple
example to demonstrate this.


532
00:28:52,486 --> 00:28:55,486
This function takes
four integer parameters


533
00:28:55,486 --> 00:28:57,496
and it computes a
simple expression,


534
00:28:57,806 --> 00:28:59,716
a times b plus c times d.


535
00:29:01,456 --> 00:29:02,516
What does the code look


536
00:29:02,516 --> 00:29:05,576
like when the compiler
generates a single fused


537
00:29:05,576 --> 00:29:06,976
multipy-add instruction?


538
00:29:08,186 --> 00:29:12,446
Well, it will compute a times
b, then the multiply-add had


539
00:29:12,516 --> 00:29:13,876
to wait for that result.


540
00:29:14,346 --> 00:29:18,536
And the multiply-add
will compute the value


541
00:29:18,686 --> 00:29:19,676
of our expression.


542
00:29:20,176 --> 00:29:21,166
How long does this take?


543
00:29:21,166 --> 00:29:24,156
It takes four cycle
for the multiply,


544
00:29:24,156 --> 00:29:25,756
four cycle for the add.


545
00:29:26,216 --> 00:29:31,806
So in total, that simple
sequence takes eight cycles.


546
00:29:32,846 --> 00:29:35,896
When we generate 2
multiplies and 1 add,


547
00:29:36,886 --> 00:29:38,136
how can that be faster?


548
00:29:38,606 --> 00:29:40,096
Let's take a look
at that sequence.


549
00:29:41,056 --> 00:29:43,746
First we issue the
compiler issues a multiply,


550
00:29:43,746 --> 00:29:47,156
computes a times b,
then computes c times d.


551
00:29:47,316 --> 00:29:49,956
Eventually adds the result.


552
00:29:50,906 --> 00:29:54,806
The secret here is that modern
processors have instruction


553
00:29:54,806 --> 00:29:55,776
level parallelism.


554
00:29:56,056 --> 00:29:59,506
They can execute two
or more multiplies


555
00:30:01,076 --> 00:30:03,426
at the same time, in parallel.


556
00:30:03,826 --> 00:30:05,576
So what is happening here is


557
00:30:05,576 --> 00:30:08,216
that the two multiplies
get executed in parallel.


558
00:30:08,446 --> 00:30:12,276
So within four cycles, we don't
get the result of one multiply,


559
00:30:12,276 --> 00:30:14,326
but the results of
two multiplies.


560
00:30:14,606 --> 00:30:18,716
Then we just add the results
in one cycle, and now we see


561
00:30:18,716 --> 00:30:22,756
that for this to compute the
value of this expression,


562
00:30:22,846 --> 00:30:24,786
we only need five cycles.


563
00:30:24,916 --> 00:30:28,076
Let's compare this with
the sequence for --


564
00:30:28,526 --> 00:30:32,106
with the sequence with a fewest
multiply-add and now we see


565
00:30:32,106 --> 00:30:35,846
that for this simple sequence,
we get a speed up of about 2x.


566
00:30:37,226 --> 00:30:40,726
So with selective fused
multiply-add, you can now speed


567
00:30:40,726 --> 00:30:43,536
up many simple expressions
in your app.


568
00:30:44,146 --> 00:30:46,116
And that is fused multiply-add.


569
00:30:47,516 --> 00:30:52,976
[ Applause ]


570
00:30:53,476 --> 00:30:56,976
So let's move on to
arm64 cache tuning.


571
00:30:58,326 --> 00:31:02,346
Here I will talk about two
techniques: The compiler has


572
00:31:02,386 --> 00:31:07,006
to impact which data gets stored
into the cache and the data


573
00:31:07,006 --> 00:31:08,636
that gets stored into the cache,


574
00:31:08,706 --> 00:31:10,686
impact the performance
of your app.


575
00:31:11,896 --> 00:31:14,996
Before we go into the details,
what the compiler is doing,


576
00:31:15,266 --> 00:31:17,846
I want to quickly review
the memory hierarchy.


577
00:31:18,506 --> 00:31:20,176
At the top, you see
the main memory


578
00:31:20,176 --> 00:31:22,546
that hosts the program
variables.


579
00:31:24,186 --> 00:31:26,176
And it's at the top.


580
00:31:26,176 --> 00:31:27,806
At the bottom, you
see the registers.


581
00:31:27,806 --> 00:31:31,656
It's very slow to load data from
main memory to the registers.


582
00:31:31,836 --> 00:31:35,246
To bridge that gap, there's
a cache, a temporary storage.


583
00:31:35,606 --> 00:31:37,546
It's about 10,000
to 100 times --


584
00:31:37,616 --> 00:31:41,516
to 100,000 times smaller than
main memory, but it's much,


585
00:31:41,516 --> 00:31:43,866
much faster to load
data out of the cache.


586
00:31:43,866 --> 00:31:46,276
About 10 times to
100 times faster.


587
00:31:47,066 --> 00:31:49,146
Then data is loaded
out of main memory.


588
00:31:50,336 --> 00:31:53,216
We not only load a
single register value,


589
00:31:53,216 --> 00:31:56,056
but an entire cache
line out of main memory.


590
00:31:56,296 --> 00:31:59,166
So cache line contains more
than one register value.


591
00:32:00,126 --> 00:32:02,696
The reason why this design
is usually so successful,


592
00:32:02,696 --> 00:32:06,966
is because your program data
have two locality properties:


593
00:32:07,206 --> 00:32:10,066
Temporal locality
and spatial locality.


594
00:32:10,676 --> 00:32:14,176
Temporal locality simply means
the data your program accesses


595
00:32:14,176 --> 00:32:16,446
now, it will access
this very soon again.


596
00:32:16,916 --> 00:32:22,316
Spatial locality simply means
data your program accesses now,


597
00:32:22,386 --> 00:32:24,226
it will also access
neighboring data.


598
00:32:24,776 --> 00:32:27,036
So when you access
an array field,


599
00:32:27,036 --> 00:32:29,396
it will also access
the field next to it.


600
00:32:29,596 --> 00:32:33,216
When it accesses a field
in your data structure,


601
00:32:33,216 --> 00:32:35,436
it will also access
a field next to that.


602
00:32:35,816 --> 00:32:37,056
And so on and so forth.


603
00:32:37,056 --> 00:32:42,496
Now, you look at this
design, and you wonder --


604
00:32:42,636 --> 00:32:46,106
so it's so fast to load
data out of the cache.


605
00:32:46,666 --> 00:32:50,376
Can we somehow magically
preload the data into the cache,


606
00:32:50,376 --> 00:32:53,506
out of main memory while the
processor is doing some other


607
00:32:53,506 --> 00:32:56,366
operations, so that all
data get loaded quickly


608
00:32:56,456 --> 00:32:58,726
when we need them, when
our programs need them?


609
00:32:58,956 --> 00:33:05,206
It might surprise you, on your
processor, in your iPhone,


610
00:33:05,366 --> 00:33:07,076
that magic is already happening.


611
00:33:07,846 --> 00:33:08,946
It's hardware prefetching.


612
00:33:09,456 --> 00:33:13,306
The processor looks at each
address that is loaded,


613
00:33:13,786 --> 00:33:16,046
tries to find pattern
in those addresses.


614
00:33:16,276 --> 00:33:17,466
When it finds a pattern,


615
00:33:17,726 --> 00:33:21,356
it predicts which data your
program will need in the future,


616
00:33:21,676 --> 00:33:24,636
prefetches this data out
of main memory, again,


617
00:33:24,636 --> 00:33:27,496
while other complications
are happening, puts the data


618
00:33:27,496 --> 00:33:30,536
into the cache and eventually
when your program needs them,


619
00:33:30,726 --> 00:33:33,006
the program can load them
quickly, out of the cache.


620
00:33:34,236 --> 00:33:36,996
Now this year, we worked
with the hardware architects


621
00:33:37,536 --> 00:33:41,436
to see how the compiler
can even make


622
00:33:41,436 --> 00:33:44,346
that prefetching magic
work better for your apps.


623
00:33:45,836 --> 00:33:47,076
And we found a few patterns.


624
00:33:47,076 --> 00:33:51,186
So what the compiler does now,
it analyzes your source code,


625
00:33:51,626 --> 00:33:54,496
predicts which data your
app will need in the future,


626
00:33:55,336 --> 00:33:59,916
issues prefetch instructions
for this app, for this data.


627
00:34:00,326 --> 00:34:01,656
Now while your app is running,


628
00:34:02,026 --> 00:34:05,076
the prefetch instruction
executes, fetches the data


629
00:34:05,306 --> 00:34:07,476
from main memory, puts
it into the cache,


630
00:34:09,106 --> 00:34:13,085
and when your program is ready
to need those data, it simply


631
00:34:13,085 --> 00:34:14,916
and quickly loads
it out of the cache.


632
00:34:14,916 --> 00:34:17,136
Your program no longer
has to wait for the data


633
00:34:17,136 --> 00:34:18,815
to be loaded out of main memory.


634
00:34:19,496 --> 00:34:21,726
And that is the magic
of software prefetching.


635
00:34:25,126 --> 00:34:25,616
So far--


636
00:34:26,516 --> 00:34:31,775
[ Applause ]


637
00:34:32,275 --> 00:34:33,746
-- so far, I have talked


638
00:34:33,746 --> 00:34:36,916
about optimizations the compiler
does automatically for you.


639
00:34:37,996 --> 00:34:39,585
For the next optimization,


640
00:34:39,726 --> 00:34:42,016
non-temporal stores it
will need your help.


641
00:34:42,525 --> 00:34:47,866
To understand what is going on
there, we need to take a look


642
00:34:48,025 --> 00:34:53,206
at what is happening when data
is stored into main memory.


643
00:34:54,036 --> 00:34:58,096
So again, let's take a look
at our memory hierarchy,


644
00:34:58,286 --> 00:35:02,516
and assume we have a simple
assignment in our program


645
00:35:02,516 --> 00:35:06,306
where we want to assign a
value of say 100, to a variable


646
00:35:06,306 --> 00:35:09,596
in main memory, which has
the current value of 55.


647
00:35:10,746 --> 00:35:14,016
The first thing that is
happening, the old data


648
00:35:14,016 --> 00:35:16,386
from the address you want
to store the new value


649
00:35:16,386 --> 00:35:20,026
in is loaded into the cache.


650
00:35:20,026 --> 00:35:23,646
And since we load data from main
memory, not only the old data


651
00:35:23,646 --> 00:35:26,776
for that variable is loaded,
but also the neighboring data --


652
00:35:27,436 --> 00:35:28,606
also the neighboring data


653
00:35:28,726 --> 00:35:30,416
because we fill the
entire cache line.


654
00:35:31,866 --> 00:35:36,586
In the second step, we store
the value in our register,


655
00:35:37,116 --> 00:35:41,786
into the cache line,
in the cache.


656
00:35:43,286 --> 00:35:46,086
And then finally, the
data in the cache line


657
00:35:46,176 --> 00:35:48,236
or the cache line is
needed for different data


658
00:35:48,346 --> 00:35:50,596
and our values get written back


659
00:35:50,596 --> 00:35:52,426
in the third step
to main memory.


660
00:35:52,756 --> 00:35:54,996
So it's a three step process.


661
00:35:56,166 --> 00:35:58,476
Loading data from main
memory into the cache,


662
00:35:58,606 --> 00:36:00,776
store the register
value into the cache,


663
00:36:00,886 --> 00:36:04,056
and then eventually store the
data back into main memory.


664
00:36:04,946 --> 00:36:09,466
And this all because data
usually have the locality


665
00:36:09,466 --> 00:36:12,456
property, and specifically
temporal locality.


666
00:36:12,926 --> 00:36:16,226
But what if your data do
not have temporal locality?


667
00:36:16,226 --> 00:36:22,566
Wouldn't it be much faster to
simply store the value directly


668
00:36:22,686 --> 00:36:29,146
from the register into the main
memory in just one, single step?


669
00:36:30,626 --> 00:36:34,096
This is exactly what
non-temporal stores do.


670
00:36:34,326 --> 00:36:36,756
They avoid the extra
load of a cache line,


671
00:36:37,596 --> 00:36:41,606
and also the other benefit of
that you have is since you know


672
00:36:41,606 --> 00:36:45,686
that the data are no
longer needed, that don't go


673
00:36:45,686 --> 00:36:48,956
into the cache, so the cache
can now have other data


674
00:36:48,956 --> 00:36:50,616
for your app that
are more useful.


675
00:36:51,626 --> 00:36:53,436
The way you instruct
the compiler


676
00:36:53,566 --> 00:36:57,126
to generate non-temporal stores
is with a compiler builtin.


677
00:36:57,806 --> 00:37:02,096
So you use the builtin
to instruct the compiler


678
00:37:02,096 --> 00:37:03,956
to generate the non-temporal
stores.


679
00:37:04,186 --> 00:37:05,516
It takes two parameters.


680
00:37:05,646 --> 00:37:08,096
The value you want to
store, and the address


681
00:37:08,196 --> 00:37:09,646
where you want to store it to.


682
00:37:11,606 --> 00:37:14,246
When do you want to use
non-temporal stores?


683
00:37:14,506 --> 00:37:17,236
When there is no [inaudible]
use, no temporal locality


684
00:37:17,786 --> 00:37:22,786
in your code, you copy a large
chunk of data, preferably larger


685
00:37:22,786 --> 00:37:26,706
than the size of the cache,
and to make it really count


686
00:37:26,706 --> 00:37:29,146
for your app, it should be
where the performance is hiding.


687
00:37:29,416 --> 00:37:30,706
So it should be in hot loops.


688
00:37:31,576 --> 00:37:34,006
And you don't want to use a
non-temporal storage basically


689
00:37:34,006 --> 00:37:36,506
when any of these
condition does not hold.


690
00:37:37,026 --> 00:37:40,366
What can you gain from this?


691
00:37:41,116 --> 00:37:41,956
So in this slide?


692
00:37:42,986 --> 00:37:47,066
We look at the performance gains
from the non-temporal stores


693
00:37:47,506 --> 00:37:51,396
on three benchmarks that contain
hot loops that look similar


694
00:37:51,396 --> 00:37:53,526
to the example that
I showed you before.


695
00:37:54,106 --> 00:37:55,786
So what this data show you is


696
00:37:55,856 --> 00:37:58,506
that for very common
loop bodies,


697
00:37:59,056 --> 00:38:02,486
you get very significant
speed ups in the range


698
00:38:02,486 --> 00:38:05,026
from 30 to 40 percent.


699
00:38:05,926 --> 00:38:09,106
And that is what non-temporal
stores can hopefully do


700
00:38:09,106 --> 00:38:10,966
for many hot loops in your app.


701
00:38:12,516 --> 00:38:19,076
[ Applause ]


702
00:38:19,576 --> 00:38:20,536
It has been a long journey.


703
00:38:21,796 --> 00:38:25,136
We have looked at many,
many new features and a lot


704
00:38:25,136 --> 00:38:26,786
of great new optimizations


705
00:38:27,056 --> 00:38:29,856
that the new compiler
brings to your app.


706
00:38:30,536 --> 00:38:35,166
So we've talked about the
Apple LLVM compiler is based


707
00:38:35,166 --> 00:38:36,506
on an open source project.


708
00:38:36,506 --> 00:38:39,286
You can interact with us in
the open source community,


709
00:38:39,646 --> 00:38:42,196
even provide patches for
your favorite compiler.


710
00:38:42,746 --> 00:38:44,806
We have seen many
great new features


711
00:38:44,806 --> 00:38:46,706
like the objective
class properties


712
00:38:47,126 --> 00:38:50,216
and C++11 thread
local storage support.


713
00:38:50,626 --> 00:38:57,656
Now the new Libc ++ has
full support for C++14


714
00:38:57,996 --> 00:39:01,816
and has many new improved
features, but keep in mind,


715
00:39:01,936 --> 00:39:04,516
that Libstandardc++
is deprecated.


716
00:39:05,086 --> 00:39:07,026
You get many more
warning and diagnostics


717
00:39:07,066 --> 00:39:09,026
to make your code
cleaner than ever before.


718
00:39:09,746 --> 00:39:12,266
And you heard about this
fantastic new feature,


719
00:39:12,496 --> 00:39:15,926
incremental LTO that gives you
basically the same performance


720
00:39:16,146 --> 00:39:19,936
as monolithic LTO, and really,
truly, awesome compile time.


721
00:39:20,576 --> 00:39:22,776
And we talked about a
number of optimizations


722
00:39:22,776 --> 00:39:25,976
in the code generator that
automatically should speed


723
00:39:25,976 --> 00:39:29,896
up all your apps,
and we finally talked


724
00:39:29,896 --> 00:39:34,196
about long term pro stores where
the compiler provides the means,


725
00:39:34,906 --> 00:39:37,266
but you provide the smarts,
and then to use them.


726
00:39:38,226 --> 00:39:41,206
So I hope all this convinces you


727
00:39:41,206 --> 00:39:44,036
that we put together a really
great compiler release.


728
00:39:44,036 --> 00:39:46,296
We couldn't be happier about it.


729
00:39:46,516 --> 00:39:47,766
Please get your hands on it.


730
00:39:47,976 --> 00:39:50,806
Check it out, what it
can do for your app.


731
00:39:51,596 --> 00:39:55,286
You'll find more
information on our website.


732
00:39:56,006 --> 00:40:00,276
There are a number of
really cool talks out there


733
00:40:01,546 --> 00:40:03,016
that you should be
interested in.


734
00:40:03,216 --> 00:40:03,846
Check them out.


735
00:40:04,266 --> 00:40:05,516
Thank you all for watching.


736
00:40:05,686 --> 00:40:06,476
Thank you for coming.


737
00:40:06,476 --> 00:40:08,716
Have a great time
at the conference.


738
00:40:09,016 --> 00:40:11,000
[ Applause ]

