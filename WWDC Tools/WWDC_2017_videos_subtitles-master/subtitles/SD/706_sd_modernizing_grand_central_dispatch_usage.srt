1
00:00:18,516 --> 00:00:21,966
[ Applause ]


2
00:00:22,466 --> 00:00:22,986
>> Good morning.


3
00:00:23,126 --> 00:00:24,386
And welcome to the modernizing


4
00:00:24,386 --> 00:00:25,586
Grand Central Dispatch Usage


5
00:00:25,616 --> 00:00:25,976
Session.


6
00:00:26,436 --> 00:00:27,806
I'm Daniel Chimene from the Core


7
00:00:27,806 --> 00:00:29,566
Darwin Team and my colleagues


8
00:00:29,566 --> 00:00:31,686
and I are here today to show you


9
00:00:31,766 --> 00:00:33,036
how you can take advantage of


10
00:00:33,036 --> 00:00:34,746
Grand Central Dispatch to get


11
00:00:34,746 --> 00:00:35,746
the best performance in your


12
00:00:35,746 --> 00:00:36,276
application.


13
00:00:37,536 --> 00:00:39,276
As app developers, you spent


14
00:00:39,276 --> 00:00:40,956
hundreds or thousands of hours


15
00:00:41,026 --> 00:00:42,706
building amazing experiences for


16
00:00:42,786 --> 00:00:43,416
your users.


17
00:00:43,476 --> 00:00:45,166
Taking advantage of our powerful


18
00:00:45,166 --> 00:00:45,616
devices.


19
00:00:46,616 --> 00:00:47,976
You want your users to be able


20
00:00:47,976 --> 00:00:49,096
to have a great experience.


21
00:00:49,596 --> 00:00:51,386
Not just on one device, but


22
00:00:51,386 --> 00:00:52,526
across all the variety of


23
00:00:52,526 --> 00:00:56,196
devices that Apple makes.


24
00:00:56,776 --> 00:00:58,476
GCD is designed to help you


25
00:00:58,476 --> 00:00:59,566
dynamically scale your


26
00:00:59,566 --> 00:01:00,356
application code.


27
00:01:00,456 --> 00:01:01,456
From the single core Apple


28
00:01:01,456 --> 00:01:03,056
Watch, all the way up to a mini


29
00:01:03,056 --> 00:01:03,536
core Mac.


30
00:01:04,486 --> 00:01:05,495
You don't want to have to worry


31
00:01:05,495 --> 00:01:07,086
too much about what kind of


32
00:01:07,086 --> 00:01:08,246
hardware your users are running.


33
00:01:09,116 --> 00:01:10,026
But there are problematic


34
00:01:10,096 --> 00:01:11,386
problems that can affect the


35
00:01:11,386 --> 00:01:13,216
scalability and the efficiency


36
00:01:13,676 --> 00:01:14,246
of your code.


37
00:01:14,296 --> 00:01:15,796
Both on the low end and on the


38
00:01:15,796 --> 00:01:16,166
high end.


39
00:01:17,166 --> 00:01:17,986
That's what we're here to talk


40
00:01:17,986 --> 00:01:18,486
about today.


41
00:01:19,326 --> 00:01:20,906
We want to help you ensure that


42
00:01:21,026 --> 00:01:22,216
all the work you're putting into


43
00:01:22,216 --> 00:01:23,206
your app to make it a great


44
00:01:23,206 --> 00:01:24,586
experience for your users


45
00:01:25,206 --> 00:01:26,846
translates across all of these


46
00:01:26,846 --> 00:01:27,306
devices.


47
00:01:28,986 --> 00:01:30,536
You may have been using GCD


48
00:01:30,536 --> 00:01:33,166
API's like dispatch async, and


49
00:01:33,166 --> 00:01:34,446
others to create cues and


50
00:01:34,446 --> 00:01:35,426
dispatch work to the system.


51
00:01:36,336 --> 00:01:37,226
These are only some of the


52
00:01:37,226 --> 00:01:38,476
interfaces to the concurrency


53
00:01:38,476 --> 00:01:40,206
technology that we call Grand


54
00:01:40,206 --> 00:01:40,976
Central Dispatch.


55
00:01:41,686 --> 00:01:42,896
Today, we're going to take a


56
00:01:42,896 --> 00:01:44,476
peek under the covers of GCD.


57
00:01:44,506 --> 00:01:46,576
This is an advanced session


58
00:01:46,666 --> 00:01:47,846
packed full of information.


59
00:01:48,366 --> 00:01:49,316
So, let's get started right


60
00:01:49,316 --> 00:01:50,566
away, by looking at our


61
00:01:50,566 --> 00:01:50,926
hardware.


62
00:01:52,136 --> 00:01:53,656
The amazing chips in our devices


63
00:01:53,656 --> 00:01:54,916
have been getting faster and


64
00:01:54,916 --> 00:01:55,806
faster over time.


65
00:01:56,276 --> 00:01:57,826
However, much of the speed is


66
00:01:57,826 --> 00:01:58,756
not just because the chips


67
00:01:58,756 --> 00:01:59,836
themselves are getting faster,


68
00:02:00,316 --> 00:02:01,026
but because they're getting


69
00:02:01,026 --> 00:02:02,386
smarter, and smarter about


70
00:02:02,386 --> 00:02:03,066
running your code.


71
00:02:03,166 --> 00:02:04,496
And they're learning from what


72
00:02:04,496 --> 00:02:06,086
your code does over time to


73
00:02:06,086 --> 00:02:07,136
operate more efficiently.


74
00:02:07,646 --> 00:02:12,246
However, if your code goes off


75
00:02:12,316 --> 00:02:13,816
core, because it's completed its


76
00:02:13,816 --> 00:02:15,526
task, then it may no longer be


77
00:02:15,526 --> 00:02:16,836
able to take advantage of the


78
00:02:16,906 --> 00:02:18,126
history that that core has built


79
00:02:18,126 --> 00:02:18,366
up.


80
00:02:18,476 --> 00:02:20,186
And you might leave performance


81
00:02:20,186 --> 00:02:21,656
on the table when you come back


82
00:02:21,846 --> 00:02:22,336
on core.


83
00:02:23,286 --> 00:02:24,646
We've even seen examples of this


84
00:02:24,646 --> 00:02:25,996
on our own frameworks, when we


85
00:02:25,996 --> 00:02:27,326
applied some of the optimization


86
00:02:27,366 --> 00:02:28,236
techniques that we're going to


87
00:02:28,236 --> 00:02:30,436
discuss today, we saw large


88
00:02:30,436 --> 00:02:33,086
speed ups from simple changes to


89
00:02:33,086 --> 00:02:34,106
avoid these problematic


90
00:02:34,106 --> 00:02:34,426
patterns.


91
00:02:36,856 --> 00:02:38,916
So, using these techniques lets


92
00:02:38,916 --> 00:02:40,406
you bring high performance apps


93
00:02:40,566 --> 00:02:41,966
to more users with less work.


94
00:02:42,666 --> 00:02:44,296
Today, we're going to give you


95
00:02:44,296 --> 00:02:45,316
some insight into what our


96
00:02:45,316 --> 00:02:46,776
system is doing under the covers


97
00:02:47,046 --> 00:02:47,756
with your code.


98
00:02:47,756 --> 00:02:49,316
So, you can tune your code to


99
00:02:49,316 --> 00:02:50,916
take the best advantage of what


100
00:02:50,916 --> 00:02:51,886
GCD has to offer.


101
00:02:52,676 --> 00:02:53,546
We're going to discuss a few


102
00:02:53,546 --> 00:02:54,086
things today.


103
00:02:55,126 --> 00:02:56,526
First, we're going to discuss


104
00:02:56,526 --> 00:02:57,736
how you can best express


105
00:02:57,946 --> 00:02:59,386
parallelism and concurrency.


106
00:03:00,026 --> 00:03:01,996
How you can chose the best way


107
00:03:02,036 --> 00:03:03,906
to express concurrency to grand


108
00:03:03,906 --> 00:03:04,636
central dispatch.


109
00:03:05,546 --> 00:03:07,036
We're going to introduce Unified


110
00:03:07,076 --> 00:03:08,466
Queue Identity, which is a major


111
00:03:08,466 --> 00:03:09,566
under the hood improvement to


112
00:03:09,566 --> 00:03:10,646
GCD that we're publishing this


113
00:03:10,646 --> 00:03:10,846
year.


114
00:03:11,666 --> 00:03:12,626
And we're finally going to show


115
00:03:12,626 --> 00:03:13,886
you how you can find problem


116
00:03:13,886 --> 00:03:15,136
spots in your code, with


117
00:03:15,136 --> 00:03:15,596
instruments.


118
00:03:16,626 --> 00:03:17,766
So, let's start by discussing


119
00:03:17,766 --> 00:03:19,196
parallelism and concurrency.


120
00:03:20,486 --> 00:03:24,506
So, for the purpose of this


121
00:03:24,546 --> 00:03:25,846
talk, we're talking about


122
00:03:25,946 --> 00:03:27,506
parallelism which is about how


123
00:03:27,506 --> 00:03:30,006
your code executes in parallel,


124
00:03:30,476 --> 00:03:31,686
simultaneously across many


125
00:03:31,686 --> 00:03:32,326
different cores.


126
00:03:32,756 --> 00:03:34,566
Concurrency is about how you


127
00:03:34,566 --> 00:03:35,806
compose the independent


128
00:03:35,806 --> 00:03:37,076
components of your application


129
00:03:37,386 --> 00:03:38,236
to run concurrently.


130
00:03:38,856 --> 00:03:40,186
The easy way to separate these


131
00:03:40,186 --> 00:03:41,796
two concepts in your mind, is to


132
00:03:41,796 --> 00:03:42,846
realize that parallelism is


133
00:03:42,846 --> 00:03:43,826
something that usually requires


134
00:03:43,826 --> 00:03:45,146
multiple cores and you want to


135
00:03:45,146 --> 00:03:46,166
use them all at the same time.


136
00:03:46,526 --> 00:03:47,696
And concurrency is something


137
00:03:47,696 --> 00:03:48,866
that you can do even on a single


138
00:03:48,866 --> 00:03:49,456
core system.


139
00:03:49,456 --> 00:03:51,016
It's about how you interpose the


140
00:03:51,016 --> 00:03:52,496
different tasks that are part of


141
00:03:52,496 --> 00:03:53,126
your application.


142
00:03:53,576 --> 00:03:55,296
So, let's start by talking about


143
00:03:55,296 --> 00:03:57,056
parallelism and how you might us


144
00:03:57,056 --> 00:03:58,576
it when you're writing an app.


145
00:03:59,676 --> 00:04:02,466
So, let's imagine you make an


146
00:04:02,466 --> 00:04:04,096
app and it processes huge


147
00:04:04,096 --> 00:04:04,596
images.


148
00:04:04,856 --> 00:04:05,916
And you want to be able to take


149
00:04:05,916 --> 00:04:07,136
advantage of the many cores on a


150
00:04:07,136 --> 00:04:08,566
Mac Pro to be able to process


151
00:04:08,566 --> 00:04:09,386
those images faster.


152
00:04:10,336 --> 00:04:11,936
What you do is break up that


153
00:04:11,936 --> 00:04:12,746
image into chunks.


154
00:04:13,576 --> 00:04:15,566
And have each core process those


155
00:04:15,566 --> 00:04:18,146
chunks in parallel.


156
00:04:19,466 --> 00:04:20,366
This gives you a speed up,


157
00:04:20,606 --> 00:04:21,236
because the cores are


158
00:04:21,236 --> 00:04:22,966
simultaneously working on


159
00:04:22,966 --> 00:04:23,996
different parts of the image.


160
00:04:25,346 --> 00:04:26,926
So, how do you implement this?


161
00:04:27,066 --> 00:04:29,506
Well first, you should stop and


162
00:04:29,506 --> 00:04:31,126
consider whether or not you can


163
00:04:31,126 --> 00:04:32,276
take advantage of our system


164
00:04:32,276 --> 00:04:32,736
frameworks.


165
00:04:33,396 --> 00:04:35,596
For example, accelerate has


166
00:04:35,656 --> 00:04:37,246
built-in support for parallel


167
00:04:37,246 --> 00:04:38,876
execution of advanced image


168
00:04:38,876 --> 00:04:39,436
algorithms.


169
00:04:40,066 --> 00:04:42,206
Metal and core image can take


170
00:04:42,206 --> 00:04:44,236
advantage of the powerful GPU.


171
00:04:45,846 --> 00:04:47,176
Well, let's say you've decided


172
00:04:47,176 --> 00:04:49,186
to implement this yourself, GCD


173
00:04:49,186 --> 00:04:50,596
gives you a tool that lets you


174
00:04:50,596 --> 00:04:52,146
easily express this pattern.


175
00:04:52,956 --> 00:04:54,396
The way you express parallels in


176
00:04:54,396 --> 00:04:55,696
GCD is with the API called


177
00:04:55,696 --> 00:04:56,516
concurrentPerform.


178
00:04:57,246 --> 00:04:58,966
This lets the framework optimize


179
00:04:58,966 --> 00:05:00,296
the parallel case because it


180
00:05:00,296 --> 00:05:01,336
knows that you're trying to do a


181
00:05:01,446 --> 00:05:03,036
parallel computation across all


182
00:05:03,036 --> 00:05:03,586
the cores.


183
00:05:04,586 --> 00:05:06,466
concurrentPerform is a parallel


184
00:05:06,466 --> 00:05:07,986
for loop that automatically load


185
00:05:07,986 --> 00:05:09,366
balances your computation across


186
00:05:09,416 --> 00:05:11,896
all the cores in the system.


187
00:05:12,086 --> 00:05:13,326
When you use this with Swift, it


188
00:05:13,326 --> 00:05:14,446
automatically chooses the


189
00:05:14,446 --> 00:05:15,936
correct context to run all your


190
00:05:15,936 --> 00:05:16,616
computation in.


191
00:05:17,206 --> 00:05:18,386
This year, we've brought that


192
00:05:18,386 --> 00:05:19,746
same power to the objective C


193
00:05:19,746 --> 00:05:21,996
interface dispatch apply with


194
00:05:21,996 --> 00:05:23,406
the dispatch apply auto keeper.


195
00:05:24,326 --> 00:05:25,656
This replaces the Q argument


196
00:05:26,436 --> 00:05:28,106
allowing the system to choose


197
00:05:28,106 --> 00:05:29,156
the right context to run your


198
00:05:29,156 --> 00:05:30,096
code in automatically.


199
00:05:31,416 --> 00:05:32,646
So, now let's take a look at


200
00:05:32,646 --> 00:05:33,716
this other parameter, which is


201
00:05:33,716 --> 00:05:34,636
the iteration count.


202
00:05:35,256 --> 00:05:36,256
This is how many times your


203
00:05:36,256 --> 00:05:37,416
block is called in parallel


204
00:05:37,416 --> 00:05:38,086
across the system.


205
00:05:39,456 --> 00:05:40,706
How do you choose a good value


206
00:05:40,706 --> 00:05:40,936
here?


207
00:05:41,776 --> 00:05:43,236
You might imagine that a good


208
00:05:43,236 --> 00:05:44,346
value would be the number of


209
00:05:44,406 --> 00:05:44,836
cores.


210
00:05:45,296 --> 00:05:47,376
Let's imagine that we're


211
00:05:47,376 --> 00:05:48,436
executing our workload on a


212
00:05:48,436 --> 00:05:49,246
three-core system.


213
00:05:50,006 --> 00:05:51,486
Here, you can see the ideal


214
00:05:51,486 --> 00:05:53,216
case, where three blocks run in


215
00:05:53,216 --> 00:05:54,396
parallel on all three cores.


216
00:05:55,016 --> 00:05:56,626
The real world isn't necessarily


217
00:05:56,626 --> 00:05:57,166
this perfect.


218
00:05:57,856 --> 00:05:58,906
What might happens if the third


219
00:05:58,906 --> 00:06:00,346
core here is taken up for awhile


220
00:06:00,346 --> 00:06:02,166
with UI rendering?


221
00:06:03,356 --> 00:06:05,146
Well, what happens is the load


222
00:06:05,146 --> 00:06:06,696
balancer has to move that third


223
00:06:06,696 --> 00:06:09,136
block over to the first core in


224
00:06:09,136 --> 00:06:10,206
order to execute it, because


225
00:06:10,206 --> 00:06:11,166
it's the third course taken up.


226
00:06:11,566 --> 00:06:13,796
And we get a bubble of idle CPU.


227
00:06:13,946 --> 00:06:15,666
We could have taken advantage of


228
00:06:15,666 --> 00:06:17,556
this time, to do more parallel


229
00:06:17,556 --> 00:06:17,766
work.


230
00:06:18,166 --> 00:06:19,486
And so, instead our workload


231
00:06:19,486 --> 00:06:19,986
took longer.


232
00:06:21,296 --> 00:06:22,236
How can we fix that?


233
00:06:22,586 --> 00:06:24,166
Well, we can increase the


234
00:06:24,166 --> 00:06:26,866
iteration count and give the


235
00:06:26,866 --> 00:06:28,256
load balancer more flexibility.


236
00:06:29,766 --> 00:06:30,216
It looks good.


237
00:06:30,466 --> 00:06:31,186
That hole is gone.


238
00:06:31,846 --> 00:06:32,796
There's actually another hole


239
00:06:32,796 --> 00:06:35,676
over here, on the third core.


240
00:06:36,186 --> 00:06:37,166
We could take advantage of that


241
00:06:37,206 --> 00:06:37,736
time as well.


242
00:06:39,756 --> 00:06:42,226
So, as Tim said on Monday, let's


243
00:06:42,226 --> 00:06:43,996
turn the iteration cup up to 11.


244
00:06:43,996 --> 00:06:48,986
There. We filled the hole, and


245
00:06:48,986 --> 00:06:50,106
we have efficient execution.


246
00:06:50,536 --> 00:06:51,676
We're using all of the available


247
00:06:51,676 --> 00:06:52,726
resources on the system until we


248
00:06:52,726 --> 00:06:53,116
finish.


249
00:06:53,606 --> 00:06:55,576
This is still a very simplistic


250
00:06:55,576 --> 00:06:56,036
example.


251
00:06:56,336 --> 00:06:57,546
To deal with the real-world


252
00:06:57,546 --> 00:06:59,216
complexity, you want to use an


253
00:06:59,276 --> 00:07:01,246
order of magnitude more, say


254
00:07:01,426 --> 00:07:02,066
1000.


255
00:07:03,136 --> 00:07:04,126
You can use a large enough


256
00:07:04,126 --> 00:07:05,526
iteration count so the load


257
00:07:05,526 --> 00:07:07,206
balancer has the flexibility to


258
00:07:07,206 --> 00:07:08,376
fill gaps in the system and take


259
00:07:08,376 --> 00:07:10,356
the maximum of your amount of


260
00:07:10,356 --> 00:07:11,456
advantage of the available


261
00:07:11,456 --> 00:07:12,366
resources of the system.


262
00:07:13,116 --> 00:07:14,456
However, you should be sure to


263
00:07:14,456 --> 00:07:15,816
balance the overhead of the load


264
00:07:15,816 --> 00:07:17,826
balancer versus the useful work


265
00:07:17,956 --> 00:07:19,866
that each block in your parallel


266
00:07:19,866 --> 00:07:20,596
for loop does.


267
00:07:21,706 --> 00:07:23,266
Remember that not every CPU is


268
00:07:23,266 --> 00:07:24,306
available to you all the time.


269
00:07:24,306 --> 00:07:26,056
There are many tasks running


270
00:07:26,136 --> 00:07:27,326
concurrently on the system.


271
00:07:27,636 --> 00:07:29,516
And additionally, not every


272
00:07:29,516 --> 00:07:30,606
worker thread will make equal


273
00:07:30,606 --> 00:07:31,006
progress.


274
00:07:32,496 --> 00:07:34,516
So, to recap, if you have a


275
00:07:34,516 --> 00:07:36,486
parallel problem, make sure to


276
00:07:36,486 --> 00:07:37,696
leverage the system frameworks


277
00:07:37,696 --> 00:07:38,416
that are available to you.


278
00:07:38,506 --> 00:07:40,476
You can use their power to solve


279
00:07:40,476 --> 00:07:41,006
your problem.


280
00:07:42,056 --> 00:07:43,326
Additionally, make sure to take


281
00:07:43,326 --> 00:07:44,696
advantage of the automatic load


282
00:07:44,696 --> 00:07:45,576
balancing inside


283
00:07:45,576 --> 00:07:46,356
concurrentPerform.


284
00:07:46,356 --> 00:07:47,926
Give it the flexibility to do


285
00:07:47,926 --> 00:07:48,746
what it does best.


286
00:07:49,496 --> 00:07:51,106
So, that's the discussion about


287
00:07:51,106 --> 00:07:51,636
parallelism.


288
00:07:51,936 --> 00:07:52,906
Now, let's switch to the main


289
00:07:52,906 --> 00:07:54,146
topic for today, which is


290
00:07:54,146 --> 00:07:54,806
concurrency.


291
00:07:56,196 --> 00:07:58,456
So, concurrency.


292
00:07:59,586 --> 00:08:00,886
Let's image you're writing a


293
00:08:00,886 --> 00:08:01,776
simple news app.


294
00:08:02,896 --> 00:08:03,656
How would you structure it?


295
00:08:03,856 --> 00:08:05,696
Well, you start by breaking it


296
00:08:05,696 --> 00:08:06,776
up into the independent


297
00:08:06,776 --> 00:08:08,456
subsystems that make up the app.


298
00:08:09,236 --> 00:08:10,216
Thinking about how you might


299
00:08:10,216 --> 00:08:11,766
break up a news app into its


300
00:08:11,766 --> 00:08:13,236
independent subsystems, you


301
00:08:13,236 --> 00:08:14,796
might have a UI component that


302
00:08:14,796 --> 00:08:15,806
renders the UI, that's the main


303
00:08:15,806 --> 00:08:16,116
thread.


304
00:08:16,706 --> 00:08:18,266
You might also have a database


305
00:08:18,626 --> 00:08:20,336
that stores those articles.


306
00:08:20,736 --> 00:08:21,826
And you might have a networking


307
00:08:21,826 --> 00:08:22,926
subsystem that fetches those


308
00:08:22,926 --> 00:08:23,796
articles from the network.


309
00:08:23,796 --> 00:08:25,606
To give you a better picture of


310
00:08:25,736 --> 00:08:26,926
how this app works and breaking


311
00:08:26,926 --> 00:08:28,466
it up into subsystems gives you


312
00:08:28,466 --> 00:08:30,176
an advantage, let's visualize


313
00:08:30,616 --> 00:08:31,906
how that executes concurrently


314
00:08:31,956 --> 00:08:32,746
on a modern system.


315
00:08:33,426 --> 00:08:36,626
So, let's say here's a timeline


316
00:08:36,785 --> 00:08:38,826
that shows at the top the CPU


317
00:08:38,826 --> 00:08:39,155
track.


318
00:08:39,155 --> 00:08:40,176
Let's image that we only have


319
00:08:40,176 --> 00:08:41,076
one CPU remaining.


320
00:08:41,076 --> 00:08:42,216
The other CPUs are busy for some


321
00:08:42,216 --> 00:08:42,446
reason.


322
00:08:43,145 --> 00:08:44,275
We only have one core available.


323
00:08:44,736 --> 00:08:46,386
At any time only one of these


324
00:08:46,386 --> 00:08:47,696
threads can run on that CPU.


325
00:08:48,526 --> 00:08:50,116
So, what happens when a user


326
00:08:50,116 --> 00:08:51,696
clicks the button and refreshes


327
00:08:51,696 --> 00:08:53,356
the article list in the news


328
00:08:53,356 --> 00:08:53,526
app?


329
00:08:54,126 --> 00:08:55,286
Well, these interface renders


330
00:08:55,286 --> 00:08:56,676
the response to that button and


331
00:08:56,946 --> 00:08:58,026
then sends an asynchronous


332
00:08:58,026 --> 00:08:58,876
across the database.


333
00:09:00,136 --> 00:09:01,346
And then the database decides it


334
00:09:01,346 --> 00:09:02,736
needs to refresh the articles,


335
00:09:02,976 --> 00:09:04,266
which it chooses another command


336
00:09:04,846 --> 00:09:05,946
to the networking subsystem.


337
00:09:06,886 --> 00:09:08,096
However, at this point, the user


338
00:09:08,096 --> 00:09:08,956
touches the app again.


339
00:09:09,936 --> 00:09:12,146
And because the database is done


340
00:09:12,216 --> 00:09:13,086
off the main thread of the


341
00:09:13,086 --> 00:09:14,896
application, the OS can


342
00:09:14,896 --> 00:09:16,236
immediately switch the CPU to


343
00:09:16,236 --> 00:09:18,026
working on the UI thread, and it


344
00:09:18,026 --> 00:09:19,126
can respond immediately to the


345
00:09:19,126 --> 00:09:20,696
user without having to wait for


346
00:09:20,886 --> 00:09:21,986
the database thread to complete.


347
00:09:23,616 --> 00:09:25,126
This is the advantage of moving


348
00:09:25,236 --> 00:09:26,386
work off the main thread.


349
00:09:28,366 --> 00:09:30,016
When the user interface is done


350
00:09:30,016 --> 00:09:32,296
responding, the CPU can then


351
00:09:32,296 --> 00:09:33,266
switch back to the database


352
00:09:33,316 --> 00:09:34,326
thread, and then finish the


353
00:09:34,326 --> 00:09:35,296
networking task as well.


354
00:09:36,496 --> 00:09:37,776
So, taking advantage of


355
00:09:37,776 --> 00:09:39,476
concurrency like this lets you


356
00:09:39,476 --> 00:09:40,696
build responsive apps.


357
00:09:40,736 --> 00:09:42,616
The main thread can always


358
00:09:42,616 --> 00:09:43,806
respond to the user's action


359
00:09:44,076 --> 00:09:45,256
without having to wait for other


360
00:09:45,256 --> 00:09:46,146
parts of your application to


361
00:09:46,146 --> 00:09:46,516
complete.


362
00:09:46,806 --> 00:09:48,886
So, let's take a look at what


363
00:09:48,956 --> 00:09:50,376
that looks like to the CPU.


364
00:09:51,416 --> 00:09:52,716
These white lines above here


365
00:09:52,896 --> 00:09:54,266
show the content switches


366
00:09:54,316 --> 00:09:55,206
between the subsystems.


367
00:09:55,946 --> 00:09:57,616
A contact switch is when the CPU


368
00:09:57,616 --> 00:09:58,686
switches between these different


369
00:09:58,686 --> 00:10:00,356
subsystems or threads that make


370
00:10:00,356 --> 00:10:01,146
up your application.


371
00:10:02,066 --> 00:10:03,146
If you want to visualize what


372
00:10:03,196 --> 00:10:04,226
this looks like to your


373
00:10:04,226 --> 00:10:05,636
application, you can use


374
00:10:05,636 --> 00:10:07,226
instrument system trace, which


375
00:10:07,226 --> 00:10:08,856
shows you what the CPUs and the


376
00:10:08,856 --> 00:10:10,016
threads are doing when they're


377
00:10:10,016 --> 00:10:11,026
running in your application.


378
00:10:11,446 --> 00:10:12,806
If you want to learn more about


379
00:10:12,806 --> 00:10:14,316
this, you can watch the "System


380
00:10:14,316 --> 00:10:15,716
Trace In-Depth" Talk from last


381
00:10:15,716 --> 00:10:16,636
year where the instrument's team


382
00:10:16,636 --> 00:10:17,946
described how you use system


383
00:10:17,946 --> 00:10:18,226
trace.


384
00:10:19,276 --> 00:10:21,186
So, this concept of context


385
00:10:21,186 --> 00:10:22,576
switching is where the power of


386
00:10:22,576 --> 00:10:23,596
concurrency comes from.


387
00:10:24,096 --> 00:10:25,546
Let's look at when these context


388
00:10:25,546 --> 00:10:27,006
switch might happen and what


389
00:10:27,006 --> 00:10:27,496
causes them.


390
00:10:28,366 --> 00:10:30,646
Well, they can start when a high


391
00:10:30,646 --> 00:10:32,226
priority thread needs the CPU as


392
00:10:32,226 --> 00:10:33,686
we saw earlier, with the UI


393
00:10:33,686 --> 00:10:34,806
thread pre-empting the database


394
00:10:34,806 --> 00:10:35,056
thread.


395
00:10:35,966 --> 00:10:37,366
It can also happen when a thread


396
00:10:37,366 --> 00:10:39,646
finishes its current work, or


397
00:10:39,826 --> 00:10:40,796
it's waiting to acquire


398
00:10:40,796 --> 00:10:41,356
resource.


399
00:10:41,736 --> 00:10:42,506
Or it's waiting for an


400
00:10:42,506 --> 00:10:43,406
asynchronous request to


401
00:10:43,406 --> 00:10:43,736
complete.


402
00:10:44,856 --> 00:10:46,446
However, with this great power


403
00:10:46,446 --> 00:10:48,406
of concurrency comes great


404
00:10:48,406 --> 00:10:49,576
responsibility as well.


405
00:10:49,766 --> 00:10:51,216
You can have too much of a good


406
00:10:51,216 --> 00:10:51,436
thing.


407
00:10:52,896 --> 00:10:54,266
Let's say you're switching


408
00:10:54,266 --> 00:10:56,116
between the network and database


409
00:10:56,116 --> 00:10:57,666
threads on your CPU.


410
00:10:58,386 --> 00:10:59,626
A few context switches is fine,


411
00:10:59,626 --> 00:11:00,586
that's the power of concurrency


412
00:11:00,586 --> 00:11:01,236
you're switching between


413
00:11:01,236 --> 00:11:01,936
different tasks.


414
00:11:02,616 --> 00:11:04,146
However, if you're doing this


415
00:11:04,146 --> 00:11:05,686
thousands of times in really


416
00:11:05,816 --> 00:11:07,706
rapid succession, you run into


417
00:11:07,706 --> 00:11:08,046
trouble.


418
00:11:08,126 --> 00:11:08,726
You're starting to lose


419
00:11:08,786 --> 00:11:10,776
performance because each white


420
00:11:10,776 --> 00:11:12,656
bar here is a context switch.


421
00:11:13,066 --> 00:11:14,066
And the overhead of a context


422
00:11:14,066 --> 00:11:14,726
switch adds up.


423
00:11:14,726 --> 00:11:16,076
It's not just the time we spend


424
00:11:16,076 --> 00:11:17,156
executing the context switch,


425
00:11:17,486 --> 00:11:18,606
it's also that history that the


426
00:11:18,606 --> 00:11:19,816
core has built up, it has to


427
00:11:19,846 --> 00:11:21,076
regain that history after every


428
00:11:21,076 --> 00:11:22,306
contact switch.


429
00:11:22,996 --> 00:11:24,666
There are also other affects you


430
00:11:24,666 --> 00:11:25,276
might experience.


431
00:11:25,556 --> 00:11:26,626
For example, there might be


432
00:11:26,626 --> 00:11:28,566
others ahead of you in line for


433
00:11:28,566 --> 00:11:29,506
access to the CPU.


434
00:11:30,486 --> 00:11:32,356
You have to wait each time you


435
00:11:32,356 --> 00:11:33,966
context switch for the rest of


436
00:11:33,966 --> 00:11:35,316
the queue to drain out and so


437
00:11:35,316 --> 00:11:37,996
you may be delayed by somebody


438
00:11:37,996 --> 00:11:38,926
else ahead of you in line.


439
00:11:39,616 --> 00:11:41,026
So, let's look about what might


440
00:11:41,056 --> 00:11:42,336
cause excessive context


441
00:11:42,336 --> 00:11:42,576
switching.


442
00:11:43,696 --> 00:11:45,446
So, there's three main causes


443
00:11:45,446 --> 00:11:46,376
we're going to talk about today.


444
00:11:46,966 --> 00:11:48,416
First, repeatedly waiting for


445
00:11:48,416 --> 00:11:49,786
exclusive access to contended


446
00:11:49,786 --> 00:11:50,336
resources.


447
00:11:50,896 --> 00:11:52,086
Repeatedly switching between


448
00:11:52,086 --> 00:11:53,836
independent operations, and


449
00:11:53,836 --> 00:11:55,306
repeatedly bouncing an operation


450
00:11:55,446 --> 00:11:56,136
between threads.


451
00:11:56,956 --> 00:11:57,936
You note, that I repeated the


452
00:11:57,936 --> 00:11:59,846
word repeatedly several times.


453
00:12:00,596 --> 00:12:01,376
That's intentional.


454
00:12:03,336 --> 00:12:04,856
Context switching a few times is


455
00:12:04,856 --> 00:12:06,396
okay, that's how concurrency


456
00:12:06,396 --> 00:12:07,436
works, that's the power that


457
00:12:07,436 --> 00:12:07,986
we're giving you.


458
00:12:08,646 --> 00:12:10,206
However, when you repeat it too


459
00:12:10,206 --> 00:12:11,746
many times, the cost start to


460
00:12:11,746 --> 00:12:12,126
add up.


461
00:12:13,376 --> 00:12:14,576
So, let's start by looking at


462
00:12:14,576 --> 00:12:15,796
the first case, which is


463
00:12:15,796 --> 00:12:17,176
exclusive access to contended


464
00:12:17,176 --> 00:12:17,826
resources.


465
00:12:18,786 --> 00:12:19,546
When can this happen?


466
00:12:20,306 --> 00:12:22,696
Well, the primary case in which


467
00:12:22,696 --> 00:12:24,176
this happens is when you have a


468
00:12:24,176 --> 00:12:26,046
lock and a bunch of threads are


469
00:12:26,046 --> 00:12:28,176
all trying to acquire that lock.


470
00:12:29,486 --> 00:12:30,716
So, how can you tell if this is


471
00:12:30,716 --> 00:12:31,686
occurring in your application?


472
00:12:31,896 --> 00:12:33,426
Well, we can go back to system


473
00:12:33,426 --> 00:12:33,786
trace.


474
00:12:34,346 --> 00:12:35,736
We can visualize what it looks


475
00:12:35,736 --> 00:12:36,466
like in instruments.


476
00:12:37,246 --> 00:12:38,776
So, let's say this shows us that


477
00:12:38,776 --> 00:12:40,306
we have many threads running for


478
00:12:40,306 --> 00:12:41,386
a very short time and they're


479
00:12:41,386 --> 00:12:42,616
all handing off to each other in


480
00:12:42,616 --> 00:12:43,376
a little cascade.


481
00:12:44,276 --> 00:12:45,446
Let's focus on the first thread


482
00:12:45,446 --> 00:12:46,306
and see what instruments it's


483
00:12:46,306 --> 00:12:46,756
telling us.


484
00:12:47,896 --> 00:12:49,746
We have this blue track, which


485
00:12:49,746 --> 00:12:51,846
shows when a thread is on CPU.


486
00:12:52,526 --> 00:12:53,856
And the red track shows when


487
00:12:53,856 --> 00:12:54,686
it's making a sys call.


488
00:12:55,136 --> 00:12:56,776
In this case, it's making


489
00:12:56,776 --> 00:12:58,586
[inaudible].


490
00:12:58,586 --> 00:13:00,336
This shows that most of its time


491
00:13:00,336 --> 00:13:01,276
is waiting for the [inaudible]


492
00:13:01,276 --> 00:13:02,216
to become available.


493
00:13:02,216 --> 00:13:04,536
And the on core time is very


494
00:13:04,536 --> 00:13:06,096
short at only 10 microseconds.


495
00:13:06,996 --> 00:13:08,316
And there are a lot of context


496
00:13:08,316 --> 00:13:09,596
switches going on on the system,


497
00:13:09,926 --> 00:13:10,596
which it shows you on the


498
00:13:10,596 --> 00:13:11,566
context switches track at the


499
00:13:11,566 --> 00:13:11,836
top.


500
00:13:13,806 --> 00:13:15,636
So, what's causing this?


501
00:13:15,856 --> 00:13:17,006
Let's go back to look at our


502
00:13:17,006 --> 00:13:19,456
simple timeline and see how you


503
00:13:19,736 --> 00:13:23,636
excess contingent could be


504
00:13:23,636 --> 00:13:24,146
playing out.


505
00:13:25,056 --> 00:13:25,956
So, you see this sort of


506
00:13:25,956 --> 00:13:27,526
staircase pattern in time.


507
00:13:27,996 --> 00:13:29,056
Where each thread is running for


508
00:13:29,056 --> 00:13:30,416
a short time, and then giving up


509
00:13:30,416 --> 00:13:31,446
the CPU to the next thread,


510
00:13:31,626 --> 00:13:32,976
rinse and repeat for a long


511
00:13:32,976 --> 00:13:33,196
time.


512
00:13:33,906 --> 00:13:35,266
You want your work to look more


513
00:13:35,266 --> 00:13:35,826
like this.


514
00:13:36,696 --> 00:13:38,456
Where you have the CPU can focus


515
00:13:38,456 --> 00:13:39,926
on one thing at a time, get it


516
00:13:39,926 --> 00:13:41,096
done, and then work on the next


517
00:13:41,186 --> 00:13:41,376
task.


518
00:13:42,336 --> 00:13:43,526
So, what's going on here that


519
00:13:43,526 --> 00:13:44,536
causes that staircase?


520
00:13:44,706 --> 00:13:45,876
Let's zoom in to one of these


521
00:13:45,876 --> 00:13:46,566
stair steps.


522
00:13:47,616 --> 00:13:49,476
So, here we're focusing on two


523
00:13:49,476 --> 00:13:50,666
threads, the green thread and


524
00:13:50,666 --> 00:13:51,226
the blue thread.


525
00:13:51,776 --> 00:13:53,546
And we have a CPU on top.


526
00:13:54,246 --> 00:13:55,476
We've added a new lock track


527
00:13:55,566 --> 00:13:56,806
here that shows the state of the


528
00:13:56,806 --> 00:13:58,106
lock and what thread owns it.


529
00:13:58,336 --> 00:14:00,126
In this case, the blue thread


530
00:14:00,126 --> 00:14:01,676
owns the block, and the green


531
00:14:01,676 --> 00:14:02,206
thread is waiting.


532
00:14:03,456 --> 00:14:04,366
So, when the blue thread


533
00:14:04,506 --> 00:14:06,166
unlocks, the ownership of that


534
00:14:06,166 --> 00:14:07,296
lock is transferred to the green


535
00:14:07,296 --> 00:14:08,276
thread, because it's next in


536
00:14:08,276 --> 00:14:08,556
line.


537
00:14:09,446 --> 00:14:11,246
However, when the blue thread


538
00:14:11,676 --> 00:14:12,926
turns around and grabs the lock


539
00:14:12,926 --> 00:14:14,856
again, it can't because the lock


540
00:14:14,856 --> 00:14:15,706
is reserved for the green


541
00:14:15,706 --> 00:14:15,966
thread.


542
00:14:16,666 --> 00:14:18,216
It forces at context switch


543
00:14:18,216 --> 00:14:19,436
because we now have to do


544
00:14:19,436 --> 00:14:20,106
something else.


545
00:14:20,486 --> 00:14:21,436
And we switch to the green


546
00:14:21,436 --> 00:14:23,496
thread, and the CPU can then


547
00:14:24,106 --> 00:14:25,346
finish the lock and we can


548
00:14:25,346 --> 00:14:25,696
repeat.


549
00:14:27,056 --> 00:14:28,056
Sometimes this is useful.


550
00:14:28,276 --> 00:14:29,376
You want every thread that's


551
00:14:29,376 --> 00:14:30,256
waiting on the lock to get a


552
00:14:30,256 --> 00:14:31,886
chance to acquire the resource,


553
00:14:32,606 --> 00:14:34,756
however, what if you had a lock


554
00:14:34,806 --> 00:14:35,746
that works a different way.


555
00:14:36,536 --> 00:14:38,696
Let's start again by looking at


556
00:14:38,696 --> 00:14:39,856
what an unfair lock does.


557
00:14:40,866 --> 00:14:43,006
So, this time when blue thread


558
00:14:43,006 --> 00:14:45,006
unlocks, the lock isn't


559
00:14:45,056 --> 00:14:45,456
reserved.


560
00:14:45,856 --> 00:14:46,946
The ownership of the lock is up


561
00:14:46,946 --> 00:14:47,616
for grabs.


562
00:14:48,616 --> 00:14:50,156
Blue can take the lock again,


563
00:14:50,536 --> 00:14:52,026
and it can immediately reacquire


564
00:14:52,026 --> 00:14:54,066
and stay on CPU without forcing


565
00:14:54,066 --> 00:14:54,786
a context switch.


566
00:14:55,716 --> 00:14:56,876
This might make it difficult for


567
00:14:56,876 --> 00:14:57,896
the green thread to actually get


568
00:14:57,896 --> 00:14:59,786
a chance at the lock, but it


569
00:14:59,786 --> 00:15:00,966
reduces the number of context


570
00:15:00,966 --> 00:15:02,346
switches the blue thread has to


571
00:15:02,886 --> 00:15:06,466
have in order to reacquire the


572
00:15:06,466 --> 00:15:06,636
lock.


573
00:15:08,016 --> 00:15:09,106
So, to recap when we're talking


574
00:15:09,106 --> 00:15:10,276
about lock contention, you


575
00:15:10,276 --> 00:15:11,716
actually want to make sure to


576
00:15:11,716 --> 00:15:12,986
measure your application and


577
00:15:12,986 --> 00:15:14,616
system trace and see if you have


578
00:15:14,616 --> 00:15:15,036
an issue.


579
00:15:15,496 --> 00:15:17,586
If you do, often the unfair lock


580
00:15:17,866 --> 00:15:19,296
works best for objects,


581
00:15:19,646 --> 00:15:21,046
properties, for global state in


582
00:15:21,046 --> 00:15:22,266
your application that may have


583
00:15:22,266 --> 00:15:23,926
taken a drop many, many times.


584
00:15:25,526 --> 00:15:26,486
There's one other thing I want


585
00:15:26,486 --> 00:15:27,186
to talk about when we're


586
00:15:27,186 --> 00:15:28,326
mentioning locks, and that is


587
00:15:28,326 --> 00:15:29,096
lock ownership.


588
00:15:29,866 --> 00:15:31,276
So, remember the lock track we


589
00:15:31,276 --> 00:15:33,386
had earlier, the runtime knows


590
00:15:33,386 --> 00:15:34,386
which thread will unlock the


591
00:15:34,386 --> 00:15:34,876
lock next.


592
00:15:36,016 --> 00:15:37,256
We can take advantage of that


593
00:15:37,256 --> 00:15:38,796
power to automatically result


594
00:15:38,796 --> 00:15:40,056
priority inversions in your app


595
00:15:40,496 --> 00:15:41,666
between the waiters and the


596
00:15:41,666 --> 00:15:42,356
owners of the lock.


597
00:15:42,776 --> 00:15:44,216
And even enable other


598
00:15:44,216 --> 00:15:46,056
optimizations, like directed CPU


599
00:15:46,056 --> 00:15:47,166
handoff to the owning thread.


600
00:15:47,756 --> 00:15:48,866
Pierre is going to discuss this


601
00:15:49,156 --> 00:15:50,356
later in our talk, when talking


602
00:15:50,356 --> 00:15:51,096
about dispatch sync.


603
00:15:52,196 --> 00:15:54,286
We often get the question, which


604
00:15:54,346 --> 00:15:55,726
primitives have this power and


605
00:15:55,726 --> 00:15:56,426
which ones don't.


606
00:15:57,476 --> 00:15:58,736
Let's take a look at which


607
00:15:58,736 --> 00:15:59,776
low-level primitives do this


608
00:15:59,836 --> 00:16:00,106
today.


609
00:16:01,506 --> 00:16:03,096
So, primitives with a single


610
00:16:03,146 --> 00:16:04,536
known owner have this power.


611
00:16:04,926 --> 00:16:06,536
Things like serial queues and OS


612
00:16:06,536 --> 00:16:07,096
unfair lock.


613
00:16:08,186 --> 00:16:10,616
However, asymmetric primitives,


614
00:16:10,616 --> 00:16:11,976
like dispatch semaphore and


615
00:16:11,976 --> 00:16:13,466
dispatch group don't have this


616
00:16:13,536 --> 00:16:14,666
power, because the runtime


617
00:16:14,666 --> 00:16:15,826
doesn't know what thread will


618
00:16:15,826 --> 00:16:16,886
single the sub primitive.


619
00:16:18,636 --> 00:16:20,286
Finally, primitives with


620
00:16:20,286 --> 00:16:22,106
multiple owners like private,


621
00:16:22,106 --> 00:16:23,416
concurrent queues and read or


622
00:16:23,416 --> 00:16:24,736
writer locks, the systems


623
00:16:24,736 --> 00:16:25,726
doesn't take advantage of that


624
00:16:25,776 --> 00:16:26,626
today, because there isn't a


625
00:16:26,626 --> 00:16:27,236
single owner.


626
00:16:27,236 --> 00:16:29,516
When you're picking a primitive


627
00:16:30,036 --> 00:16:31,576
consider whether or not your use


628
00:16:31,646 --> 00:16:32,936
case involves threads of


629
00:16:32,936 --> 00:16:33,656
different priorities


630
00:16:33,656 --> 00:16:34,226
interacting.


631
00:16:35,096 --> 00:16:37,096
In like a high party UI thread


632
00:16:37,096 --> 00:16:38,416
with a lower priority background


633
00:16:38,416 --> 00:16:38,636
thread.


634
00:16:39,836 --> 00:16:41,456
If so, you might want to take


635
00:16:41,456 --> 00:16:42,416
advantage of a primitive


636
00:16:42,456 --> 00:16:44,736
ownership that ensures that your


637
00:16:44,886 --> 00:16:46,416
UI thread doesn't get delayed by


638
00:16:46,416 --> 00:16:47,336
waiting on a lower priority


639
00:16:47,336 --> 00:16:48,606
background thread.


640
00:16:49,256 --> 00:16:51,056
So, in summary, these


641
00:16:51,126 --> 00:16:52,526
inefficient behaviors often


642
00:16:52,526 --> 00:16:53,496
emerge in properties of your


643
00:16:53,496 --> 00:16:54,126
application.


644
00:16:54,486 --> 00:16:55,806
It's not easy to find these


645
00:16:55,846 --> 00:16:56,976
problems just by looking at your


646
00:16:56,976 --> 00:16:57,336
code.


647
00:16:57,816 --> 00:16:59,196
You should observe it in


648
00:16:59,196 --> 00:17:00,846
instrument system trace to


649
00:17:00,846 --> 00:17:02,446
visualize your apps true, real


650
00:17:02,446 --> 00:17:04,425
behavior and so you can use the


651
00:17:04,486 --> 00:17:05,526
right lock for the job.


652
00:17:06,526 --> 00:17:07,415
So, I've just discussed the


653
00:17:07,516 --> 00:17:09,016
first cause on our context


654
00:17:09,016 --> 00:17:10,165
switching list, which is


655
00:17:10,165 --> 00:17:11,246
exclusive access.


656
00:17:12,566 --> 00:17:13,925
To discuss some other ways your


657
00:17:13,925 --> 00:17:15,256
apps can experience excessive


658
00:17:15,256 --> 00:17:16,445
context switching, I'm going to


659
00:17:16,445 --> 00:17:18,726
bring out my Daniel Steffen to


660
00:17:19,056 --> 00:17:20,476
talk to you about how you can


661
00:17:20,526 --> 00:17:21,856
organize your concurrency with


662
00:17:21,856 --> 00:17:23,886
GCD, to avoid these pitfalls.


663
00:17:26,516 --> 00:17:31,126
[ Applause ]


664
00:17:31,626 --> 00:17:31,866
>> All right.


665
00:17:32,076 --> 00:17:32,666
Thank you, Daniel.


666
00:17:34,336 --> 00:17:35,486
So, we've got a lot to cover


667
00:17:35,486 --> 00:17:37,036
today, so I won't be able to go


668
00:17:37,036 --> 00:17:38,556
into too many details on the


669
00:17:38,556 --> 00:17:39,856
fundamentals of GCD.


670
00:17:40,406 --> 00:17:41,966
If you're new to the technology,


671
00:17:42,376 --> 00:17:43,686
or need a bit of a refresher,


672
00:17:43,796 --> 00:17:44,936
here are some of the sessions at


673
00:17:44,936 --> 00:17:47,286
previous WWDC conferences that


674
00:17:47,286 --> 00:17:48,746
covered GCD and the enhancements


675
00:17:48,746 --> 00:17:49,626
that we've made to it over the


676
00:17:49,626 --> 00:17:50,106
years.


677
00:17:50,686 --> 00:17:51,866
So, I encourage you to go and


678
00:17:51,866 --> 00:17:52,806
see those on video.


679
00:17:53,496 --> 00:17:55,266
We do need a few of the basic


680
00:17:55,266 --> 00:17:56,876
concepts of GCD today however,


681
00:17:56,876 --> 00:17:58,776
starting with the serial


682
00:17:58,926 --> 00:17:59,566
dispatch queue.


683
00:18:00,566 --> 00:18:01,936
This is really our fundamental


684
00:18:02,446 --> 00:18:03,746
synchronization primitive in


685
00:18:03,746 --> 00:18:04,086
GCD.


686
00:18:04,086 --> 00:18:06,266
It provides you with mutual


687
00:18:06,266 --> 00:18:07,706
exclusion as well as FIFO


688
00:18:07,736 --> 00:18:07,996
ordering.


689
00:18:07,996 --> 00:18:09,836
This is one of these ordered and


690
00:18:10,106 --> 00:18:11,426
fair primitives that Daniel just


691
00:18:11,426 --> 00:18:11,796
mentioned.


692
00:18:13,296 --> 00:18:15,196
And it has a concurrent atomic


693
00:18:15,196 --> 00:18:16,406
in queue operation, so it's find


694
00:18:16,406 --> 00:18:17,406
for multiple threads to in


695
00:18:17,406 --> 00:18:18,736
queue, operations into the queue


696
00:18:18,736 --> 00:18:20,396
at the same time, as well as a


697
00:18:20,396 --> 00:18:21,706
single DQI thread that the


698
00:18:21,706 --> 00:18:23,216
system provides to execute


699
00:18:23,216 --> 00:18:24,376
asynchronous work out of the


700
00:18:24,376 --> 00:18:24,676
queue.


701
00:18:25,416 --> 00:18:26,746
So, let's look at an example of


702
00:18:26,846 --> 00:18:27,496
this in action.


703
00:18:27,496 --> 00:18:29,436
Here we're creating a serial


704
00:18:29,466 --> 00:18:30,926
queue by calling the dispatch


705
00:18:30,926 --> 00:18:33,256
queue constructor and that will


706
00:18:33,256 --> 00:18:34,976
give you a piece of memory that


707
00:18:34,976 --> 00:18:36,426
as long as you haven't used it


708
00:18:36,426 --> 00:18:37,506
yet, it's just in your


709
00:18:37,506 --> 00:18:38,066
application.


710
00:18:38,826 --> 00:18:40,006
Now, imagine there's two threads


711
00:18:40,006 --> 00:18:41,046
that come along in call D


712
00:18:41,046 --> 00:18:43,086
queue.async method to submit


713
00:18:43,086 --> 00:18:44,416
some asynchronous work into this


714
00:18:44,416 --> 00:18:44,736
queue.


715
00:18:45,396 --> 00:18:46,336
As mentioned, it's find for


716
00:18:46,336 --> 00:18:47,646
multiple threads to do this, and


717
00:18:47,646 --> 00:18:49,546
the items will just get in queue


718
00:18:49,546 --> 00:18:50,726
in the order that they appeared.


719
00:18:52,296 --> 00:18:53,186
And because this is the


720
00:18:53,186 --> 00:18:54,946
asynchronous method, this method


721
00:18:54,946 --> 00:18:57,846
returns and the threads can go


722
00:18:57,846 --> 00:18:59,486
on their way, so maybe this


723
00:18:59,656 --> 00:19:00,776
first thread eventually calls


724
00:19:00,776 --> 00:19:01,526
queue.sync.


725
00:19:01,966 --> 00:19:02,886
This is the way you interact


726
00:19:02,886 --> 00:19:03,936
synchronously with the queue.


727
00:19:04,316 --> 00:19:05,356
And because this is an ordered


728
00:19:05,356 --> 00:19:06,526
primitive here, what this does


729
00:19:06,526 --> 00:19:07,566
is it will in queue a


730
00:19:07,566 --> 00:19:09,726
placeholder into the queue so


731
00:19:09,726 --> 00:19:12,716
that the thread can wait until


732
00:19:12,716 --> 00:19:13,526
it is its turn.


733
00:19:14,556 --> 00:19:16,876
And now, there's this automatic


734
00:19:17,136 --> 00:19:18,506
worker thread that will come


735
00:19:18,506 --> 00:19:19,826
along to execute the


736
00:19:19,826 --> 00:19:21,556
asynchronous work items, until


737
00:19:21,556 --> 00:19:22,896
you get to that placeholder at


738
00:19:22,896 --> 00:19:24,446
which point the ownership of the


739
00:19:24,446 --> 00:19:25,686
queue will transfer to the


740
00:19:25,746 --> 00:19:27,136
thread waiting in queue.sync so


741
00:19:27,136 --> 00:19:29,056
that it can execute its block.


742
00:19:30,126 --> 00:19:32,746
So, the next concept that we'll


743
00:19:32,746 --> 00:19:34,876
need is the dispatch source.


744
00:19:35,106 --> 00:19:36,286
This is our event monitoring


745
00:19:36,286 --> 00:19:37,456
primitive in GCD.


746
00:19:37,846 --> 00:19:39,446
Here we are setting one up to


747
00:19:39,446 --> 00:19:40,536
monitor a default descriptive


748
00:19:40,536 --> 00:19:42,646
for readability if you make read


749
00:19:42,646 --> 00:19:43,436
source constructor.


750
00:19:43,806 --> 00:19:45,656
You pass it in a queue which is


751
00:19:45,686 --> 00:19:46,996
the target queue of the source,


752
00:19:47,336 --> 00:19:48,906
which is where we execute the


753
00:19:48,906 --> 00:19:50,126
event handle of the source,


754
00:19:50,446 --> 00:19:51,496
which here just reads from the


755
00:19:51,496 --> 00:19:52,246
default descriptor.


756
00:19:53,046 --> 00:19:54,166
This target queue is also where


757
00:19:54,166 --> 00:19:55,406
you might put other work that


758
00:19:55,406 --> 00:19:56,646
should be serialized with this


759
00:19:56,646 --> 00:19:58,426
operation, such as processing


760
00:19:58,426 --> 00:19:59,656
the data that was read.


761
00:20:00,816 --> 00:20:02,206
Then, we set the cancel handler


762
00:20:02,206 --> 00:20:03,396
for the source, which is how


763
00:20:03,396 --> 00:20:04,786
sources implement the


764
00:20:04,816 --> 00:20:06,326
invalidation pattern.


765
00:20:07,056 --> 00:20:08,066
And finally, when everything is


766
00:20:08,096 --> 00:20:09,166
set up, you call source and


767
00:20:09,166 --> 00:20:10,566
activate to start monitoring.


768
00:20:11,196 --> 00:20:12,736
So, it's worth noting that


769
00:20:12,826 --> 00:20:14,546
sources are really just an


770
00:20:14,546 --> 00:20:15,806
instance of a more general


771
00:20:15,806 --> 00:20:17,606
pattern throughout the OS, where


772
00:20:17,706 --> 00:20:19,116
you have objects that deliver


773
00:20:19,116 --> 00:20:20,566
events to you on a target queue


774
00:20:20,566 --> 00:20:21,486
that you specify.


775
00:20:22,086 --> 00:20:23,696
So, if you're familiar with XPC,


776
00:20:23,696 --> 00:20:25,476
that would be another example of


777
00:20:25,546 --> 00:20:26,666
that XPC connections.


778
00:20:27,986 --> 00:20:30,246
And, it's worth noting that all


779
00:20:30,246 --> 00:20:31,086
of everything we're telling you


780
00:20:31,086 --> 00:20:32,406
today about sources really


781
00:20:32,406 --> 00:20:33,926
applies to all such objects in


782
00:20:33,926 --> 00:20:34,266
general.


783
00:20:36,286 --> 00:20:37,616
So, putting these two concepts


784
00:20:37,616 --> 00:20:39,526
together, we get what we call


785
00:20:39,526 --> 00:20:40,606
the target queue hierarchy.


786
00:20:41,346 --> 00:20:46,156
So, here we have two sources


787
00:20:46,156 --> 00:20:48,266
with their associated target


788
00:20:48,296 --> 00:20:50,466
queues, S1, S2 and the queue is


789
00:20:50,536 --> 00:20:50,886
Q1 and Q2.


790
00:20:50,886 --> 00:20:52,416
And we can form a little tree


791
00:20:52,416 --> 00:20:53,776
out of this situation by adding


792
00:20:53,776 --> 00:20:54,856
yet another serial queue to the


793
00:20:54,856 --> 00:20:57,296
mix, by adding mutual exclusion


794
00:20:57,296 --> 00:20:58,706
queue, EQ, at the bottom.


795
00:20:59,336 --> 00:21:00,636
The way we do this is simply by


796
00:21:00,636 --> 00:21:02,006
passing in the optional target


797
00:21:02,006 --> 00:21:03,486
argument into the dispatch queue


798
00:21:03,486 --> 00:21:03,996
constructor.


799
00:21:03,996 --> 00:21:06,786
So, this gives you a shared


800
00:21:06,786 --> 00:21:08,216
single mutual exclusion context


801
00:21:08,216 --> 00:21:09,096
for this whole tree.


802
00:21:09,506 --> 00:21:11,466
Only one of the sources or one


803
00:21:11,466 --> 00:21:12,636
item in one of the queues can


804
00:21:12,636 --> 00:21:13,686
execute at one time.


805
00:21:14,426 --> 00:21:16,086
But it preserves the independent


806
00:21:16,086 --> 00:21:17,606
individual queue order for queue


807
00:21:17,606 --> 00:21:18,756
1 and queue 2.


808
00:21:19,216 --> 00:21:20,456
So, let's look at what I mean by


809
00:21:20,456 --> 00:21:21,586
that.


810
00:21:22,376 --> 00:21:23,876
Here I have the two queues,


811
00:21:23,876 --> 00:21:25,806
queue 1 and queue 2 with them


812
00:21:25,806 --> 00:21:27,036
queued in a specific order.


813
00:21:27,796 --> 00:21:28,936
And because we have this extra


814
00:21:28,936 --> 00:21:30,476
serial queue at the bottom, and


815
00:21:30,476 --> 00:21:32,186
the executes, they will execute


816
00:21:32,266 --> 00:21:34,516
in EQ and there will be a single


817
00:21:34,516 --> 00:21:35,626
worker thread executing these


818
00:21:35,626 --> 00:21:36,916
items giving you that mutual


819
00:21:36,916 --> 00:21:38,386
exclusion property, only one


820
00:21:38,386 --> 00:21:39,686
item executing at one time.


821
00:21:39,736 --> 00:21:41,476
But as you can see, the items


822
00:21:41,476 --> 00:21:42,756
from both queues can execute


823
00:21:42,796 --> 00:21:44,736
interleafed while preserving the


824
00:21:44,736 --> 00:21:45,996
individual order that they had


825
00:21:45,996 --> 00:21:47,056
in their original queues.


826
00:21:47,606 --> 00:21:51,236
So, the last concept that we'll


827
00:21:51,236 --> 00:21:53,026
need today, is the notion of


828
00:21:53,026 --> 00:21:54,026
quality of service.


829
00:21:55,296 --> 00:21:56,906
Here is a fairly deed concept


830
00:21:56,906 --> 00:21:57,876
that was talked about in some


831
00:21:57,876 --> 00:21:58,886
detail in the past.


832
00:21:58,886 --> 00:22:00,006
In particular, in the power


833
00:22:00,006 --> 00:22:01,166
performance and diagnostics


834
00:22:01,166 --> 00:22:03,726
session in 2014.


835
00:22:03,726 --> 00:22:05,576
So, if this is new to you, I


836
00:22:05,576 --> 00:22:07,066
would encourage you to go and


837
00:22:07,066 --> 00:22:07,606
watch that.


838
00:22:08,456 --> 00:22:09,826
But what we'll need today from


839
00:22:09,826 --> 00:22:11,556
this is really mostly it's


840
00:22:11,916 --> 00:22:13,226
abstract notion of priority.


841
00:22:14,686 --> 00:22:16,486
And we'll use the terms QOS and


842
00:22:16,486 --> 00:22:18,446
priority somewhat


843
00:22:18,446 --> 00:22:19,456
interchangeably in the rest of


844
00:22:19,496 --> 00:22:19,966
the session.


845
00:22:21,266 --> 00:22:22,536
We have four quality of service


846
00:22:22,536 --> 00:22:23,506
classes on the system.


847
00:22:23,746 --> 00:22:25,056
From the highest user


848
00:22:25,056 --> 00:22:27,136
interactive UI to user


849
00:22:27,136 --> 00:22:30,216
initiated, or IN, utility, UT to


850
00:22:30,366 --> 00:22:31,516
background BG.


851
00:22:32,056 --> 00:22:32,866
The lowest priority.


852
00:22:33,986 --> 00:22:35,366
So, let's look at how we would


853
00:22:35,366 --> 00:22:36,876
combine this concept of quality


854
00:22:36,876 --> 00:22:38,346
of service with the target queue


855
00:22:38,346 --> 00:22:39,326
hierarchy that we just looked


856
00:22:39,326 --> 00:22:39,516
at.


857
00:22:40,286 --> 00:22:41,856
In this hierarchy, every node in


858
00:22:41,856 --> 00:22:42,746
the tree can actually have a


859
00:22:42,746 --> 00:22:43,826
quality of service label


860
00:22:43,826 --> 00:22:44,646
associated with it.


861
00:22:45,436 --> 00:22:47,086
So, for instance the source 2


862
00:22:47,086 --> 00:22:48,146
might be relevant to the user


863
00:22:48,146 --> 00:22:48,716
interface.


864
00:22:49,096 --> 00:22:50,156
It might be monitored for an


865
00:22:50,156 --> 00:22:51,376
event where we should update the


866
00:22:51,376 --> 00:22:52,556
UI as soon as the event


867
00:22:52,556 --> 00:22:52,876
triggers.


868
00:22:52,876 --> 00:22:55,036
So, it could be that we want to


869
00:22:55,036 --> 00:22:56,546
put the UI label onto the


870
00:22:56,546 --> 00:22:57,046
source.


871
00:22:57,916 --> 00:22:59,006
Another common use source would


872
00:22:59,006 --> 00:23:01,546
be to put a label on the mutual


873
00:23:01,546 --> 00:23:03,066
exclusion queue to provide a


874
00:23:03,066 --> 00:23:04,866
flow of execution so that


875
00:23:04,866 --> 00:23:06,296
nothing in this tree can execute


876
00:23:06,296 --> 00:23:08,146
below this level, so UT in this


877
00:23:08,146 --> 00:23:08,606
example.


878
00:23:09,106 --> 00:23:11,946
And now if anything else in this


879
00:23:11,946 --> 00:23:13,636
queue fires, for instance source


880
00:23:13,636 --> 00:23:16,516
1, we will be using this flow


881
00:23:16,516 --> 00:23:18,016
for the tree if it doesn't have


882
00:23:18,016 --> 00:23:19,076
its own quality of service


883
00:23:19,076 --> 00:23:19,626
associated.


884
00:23:21,426 --> 00:23:23,816
And source firing is really just


885
00:23:23,816 --> 00:23:26,116
an async executes from the


886
00:23:26,116 --> 00:23:26,516
kernel.


887
00:23:26,886 --> 00:23:28,576
And the same as before happens,


888
00:23:28,576 --> 00:23:29,806
we end queue the source handler


889
00:23:30,206 --> 00:23:31,366
eventually into the mutual


890
00:23:31,366 --> 00:23:32,656
exclusion queue for execution.


891
00:23:34,176 --> 00:23:36,536
For asyncs from user space, your


892
00:23:36,536 --> 00:23:37,666
quality of service is usually


893
00:23:37,666 --> 00:23:38,706
determined from the thread that


894
00:23:38,706 --> 00:23:39,746
called queueu.async.


895
00:23:39,816 --> 00:23:41,646
Now, we have a user initiated


896
00:23:41,646 --> 00:23:44,696
thread that makes item at IN


897
00:23:44,696 --> 00:23:46,586
into the queue and for execution


898
00:23:46,586 --> 00:23:47,666
into E queue eventually.


899
00:23:48,296 --> 00:23:49,626
And now, maybe we have the


900
00:23:49,626 --> 00:23:50,876
source 2 that flies with this


901
00:23:50,876 --> 00:23:53,086
very high priority UI relevant


902
00:23:53,086 --> 00:23:54,836
event that executes its event


903
00:23:54,836 --> 00:23:56,156
handler, and queues its event


904
00:23:56,156 --> 00:23:57,216
handler into EQ.


905
00:23:58,216 --> 00:23:59,306
So, now you'll notice that we


906
00:23:59,306 --> 00:24:01,076
have a priority inversion


907
00:24:01,076 --> 00:24:01,616
situation.


908
00:24:01,616 --> 00:24:02,756
We have three items in queue


909
00:24:02,806 --> 00:24:04,136
with a very high priority item


910
00:24:04,136 --> 00:24:06,116
at the end preceded by some low


911
00:24:06,156 --> 00:24:06,796
priority items.


912
00:24:07,166 --> 00:24:08,216
And these have to execute in


913
00:24:08,216 --> 00:24:08,506
order.


914
00:24:09,716 --> 00:24:10,686
The system resolves this


915
00:24:10,686 --> 00:24:12,036
inversion for you by bringing up


916
00:24:12,036 --> 00:24:14,056
a worker thread at the highest


917
00:24:14,306 --> 00:24:15,696
priority of anything that is


918
00:24:15,766 --> 00:24:16,466
currently in queue.


919
00:24:16,466 --> 00:24:19,116
And it's worth keeping this


920
00:24:19,116 --> 00:24:20,196
little tree on the right hand


921
00:24:20,196 --> 00:24:21,306
side here in mind because it


922
00:24:21,306 --> 00:24:22,436
comes up again later in the


923
00:24:22,436 --> 00:24:22,776
session.


924
00:24:24,166 --> 00:24:28,376
And with that let's move on to


925
00:24:28,376 --> 00:24:29,666
our main topic of the section


926
00:24:29,666 --> 00:24:31,016
which is how to use what we just


927
00:24:31,016 --> 00:24:32,216
learned to express good


928
00:24:32,216 --> 00:24:34,956
granularity of concurrency to


929
00:24:35,636 --> 00:24:35,726
GCD.


930
00:24:35,956 --> 00:24:36,946
Let's go back to our news


931
00:24:36,946 --> 00:24:37,836
application that Daniel


932
00:24:37,836 --> 00:24:39,526
introduced earlier and focus on


933
00:24:39,526 --> 00:24:40,806
the networking subsystem for a


934
00:24:40,806 --> 00:24:43,626
little bit.


935
00:24:43,626 --> 00:24:45,116
In a networking subsystem,


936
00:24:45,116 --> 00:24:46,056
you'll have to monitor some


937
00:24:46,056 --> 00:24:47,056
network connections in the


938
00:24:47,056 --> 00:24:47,456
kernel.


939
00:24:47,936 --> 00:24:49,246
And with GCD you'll do that with


940
00:24:49,246 --> 00:24:50,256
a dispatch source, and the


941
00:24:50,296 --> 00:24:51,326
dispatch queue like you just


942
00:24:51,326 --> 00:24:51,596
saw.


943
00:24:51,596 --> 00:24:53,426
But of course in any networking


944
00:24:53,426 --> 00:24:54,706
subsystem you usually not just


945
00:24:54,706 --> 00:24:55,746
have one network connection,


946
00:24:55,746 --> 00:24:58,036
you'll have many of them and


947
00:24:58,036 --> 00:24:59,276
they will all replicate the same


948
00:24:59,276 --> 00:24:59,876
setup.


949
00:25:00,716 --> 00:25:01,766
So, let's focus on the right


950
00:25:01,766 --> 00:25:03,036
hand side on the three


951
00:25:03,036 --> 00:25:04,806
connections here and see how the


952
00:25:04,806 --> 00:25:05,306
execute.


953
00:25:06,936 --> 00:25:07,896
If the first connection


954
00:25:07,896 --> 00:25:09,736
triggers, just like the same


955
00:25:09,736 --> 00:25:11,146
thing we just saw happens, we


956
00:25:11,146 --> 00:25:12,496
will end queue the event handler


957
00:25:12,496 --> 00:25:14,206
for that source onto its target


958
00:25:14,206 --> 00:25:14,366
queue.


959
00:25:14,456 --> 00:25:16,206
Of course if the other two


960
00:25:16,386 --> 00:25:17,406
connections fire at the same


961
00:25:17,406 --> 00:25:18,546
time, they'll still replicated


962
00:25:18,546 --> 00:25:19,386
and you'll end up with three


963
00:25:19,386 --> 00:25:21,056
queues with an event handler end


964
00:25:21,056 --> 00:25:21,296
queued.


965
00:25:22,026 --> 00:25:23,516
And because you have these three


966
00:25:23,646 --> 00:25:24,846
independent serial queues at the


967
00:25:24,846 --> 00:25:26,166
bottom, you've really asked the


968
00:25:26,166 --> 00:25:27,606
system to provide you with three


969
00:25:27,606 --> 00:25:28,646
independent concurrency


970
00:25:28,646 --> 00:25:29,236
contexts.


971
00:25:29,676 --> 00:25:30,806
If all these become active at


972
00:25:30,806 --> 00:25:32,846
once, the system will oblige and


973
00:25:32,846 --> 00:25:34,356
create three threads for you to


974
00:25:34,356 --> 00:25:35,516
execute the event handlers.


975
00:25:36,616 --> 00:25:37,616
Now, this may be what you


976
00:25:37,616 --> 00:25:39,016
wanted, and maybe exactly what


977
00:25:39,016 --> 00:25:40,856
you were after, but it is quite


978
00:25:40,856 --> 00:25:42,786
common for these event handlers


979
00:25:42,786 --> 00:25:45,236
to be small and only read some


980
00:25:45,276 --> 00:25:46,576
data from the network and in


981
00:25:46,576 --> 00:25:47,486
queue it into a common data


982
00:25:47,486 --> 00:25:47,876
structure.


983
00:25:48,696 --> 00:25:50,186
Additionally, as we saw before,


984
00:25:50,186 --> 00:25:51,106
you don't have just three


985
00:25:51,106 --> 00:25:52,326
connections, you may have many,


986
00:25:52,326 --> 00:25:53,836
many of them if you have a


987
00:25:53,836 --> 00:25:55,576
number of network connections in


988
00:25:55,676 --> 00:25:56,336
your subsystem.


989
00:25:57,096 --> 00:25:58,246
So, this can leave to a


990
00:25:58,246 --> 00:25:59,436
situation where you have this


991
00:25:59,506 --> 00:26:00,726
kind of context switching


992
00:26:00,726 --> 00:26:02,086
pattern, and excessive context


993
00:26:02,086 --> 00:26:03,006
switching that Daniel talked


994
00:26:03,006 --> 00:26:04,636
about where you execute a small


995
00:26:04,636 --> 00:26:06,196
amount of work, context switch


996
00:26:06,196 --> 00:26:07,176
to another thread and do that


997
00:26:07,176 --> 00:26:08,526
again, and again, and again.


998
00:26:09,146 --> 00:26:10,816
So, how can we improve on this


999
00:26:10,816 --> 00:26:12,466
situation in this example here?


1000
00:26:13,286 --> 00:26:15,066
We can apply the single mutual


1001
00:26:15,066 --> 00:26:16,326
exclusion context idea that we


1002
00:26:16,326 --> 00:26:18,076
just talked about by simply


1003
00:26:18,076 --> 00:26:19,426
putting in an additional serial


1004
00:26:19,426 --> 00:26:21,056
queue at the bottom and forming


1005
00:26:21,056 --> 00:26:23,436
a hierarchy, you can get a


1006
00:26:23,436 --> 00:26:24,776
single mutual exclusion context


1007
00:26:24,776 --> 00:26:26,006
for all of these connections.


1008
00:26:26,756 --> 00:26:27,906
And if they fire at the same


1009
00:26:27,906 --> 00:26:29,386
time, the same thing as before


1010
00:26:29,386 --> 00:26:30,436
will happen, the event handlers


1011
00:26:30,436 --> 00:26:31,446
will get end queued onto the


1012
00:26:31,446 --> 00:26:33,046
target queues, but because


1013
00:26:33,076 --> 00:26:33,986
there's an additional serial


1014
00:26:33,986 --> 00:26:35,526
queue at the bottom here, it's a


1015
00:26:35,526 --> 00:26:36,626
single thread that will come and


1016
00:26:36,626 --> 00:26:38,306
execute them in order instead of


1017
00:26:38,376 --> 00:26:39,516
the multiple threads that we had


1018
00:26:39,546 --> 00:26:39,966
before.


1019
00:26:41,126 --> 00:26:42,276
So, this seems like a really


1020
00:26:42,276 --> 00:26:44,236
simple change but it is exactly


1021
00:26:44,236 --> 00:26:45,816
the type of change that lead to


1022
00:26:45,816 --> 00:26:46,986
the 1.3 X performance


1023
00:26:46,986 --> 00:26:48,356
improvement in some of our own


1024
00:26:48,436 --> 00:26:50,706
code that Daniel talked about


1025
00:26:50,706 --> 00:26:52,246
earlier in the session.


1026
00:26:53,256 --> 00:26:57,436
So, this is one example of how


1027
00:26:57,436 --> 00:26:58,956
we can avoid the problematic


1028
00:26:58,956 --> 00:27:00,186
pattern of repeatedly switching


1029
00:27:00,186 --> 00:27:01,506
between independent operations.


1030
00:27:02,426 --> 00:27:03,616
But it really comes under the


1031
00:27:03,616 --> 00:27:04,836
general heading of avoiding


1032
00:27:05,146 --> 00:27:06,696
unwanted and unbounded


1033
00:27:06,696 --> 00:27:08,036
concurrency in application.


1034
00:27:09,346 --> 00:27:10,746
One way you can get that is by


1035
00:27:10,746 --> 00:27:11,916
having many queues becoming


1036
00:27:11,916 --> 00:27:12,806
active all at once.


1037
00:27:13,366 --> 00:27:15,196
And one example of this is that


1038
00:27:15,196 --> 00:27:16,366
independent requiring source


1039
00:27:16,396 --> 00:27:17,906
pattern that we just say.


1040
00:27:18,236 --> 00:27:19,196
You can also get this if you


1041
00:27:19,196 --> 00:27:20,356
have independent or object


1042
00:27:20,476 --> 00:27:21,036
queues.


1043
00:27:21,556 --> 00:27:22,806
If many objects in your


1044
00:27:22,806 --> 00:27:23,596
application have their own


1045
00:27:23,596 --> 00:27:24,866
serial queues and you put


1046
00:27:24,866 --> 00:27:26,076
asynchronous work into them at


1047
00:27:26,076 --> 00:27:27,276
the same time you can get


1048
00:27:27,276 --> 00:27:29,196
exactly the same phenomenon.


1049
00:27:31,046 --> 00:27:32,086
You can also see this if you


1050
00:27:32,086 --> 00:27:33,476
have many work items submitted


1051
00:27:33,476 --> 00:27:34,726
to the global concurrent queue


1052
00:27:34,726 --> 00:27:35,386
at the same time.


1053
00:27:36,406 --> 00:27:37,566
In particular if there's work


1054
00:27:37,566 --> 00:27:38,176
items block.


1055
00:27:38,726 --> 00:27:40,276
The way the global concurrent


1056
00:27:40,276 --> 00:27:41,496
queue works is that it corrects


1057
00:27:41,496 --> 00:27:42,966
more threads when existing


1058
00:27:42,966 --> 00:27:44,316
threads block to give you a


1059
00:27:44,316 --> 00:27:45,236
continuing good level of


1060
00:27:45,236 --> 00:27:46,566
concurrency in your application.


1061
00:27:47,026 --> 00:27:48,276
But if those threads then block


1062
00:27:48,276 --> 00:27:50,096
again, you can get something


1063
00:27:50,346 --> 00:27:51,276
that we call the thread


1064
00:27:51,276 --> 00:27:51,836
explosion.


1065
00:27:52,876 --> 00:27:53,986
This is a topic that we went


1066
00:27:53,986 --> 00:27:55,196
into some detail in the


1067
00:27:55,196 --> 00:27:56,256
"Building Responses and


1068
00:27:56,256 --> 00:27:57,186
Efficient Apps with GCD" in


1069
00:27:57,186 --> 00:27:59,146
2015.


1070
00:27:59,636 --> 00:28:01,516
So, if this sounds new to you,


1071
00:28:01,566 --> 00:28:02,506
I'd encourage you to go and


1072
00:28:02,506 --> 00:28:03,216
watch that session.


1073
00:28:04,826 --> 00:28:06,096
So, how do you choose the right


1074
00:28:06,096 --> 00:28:07,176
amount of concurrency in your


1075
00:28:07,176 --> 00:28:08,316
application to avoid these


1076
00:28:08,316 --> 00:28:09,116
problematic patterns?


1077
00:28:10,286 --> 00:28:11,606
One idea that we've recommended


1078
00:28:11,606 --> 00:28:13,556
to you in the past is to use one


1079
00:28:13,556 --> 00:28:14,536
queue for subsystem.


1080
00:28:15,816 --> 00:28:17,116
So, here back in our news


1081
00:28:17,116 --> 00:28:18,716
application, we already have one


1082
00:28:18,716 --> 00:28:19,816
queue for the user interface,


1083
00:28:19,816 --> 00:28:21,086
the main queue and we could


1084
00:28:21,086 --> 00:28:22,356
choose one queue for the


1085
00:28:22,356 --> 00:28:24,076
networking and 1 queue for the


1086
00:28:24,076 --> 00:28:25,776
database subsystem in addition.


1087
00:28:27,016 --> 00:28:28,666
But what we've learned today a


1088
00:28:28,666 --> 00:28:29,676
more general way to think of


1089
00:28:29,736 --> 00:28:31,036
this is really to use one queue


1090
00:28:31,036 --> 00:28:32,336
hierarchy per subsystem.


1091
00:28:33,656 --> 00:28:37,066
This gives you a mutual


1092
00:28:37,066 --> 00:28:37,996
exclusion context for the


1093
00:28:37,996 --> 00:28:39,806
subsystem, and you can leave the


1094
00:28:39,806 --> 00:28:41,516
rest of the queue event sub


1095
00:28:41,516 --> 00:28:42,936
structing and subsystem alone


1096
00:28:43,306 --> 00:28:45,106
and just target that network


1097
00:28:45,246 --> 00:28:47,406
queue or database queue that


1098
00:28:47,406 --> 00:28:49,486
underlies the bottom of your


1099
00:28:49,486 --> 00:28:50,406
queue hierarchies.


1100
00:28:53,296 --> 00:28:56,756
But, that may be a bit too


1101
00:28:56,756 --> 00:28:57,756
simplistic a pattern for a


1102
00:28:57,756 --> 00:28:59,226
complex application or a complex


1103
00:28:59,226 --> 00:28:59,766
subsystem.


1104
00:29:00,176 --> 00:29:01,306
The main thing that is important


1105
00:29:01,306 --> 00:29:02,476
here is to have a fixed number


1106
00:29:02,476 --> 00:29:03,526
of serial queue hierarchies in


1107
00:29:03,526 --> 00:29:04,146
your application.


1108
00:29:04,676 --> 00:29:05,706
So, it may make sense to have


1109
00:29:05,706 --> 00:29:07,236
additional queue hierarchies for


1110
00:29:07,236 --> 00:29:08,646
a complicated subsystem, say a


1111
00:29:08,646 --> 00:29:10,286
secondary one for slower work,


1112
00:29:10,286 --> 00:29:11,516
or larger work items, so that


1113
00:29:11,556 --> 00:29:13,286
the first one, the primary one


1114
00:29:13,286 --> 00:29:14,276
can keep the subsystem


1115
00:29:14,276 --> 00:29:15,796
responsive to requests coming in


1116
00:29:15,796 --> 00:29:16,366
from outside.


1117
00:29:17,396 --> 00:29:19,596
Another thing that's important


1118
00:29:19,596 --> 00:29:20,976
to think about in this context


1119
00:29:21,426 --> 00:29:23,166
is the granularity of the work


1120
00:29:23,306 --> 00:29:24,656
submitted to those subsystems.


1121
00:29:25,856 --> 00:29:26,936
You want to use fairly large


1122
00:29:26,936 --> 00:29:28,276
work items when you move between


1123
00:29:28,276 --> 00:29:30,146
subsystems to get a picture like


1124
00:29:30,146 --> 00:29:31,776
what we say earlier in the


1125
00:29:31,776 --> 00:29:34,176
session, where the CP is able to


1126
00:29:34,176 --> 00:29:35,536
execute your subsystem for long


1127
00:29:35,536 --> 00:29:37,726
enough to reach an efficient


1128
00:29:37,726 --> 00:29:38,396
performance state.


1129
00:29:39,986 --> 00:29:40,786
Once you're inside the


1130
00:29:40,786 --> 00:29:41,956
subsystem, say the networking


1131
00:29:41,956 --> 00:29:42,756
subsystem here.


1132
00:29:43,246 --> 00:29:44,736
It may make sense to subdivide


1133
00:29:44,986 --> 00:29:47,676
into smaller block items and


1134
00:29:47,676 --> 00:29:49,166
have a finer granularity to


1135
00:29:49,166 --> 00:29:50,326
improve the responsiveness of


1136
00:29:50,326 --> 00:29:51,156
that subsystem.


1137
00:29:51,606 --> 00:29:52,726
For instance, you can to that by


1138
00:29:52,726 --> 00:29:53,946
splitting up your work and


1139
00:29:53,946 --> 00:29:55,956
re-asyncing to another queue in


1140
00:29:55,956 --> 00:29:56,746
your queue hierarchy.


1141
00:29:56,746 --> 00:29:57,646
And that doesn't introduce a


1142
00:29:57,646 --> 00:29:58,686
context switch because you're


1143
00:29:58,686 --> 00:30:02,926
already in that one subsystem.


1144
00:30:03,426 --> 00:30:04,706
So, in summary, what have we


1145
00:30:04,706 --> 00:30:05,676
looked at in this section?


1146
00:30:06,956 --> 00:30:08,326
We saw how we can organize


1147
00:30:08,326 --> 00:30:09,956
queues and sources into serial


1148
00:30:09,956 --> 00:30:10,726
queue hierarchies.


1149
00:30:11,246 --> 00:30:12,946
How to use a fixed number of the


1150
00:30:12,946 --> 00:30:14,526
queue hierarchies to give GCD a


1151
00:30:14,526 --> 00:30:16,096
good granularity of concurrency.


1152
00:30:16,676 --> 00:30:18,796
And how to size your work items


1153
00:30:18,796 --> 00:30:20,406
appropriately earlier in the


1154
00:30:20,406 --> 00:30:22,176
section for parallel work and


1155
00:30:22,176 --> 00:30:24,806
here for concurrent work inside


1156
00:30:24,806 --> 00:30:26,346
the subsystem as well as between


1157
00:30:26,346 --> 00:30:26,916
subsystems.


1158
00:30:27,556 --> 00:30:29,186
And with this, I'll hand it over


1159
00:30:29,186 --> 00:30:30,866
to Pierre to dive into how we


1160
00:30:30,866 --> 00:30:32,746
have improved GCD to always


1161
00:30:32,746 --> 00:30:33,916
execute the queue hierarchy on a


1162
00:30:33,916 --> 00:30:35,216
single thread and how you can


1163
00:30:35,216 --> 00:30:36,926
modernize your code to take


1164
00:30:36,926 --> 00:30:37,686
advantage of this.


1165
00:30:38,516 --> 00:30:43,876
[ Applause ]


1166
00:30:44,376 --> 00:30:45,006
>> Thank you Daniel.


1167
00:30:46,506 --> 00:30:49,036
So, indeed we have completely


1168
00:30:49,036 --> 00:30:51,076
reinvented the internals of GCD


1169
00:30:51,076 --> 00:30:52,826
this year to eliminate some


1170
00:30:52,826 --> 00:30:54,936
unwanted context switches and


1171
00:30:54,936 --> 00:30:56,836
execute single queue hierarchies


1172
00:30:56,836 --> 00:30:58,266
like the ones that Daniel showed


1173
00:30:58,686 --> 00:30:59,456
on the single thread.


1174
00:31:00,476 --> 00:31:02,696
To do so we have created a new


1175
00:31:03,156 --> 00:31:04,476
kind of concepts that we call


1176
00:31:04,786 --> 00:31:06,626
Unified Queue Identity that let


1177
00:31:06,626 --> 00:31:07,296
us do that.


1178
00:31:07,296 --> 00:31:09,126
And we will walk you through how


1179
00:31:09,896 --> 00:31:10,716
it works.


1180
00:31:11,486 --> 00:31:16,046
So, really this part of the talk


1181
00:31:16,046 --> 00:31:17,626
will focus on a single queue


1182
00:31:17,626 --> 00:31:19,386
hierarchy, like the ones Daniel


1183
00:31:19,386 --> 00:31:19,956
showed earlier.


1184
00:31:20,516 --> 00:31:21,696
However, we'll work on


1185
00:31:21,696 --> 00:31:23,956
simplified ones with the sources


1186
00:31:23,956 --> 00:31:25,946
at the top, and your mutual


1187
00:31:25,946 --> 00:31:27,896
exclusion context at the bottom.


1188
00:31:27,896 --> 00:31:28,976
The internal GCD notes are not


1189
00:31:28,976 --> 00:31:31,966
quite given for that part of the


1190
00:31:33,236 --> 00:31:33,346
talk.


1191
00:31:33,676 --> 00:31:35,436
So, when you create an


1192
00:31:35,436 --> 00:31:36,196
[inaudible] context you use to


1193
00:31:36,196 --> 00:31:38,876
dispatch queue constructor, that


1194
00:31:38,906 --> 00:31:40,266
creates just a piece of memory


1195
00:31:40,266 --> 00:31:41,426
in your application that is a


1196
00:31:41,426 --> 00:31:41,706
note.


1197
00:31:42,306 --> 00:31:43,476
And one of the first things that


1198
00:31:43,476 --> 00:31:45,786
you may do is to dispatch recent


1199
00:31:45,786 --> 00:31:47,636
coded items to it.


1200
00:31:48,146 --> 00:31:49,496
So, you will have code in your


1201
00:31:49,496 --> 00:31:51,946
application that will here and


1202
00:31:51,946 --> 00:31:53,946
queue a [inaudible] on the


1203
00:31:53,946 --> 00:31:56,976
queue, when that happened before


1204
00:31:56,976 --> 00:31:58,836
we used to request a thread


1205
00:31:58,836 --> 00:32:01,486
anonymously to the system.


1206
00:32:01,666 --> 00:32:03,846
And the resolution of what that


1207
00:32:03,846 --> 00:32:05,686
was meant to do happens late


1208
00:32:05,776 --> 00:32:06,686
inside your application.


1209
00:32:08,246 --> 00:32:09,996
In this case, we change that and


1210
00:32:09,996 --> 00:32:11,836
what we do is that we create our


1211
00:32:11,876 --> 00:32:13,676
counter object, the Unified


1212
00:32:13,676 --> 00:32:15,536
Queue Identity that is tied to


1213
00:32:15,536 --> 00:32:17,716
your queue and is exactly meant


1214
00:32:17,716 --> 00:32:18,806
to represent your queue in the


1215
00:32:18,806 --> 00:32:19,156
kernel.


1216
00:32:20,076 --> 00:32:21,536
We can tie that object with the


1217
00:32:21,536 --> 00:32:23,026
required hierarchy to execute to


1218
00:32:23,026 --> 00:32:24,676
work, which here is backup.


1219
00:32:25,276 --> 00:32:27,276
And that causes the system to


1220
00:32:27,276 --> 00:32:28,036
ask for a thread.


1221
00:32:28,856 --> 00:32:31,066
The thread request, that dotted


1222
00:32:31,066 --> 00:32:33,656
line on the slide, may not be


1223
00:32:33,656 --> 00:32:35,886
fulfilled for some time, because


1224
00:32:35,886 --> 00:32:36,996
here that's a background thread,


1225
00:32:37,236 --> 00:32:38,506
and maybe the system is loaded


1226
00:32:38,506 --> 00:32:40,046
enough that it's not even worth


1227
00:32:40,046 --> 00:32:44,606
giving you a thread for it.


1228
00:32:44,606 --> 00:32:45,926
Later on, some other path of


1229
00:32:46,086 --> 00:32:49,176
your application may actually


1230
00:32:49,176 --> 00:32:50,726
try to en queue more work.


1231
00:32:50,726 --> 00:32:51,216
Here a UT [inaudible] that is


1232
00:32:51,216 --> 00:32:53,526
slightly higher priority.


1233
00:32:53,996 --> 00:32:55,956
We can use the queue identity,


1234
00:32:56,086 --> 00:32:57,496
the unified identity in the


1235
00:32:57,496 --> 00:32:58,586
catalog to look and solve the


1236
00:32:58,586 --> 00:32:59,866
priority inversion, and elevate


1237
00:32:59,866 --> 00:33:00,916
the priority of that thread


1238
00:33:00,916 --> 00:33:01,426
request.


1239
00:33:01,906 --> 00:33:03,196
It may be that is the small


1240
00:33:03,196 --> 00:33:04,846
nudge that the system needed to


1241
00:33:04,846 --> 00:33:06,206
actually give you a thread here


1242
00:33:06,206 --> 00:33:07,736
to execute your work.


1243
00:33:08,016 --> 00:33:09,116
But this thread is in the


1244
00:33:09,116 --> 00:33:10,406
scheduler queues not yet on


1245
00:33:10,406 --> 00:33:10,626
call.


1246
00:33:10,626 --> 00:33:11,356
Not executing.


1247
00:33:11,946 --> 00:33:13,326
And the reason why is because


1248
00:33:13,326 --> 00:33:14,456
there is another thread in your


1249
00:33:14,456 --> 00:33:15,676
application that is interacting


1250
00:33:15,676 --> 00:33:16,746
with a queue and working


1251
00:33:16,746 --> 00:33:18,046
synchronously at a higher


1252
00:33:18,046 --> 00:33:19,756
priority, even, usually shaded.


1253
00:33:20,286 --> 00:33:23,056
Now that we have that Unified


1254
00:33:23,056 --> 00:33:24,486
Queue Identity, we can actually


1255
00:33:24,736 --> 00:33:26,346
since that thread has to block


1256
00:33:26,566 --> 00:33:27,826
to en queue the placeholder that


1257
00:33:27,826 --> 00:33:29,716
Daniel told you about a bit


1258
00:33:29,716 --> 00:33:31,956
earlier, we can block the


1259
00:33:31,956 --> 00:33:33,726
synchronous execution of that


1260
00:33:33,836 --> 00:33:35,196
thread on the Unified Queue


1261
00:33:35,196 --> 00:33:35,676
Identity.


1262
00:33:35,676 --> 00:33:36,816
The same on that we use for


1263
00:33:36,816 --> 00:33:38,226
asynchronous work, [inaudible].


1264
00:33:38,226 --> 00:33:41,046
But now that we unified the


1265
00:33:41,046 --> 00:33:43,056
asynchronous and the synchronous


1266
00:33:43,266 --> 00:33:44,786
part of the queue in a single


1267
00:33:44,786 --> 00:33:46,386
identity, we can apply an


1268
00:33:46,866 --> 00:33:48,566
optimization and delicately


1269
00:33:48,566 --> 00:33:49,686
switch the thread that's


1270
00:33:49,686 --> 00:33:51,266
blocking you by passing the


1271
00:33:51,266 --> 00:33:52,536
scheduler queue and registering


1272
00:33:52,536 --> 00:33:54,246
the queue delays that Daniel


1273
00:33:54,406 --> 00:33:55,936
introduced while talking about


1274
00:33:55,936 --> 00:33:56,896
the scheduler very early.


1275
00:33:58,016 --> 00:33:59,826
So, that's how the unified queue


1276
00:33:59,826 --> 00:34:00,986
identity is used for for


1277
00:34:00,986 --> 00:34:02,226
asynchronous and synchronous


1278
00:34:02,226 --> 00:34:02,846
work items.


1279
00:34:05,396 --> 00:34:07,256
Now, how did we use that for


1280
00:34:07,256 --> 00:34:07,816
events?


1281
00:34:08,016 --> 00:34:08,906
Why is it useful?


1282
00:34:09,846 --> 00:34:11,606
So, that is the small tree that


1283
00:34:11,606 --> 00:34:13,766
we've been using so far, let's


1284
00:34:13,766 --> 00:34:15,505
look at the creation of these


1285
00:34:15,505 --> 00:34:16,065
sources.


1286
00:34:17,356 --> 00:34:18,516
When you create the source using


1287
00:34:18,516 --> 00:34:20,716
the makeResource factory button,


1288
00:34:20,716 --> 00:34:22,476
you set a bunch of events, of


1289
00:34:22,476 --> 00:34:23,396
favorite handlers and


1290
00:34:23,396 --> 00:34:24,036
properties.


1291
00:34:24,516 --> 00:34:25,536
But what is really interesting


1292
00:34:25,536 --> 00:34:26,576
is what happens when you


1293
00:34:26,576 --> 00:34:28,386
activating the object.


1294
00:34:29,335 --> 00:34:31,576
This is actually at that moment,


1295
00:34:31,966 --> 00:34:33,755
that we will notice that utility


1296
00:34:33,886 --> 00:34:35,545
are QOS, at which the handler


1297
00:34:35,545 --> 00:34:36,716
for your source will always


1298
00:34:36,716 --> 00:34:37,266
execute.


1299
00:34:37,496 --> 00:34:38,806
Because it's inherited from your


1300
00:34:38,806 --> 00:34:39,406
queue hierarchy.


1301
00:34:39,716 --> 00:34:42,096
We will also know now, with the


1302
00:34:42,096 --> 00:34:44,795
new system, that the handler


1303
00:34:44,795 --> 00:34:46,456
will eventually execute that in


1304
00:34:46,576 --> 00:34:49,396
queue execution mature exclusion


1305
00:34:49,396 --> 00:34:49,936
context.


1306
00:34:50,815 --> 00:34:52,996
And will now register the source


1307
00:34:52,996 --> 00:34:55,156
at front with the sync unified


1308
00:34:55,156 --> 00:34:57,136
identity that I just talked


1309
00:34:57,136 --> 00:34:59,296
about a bit earlier.


1310
00:34:59,606 --> 00:35:01,796
If we look at the higher UI QOS


1311
00:35:02,016 --> 00:35:04,506
source that we have on the tree,


1312
00:35:05,146 --> 00:35:06,696
the way we are treated is very


1313
00:35:06,696 --> 00:35:09,276
similar of the first one, except


1314
00:35:09,276 --> 00:35:10,526
that when you're setting the


1315
00:35:10,586 --> 00:35:12,746
handler here you're specifying


1316
00:35:12,746 --> 00:35:14,406
the QOS that you actually want.


1317
00:35:15,366 --> 00:35:16,576
And again, what's interesting is


1318
00:35:16,576 --> 00:35:18,126
what happens at activation.


1319
00:35:18,126 --> 00:35:19,886
That is when we the snapshot and


1320
00:35:19,886 --> 00:35:21,936
like before when we got the


1321
00:35:21,936 --> 00:35:24,146
utility QOS from your hierarchy,


1322
00:35:24,436 --> 00:35:25,876
here we get it from your hint.


1323
00:35:26,796 --> 00:35:27,946
We still recall the fact that


1324
00:35:27,986 --> 00:35:29,606
they will execute both the


1325
00:35:29,606 --> 00:35:31,296
sources on the same execution


1326
00:35:31,296 --> 00:35:31,836
context.


1327
00:35:31,836 --> 00:35:33,376
And will register that second


1328
00:35:33,376 --> 00:35:34,486
source up front again, which


1329
00:35:34,706 --> 00:35:36,506
with some unified identity in


1330
00:35:37,096 --> 00:35:39,366
the kernel.


1331
00:35:39,506 --> 00:35:41,746
So, really what we're trying to


1332
00:35:41,746 --> 00:35:43,866
solve with that quite complex


1333
00:35:43,956 --> 00:35:46,446
identity is a problem that we


1334
00:35:46,446 --> 00:35:48,136
had in previous phases of the


1335
00:35:48,206 --> 00:35:51,316
OS, where related operations


1336
00:35:51,316 --> 00:35:52,846
would actually bounce off the


1337
00:35:52,846 --> 00:35:53,466
old threads.


1338
00:35:53,606 --> 00:35:54,766
Let's look at how it used to


1339
00:35:54,766 --> 00:35:55,116
work.


1340
00:35:56,086 --> 00:35:58,436
So, remember that's our queue


1341
00:35:58,436 --> 00:36:01,326
hierarchy, and let's bring up


1342
00:36:01,406 --> 00:36:02,536
the timeline that you've seen a


1343
00:36:02,536 --> 00:36:05,146
bunch of times now in our talk.


1344
00:36:05,276 --> 00:36:07,346
At the top, the CPU, but now


1345
00:36:07,346 --> 00:36:08,896
there is a new tack, the


1346
00:36:08,896 --> 00:36:10,756
exclusion queue card that will


1347
00:36:10,756 --> 00:36:12,586
show you what is executing at


1348
00:36:12,586 --> 00:36:15,696
any given moment on that queue.


1349
00:36:15,926 --> 00:36:17,506
So, that's really how the


1350
00:36:17,556 --> 00:36:19,366
runtime used to work before this


1351
00:36:19,366 --> 00:36:22,036
phase in macOS Sierra and iOS


1352
00:36:22,036 --> 00:36:22,306
10.


1353
00:36:22,616 --> 00:36:24,616
So, let's look at what happens


1354
00:36:24,616 --> 00:36:25,966
if the first source fails.


1355
00:36:26,626 --> 00:36:28,286
Before, like I said thread


1356
00:36:28,286 --> 00:36:29,396
requests were anonymous.


1357
00:36:29,396 --> 00:36:30,646
We would ask for an anonymous


1358
00:36:30,646 --> 00:36:31,846
thread, deliver the event on the


1359
00:36:31,846 --> 00:36:33,656
thread and then we would look at


1360
00:36:33,656 --> 00:36:33,976
the event.


1361
00:36:34,856 --> 00:36:36,706
And when we look at the event


1362
00:36:36,706 --> 00:36:38,436
inside your application, that is


1363
00:36:38,436 --> 00:36:39,996
only then that we realize that


1364
00:36:39,996 --> 00:36:41,966
this event is meant to run on a


1365
00:36:41,966 --> 00:36:42,206
queue.


1366
00:36:42,206 --> 00:36:43,876
We would then queue the event


1367
00:36:43,876 --> 00:36:44,276
handler.


1368
00:36:44,416 --> 00:36:46,926
But since the queue is


1369
00:36:46,926 --> 00:36:48,156
unclaimed, the thread could


1370
00:36:48,156 --> 00:36:50,856
actually become that queue and


1371
00:36:50,856 --> 00:36:52,356
start executing given handler


1372
00:36:52,766 --> 00:36:53,596
for your source.


1373
00:36:54,306 --> 00:36:54,886
And we do so.


1374
00:36:55,486 --> 00:36:56,926
Now, the interesting thing is


1375
00:36:56,926 --> 00:36:58,236
what happens when the second


1376
00:36:58,236 --> 00:36:59,346
source that is higher priority


1377
00:36:59,346 --> 00:36:59,896
fires?


1378
00:37:00,716 --> 00:37:01,626
The same actually.


1379
00:37:02,116 --> 00:37:03,726
Since it's a hierarchy QOS here,


1380
00:37:03,906 --> 00:37:05,146
higher priority that's what


1381
00:37:05,146 --> 00:37:06,216
you're executing right now.


1382
00:37:07,426 --> 00:37:08,566
We would bring up a new


1383
00:37:08,626 --> 00:37:10,076
anonymous thread deliver that


1384
00:37:10,136 --> 00:37:11,256
higher priority event on the


1385
00:37:11,306 --> 00:37:11,566
thread.


1386
00:37:13,196 --> 00:37:14,416
And look at what that event


1387
00:37:14,416 --> 00:37:14,886
means.


1388
00:37:15,036 --> 00:37:16,136
And we will notice that it is


1389
00:37:16,136 --> 00:37:17,096
for exactly the same queue


1390
00:37:17,096 --> 00:37:17,966
hierarchy only then.


1391
00:37:18,036 --> 00:37:19,676
And then queue the handler after


1392
00:37:19,676 --> 00:37:20,756
the one we just pre emptied.


1393
00:37:21,596 --> 00:37:23,186
As you see, we closed our first


1394
00:37:23,606 --> 00:37:24,176
context switch.


1395
00:37:24,716 --> 00:37:26,176
It was of that higher priority


1396
00:37:26,176 --> 00:37:26,556
event.


1397
00:37:27,136 --> 00:37:28,616
But, we cannot make for what


1398
00:37:28,616 --> 00:37:29,636
progress, because unlike the


1399
00:37:29,636 --> 00:37:30,816
first time, that second thread


1400
00:37:30,816 --> 00:37:32,166
cannot take over the queue it is


1401
00:37:32,166 --> 00:37:33,156
already associated with a


1402
00:37:33,156 --> 00:37:33,676
thread.


1403
00:37:33,946 --> 00:37:34,906
We cannot take it over.


1404
00:37:35,476 --> 00:37:36,286
So, the thread is done.


1405
00:37:36,526 --> 00:37:38,126
Which as Daniel explained one


1406
00:37:38,126 --> 00:37:39,826
reason why you context switch


1407
00:37:39,826 --> 00:37:40,096
again.


1408
00:37:40,766 --> 00:37:41,676
And that's what we do, we


1409
00:37:41,676 --> 00:37:42,966
context switch back to the first


1410
00:37:42,966 --> 00:37:43,946
thread that is the one that can


1411
00:37:43,946 --> 00:37:44,886
actually make progress.


1412
00:37:44,886 --> 00:37:46,836
We execute the rest of the first


1413
00:37:46,836 --> 00:37:48,276
handle and finally move to the


1414
00:37:48,276 --> 00:37:48,696
second one.


1415
00:37:50,066 --> 00:37:52,446
So, as you can see, we use two


1416
00:37:52,536 --> 00:37:54,216
threads and two context switches


1417
00:37:54,286 --> 00:37:55,646
that you really didn't want for


1418
00:37:55,646 --> 00:37:56,956
a single execution context.


1419
00:37:57,476 --> 00:38:00,996
We fixed that using Unified


1420
00:38:01,036 --> 00:38:02,996
Identity in macOS High Sierra


1421
00:38:03,066 --> 00:38:04,686
and iOS 11.


1422
00:38:05,826 --> 00:38:09,266
We got rid of that thread.


1423
00:38:10,156 --> 00:38:12,566
And we also, of course got rid


1424
00:38:12,566 --> 00:38:14,846
of the two context switches that


1425
00:38:14,846 --> 00:38:16,416
we had, that were unwanted.


1426
00:38:17,396 --> 00:38:18,716
And of course, its important


1427
00:38:18,716 --> 00:38:20,726
because unlike what happened


1428
00:38:20,776 --> 00:38:22,106
when Daniel showed you the


1429
00:38:22,106 --> 00:38:24,916
pre-emption with that UI touch


1430
00:38:24,916 --> 00:38:26,996
event, where we could take


1431
00:38:26,996 --> 00:38:28,646
advantage of the fact that we


1432
00:38:28,756 --> 00:38:30,596
actually had two threads that


1433
00:38:30,596 --> 00:38:32,186
were independent to be more


1434
00:38:32,186 --> 00:38:33,266
responsive for application.


1435
00:38:33,626 --> 00:38:35,276
Here, we didn't benefit from any


1436
00:38:35,276 --> 00:38:36,276
of these context switches,


1437
00:38:36,566 --> 00:38:38,206
because these event handler, S1


1438
00:38:38,206 --> 00:38:40,586
and M2 had to execute in order


1439
00:38:40,586 --> 00:38:41,126
anyways.


1440
00:38:41,126 --> 00:38:42,506
So, knowing about that event


1441
00:38:42,506 --> 00:38:43,546
early was not useful.


1442
00:38:43,546 --> 00:38:46,046
And if you look at how this


1443
00:38:46,046 --> 00:38:48,006
actually, what the flow is


1444
00:38:48,006 --> 00:38:49,916
today, it looks more like this.


1445
00:38:50,816 --> 00:38:53,026
What happened here?


1446
00:38:54,936 --> 00:38:56,926
The most important thing on that


1447
00:38:56,926 --> 00:38:58,346
flow is that now if you look at


1448
00:38:58,346 --> 00:39:00,106
the thread, it's called EQ,


1449
00:39:00,386 --> 00:39:01,516
because that's the part of the


1450
00:39:01,516 --> 00:39:03,206
unified identity, the thread and


1451
00:39:03,206 --> 00:39:04,636
the EQ are basically the same


1452
00:39:04,636 --> 00:39:05,096
object.


1453
00:39:05,386 --> 00:39:06,696
And the kernel knows that it's


1454
00:39:06,696 --> 00:39:08,476
really executing a queue, which


1455
00:39:08,476 --> 00:39:09,976
is reflected on the CPU tab, you


1456
00:39:09,976 --> 00:39:11,186
don't see the events anymore,


1457
00:39:11,186 --> 00:39:12,286
it's just running your queue.


1458
00:39:14,476 --> 00:39:16,256
However, you might ask, how did


1459
00:39:16,256 --> 00:39:19,206
we manage to deliver the event,


1460
00:39:19,456 --> 00:39:20,896
that second event without


1461
00:39:20,896 --> 00:39:21,726
requiring a hamper.


1462
00:39:21,846 --> 00:39:23,566
That is actually a good


1463
00:39:24,696 --> 00:39:24,916
question.


1464
00:39:25,446 --> 00:39:27,266
When the event fires, now we


1465
00:39:27,266 --> 00:39:28,756
know where it will execute,


1466
00:39:28,756 --> 00:39:29,936
where you will handle it.


1467
00:39:30,326 --> 00:39:31,686
We just mark the thread.


1468
00:39:32,486 --> 00:39:33,416
No helper needed.


1469
00:39:34,226 --> 00:39:38,266
And at the first possible time,


1470
00:39:38,986 --> 00:39:40,786
we will notice that thread was


1471
00:39:40,786 --> 00:39:42,006
marked with you have pending


1472
00:39:42,006 --> 00:39:42,486
events.


1473
00:39:42,966 --> 00:39:44,356
And when we de-queue the events,


1474
00:39:44,356 --> 00:39:46,596
one needs to hide time, hide


1475
00:39:46,596 --> 00:39:47,556
after the first handler


1476
00:39:47,556 --> 00:39:48,116
finishes.


1477
00:39:48,696 --> 00:39:50,066
We can grab the events from the


1478
00:39:50,066 --> 00:39:51,566
kernel, look at them, and then


1479
00:39:51,606 --> 00:39:53,346
queue their handlers on your


1480
00:39:54,006 --> 00:39:54,246
hierarchy.


1481
00:39:54,796 --> 00:39:57,726
So, why did we go through that


1482
00:39:57,726 --> 00:39:59,016
quite complex explanation?


1483
00:40:00,046 --> 00:40:01,856
That's so that you can


1484
00:40:01,856 --> 00:40:03,146
understand how to best take


1485
00:40:03,146 --> 00:40:04,676
advantage of the runtime


1486
00:40:04,676 --> 00:40:05,126
behavior.


1487
00:40:06,086 --> 00:40:07,366
Because clearly, the runtime


1488
00:40:07,366 --> 00:40:08,986
uses every possible hint you're


1489
00:40:08,986 --> 00:40:10,996
giving us to optimize behavior


1490
00:40:10,996 --> 00:40:11,756
in your application.


1491
00:40:12,766 --> 00:40:14,716
And admittance buttons to know


1492
00:40:14,716 --> 00:40:17,356
how to hint and when to hint the


1493
00:40:17,356 --> 00:40:18,866
runtime correctly so that we


1494
00:40:18,866 --> 00:40:19,886
make the right decisions.


1495
00:40:20,386 --> 00:40:24,196
Which brings me to what should


1496
00:40:24,196 --> 00:40:26,226
you do to existing code bases to


1497
00:40:26,226 --> 00:40:28,896
take advantage of all that core


1498
00:40:28,896 --> 00:40:32,966
technology that we rebuilt.


1499
00:40:33,156 --> 00:40:35,966
Now, actually two steps to


1500
00:40:35,966 --> 00:40:37,156
follow to take the full


1501
00:40:37,156 --> 00:40:38,706
advantage of that technology.


1502
00:40:38,936 --> 00:40:41,116
The first one is no mutation


1503
00:40:41,116 --> 00:40:41,966
after activation.


1504
00:40:42,666 --> 00:40:44,036
And the second one is paying


1505
00:40:44,276 --> 00:40:46,796
extra care with extra attention


1506
00:40:46,796 --> 00:40:47,566
to your target queue


1507
00:40:47,566 --> 00:40:48,126
hierarchies.


1508
00:40:49,126 --> 00:40:49,856
So, what does that mean?


1509
00:40:51,236 --> 00:40:52,576
No mutation past activation


1510
00:40:52,656 --> 00:40:53,806
really means that when you have


1511
00:40:53,806 --> 00:40:55,396
any kind of property on a


1512
00:40:55,396 --> 00:40:57,216
dispatch object, you can send


1513
00:40:57,216 --> 00:40:58,446
them, well as soon as you


1514
00:40:58,446 --> 00:40:59,816
activate the object, you should


1515
00:40:59,816 --> 00:41:01,056
stop mutating them.


1516
00:41:01,636 --> 00:41:04,786
The second example, that's our


1517
00:41:04,786 --> 00:41:05,976
source that we've seen quite a


1518
00:41:05,976 --> 00:41:07,466
few times already in the talk.


1519
00:41:07,956 --> 00:41:08,636
That [inaudible] for the


1520
00:41:08,636 --> 00:41:09,836
ability.


1521
00:41:09,836 --> 00:41:12,686
And you're setting a bunch of


1522
00:41:12,686 --> 00:41:14,606
properties, handlers; the event


1523
00:41:14,606 --> 00:41:15,806
handler, the consent handler.


1524
00:41:15,806 --> 00:41:16,826
You may have registration


1525
00:41:16,826 --> 00:41:17,406
handlers.


1526
00:41:18,196 --> 00:41:19,536
You can even change them a few


1527
00:41:19,536 --> 00:41:20,826
times, that's fine, you can


1528
00:41:20,826 --> 00:41:21,496
change your mind.


1529
00:41:21,946 --> 00:41:23,956
And then you activate the


1530
00:41:23,956 --> 00:41:24,346
source.


1531
00:41:25,706 --> 00:41:27,496
The contact here is that you


1532
00:41:27,496 --> 00:41:29,556
should stop mutate your objects.


1533
00:41:30,246 --> 00:41:31,376
It's very tempting to,


1534
00:41:31,706 --> 00:41:33,076
after-the-fact, for example


1535
00:41:33,076 --> 00:41:33,976
change the target queue of your


1536
00:41:33,976 --> 00:41:34,386
source.


1537
00:41:34,386 --> 00:41:35,486
That will cause problems.


1538
00:41:35,786 --> 00:41:37,356
And the reason why is exactly


1539
00:41:37,356 --> 00:41:38,956
what I showed a bit earlier, at


1540
00:41:38,956 --> 00:41:40,876
activate time we take a snapshot


1541
00:41:40,916 --> 00:41:42,006
of the properties of your


1542
00:41:42,006 --> 00:41:43,306
objects, and we will take


1543
00:41:43,306 --> 00:41:44,906
decisions in the future based on


1544
00:41:44,906 --> 00:41:45,556
that snapshot.


1545
00:41:45,686 --> 00:41:48,486
And if you change the target


1546
00:41:48,486 --> 00:41:50,196
queue hierarchy after-the-fact,


1547
00:41:50,476 --> 00:41:51,756
it will hinder that snapshot


1548
00:41:51,756 --> 00:41:54,256
stale and that will defeat a


1549
00:41:54,256 --> 00:41:55,536
bunch of very important


1550
00:41:55,536 --> 00:41:56,846
optimization such as the


1551
00:41:56,846 --> 00:41:59,726
priority inversion avoidance


1552
00:41:59,726 --> 00:42:01,096
[inaudible] the direct handoff


1553
00:42:01,186 --> 00:42:02,306
that we have for the dispatch


1554
00:42:02,306 --> 00:42:03,686
sync that I presented earlier,


1555
00:42:03,686 --> 00:42:05,756
are all defensive and


1556
00:42:05,756 --> 00:42:07,056
deliverable optimizations that


1557
00:42:07,056 --> 00:42:09,976
we just went through.


1558
00:42:10,896 --> 00:42:12,586
And I insist on the points that


1559
00:42:12,586 --> 00:42:13,966
Daniel made early on, which is


1560
00:42:13,966 --> 00:42:16,476
that many of you probably never


1561
00:42:16,886 --> 00:42:18,476
had to create a dispatch source


1562
00:42:18,476 --> 00:42:19,196
in your application.


1563
00:42:19,196 --> 00:42:20,576
And this is fine, this is really


1564
00:42:20,576 --> 00:42:21,396
how it's supposed to work.


1565
00:42:22,236 --> 00:42:23,846
You probably actually use them a


1566
00:42:23,846 --> 00:42:25,906
lot of them through system


1567
00:42:25,906 --> 00:42:26,476
frameworks.


1568
00:42:26,476 --> 00:42:27,286
It's a shame you have a


1569
00:42:27,286 --> 00:42:28,696
framework that you have to then


1570
00:42:28,696 --> 00:42:30,666
dispatch queue to because they


1571
00:42:30,666 --> 00:42:33,166
are asyncing some notifications


1572
00:42:33,166 --> 00:42:34,606
on the queue on your behalf.


1573
00:42:34,706 --> 00:42:36,106
Behind the scenes, they have one


1574
00:42:36,106 --> 00:42:36,886
of these sources.


1575
00:42:37,216 --> 00:42:38,056
So, if you're changing the


1576
00:42:38,056 --> 00:42:39,766
assumptions of the system, you


1577
00:42:39,766 --> 00:42:42,106
will actually break all of these


1578
00:42:42,106 --> 00:42:45,666
optimizations as well.


1579
00:42:46,546 --> 00:42:48,806
So, I hope a made a point really


1580
00:42:48,806 --> 00:42:50,176
clear that to target your


1581
00:42:50,176 --> 00:42:52,396
hierarchy is essential and you


1582
00:42:52,396 --> 00:42:53,266
have to protect it.


1583
00:42:54,656 --> 00:42:55,226
What does that mean?


1584
00:42:56,466 --> 00:42:57,256
And how to do that?


1585
00:42:58,756 --> 00:43:00,156
The first way, which is a very


1586
00:43:00,156 --> 00:43:01,386
simple device, is that when


1587
00:43:01,386 --> 00:43:03,796
you're building one, start from


1588
00:43:03,796 --> 00:43:05,876
the bottom and build it toward


1589
00:43:06,496 --> 00:43:07,306
the top.


1590
00:43:07,496 --> 00:43:09,016
When you show that card from the


1591
00:43:09,016 --> 00:43:11,706
slide build up, as you see,


1592
00:43:12,086 --> 00:43:13,906
these wider holes there, they


1593
00:43:13,906 --> 00:43:14,846
are your target queue


1594
00:43:14,846 --> 00:43:15,516
relationships.


1595
00:43:15,766 --> 00:43:18,146
None of them have to be mutated


1596
00:43:18,146 --> 00:43:19,886
if you [inaudible] in that


1597
00:43:20,566 --> 00:43:20,956
order.


1598
00:43:21,036 --> 00:43:22,656
However, when you have a large


1599
00:43:22,656 --> 00:43:24,156
application, or you're hiding


1600
00:43:24,156 --> 00:43:25,486
your frameworks and you're


1601
00:43:25,486 --> 00:43:26,996
bending one of these queues to


1602
00:43:26,996 --> 00:43:30,056
another part of your engineering


1603
00:43:30,166 --> 00:43:32,406
company, you may want to have


1604
00:43:32,406 --> 00:43:33,706
stronger guarantees than that.


1605
00:43:34,256 --> 00:43:35,516
You may want to lockdown these


1606
00:43:35,516 --> 00:43:36,756
relationships, so that really no


1607
00:43:36,756 --> 00:43:37,726
one can mutate them


1608
00:43:37,726 --> 00:43:39,026
after-the-fact.


1609
00:43:39,576 --> 00:43:41,846
This is actually something that


1610
00:43:41,846 --> 00:43:43,306
you can do with the technology


1611
00:43:43,306 --> 00:43:44,746
that we call set a queue


1612
00:43:44,746 --> 00:43:45,256
hierarchy.


1613
00:43:45,416 --> 00:43:47,236
We introduced it last year, and


1614
00:43:47,736 --> 00:43:49,456
actually if you are using Swift


1615
00:43:49,456 --> 00:43:50,876
3, then you can stop listening


1616
00:43:50,876 --> 00:43:51,976
to me, because you're already in


1617
00:43:51,976 --> 00:43:53,146
that form and that the only


1618
00:43:53,146 --> 00:43:54,616
world you're living.


1619
00:43:55,606 --> 00:43:57,066
However, if you have an existing


1620
00:43:57,066 --> 00:43:58,186
cloud based, or you use older


1621
00:43:58,186 --> 00:44:01,056
versions than of Swift, you need


1622
00:44:01,056 --> 00:44:03,076
to do some extra steps.


1623
00:44:03,736 --> 00:44:06,756
So, let's focus on the


1624
00:44:06,756 --> 00:44:08,626
relationship between Q1 and EQ


1625
00:44:08,896 --> 00:44:09,046
here.


1626
00:44:09,776 --> 00:44:12,266
When you created that with


1627
00:44:12,266 --> 00:44:13,886
Objective-C you probably hold


1628
00:44:13,886 --> 00:44:15,356
code that looks like this.


1629
00:44:15,436 --> 00:44:17,346
You create your queue and then


1630
00:44:17,406 --> 00:44:19,216
in the second step, you will set


1631
00:44:19,406 --> 00:44:21,816
your target queue of Q1 to EQ.


1632
00:44:22,156 --> 00:44:23,916
That is not protecting your


1633
00:44:23,916 --> 00:44:24,566
queue hierarchy.


1634
00:44:24,566 --> 00:44:26,416
Anyone can come along and call


1635
00:44:26,416 --> 00:44:27,666
dispatch target queue again and


1636
00:44:27,666 --> 00:44:28,826
break all your assumptions.


1637
00:44:29,056 --> 00:44:30,126
That's not totally great.


1638
00:44:30,906 --> 00:44:32,986
There is a simple step to just


1639
00:44:32,986 --> 00:44:34,316
fix that code into a way that is


1640
00:44:34,316 --> 00:44:37,206
safe, which is to adopt a new


1641
00:44:37,206 --> 00:44:38,536
API we introduced last year,


1642
00:44:38,736 --> 00:44:39,966
which is dispatch queue create


1643
00:44:39,966 --> 00:44:42,636
with target, which in a single


1644
00:44:42,636 --> 00:44:44,476
automatic step will create the


1645
00:44:44,476 --> 00:44:46,606
queue, set the queue hierarchy


1646
00:44:46,606 --> 00:44:48,116
height, and protect it.


1647
00:44:48,676 --> 00:44:50,066
And that's it.


1648
00:44:50,536 --> 00:44:51,706
These were the two steps to


1649
00:44:51,706 --> 00:44:53,526
follow for you to really work


1650
00:44:53,526 --> 00:44:54,386
with the [inaudible] well.


1651
00:44:54,386 --> 00:44:58,306
Other, a bit like the mutated


1652
00:44:58,306 --> 00:45:00,296
case that Daniel walked you


1653
00:45:00,296 --> 00:45:02,566
through early on, finding when


1654
00:45:02,566 --> 00:45:04,316
you're doing one of these things


1655
00:45:04,316 --> 00:45:05,836
wrong is fairly challenging,


1656
00:45:05,836 --> 00:45:07,226
especially on the large cloud


1657
00:45:07,226 --> 00:45:07,646
base.


1658
00:45:07,646 --> 00:45:09,256
Finding that in an existing


1659
00:45:09,256 --> 00:45:10,826
cloud base full code inspection


1660
00:45:10,886 --> 00:45:11,436
is hard.


1661
00:45:12,356 --> 00:45:14,736
This is why we created a new GCD


1662
00:45:14,736 --> 00:45:16,626
performance instruments tool to


1663
00:45:16,626 --> 00:45:18,346
find problem spots in an


1664
00:45:18,346 --> 00:45:19,406
existing application.


1665
00:45:19,976 --> 00:45:21,096
And I will call Daniel back to


1666
00:45:21,096 --> 00:45:23,806
the stage to demo for you.


1667
00:45:24,516 --> 00:45:28,866
[ Applause ]


1668
00:45:29,366 --> 00:45:29,896
>> Thank you, Pierre.


1669
00:45:30,426 --> 00:45:32,766
All right to start out with


1670
00:45:32,766 --> 00:45:34,166
please note that this GCD


1671
00:45:34,166 --> 00:45:35,216
performance instrument that


1672
00:45:35,216 --> 00:45:37,156
we'll see is not yet present in


1673
00:45:37,156 --> 00:45:38,396
the version of XCode 9 that you


1674
00:45:38,396 --> 00:45:39,746
have, but it will be available


1675
00:45:39,746 --> 00:45:41,996
in an upcoming seed of XCode 9.


1676
00:45:41,996 --> 00:45:44,566
So, for this demo, let's analyze


1677
00:45:44,596 --> 00:45:46,176
the execution of our sample news


1678
00:45:46,176 --> 00:45:47,486
application in some detail.


1679
00:45:48,596 --> 00:45:49,566
So, what happens here if you


1680
00:45:49,566 --> 00:45:51,146
click this connect button at the


1681
00:45:51,146 --> 00:45:53,046
bottom, is that this app creates


1682
00:45:53,046 --> 00:45:54,376
a number of network connections


1683
00:45:54,486 --> 00:45:56,336
to a server, to read lists of


1684
00:45:56,336 --> 00:45:57,736
URLs from, which are then


1685
00:45:57,736 --> 00:45:59,196
displayed in the WebViews


1686
00:45:59,196 --> 00:46:00,246
whenever the refresh button is


1687
00:46:00,246 --> 00:46:00,386
hit.


1688
00:46:01,146 --> 00:46:02,776
So, let's jump into XCode to see


1689
00:46:02,776 --> 00:46:03,866
how we are setting up those


1690
00:46:03,866 --> 00:46:04,576
network connections.


1691
00:46:05,446 --> 00:46:07,686
So, here we are in XCode in the


1692
00:46:07,686 --> 00:46:09,806
create connections method, which


1693
00:46:09,806 --> 00:46:10,956
does just that.


1694
00:46:11,376 --> 00:46:12,006
It's very simple.


1695
00:46:12,006 --> 00:46:13,716
We have a for loop, maybe just


1696
00:46:13,716 --> 00:46:15,436
create some sockets and connect


1697
00:46:15,436 --> 00:46:16,336
them to our server.


1698
00:46:17,266 --> 00:46:18,946
And we monitor that socket for


1699
00:46:18,946 --> 00:46:20,086
readability with one of these


1700
00:46:20,146 --> 00:46:21,696
dispatch read sources that we've


1701
00:46:21,696 --> 00:46:22,896
seen so many times already in


1702
00:46:22,896 --> 00:46:23,396
this session.


1703
00:46:23,396 --> 00:46:24,796
And here it is the trusted the


1704
00:46:24,796 --> 00:46:25,506
see API.


1705
00:46:26,546 --> 00:46:27,946
We then set up the event handler


1706
00:46:27,946 --> 00:46:29,506
block for that dispatch source


1707
00:46:29,906 --> 00:46:30,216
here.


1708
00:46:30,366 --> 00:46:32,146
And when the socket becomes


1709
00:46:32,146 --> 00:46:33,346
readable, we just read from it


1710
00:46:33,346 --> 00:46:34,756
with the read system call until


1711
00:46:34,756 --> 00:46:36,016
there is no more data available.


1712
00:46:36,756 --> 00:46:38,416
Once we have the data, we pass


1713
00:46:38,416 --> 00:46:40,586
it to our database, subsystem in


1714
00:46:40,586 --> 00:46:41,576
the application with this


1715
00:46:41,606 --> 00:46:42,556
process 0 method.


1716
00:46:43,706 --> 00:46:45,076
So, let's build and run, and


1717
00:46:45,076 --> 00:46:46,206
take a system trace of this


1718
00:46:46,206 --> 00:46:47,416
application and see how it


1719
00:46:47,416 --> 00:46:48,016
executes.


1720
00:46:48,016 --> 00:46:52,246
So here we are in instruments,


1721
00:46:52,286 --> 00:46:54,146
in system trace, and in addition


1722
00:46:54,146 --> 00:46:55,256
to the usual tacks in system


1723
00:46:55,256 --> 00:46:56,736
trace, we've added this new GCD


1724
00:46:56,736 --> 00:46:57,686
performance instrument.


1725
00:46:58,096 --> 00:46:59,436
When we click on there, we see a


1726
00:46:59,436 --> 00:47:01,056
number of performance events


1727
00:47:01,056 --> 00:47:02,456
that have been reported for


1728
00:47:02,456 --> 00:47:03,346
performance problems.


1729
00:47:03,746 --> 00:47:05,336
One of these is this mutation


1730
00:47:05,336 --> 00:47:06,456
after activation event, that we


1731
00:47:06,456 --> 00:47:08,256
can also see when we go and mass


1732
00:47:08,256 --> 00:47:08,886
over the timeline.


1733
00:47:09,316 --> 00:47:10,576
You can also click on one of the


1734
00:47:10,576 --> 00:47:12,176
other events here, such as this,


1735
00:47:12,176 --> 00:47:13,486
re-target after activation


1736
00:47:13,486 --> 00:47:13,836
event.


1737
00:47:14,506 --> 00:47:15,846
And the list will take us


1738
00:47:15,846 --> 00:47:16,626
directly there.


1739
00:47:16,626 --> 00:47:18,036
If you want more details on


1740
00:47:18,036 --> 00:47:19,646
this, we can disclose the


1741
00:47:19,646 --> 00:47:20,806
backtrace on the right hand side


1742
00:47:20,806 --> 00:47:22,306
is instruments which will show


1743
00:47:22,306 --> 00:47:23,636
us where exactly this event


1744
00:47:23,636 --> 00:47:24,736
occurred in your application.


1745
00:47:25,166 --> 00:47:26,416
So, here for instance it is in


1746
00:47:26,416 --> 00:47:28,226
our create connections method.


1747
00:47:29,546 --> 00:47:30,956
If we double click on this


1748
00:47:30,956 --> 00:47:32,306
frame, instruments will show us


1749
00:47:32,306 --> 00:47:33,386
directly the line of code where


1750
00:47:33,766 --> 00:47:34,696
the problem occurred.


1751
00:47:35,396 --> 00:47:36,786
This is actually a target queue


1752
00:47:36,786 --> 00:47:38,176
call here that indeed occurs


1753
00:47:38,176 --> 00:47:38,966
after activate.


1754
00:47:39,416 --> 00:47:40,606
Like, this is the pattern up


1755
00:47:40,606 --> 00:47:41,476
here just told you about.


1756
00:47:41,476 --> 00:47:43,006
To go and fix that, we can jump


1757
00:47:43,136 --> 00:47:44,376
directly into XCode with the


1758
00:47:44,376 --> 00:47:45,676
open file and XCode button and


1759
00:47:45,676 --> 00:47:46,196
instruments.


1760
00:47:46,676 --> 00:47:48,416
So, here we are at that dispatch


1761
00:47:48,416 --> 00:47:50,336
the target queue line and indeed


1762
00:47:50,396 --> 00:47:51,716
it, as well as the dispatch


1763
00:47:51,716 --> 00:47:53,816
source at event handler set up


1764
00:47:53,816 --> 00:47:54,936
happens after activate.


1765
00:47:55,486 --> 00:47:56,896
So, here in this example, it's


1766
00:47:56,896 --> 00:47:57,636
really easy to fix.


1767
00:47:57,636 --> 00:47:58,936
We just move these two lines


1768
00:47:59,356 --> 00:48:00,206
down below.


1769
00:48:00,206 --> 00:48:01,706
And we have fixed the problem.


1770
00:48:01,706 --> 00:48:03,386
We have activate after we set up


1771
00:48:03,426 --> 00:48:04,856
the source, and not before.


1772
00:48:05,526 --> 00:48:06,616
So, let's jump back into


1773
00:48:06,616 --> 00:48:08,156
instruments and see what we can


1774
00:48:08,156 --> 00:48:09,556
see in the system trace now.


1775
00:48:09,856 --> 00:48:11,176
It looks the same as before,


1776
00:48:11,176 --> 00:48:12,696
except when you click on the GCD


1777
00:48:12,696 --> 00:48:14,196
performance track, you will see


1778
00:48:14,196 --> 00:48:15,336
there is no more significant


1779
00:48:15,336 --> 00:48:16,476
performance problems detected.


1780
00:48:16,476 --> 00:48:17,836
And that's what you ought to see


1781
00:48:17,836 --> 00:48:18,686
if you use this instrument.


1782
00:48:18,836 --> 00:48:20,466
So, of course this was very


1783
00:48:20,466 --> 00:48:21,686
simple in this application.


1784
00:48:21,686 --> 00:48:22,686
You may have to do some work.


1785
00:48:24,566 --> 00:48:25,806
So, let's focus on the points


1786
00:48:25,806 --> 00:48:26,916
track in the application.


1787
00:48:26,916 --> 00:48:28,096
This shows us a number of


1788
00:48:28,146 --> 00:48:29,366
network event handlers.


1789
00:48:29,816 --> 00:48:31,106
And these are the source event


1790
00:48:31,106 --> 00:48:32,166
handlers in our application.


1791
00:48:32,666 --> 00:48:34,066
How did you manage to make these


1792
00:48:34,066 --> 00:48:35,006
show up in instruments?


1793
00:48:35,606 --> 00:48:36,246
That's actually really


1794
00:48:36,246 --> 00:48:37,356
interesting to understand


1795
00:48:37,356 --> 00:48:38,386
because it's something you can


1796
00:48:38,386 --> 00:48:39,916
apply to your own code to


1797
00:48:39,916 --> 00:48:41,646
understand how it executes in


1798
00:48:41,676 --> 00:48:42,176
instruments.


1799
00:48:43,806 --> 00:48:45,556
Well, going back to XCode in our


1800
00:48:45,556 --> 00:48:47,486
create connections method, when


1801
00:48:47,486 --> 00:48:49,516
we set up our source and its


1802
00:48:49,516 --> 00:48:50,456
source event handlers, we are


1803
00:48:50,456 --> 00:48:51,976
interested in the execution of


1804
00:48:52,046 --> 00:48:53,806
that event handler, and try to


1805
00:48:53,806 --> 00:48:54,926
understand its timing.


1806
00:48:55,506 --> 00:48:57,016
To see that instruments, we've


1807
00:48:57,016 --> 00:48:59,156
added the kdebug signpost start


1808
00:48:59,156 --> 00:49:00,696
function at the beginning of the


1809
00:49:00,696 --> 00:49:02,776
handler, and the kdebug signpost


1810
00:49:02,886 --> 00:49:03,936
end function at the end.


1811
00:49:03,936 --> 00:49:05,836
And that is all it takes for the


1812
00:49:05,836 --> 00:49:07,346
section of code to appear


1813
00:49:07,346 --> 00:49:08,486
highlighted in the points track


1814
00:49:08,946 --> 00:49:10,096
in instrument system trace.


1815
00:49:10,396 --> 00:49:11,836
So, if you switch back to


1816
00:49:11,836 --> 00:49:13,766
instruments, that is these red


1817
00:49:13,866 --> 00:49:15,436
dots at the pop in the points


1818
00:49:15,436 --> 00:49:16,826
track and we can see in the back


1819
00:49:16,826 --> 00:49:18,706
trace that it matches our event


1820
00:49:18,706 --> 00:49:20,486
handler for one of these events.


1821
00:49:21,566 --> 00:49:22,726
If you zoom in on one of these


1822
00:49:22,726 --> 00:49:24,416
interesting looking areas in the


1823
00:49:24,416 --> 00:49:26,226
points track, here, you can see


1824
00:49:26,226 --> 00:49:27,956
that there is a number of event


1825
00:49:27,956 --> 00:49:29,876
handlers that are occurring very


1826
00:49:29,876 --> 00:49:30,946
close together.


1827
00:49:31,156 --> 00:49:33,276
And by mousing over we can


1828
00:49:33,276 --> 00:49:34,066
actually see that they're


1829
00:49:34,066 --> 00:49:35,426
execute for very short amounts


1830
00:49:35,426 --> 00:49:35,836
of time.


1831
00:49:36,356 --> 00:49:38,286
The pop-up will tell us the


1832
00:49:38,286 --> 00:49:39,746
amount of time it has executed


1833
00:49:39,946 --> 00:49:40,826
and we can even see that


1834
00:49:40,826 --> 00:49:42,186
sometimes we have overlapping


1835
00:49:42,186 --> 00:49:43,226
event handlers that are all


1836
00:49:43,226 --> 00:49:44,886
executing concurrently at the


1837
00:49:44,926 --> 00:49:45,506
same time.


1838
00:49:45,506 --> 00:49:47,766
So, this is one of the symptoms


1839
00:49:47,766 --> 00:49:48,856
of potentially unwanted


1840
00:49:48,856 --> 00:49:50,056
concurrency in our application,


1841
00:49:50,376 --> 00:49:51,546
where something that didn't look


1842
00:49:51,546 --> 00:49:53,346
like it would cause concurrency


1843
00:49:53,556 --> 00:49:55,926
in your code, actually does run


1844
00:49:55,926 --> 00:49:57,146
in a concurrent way or multiple


1845
00:49:57,146 --> 00:49:58,906
threads and cause potentially


1846
00:49:58,906 --> 00:49:59,876
extra context switches.


1847
00:50:01,466 --> 00:50:03,076
So, to understand this better,


1848
00:50:03,076 --> 00:50:04,586
let's bring up the threads in


1849
00:50:04,586 --> 00:50:05,136
instruments.


1850
00:50:05,426 --> 00:50:06,556
And the system trace that are


1851
00:50:06,556 --> 00:50:07,506
executing this code.


1852
00:50:12,446 --> 00:50:13,726
So, here I've highlighted the


1853
00:50:13,726 --> 00:50:14,716
three worker threads that are


1854
00:50:14,716 --> 00:50:16,026
executing these event handlers.


1855
00:50:16,496 --> 00:50:17,856
And we can see as before that


1856
00:50:17,856 --> 00:50:19,366
they are executing on call


1857
00:50:19,556 --> 00:50:20,376
during this time.


1858
00:50:20,926 --> 00:50:22,316
And the time they were running.


1859
00:50:22,316 --> 00:50:23,846
But here we can see they were


1860
00:50:23,846 --> 00:50:25,866
again, running for a very short


1861
00:50:25,866 --> 00:50:27,566
amount of time in this area.


1862
00:50:27,926 --> 00:50:30,386
And we can verify that they are


1863
00:50:30,386 --> 00:50:31,686
making these read system calls


1864
00:50:31,686 --> 00:50:33,146
that we saw earlier in the event


1865
00:50:33,146 --> 00:50:33,416
handler.


1866
00:50:34,136 --> 00:50:35,766
And we can get some more detail


1867
00:50:35,766 --> 00:50:36,856
by looking at the back trace


1868
00:50:36,856 --> 00:50:38,246
again, and seeing, yes it is us


1869
00:50:38,246 --> 00:50:39,276
that is calling that read system


1870
00:50:39,276 --> 00:50:41,246
call and here it reads 97 bytes


1871
00:50:41,576 --> 00:50:42,366
from our socket.


1872
00:50:44,216 --> 00:50:45,246
And looking at the other


1873
00:50:45,246 --> 00:50:46,206
threads, the same pattern


1874
00:50:46,206 --> 00:50:46,766
repeats.


1875
00:50:46,766 --> 00:50:47,786
You can see it's the same read


1876
00:50:47,786 --> 00:50:49,206
system calls occurring there,


1877
00:50:49,206 --> 00:50:49,986
more or less at the same


1878
00:50:49,986 --> 00:50:52,466
timeframe and so on the second


1879
00:50:52,466 --> 00:50:53,496
thread here or on the first


1880
00:50:53,496 --> 00:50:53,736
thread.


1881
00:50:53,996 --> 00:50:54,856
They're really all doing the


1882
00:50:54,856 --> 00:50:56,736
same thing, and overlapping.


1883
00:50:57,886 --> 00:50:59,156
It would be much better for our


1884
00:50:59,156 --> 00:51:01,806
program if these things executed


1885
00:51:01,806 --> 00:51:02,556
on a single thread.


1886
00:51:02,556 --> 00:51:03,736
Here we don't really get any


1887
00:51:03,736 --> 00:51:05,046
benefit from the concurrency


1888
00:51:05,316 --> 00:51:06,456
because we are executing such


1889
00:51:06,456 --> 00:51:07,436
short pieces of code.


1890
00:51:07,896 --> 00:51:09,716
And we are probably getting more


1891
00:51:09,716 --> 00:51:11,236
harm than good from adding these


1892
00:51:11,236 --> 00:51:12,216
extra context switches.


1893
00:51:13,036 --> 00:51:14,496
So, let's apply the patterns


1894
00:51:14,496 --> 00:51:15,776
that we saw earlier to fix this


1895
00:51:15,776 --> 00:51:16,666
problem in this sample


1896
00:51:16,666 --> 00:51:17,216
application.


1897
00:51:17,806 --> 00:51:19,446
Jumping back into XCode, let's


1898
00:51:19,446 --> 00:51:20,576
see how we set up the target


1899
00:51:20,576 --> 00:51:21,616
queue for this source that we


1900
00:51:21,616 --> 00:51:21,836
have.


1901
00:51:23,056 --> 00:51:24,096
So, that's sort of when you


1902
00:51:24,096 --> 00:51:25,766
create this queue at the top of


1903
00:51:25,836 --> 00:51:27,256
this function framework and as


1904
00:51:27,256 --> 00:51:29,186
you can see, we do it simply by


1905
00:51:29,186 --> 00:51:30,046
calling this batch queue


1906
00:51:30,046 --> 00:51:30,566
correct.


1907
00:51:31,446 --> 00:51:32,846
And that creates an independent


1908
00:51:32,846 --> 00:51:34,016
serial queue that isn't


1909
00:51:34,016 --> 00:51:35,346
connected to anything else in


1910
00:51:35,346 --> 00:51:36,046
our application.


1911
00:51:36,046 --> 00:51:37,886
This is exactly like the case we


1912
00:51:37,886 --> 00:51:40,036
had earlier in my example of the


1913
00:51:40,036 --> 00:51:40,936
networking subsystem.


1914
00:51:41,636 --> 00:51:43,356
So, let's fix that by adding a


1915
00:51:43,356 --> 00:51:44,616
mutual exclusion context at the


1916
00:51:44,616 --> 00:51:46,196
bottom of all of these queues


1917
00:51:46,196 --> 00:51:47,106
for all of these connections.


1918
00:51:47,606 --> 00:51:49,326
And we do that by adding the, or


1919
00:51:49,326 --> 00:51:50,666
by switching to the dispatch


1920
00:51:50,666 --> 00:51:51,606
queue create with target


1921
00:51:51,606 --> 00:51:53,256
function up here introduced to


1922
00:51:53,256 --> 00:51:53,776
you earlier.


1923
00:51:54,416 --> 00:51:58,996
So, here we add dispatch queue,


1924
00:51:58,996 --> 00:51:59,746
create this target.


1925
00:52:00,206 --> 00:52:01,716
And we use a single mutual


1926
00:52:01,716 --> 00:52:02,906
exclusion queue as the target


1927
00:52:02,906 --> 00:52:03,756
queue for all of these.


1928
00:52:04,196 --> 00:52:05,246
And this is a serial queue that


1929
00:52:05,246 --> 00:52:06,596
we created somewhere else.


1930
00:52:07,416 --> 00:52:09,036
And with that, we build and run


1931
00:52:09,036 --> 00:52:09,976
again and look at the system


1932
00:52:09,976 --> 00:52:10,546
trace again.


1933
00:52:11,576 --> 00:52:12,846
And now it looks very different.


1934
00:52:13,466 --> 00:52:15,436
Here we have still the same


1935
00:52:15,436 --> 00:52:16,616
points track and we still see


1936
00:52:16,616 --> 00:52:18,136
the same network events that


1937
00:52:18,136 --> 00:52:19,426
occur, but as you can see,


1938
00:52:19,656 --> 00:52:20,596
there's no more overlapping


1939
00:52:20,596 --> 00:52:21,686
events in that track, and


1940
00:52:21,906 --> 00:52:23,066
there's a single worker thread


1941
00:52:23,066 --> 00:52:24,226
that executes this code.


1942
00:52:24,306 --> 00:52:26,376
And if we zoom in on one of


1943
00:52:26,426 --> 00:52:27,726
these clusters we can see this


1944
00:52:27,726 --> 00:52:29,066
is actually many instances of


1945
00:52:29,066 --> 00:52:30,996
that event handler executing in


1946
00:52:30,996 --> 00:52:32,306
rapid succession, which is


1947
00:52:32,306 --> 00:52:33,686
exactly what we expected.


1948
00:52:34,216 --> 00:52:36,376
And when you zoom in more on one


1949
00:52:36,376 --> 00:52:37,746
particular event, you can see


1950
00:52:37,746 --> 00:52:38,896
it's still executing for a


1951
00:52:38,896 --> 00:52:40,676
fairly short amount of time, and


1952
00:52:40,676 --> 00:52:41,716
making those same read sys


1953
00:52:41,716 --> 00:52:42,116
calls.


1954
00:52:42,786 --> 00:52:44,216
But now that is much less


1955
00:52:44,216 --> 00:52:45,266
problematic because it's all


1956
00:52:45,266 --> 00:52:46,986
happening on a single thread.


1957
00:52:49,636 --> 00:52:51,886
So, this may seem like a very


1958
00:52:51,886 --> 00:52:53,626
simple and trivial change, but


1959
00:52:53,626 --> 00:52:54,546
it's worth pointing out that


1960
00:52:54,546 --> 00:52:55,926
it's exactly this type of small


1961
00:52:55,926 --> 00:52:57,906
tweak that led to the 1.3X


1962
00:52:57,906 --> 00:52:59,306
performance improvement in some


1963
00:52:59,306 --> 00:53:00,776
of our own framework code that


1964
00:53:00,776 --> 00:53:02,016
Daniel pointed out at the


1965
00:53:02,016 --> 00:53:02,936
beginning of the session.


1966
00:53:03,426 --> 00:53:04,736
So, very small changes like this


1967
00:53:04,736 --> 00:53:05,596
can make a significant


1968
00:53:05,596 --> 00:53:06,036
difference.


1969
00:53:07,476 --> 00:53:12,256
All right so, let's look back at


1970
00:53:12,256 --> 00:53:13,166
what we've covered today.


1971
00:53:13,166 --> 00:53:15,156
Daniel, at the beginning went


1972
00:53:15,156 --> 00:53:16,606
with you over the details of how


1973
00:53:16,606 --> 00:53:18,736
not to go off core unnecessarily


1974
00:53:18,736 --> 00:53:19,976
is ever more important for


1975
00:53:19,976 --> 00:53:21,406
modern CPUs so that it can reach


1976
00:53:21,406 --> 00:53:22,456
the most efficient performance


1977
00:53:22,456 --> 00:53:22,676
state.


1978
00:53:23,506 --> 00:53:25,106
We looked at the importance of


1979
00:53:25,106 --> 00:53:26,686
sizing the workforce of power


1980
00:53:26,686 --> 00:53:28,586
workloads and for work moving


1981
00:53:28,586 --> 00:53:29,726
between subsystems in your


1982
00:53:29,726 --> 00:53:31,456
application as well as inside


1983
00:53:31,536 --> 00:53:32,266
those subsystems.


1984
00:53:33,166 --> 00:53:34,446
We talked about how to choose


1985
00:53:34,446 --> 00:53:36,476
good granularity of concurrency


1986
00:53:36,476 --> 00:53:37,916
with GCD by using a fixed number


1987
00:53:37,916 --> 00:53:38,946
of serial queue hierarchies in


1988
00:53:38,946 --> 00:53:39,596
your application.


1989
00:53:40,096 --> 00:53:41,056
And Pierre walked you through


1990
00:53:41,056 --> 00:53:42,956
how to modernize your GCD usage


1991
00:53:43,256 --> 00:53:44,356
to take full advantage of


1992
00:53:44,356 --> 00:53:46,186
improvements in the OS, in our


1993
00:53:46,186 --> 00:53:46,626
hardware.


1994
00:53:47,596 --> 00:53:49,096
And finally, we saw how we can


1995
00:53:49,096 --> 00:53:50,486
use instruments to find problems


1996
00:53:50,486 --> 00:53:51,886
spots in our application and how


1997
00:53:52,206 --> 00:53:54,826
to fix them.


1998
00:53:55,186 --> 00:53:56,256
For more information on this


1999
00:53:56,256 --> 00:53:57,926
session, I will direct you to


2000
00:53:57,926 --> 00:54:00,196
this URL where the documentation


2001
00:54:00,196 --> 00:54:02,116
links for GCD are as well as the


2002
00:54:02,116 --> 00:54:04,176
movie for the session, and we


2003
00:54:04,176 --> 00:54:05,386
have some related sessions this


2004
00:54:05,386 --> 00:54:06,566
week that might be worthwhile


2005
00:54:06,876 --> 00:54:07,616
going to.


2006
00:54:07,746 --> 00:54:09,726
Introducing Core ML already


2007
00:54:09,726 --> 00:54:11,226
having happened, the other two


2008
00:54:11,226 --> 00:54:12,386
are going to help you with


2009
00:54:12,386 --> 00:54:13,636
parallel and computing


2010
00:54:13,686 --> 00:54:14,176
[inaudible] task in your


2011
00:54:14,336 --> 00:54:15,746
application like we talked about


2012
00:54:15,746 --> 00:54:16,346
at the beginning.


2013
00:54:16,946 --> 00:54:19,426
And the last two are going to


2014
00:54:19,426 --> 00:54:20,566
help you with more performance


2015
00:54:20,566 --> 00:54:21,656
analysis and improvements of


2016
00:54:21,696 --> 00:54:23,186
different aspects of your app.


2017
00:54:23,186 --> 00:54:25,216
And with that, I'd like to thank


2018
00:54:25,216 --> 00:54:25,836
you for coming.


2019
00:54:26,176 --> 00:54:26,986
If you have any questions,


2020
00:54:26,986 --> 00:54:27,966
please come and see us at the


2021
00:54:27,966 --> 00:54:28,246
labs.


2022
00:54:29,508 --> 00:54:31,508
[ Applause ]

