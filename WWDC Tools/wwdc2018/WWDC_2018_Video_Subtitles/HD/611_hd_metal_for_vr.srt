1
00:00:07,516 --> 00:00:15,500
[ Music ]


2
00:00:17,516 --> 00:00:23,336
[ Applause ]


3
00:00:23,836 --> 00:00:24,826
>> Good morning everyone.


4
00:00:25,046 --> 00:00:26,676
My name is Karol Gasinski, and I


5
00:00:26,676 --> 00:00:28,296
am a member of GPU Software


6
00:00:28,296 --> 00:00:29,566
Architecture Team at Apple.


7
00:00:29,566 --> 00:00:32,836
We will start this session with


8
00:00:32,836 --> 00:00:35,186
a brief summary of what is new


9
00:00:35,286 --> 00:00:37,456
in [inaudible], in terms of VR


10
00:00:37,456 --> 00:00:38,196
adoption.


11
00:00:38,946 --> 00:00:40,826
Then, we will take a deep dive


12
00:00:41,046 --> 00:00:42,906
into a new Metal 2 features,


13
00:00:43,136 --> 00:00:45,016
designs specifically for the VR


14
00:00:45,016 --> 00:00:45,476
this year.


15
00:00:46,756 --> 00:00:48,576
And finally, we will end this


16
00:00:48,576 --> 00:00:50,466
session with advanced techniques


17
00:00:50,626 --> 00:00:52,236
for developing VR applications.


18
00:00:53,856 --> 00:00:56,026
Recently, we introduced new iMac


19
00:00:56,116 --> 00:00:57,976
and iMac Pro that have great


20
00:00:57,976 --> 00:01:00,776
GPUs on board; iMac is now


21
00:01:00,776 --> 00:01:01,936
equipped with [inaudible] based


22
00:01:01,936 --> 00:01:05,636
GPUs and have up to 80 gigabytes


23
00:01:05,636 --> 00:01:06,976
of video memory on board.


24
00:01:07,716 --> 00:01:09,946
While iMac Pro are equipped with


25
00:01:09,946 --> 00:01:11,786
even more advanced and even


26
00:01:11,786 --> 00:01:14,286
bigger based GPUs, with up to 16


27
00:01:14,286 --> 00:01:15,586
gigabytes of video memory.


28
00:01:16,546 --> 00:01:18,636
That's a lot of power that is


29
00:01:18,636 --> 00:01:19,536
now in your hands.


30
00:01:20,386 --> 00:01:21,516
But we are not limiting our


31
00:01:21,516 --> 00:01:23,406
service to iMac on their own.


32
00:01:24,166 --> 00:01:25,976
With recent announcement of


33
00:01:25,976 --> 00:01:28,456
extended GPU support, you can


34
00:01:28,456 --> 00:01:30,816
now turn any Mac into powerful


35
00:01:30,936 --> 00:01:34,376
workstation that gives you more


36
00:01:34,376 --> 00:01:35,116
than 10 [inaudible] of


37
00:01:35,176 --> 00:01:36,376
processing power.


38
00:01:37,126 --> 00:01:37,886
And that's not all.


39
00:01:39,106 --> 00:01:40,786
Today we are introducing plug


40
00:01:40,786 --> 00:01:43,276
and play support for HTC Vive


41
00:01:43,406 --> 00:01:44,406
head mounted display.


42
00:01:45,196 --> 00:01:48,966
It has two panels with 1,440 by


43
00:01:48,966 --> 00:01:54,826
1,600 and 650 pixels per inch.


44
00:01:54,956 --> 00:01:58,366
That's 78% increase in the


45
00:01:58,366 --> 00:02:01,516
resolution, and 57% increase in


46
00:02:01,566 --> 00:02:03,616
pixel density compared to Vive.


47
00:02:03,746 --> 00:02:06,676
And with support for better


48
00:02:06,786 --> 00:02:09,086
panels comes support for its new


49
00:02:09,186 --> 00:02:11,266
dual-camera front-facing system,


50
00:02:11,666 --> 00:02:13,526
so developers will now be able


51
00:02:13,526 --> 00:02:14,886
to use those cameras to


52
00:02:14,956 --> 00:02:16,386
experiment with pass-through


53
00:02:16,386 --> 00:02:18,036
video on Mac.


54
00:02:18,236 --> 00:02:19,486
And together with Vive Pro


55
00:02:19,486 --> 00:02:21,446
support comes improved tracking


56
00:02:21,446 --> 00:02:21,966
system.


57
00:02:23,456 --> 00:02:25,186
So, you might be wondering how


58
00:02:25,186 --> 00:02:26,426
you can start developing VR


59
00:02:26,426 --> 00:02:28,036
application on Mac OS.


60
00:02:28,906 --> 00:02:32,506
Both HTC Vive and Vive Pro work


61
00:02:32,506 --> 00:02:34,526
in conjunction with Valve


62
00:02:34,826 --> 00:02:37,456
SteamVR runtime, that provides a


63
00:02:37,456 --> 00:02:39,346
number of services including VR


64
00:02:39,346 --> 00:02:40,226
compositor.


65
00:02:40,676 --> 00:02:43,936
Valve is also making open VR


66
00:02:43,936 --> 00:02:46,016
framework that is available on


67
00:02:46,016 --> 00:02:48,066
Mac OS, so that you can do the


68
00:02:48,066 --> 00:02:49,576
map that works with SteamVR.


69
00:02:50,626 --> 00:02:52,296
We've been working very closely


70
00:02:52,296 --> 00:02:54,876
with both Valve and HTC to make


71
00:02:54,876 --> 00:02:57,516
sure that Vive Pro is supported


72
00:02:57,676 --> 00:03:00,356
in SteamVR runtime on Mac OS.


73
00:03:03,006 --> 00:03:04,796
So, now let's see how new Metal


74
00:03:04,796 --> 00:03:06,486
features that we're introducing


75
00:03:06,486 --> 00:03:07,876
can develop a [inaudible] Mac OS


76
00:03:07,876 --> 00:03:09,786
Mojave can be used to


77
00:03:09,786 --> 00:03:11,166
fiber-optimize your VR


78
00:03:11,166 --> 00:03:11,946
application.


79
00:03:12,036 --> 00:03:14,356
As a quick refresher, let's


80
00:03:15,086 --> 00:03:16,736
review current interaction


81
00:03:16,736 --> 00:03:18,096
between application and VR


82
00:03:18,136 --> 00:03:18,786
compositor.


83
00:03:18,896 --> 00:03:21,386
Application will start by


84
00:03:21,386 --> 00:03:23,106
rendering image for left and


85
00:03:23,106 --> 00:03:25,466
right eye into 30 multi-sample


86
00:03:25,516 --> 00:03:26,026
textures.


87
00:03:26,776 --> 00:03:28,306
Then it will resolve those


88
00:03:28,306 --> 00:03:30,556
images into iOS surface back


89
00:03:30,726 --> 00:03:32,456
textures that can be further


90
00:03:32,456 --> 00:03:33,816
passed to VR compositor.


91
00:03:33,816 --> 00:03:37,926
VR compositor will perform final


92
00:03:37,986 --> 00:03:39,476
processing step that will


93
00:03:39,556 --> 00:03:41,256
include [inaudible] distortion


94
00:03:41,256 --> 00:03:42,446
correction, chromatic


95
00:03:42,446 --> 00:03:43,656
aberration, and order


96
00:03:43,656 --> 00:03:44,356
operations.


97
00:03:44,986 --> 00:03:47,026
We can call it in short warp.


98
00:03:48,426 --> 00:03:49,866
Once the final image is


99
00:03:49,956 --> 00:03:52,066
produced, it can be sent to the


100
00:03:52,066 --> 00:03:53,846
headset for presentment.


101
00:03:54,936 --> 00:03:56,856
It is a lot of work here that is


102
00:03:56,916 --> 00:03:59,446
happening twice, so let's see if


103
00:03:59,446 --> 00:04:02,456
you can do something about that.


104
00:04:02,766 --> 00:04:04,126
See, now with VR application it


105
00:04:04,126 --> 00:04:05,336
wants to benefit from


106
00:04:05,336 --> 00:04:06,656
multi-sample [inaudible], it


107
00:04:07,236 --> 00:04:08,806
needed to use the dedicated


108
00:04:08,976 --> 00:04:12,016
textures per i, or single shared


109
00:04:12,016 --> 00:04:13,666
one, for both.


110
00:04:13,716 --> 00:04:15,246
But none of those layouts is


111
00:04:15,306 --> 00:04:15,726
perfect.


112
00:04:16,576 --> 00:04:18,386
The dedicated textures require


113
00:04:18,386 --> 00:04:20,505
separate draw calls and passes,


114
00:04:20,886 --> 00:04:21,846
as we just saw.


115
00:04:23,166 --> 00:04:25,246
While straight textures enable


116
00:04:25,246 --> 00:04:27,296
rendering of both eyes in single


117
00:04:27,656 --> 00:04:30,456
rendered and results pass, they


118
00:04:30,456 --> 00:04:32,206
are problematic when it comes to


119
00:04:32,276 --> 00:04:34,196
post-processing the effects.


120
00:04:34,736 --> 00:04:36,986
[Inaudible] textures have all


121
00:04:36,986 --> 00:04:38,956
the benefits of both dedicated


122
00:04:39,126 --> 00:04:41,556
and shared layouts, but


123
00:04:41,646 --> 00:04:43,026
currently they couldn't be used


124
00:04:43,176 --> 00:04:44,056
with MSAA.


125
00:04:45,816 --> 00:04:47,506
This was forcing app developers


126
00:04:47,566 --> 00:04:48,826
to use different rendering


127
00:04:48,936 --> 00:04:50,876
part-time layouts, based on the


128
00:04:51,096 --> 00:04:54,046
fact if they wanted to use MSAA


129
00:04:54,046 --> 00:04:54,406
or not.


130
00:04:54,936 --> 00:04:56,716
Or use different tricks to work


131
00:04:56,716 --> 00:04:57,646
around it.


132
00:04:57,866 --> 00:04:59,416
So let's see how we can optimize


133
00:04:59,466 --> 00:05:00,116
this rendering.


134
00:05:02,166 --> 00:05:03,966
Today, we introduce new texture


135
00:05:04,056 --> 00:05:06,026
types to the multi-sample


136
00:05:06,026 --> 00:05:07,396
already textured.


137
00:05:08,366 --> 00:05:10,726
This texture type has all the


138
00:05:10,726 --> 00:05:13,166
benefits of previously mentioned


139
00:05:13,266 --> 00:05:14,666
types without any of the


140
00:05:14,736 --> 00:05:15,336
drawbacks.


141
00:05:15,476 --> 00:05:17,806
Thanks to that it is now


142
00:05:17,806 --> 00:05:19,606
possible to separate from each


143
00:05:19,606 --> 00:05:22,516
other rendering space, which


144
00:05:22,516 --> 00:05:24,696
simplifies the post-processing


145
00:05:24,696 --> 00:05:28,286
effects, views count, so that


146
00:05:28,286 --> 00:05:30,016
application can fall back easily


147
00:05:30,016 --> 00:05:33,536
to monoscope rendering and


148
00:05:33,536 --> 00:05:35,406
control over anti-aliasing mode.


149
00:05:36,436 --> 00:05:38,786
As a result, application can now


150
00:05:38,786 --> 00:05:40,956
have single rendering files that


151
00:05:40,956 --> 00:05:43,456
can be easily adopted to any


152
00:05:43,556 --> 00:05:46,056
situation, and most important,


153
00:05:46,056 --> 00:05:48,206
that can be rendered with single


154
00:05:48,206 --> 00:05:50,736
draw and render pass in each


155
00:05:50,736 --> 00:05:51,056
case.


156
00:05:53,936 --> 00:05:55,686
So here we see code snippet for


157
00:05:55,686 --> 00:05:57,916
creation of mentioned 2D multi


158
00:05:57,916 --> 00:05:59,126
sample [inaudible] texture.


159
00:06:01,146 --> 00:06:03,446
We set up sample count to 4, as


160
00:06:03,446 --> 00:06:05,386
it's an optimal tradeoff between


161
00:06:05,426 --> 00:06:07,486
quality and performance, and at


162
00:06:07,486 --> 00:06:09,736
the same time, we set up our


163
00:06:09,826 --> 00:06:12,436
other length to 2 as we want to


164
00:06:12,436 --> 00:06:16,346
store each image for each I in


165
00:06:16,446 --> 00:06:17,226
separate slice.


166
00:06:18,676 --> 00:06:21,776
So let's see how our pipeline


167
00:06:21,776 --> 00:06:22,376
will change.


168
00:06:23,606 --> 00:06:25,126
We can now replace those 2D


169
00:06:25,126 --> 00:06:27,396
multi-sample textures with


170
00:06:27,396 --> 00:06:29,596
single 2D multi-sample


171
00:06:32,066 --> 00:06:32,156
[inaudible] one.


172
00:06:33,136 --> 00:06:34,856
So now application can render


173
00:06:34,966 --> 00:06:38,396
both I in single render pass and


174
00:06:38,396 --> 00:06:40,686
if it's using instancing, it can


175
00:06:40,976 --> 00:06:42,356
even do that in single draw


176
00:06:42,356 --> 00:06:42,596
code.


177
00:06:42,596 --> 00:06:45,606
So that already looks great, but


178
00:06:45,606 --> 00:06:47,266
we still need to resolve those


179
00:06:47,726 --> 00:06:49,976
2D multi-sample array texture


180
00:06:49,976 --> 00:06:52,346
slices into separate iOS


181
00:06:52,406 --> 00:06:54,636
[inaudible] faces before we pass


182
00:06:54,636 --> 00:06:55,716
them to compositor.


183
00:06:56,936 --> 00:06:59,156
So let's focus on our way,


184
00:06:59,156 --> 00:07:01,276
application shares textures with


185
00:07:01,306 --> 00:07:01,746
compositor.


186
00:07:01,746 --> 00:07:04,676
So now, for sharing textures, we


187
00:07:04,676 --> 00:07:05,826
use IOSurfaces.


188
00:07:06,786 --> 00:07:08,316
They are sharing textures


189
00:07:08,426 --> 00:07:10,086
between different process spaces


190
00:07:10,796 --> 00:07:14,086
and different GPUs, that we've


191
00:07:14,186 --> 00:07:15,466
got [inaudible] comes a price.


192
00:07:16,436 --> 00:07:19,466
IOSurfaces can be only used to


193
00:07:19,466 --> 00:07:22,316
share simple 2D textures, so if


194
00:07:22,316 --> 00:07:23,876
you have a multi-sampled one,


195
00:07:24,266 --> 00:07:25,996
storing [inaudible] or having


196
00:07:26,806 --> 00:07:27,386
[inaudible], they couldn't be


197
00:07:27,386 --> 00:07:27,736
shared.


198
00:07:29,046 --> 00:07:30,706
That's why today we introduce


199
00:07:30,706 --> 00:07:32,836
shareable Metal textures that


200
00:07:32,836 --> 00:07:34,676
allow your applications to share


201
00:07:34,806 --> 00:07:36,486
any type of Metal texture


202
00:07:36,796 --> 00:07:39,736
between process spaces, as long


203
00:07:39,836 --> 00:07:42,046
as these textures stay in scope


204
00:07:42,046 --> 00:07:43,106
of single GPU.


205
00:07:43,996 --> 00:07:46,726
This file features [inaudible]


206
00:07:46,856 --> 00:07:49,116
advanced view of these cases.


207
00:07:49,636 --> 00:07:51,956
For example, sharing depth of


208
00:07:52,036 --> 00:07:53,946
your scene with VR compositor.


209
00:07:54,486 --> 00:07:55,846
But, of course, it's not limited


210
00:07:55,846 --> 00:07:56,446
just to that.


211
00:07:57,256 --> 00:07:59,016
Now, let's look how those


212
00:07:59,086 --> 00:08:00,236
textures can be created.


213
00:08:02,636 --> 00:08:03,906
Because shareable textures


214
00:08:03,996 --> 00:08:06,316
allows us now to pass complex


215
00:08:06,366 --> 00:08:08,296
textures between processes, we


216
00:08:08,296 --> 00:08:10,676
will create 2D array texture


217
00:08:10,676 --> 00:08:12,006
that we will pass to VR


218
00:08:12,206 --> 00:08:12,936
compositor.


219
00:08:13,216 --> 00:08:16,116
As you can see, to do that, we


220
00:08:16,226 --> 00:08:18,126
use new methods, new shared


221
00:08:18,206 --> 00:08:19,426
texture with this creator.


222
00:08:19,976 --> 00:08:22,986
And while doing that, you need


223
00:08:23,056 --> 00:08:24,926
to remember to use private


224
00:08:25,096 --> 00:08:27,036
storage mode, as this texture


225
00:08:27,036 --> 00:08:29,286
can be only accessed by the GPU


226
00:08:29,706 --> 00:08:33,126
on which it was created.


227
00:08:33,876 --> 00:08:35,996
Now, we see a code snippet


228
00:08:36,456 --> 00:08:38,546
showing us how our VR


229
00:08:38,546 --> 00:08:41,416
application would send IOSurface


230
00:08:41,556 --> 00:08:44,186
to VR compositor in the past.


231
00:08:44,186 --> 00:08:45,526
We will now go through this code


232
00:08:45,526 --> 00:08:47,656
snippet, and see what changes


233
00:08:47,746 --> 00:08:50,676
needs to be applied to switch


234
00:08:50,676 --> 00:08:52,656
from using IOSurfaces to shared


235
00:08:52,706 --> 00:08:53,516
Metal textures.


236
00:08:54,976 --> 00:08:56,166
So we don't need those two


237
00:08:56,166 --> 00:08:59,366
IOSurfaces anymore, and those


238
00:08:59,426 --> 00:09:01,006
two textures that were backed by


239
00:09:01,006 --> 00:09:03,256
them can now be replaced with


240
00:09:03,256 --> 00:09:05,686
single shareable Metal texture


241
00:09:05,976 --> 00:09:08,246
that is over 2D array type.


242
00:09:09,236 --> 00:09:11,956
We will then assign this texture


243
00:09:12,146 --> 00:09:15,136
to both texture descriptors from


244
00:09:15,176 --> 00:09:19,116
open VRSDK, and change its type


245
00:09:19,526 --> 00:09:21,416
from IOSurface to Metal.


246
00:09:21,526 --> 00:09:25,076
After doing these few changes,


247
00:09:25,836 --> 00:09:27,826
we can submit image for the left


248
00:09:27,826 --> 00:09:31,606
and right I to the compositor.


249
00:09:32,346 --> 00:09:34,036
Compositor will now know that


250
00:09:34,036 --> 00:09:35,666
we've passed shared Metal


251
00:09:35,666 --> 00:09:37,446
texture with advanced layout,


252
00:09:37,686 --> 00:09:40,366
instead of IOSurface, and if we


253
00:09:40,366 --> 00:09:42,526
check, if its type is 2D array


254
00:09:42,856 --> 00:09:44,596
or 2D multi-sampling array.


255
00:09:45,246 --> 00:09:47,336
If it is, then compositor will


256
00:09:47,416 --> 00:09:49,736
automatically assume that image


257
00:09:49,736 --> 00:09:51,546
for the left i is stored in


258
00:09:51,546 --> 00:09:54,266
slice 0, and image for right i


259
00:09:54,266 --> 00:09:55,876
is stored in slice 1.


260
00:09:56,536 --> 00:09:57,916
So your application doesn't need


261
00:09:57,966 --> 00:09:59,396
to do anything more about that.


262
00:09:59,396 --> 00:10:03,326
And of course, sharing Metal


263
00:10:03,326 --> 00:10:05,266
textures between application and


264
00:10:05,266 --> 00:10:06,806
compositor is not the only use


265
00:10:06,856 --> 00:10:08,026
case for shareable Metal


266
00:10:08,066 --> 00:10:08,636
textures.


267
00:10:09,436 --> 00:10:11,126
So here we have simple example


268
00:10:11,126 --> 00:10:12,506
of how you can pass Metal


269
00:10:12,506 --> 00:10:14,156
texture between any two


270
00:10:14,156 --> 00:10:14,956
processes.


271
00:10:15,816 --> 00:10:17,526
So we start exactly in the same


272
00:10:17,526 --> 00:10:17,766
way.


273
00:10:18,046 --> 00:10:19,326
We create our shareable Metal


274
00:10:19,326 --> 00:10:23,206
texture, but now from this


275
00:10:23,286 --> 00:10:25,136
texture, we create special


276
00:10:25,286 --> 00:10:27,456
shared texture handle that can


277
00:10:27,456 --> 00:10:29,726
be passed between process spaces


278
00:10:30,136 --> 00:10:31,176
using cross-process


279
00:10:31,246 --> 00:10:32,556
communication connection.


280
00:10:33,236 --> 00:10:36,056
Once this handle is passed to


281
00:10:36,056 --> 00:10:39,306
other process, it can be used to


282
00:10:39,306 --> 00:10:41,496
recreate texture object.


283
00:10:42,346 --> 00:10:44,906
But while doing that, you need


284
00:10:45,006 --> 00:10:46,936
to remember to recreate your


285
00:10:47,016 --> 00:10:49,096
texture object on exactly the


286
00:10:49,096 --> 00:10:51,616
same device as it was originally


287
00:10:51,666 --> 00:10:53,326
created in other process space,


288
00:10:53,896 --> 00:10:56,176
as this texture cannot leave


289
00:10:56,176 --> 00:10:57,666
scope of GPU.


290
00:10:58,266 --> 00:11:02,476
So now let's get back to our


291
00:11:02,716 --> 00:11:05,196
pipeline and see what will


292
00:11:05,196 --> 00:11:05,576
change.


293
00:11:05,606 --> 00:11:07,806
Application can now replace


294
00:11:07,806 --> 00:11:10,466
those separate IOSurfaces with


295
00:11:10,466 --> 00:11:13,096
one 2D array texture, storing


296
00:11:13,096 --> 00:11:14,256
the image for both i's.


297
00:11:15,346 --> 00:11:17,406
This allows further optimization


298
00:11:18,026 --> 00:11:20,166
as original 2D multi-sample


299
00:11:20,166 --> 00:11:21,876
array texture can be now


300
00:11:21,876 --> 00:11:24,236
resolved in one pass as well to


301
00:11:24,606 --> 00:11:25,906
just create it shareable through


302
00:11:25,906 --> 00:11:26,736
the array texture.


303
00:11:27,706 --> 00:11:28,716
But that's not everything.


304
00:11:29,486 --> 00:11:30,786
Let's look at the compositor.


305
00:11:32,286 --> 00:11:33,196
Once we have simplified


306
00:11:33,196 --> 00:11:35,006
rendering parts on application


307
00:11:35,006 --> 00:11:36,596
site, there is nothing


308
00:11:36,716 --> 00:11:38,176
preventing compositor from


309
00:11:38,406 --> 00:11:39,726
benefiting from those new


310
00:11:39,726 --> 00:11:40,596
features as well.


311
00:11:41,246 --> 00:11:45,056
So compositor can now use those


312
00:11:45,126 --> 00:11:47,826
incoming 2D array textures and


313
00:11:47,826 --> 00:11:50,086
perform work for both i's in


314
00:11:50,086 --> 00:11:51,576
single render pass as well.


315
00:11:51,736 --> 00:11:54,836
And as you can see, we've just


316
00:11:54,836 --> 00:11:56,396
simplified the whole pipeline.


317
00:11:57,816 --> 00:11:59,246
So let's do recap of what we've


318
00:11:59,246 --> 00:11:59,766
just learned.


319
00:12:01,776 --> 00:12:03,266
We've just described two new


320
00:12:03,266 --> 00:12:04,236
Metal features.


321
00:12:04,576 --> 00:12:07,846
Shareable Metal textures, and 2D


322
00:12:07,846 --> 00:12:09,746
multi-sample array texture type.


323
00:12:09,746 --> 00:12:12,356
And the way they can be used to


324
00:12:12,406 --> 00:12:14,246
further optimize your rendering


325
00:12:14,326 --> 00:12:14,856
pipeline.


326
00:12:16,026 --> 00:12:17,406
Both features will be soon


327
00:12:17,406 --> 00:12:19,326
supported in upcoming SteamVR


328
00:12:19,326 --> 00:12:20,836
runtime updates.


329
00:12:22,016 --> 00:12:23,686
So now, let's focus on


330
00:12:23,686 --> 00:12:25,166
techniques that will allow your


331
00:12:25,166 --> 00:12:27,376
application to maximize its CPU


332
00:12:27,376 --> 00:12:28,736
and GPU utilization.


333
00:12:29,796 --> 00:12:32,276
We will divide this section into


334
00:12:32,366 --> 00:12:34,696
two subsections-- Advanced frame


335
00:12:34,696 --> 00:12:36,656
pacing and a reducing free rate.


336
00:12:38,386 --> 00:12:40,436
We will start with frame pacing.


337
00:12:41,276 --> 00:12:43,426
And in this section, we will


338
00:12:43,426 --> 00:12:45,446
analyze application frame pacing


339
00:12:45,536 --> 00:12:47,116
and how it can be optimized for


340
00:12:47,116 --> 00:12:47,266
VR.


341
00:12:48,306 --> 00:12:49,936
So let's start with simple,


342
00:12:50,146 --> 00:12:52,096
single-threaded application that


343
00:12:52,096 --> 00:12:53,646
is executing everything in


344
00:12:53,646 --> 00:12:54,476
serial monitoring.


345
00:12:55,406 --> 00:12:57,336
Such application will start its


346
00:12:57,336 --> 00:12:59,816
frame by calling WaitGet pauses,


347
00:13:00,866 --> 00:13:03,256
to receive pauses, and


348
00:13:03,256 --> 00:13:05,866
synchronize its execution to the


349
00:13:05,866 --> 00:13:07,256
frame rate of the headset.


350
00:13:09,026 --> 00:13:11,506
Both Vive and Vive Pro has


351
00:13:11,506 --> 00:13:13,286
refresh rate of 90 frames per


352
00:13:13,286 --> 00:13:14,936
second, which means the


353
00:13:14,986 --> 00:13:17,096
application has only 11.1


354
00:13:17,096 --> 00:13:18,706
milliseconds to process the


355
00:13:18,706 --> 00:13:19,316
whole frame.


356
00:13:20,086 --> 00:13:22,496
For comparison, blink of an eye


357
00:13:22,746 --> 00:13:24,756
takes about 300 milliseconds.


358
00:13:25,536 --> 00:13:27,286
So in this time, the application


359
00:13:27,286 --> 00:13:28,666
should render 50 frames.


360
00:13:30,326 --> 00:13:32,436
So once our application receives


361
00:13:32,616 --> 00:13:34,736
pauses from WaitGet pauses, it


362
00:13:34,806 --> 00:13:37,106
can start simulation of your


363
00:13:37,106 --> 00:13:37,636
trial [inaudible].


364
00:13:38,816 --> 00:13:39,886
When this simulation is


365
00:13:39,956 --> 00:13:41,676
complete, and state of all


366
00:13:41,676 --> 00:13:43,756
objects is known, application


367
00:13:43,756 --> 00:13:46,076
can continue with encoding


368
00:13:46,206 --> 00:13:48,286
command buffer that will be then


369
00:13:48,286 --> 00:13:50,386
sent to GPU for execution.


370
00:13:51,886 --> 00:13:54,786
Once GPU is done, an image for


371
00:13:54,786 --> 00:13:57,416
both i's is rendered, it can be


372
00:13:57,416 --> 00:13:59,906
sent to VR compositor for final


373
00:13:59,906 --> 00:14:01,426
post-processing, as we talked


374
00:14:01,516 --> 00:14:02,706
about a few slides before.


375
00:14:02,706 --> 00:14:07,316
After that, frames scanned out


376
00:14:07,646 --> 00:14:08,576
from memory to [inaudible] in


377
00:14:08,636 --> 00:14:09,466
the headset.


378
00:14:09,596 --> 00:14:12,266
This transfer takes additional


379
00:14:12,266 --> 00:14:15,226
frame as all pixels need to be


380
00:14:15,376 --> 00:14:17,376
updated before image can be


381
00:14:17,436 --> 00:14:17,986
presented.


382
00:14:19,296 --> 00:14:23,436
Once all pixels are updated,


383
00:14:23,566 --> 00:14:25,686
[inaudible] and user can see a


384
00:14:25,686 --> 00:14:26,106
frame.


385
00:14:27,246 --> 00:14:28,346
So as you can see from the


386
00:14:28,346 --> 00:14:29,846
moment the application receives


387
00:14:29,926 --> 00:14:32,036
pauses, to the moment image is


388
00:14:32,036 --> 00:14:34,476
really projected, it takes about


389
00:14:34,566 --> 00:14:35,896
25 milliseconds.


390
00:14:35,976 --> 00:14:39,556
That is why application receives


391
00:14:39,686 --> 00:14:41,346
pauses that are already


392
00:14:41,346 --> 00:14:43,296
predicted into the future, to


393
00:14:43,546 --> 00:14:45,116
the moment when photons will be


394
00:14:45,116 --> 00:14:47,806
emitted, so that the rendered


395
00:14:47,896 --> 00:14:49,546
image is matching the user


396
00:14:49,636 --> 00:14:49,946
pause.


397
00:14:50,066 --> 00:14:54,336
And this cascade of events


398
00:14:54,416 --> 00:14:56,046
overlapping with previous and


399
00:14:56,046 --> 00:14:58,486
next frame is creating our frame


400
00:14:58,516 --> 00:14:59,436
basing diagram.


401
00:14:59,436 --> 00:15:02,096
As you can see, in case of the


402
00:15:02,096 --> 00:15:04,966
single-threaded application, GPU


403
00:15:04,966 --> 00:15:06,446
is idle most of the time.


404
00:15:07,826 --> 00:15:08,816
So let's see if we can do


405
00:15:08,816 --> 00:15:10,346
anything about that.


406
00:15:11,776 --> 00:15:13,606
We are now switching to


407
00:15:13,606 --> 00:15:15,216
multi-threaded application,


408
00:15:15,716 --> 00:15:17,746
which separates simulation of


409
00:15:17,776 --> 00:15:20,416
its visual environment from


410
00:15:20,446 --> 00:15:22,656
encoding operations to the GPU.


411
00:15:23,496 --> 00:15:24,996
Encoding of those operations


412
00:15:24,996 --> 00:15:26,596
will now happen on separate


413
00:15:26,596 --> 00:15:27,356
rendering threads.


414
00:15:28,796 --> 00:15:29,766
Because we've separated


415
00:15:29,866 --> 00:15:31,216
simulation from encoding,


416
00:15:31,766 --> 00:15:33,596
simulation for our frame can


417
00:15:33,696 --> 00:15:36,786
happen in parallel to previous


418
00:15:36,786 --> 00:15:38,636
frame encoding of GPU


419
00:15:38,636 --> 00:15:39,336
operations.


420
00:15:40,636 --> 00:15:43,316
This means that encoding is now


421
00:15:43,316 --> 00:15:45,746
shifted area in time, and starts


422
00:15:45,796 --> 00:15:47,596
immediately after we receive


423
00:15:47,706 --> 00:15:48,656
predicted pauses.


424
00:15:49,216 --> 00:15:50,846
This means that your application


425
00:15:50,846 --> 00:15:52,436
will now have more time to


426
00:15:52,436 --> 00:15:54,736
encode the GPU [inaudible] and


427
00:15:54,736 --> 00:15:56,666
GPU will have more time to


428
00:15:56,746 --> 00:15:57,406
process it.


429
00:15:57,406 --> 00:15:59,166
So, as a result, your


430
00:15:59,166 --> 00:16:00,436
application can have better


431
00:16:00,436 --> 00:16:01,096
visualize.


432
00:16:02,856 --> 00:16:04,216
But there is one trick.


433
00:16:05,436 --> 00:16:07,386
Because simulation is now


434
00:16:07,386 --> 00:16:09,256
happening one frame in advance,


435
00:16:09,956 --> 00:16:12,196
it requires separate set of


436
00:16:12,286 --> 00:16:13,326
predicted pauses.


437
00:16:14,066 --> 00:16:16,586
This set is predicted 56


438
00:16:16,586 --> 00:16:18,956
milliseconds into the future so


439
00:16:18,956 --> 00:16:20,276
that it will match the set


440
00:16:20,366 --> 00:16:21,886
predicted for rendering thread


441
00:16:22,416 --> 00:16:24,176
and both will match the moment


442
00:16:24,206 --> 00:16:25,266
when photons are emitted.


443
00:16:27,126 --> 00:16:28,646
This diagram already looks good


444
00:16:28,646 --> 00:16:31,056
from CPU side, as we can see


445
00:16:31,056 --> 00:16:32,276
application is nicely


446
00:16:32,276 --> 00:16:33,566
distributing its work


447
00:16:33,566 --> 00:16:35,236
[inaudible] CPU course, but


448
00:16:36,236 --> 00:16:37,566
let's focus on GPU.


449
00:16:38,446 --> 00:16:43,596
As you can see, now our example


450
00:16:43,596 --> 00:16:46,696
application is encoding all


451
00:16:46,696 --> 00:16:48,576
these GPU [inaudible] for the


452
00:16:48,746 --> 00:16:51,286
whole frame into a single common


453
00:16:51,286 --> 00:16:54,016
buffer, so unless this common


454
00:16:54,016 --> 00:16:56,466
buffer is complete, GPU is


455
00:16:56,466 --> 00:16:57,316
waiting idle.


456
00:16:58,836 --> 00:17:00,306
But it's important to notice


457
00:17:00,546 --> 00:17:02,676
that encoding of GPU operations


458
00:17:02,676 --> 00:17:05,336
on a CPU takes much less time


459
00:17:05,586 --> 00:17:06,796
than processing of these


460
00:17:06,796 --> 00:17:08,165
operations on the GPU.


461
00:17:08,986 --> 00:17:10,336
So we can benefit from this


462
00:17:10,336 --> 00:17:13,935
fact, and split our encoding


463
00:17:13,935 --> 00:17:15,656
operation into a few common


464
00:17:15,656 --> 00:17:17,876
buffers while a few common


465
00:17:17,876 --> 00:17:19,276
buffer will be encoded very


466
00:17:19,276 --> 00:17:21,266
fast, with just few operations,


467
00:17:21,776 --> 00:17:23,715
and submitted to GPU as fast as


468
00:17:23,786 --> 00:17:24,336
possible.


469
00:17:25,626 --> 00:17:30,426
This way, now our encoding is


470
00:17:30,486 --> 00:17:33,016
processing in parallel to GPU


471
00:17:33,016 --> 00:17:34,626
already processing our frame,


472
00:17:35,086 --> 00:17:36,826
and as you can see, we've just


473
00:17:36,826 --> 00:17:39,926
extended the time when GPU is


474
00:17:40,036 --> 00:17:43,456
doing its work, and as a result,


475
00:17:43,456 --> 00:17:45,216
further increase amount of work


476
00:17:45,216 --> 00:17:46,706
that you can submit in a frame.


477
00:17:48,156 --> 00:17:49,226
Now, let's get back to our


478
00:17:49,266 --> 00:17:50,826
diagram, and see how it all


479
00:17:50,826 --> 00:17:51,906
looks together.


480
00:17:53,446 --> 00:17:55,636
So as you can see, now both CPU


481
00:17:55,636 --> 00:17:57,766
and GPU are fully utilized.


482
00:17:58,746 --> 00:18:00,056
So [inaudible] application is


483
00:18:00,056 --> 00:18:01,846
already very good example of


484
00:18:01,846 --> 00:18:03,966
your application, but there are


485
00:18:03,966 --> 00:18:05,446
still few things we can do.


486
00:18:07,196 --> 00:18:09,396
If you will notice, rendering


487
00:18:09,506 --> 00:18:11,626
thread is still waiting with


488
00:18:11,696 --> 00:18:13,786
encoding of any type of GPU work


489
00:18:14,306 --> 00:18:16,326
before it will receive predicted


490
00:18:16,456 --> 00:18:16,876
pauses.


491
00:18:17,566 --> 00:18:19,186
But not all [inaudible] in the


492
00:18:19,186 --> 00:18:21,486
frame requires those pauses.


493
00:18:22,536 --> 00:18:24,766
So let's analyze in more detail


494
00:18:24,886 --> 00:18:26,086
to pick our frame workloads.


495
00:18:27,486 --> 00:18:29,956
Here, you can see a list of


496
00:18:30,006 --> 00:18:31,896
workloads that may be executed


497
00:18:32,086 --> 00:18:32,916
in each frame.


498
00:18:34,056 --> 00:18:35,656
Part of them happen in screen


499
00:18:35,656 --> 00:18:37,696
space or require general


500
00:18:37,696 --> 00:18:39,766
knowledge about pause for which


501
00:18:39,766 --> 00:18:40,736
frame is rendered.


502
00:18:41,496 --> 00:18:42,726
We call such workloads


503
00:18:42,846 --> 00:18:44,116
pause-dependent ones.


504
00:18:44,986 --> 00:18:46,506
At the same time, there are


505
00:18:46,506 --> 00:18:48,746
workloads that are generic and


506
00:18:48,746 --> 00:18:50,056
can be executed without


507
00:18:50,096 --> 00:18:51,326
knowledge about pauses


508
00:18:51,326 --> 00:18:51,996
immediately.


509
00:18:53,096 --> 00:18:54,886
We call those workloads pause


510
00:18:54,886 --> 00:18:55,886
independent ones.


511
00:18:56,496 --> 00:18:59,306
So currently, our application


512
00:18:59,306 --> 00:19:01,346
was waiting for pauses to encode


513
00:19:01,346 --> 00:19:02,996
any type of work to GPU.


514
00:19:03,626 --> 00:19:05,336
But if we split those workloads


515
00:19:05,336 --> 00:19:08,636
in half, we can encode pause


516
00:19:08,636 --> 00:19:09,676
independent workloads


517
00:19:09,766 --> 00:19:11,936
immediately and then wait for


518
00:19:12,026 --> 00:19:14,046
pauses to continue with encoding


519
00:19:14,146 --> 00:19:15,156
pause-dependent ones.


520
00:19:16,126 --> 00:19:19,456
In this slide, we've already


521
00:19:19,456 --> 00:19:21,076
separated pause independent


522
00:19:21,076 --> 00:19:23,006
workloads from pause dependent


523
00:19:23,036 --> 00:19:23,346
ones.


524
00:19:24,516 --> 00:19:26,096
Pause independent workloads is


525
00:19:26,096 --> 00:19:27,346
now encoded in [inaudible]


526
00:19:27,496 --> 00:19:29,096
common buffer, and is marked


527
00:19:29,446 --> 00:19:31,066
with a little bit darker shade


528
00:19:31,276 --> 00:19:32,746
than pause-dependent workload


529
00:19:32,746 --> 00:19:33,446
following it.


530
00:19:34,336 --> 00:19:35,666
Because pause-independent


531
00:19:35,666 --> 00:19:36,886
workload can be encoded


532
00:19:36,886 --> 00:19:39,146
immediately, we will do exactly


533
00:19:39,146 --> 00:19:39,376
that.


534
00:19:39,966 --> 00:19:41,526
We will encode it as soon as the


535
00:19:41,636 --> 00:19:42,796
previous frame workload is


536
00:19:42,796 --> 00:19:43,286
encoded.


537
00:19:44,766 --> 00:19:46,866
This gives CPU more time to


538
00:19:46,866 --> 00:19:49,246
encode the GPU work, and what is


539
00:19:49,416 --> 00:19:51,646
even more important, it ensures


540
00:19:51,646 --> 00:19:54,196
us that this GPU work is already


541
00:19:54,196 --> 00:19:56,556
waiting for being executed on


542
00:19:56,556 --> 00:19:59,336
GPU so there will be exactly no


543
00:19:59,336 --> 00:20:00,576
idle time on GPU.


544
00:20:00,636 --> 00:20:01,846
As soon as previous frame is


545
00:20:01,846 --> 00:20:04,086
finished, GPU can start with the


546
00:20:04,626 --> 00:20:06,766
next one.


547
00:20:06,896 --> 00:20:09,376
The last subsection is a


548
00:20:09,376 --> 00:20:11,306
multi-GPU workload distribution.


549
00:20:13,226 --> 00:20:15,666
We can scale our workload across


550
00:20:15,666 --> 00:20:16,386
multiple GPUs.


551
00:20:16,386 --> 00:20:19,226
Current Mac Book Pro has two GPU


552
00:20:19,306 --> 00:20:21,166
on board, and while they have


553
00:20:21,166 --> 00:20:22,056
different performance


554
00:20:22,116 --> 00:20:23,616
characteristics, there is


555
00:20:23,616 --> 00:20:25,126
nothing preventing us from using


556
00:20:25,126 --> 00:20:25,366
them.


557
00:20:26,036 --> 00:20:27,926
Similarly, if each GPU is


558
00:20:27,986 --> 00:20:29,906
connected, application can use


559
00:20:29,906 --> 00:20:31,756
it for rendering to the headset


560
00:20:32,096 --> 00:20:34,866
while using Mac's primary GPU to


561
00:20:34,896 --> 00:20:35,936
offload some work.


562
00:20:39,066 --> 00:20:40,516
So we've just separated


563
00:20:40,666 --> 00:20:43,286
pause-independent work and moved


564
00:20:43,566 --> 00:20:45,256
it to a secondary GPU.


565
00:20:45,796 --> 00:20:48,436
We could do that because it was


566
00:20:48,436 --> 00:20:52,046
already encoded much earlier in


567
00:20:52,046 --> 00:20:53,576
our frame, and now this


568
00:20:53,666 --> 00:20:55,866
pause-independent workload is


569
00:20:55,866 --> 00:20:57,516
executing in parallel to


570
00:20:57,616 --> 00:20:58,986
pause-dependent workload of


571
00:20:59,066 --> 00:20:59,746
previous frame.


572
00:21:00,196 --> 00:21:01,706
As a result, we further


573
00:21:01,706 --> 00:21:03,596
increased the amount of GPU time


574
00:21:03,676 --> 00:21:04,916
that you had for your frame.


575
00:21:07,546 --> 00:21:09,856
But, by splitting this work into


576
00:21:09,856 --> 00:21:13,146
multiple GPUs, we now get to the


577
00:21:13,256 --> 00:21:14,926
point where we need a way to


578
00:21:14,926 --> 00:21:16,886
synchronize those workloads with


579
00:21:16,886 --> 00:21:17,446
each other.


580
00:21:22,676 --> 00:21:24,426
So today we introduce new


581
00:21:24,426 --> 00:21:26,006
synchronization parameters to


582
00:21:26,066 --> 00:21:27,316
deal exactly with such


583
00:21:27,316 --> 00:21:28,086
situation.


584
00:21:28,816 --> 00:21:31,196
MTL Events can now be used to


585
00:21:31,196 --> 00:21:33,616
synchronize GPU work in scope of


586
00:21:33,616 --> 00:21:35,566
single GPU across different


587
00:21:35,626 --> 00:21:38,826
Metal cues and MTL Shared Events


588
00:21:39,276 --> 00:21:41,436
extends this functionality by


589
00:21:41,436 --> 00:21:42,886
allowing it to synchronize


590
00:21:42,886 --> 00:21:44,816
workloads across different GPUs


591
00:21:44,986 --> 00:21:46,176
and even across different


592
00:21:46,236 --> 00:21:46,796
processes.


593
00:21:49,136 --> 00:21:50,996
So here we will go through the


594
00:21:51,106 --> 00:21:52,426
simple code example.


595
00:21:53,546 --> 00:21:55,406
We have our Mac, with attached


596
00:21:55,406 --> 00:21:57,486
eGPU through Thunderbolt 3


597
00:21:57,486 --> 00:21:58,166
connection.


598
00:21:58,266 --> 00:22:00,936
This eGPU will be our primary


599
00:22:00,936 --> 00:22:04,396
GPU driving the headset, so we


600
00:22:04,396 --> 00:22:06,576
can use GPU that is already in


601
00:22:06,576 --> 00:22:08,676
our Mac as secondary supporting


602
00:22:08,676 --> 00:22:09,136
GPU.


603
00:22:10,496 --> 00:22:12,766
And we will use shared event to


604
00:22:12,766 --> 00:22:16,896
synchronize workloads of both


605
00:22:18,486 --> 00:22:18,716
GPUs.


606
00:22:19,206 --> 00:22:21,816
Event initial value is zero, so


607
00:22:21,816 --> 00:22:22,996
it's important to start


608
00:22:22,996 --> 00:22:25,526
synchronization counter from 1.


609
00:22:26,176 --> 00:22:27,516
That's because when we would


610
00:22:27,516 --> 00:22:30,096
wait on just initialized event,


611
00:22:30,276 --> 00:22:32,476
its counter of zero will cause


612
00:22:32,476 --> 00:22:34,696
it to return immediately, so


613
00:22:34,696 --> 00:22:35,126
there would be no


614
00:22:35,126 --> 00:22:35,976
synchronization.


615
00:22:37,786 --> 00:22:39,276
So our rendering thread now


616
00:22:39,276 --> 00:22:41,826
starts encoding work for our


617
00:22:41,826 --> 00:22:43,836
supporting GPU immediately.


618
00:22:44,536 --> 00:22:46,426
It will encode pause-independent


619
00:22:46,426 --> 00:22:48,796
work that will happen on our


620
00:22:48,796 --> 00:22:50,996
supporting GPU course, and once


621
00:22:51,026 --> 00:22:53,426
this work is complete, its


622
00:22:53,426 --> 00:22:55,496
results will be stored in locker


623
00:22:55,646 --> 00:22:56,156
memory.


624
00:22:56,796 --> 00:22:59,146
That's why we follow with


625
00:22:59,146 --> 00:23:01,396
encoding brief operation that


626
00:23:01,396 --> 00:23:03,736
will transfer those results to


627
00:23:03,736 --> 00:23:05,806
system memory that is visible by


628
00:23:05,806 --> 00:23:06,676
both GPUs.


629
00:23:07,426 --> 00:23:08,786
And once this transfer is


630
00:23:08,786 --> 00:23:12,436
complete, our supporting GPU can


631
00:23:12,906 --> 00:23:15,216
safely signal our shared event.


632
00:23:15,866 --> 00:23:18,256
This signal will tell eGPU that


633
00:23:18,316 --> 00:23:19,846
now it's safe to take those


634
00:23:19,846 --> 00:23:20,236
results.


635
00:23:21,496 --> 00:23:23,536
So our rendering thread


636
00:23:23,536 --> 00:23:24,576
committed this [inaudible]


637
00:23:24,576 --> 00:23:26,886
common buffer, and supporting


638
00:23:26,916 --> 00:23:28,946
GPU is already processing its


639
00:23:28,946 --> 00:23:29,246
work.


640
00:23:30,016 --> 00:23:32,336
At the same time, we can start


641
00:23:32,336 --> 00:23:34,086
encoding command buffer for a


642
00:23:34,166 --> 00:23:36,026
primary GPU that is driving the


643
00:23:36,076 --> 00:23:36,486
headset.


644
00:23:37,676 --> 00:23:39,356
In this command buffer, we will


645
00:23:39,356 --> 00:23:41,626
start by waiting for our shared


646
00:23:41,626 --> 00:23:43,446
event to be sure that the data


647
00:23:43,446 --> 00:23:45,516
is in system memory, and once


648
00:23:45,516 --> 00:23:47,426
it's there, and the shared event


649
00:23:47,426 --> 00:23:50,066
is signaled, we can perform a


650
00:23:50,066 --> 00:23:51,306
brief operation that will


651
00:23:51,306 --> 00:23:52,516
transfer this data through


652
00:23:52,516 --> 00:23:54,826
Thunderbolt 3 connection, back


653
00:23:55,096 --> 00:23:57,856
to our [inaudible] GPU and once


654
00:23:57,956 --> 00:23:59,856
this transfer is complete, it's


655
00:23:59,856 --> 00:24:01,936
safe to perform pause-dependent


656
00:24:01,966 --> 00:24:05,246
work, so a second command buffer


657
00:24:05,556 --> 00:24:08,206
will signal lockout event to let


658
00:24:08,356 --> 00:24:10,166
pause-dependent work know that


659
00:24:10,166 --> 00:24:11,546
it can start executing.


660
00:24:12,366 --> 00:24:14,346
After encoding and submitting


661
00:24:14,416 --> 00:24:15,776
those two command buffers,


662
00:24:16,386 --> 00:24:18,436
rendering thread can continue as


663
00:24:18,436 --> 00:24:20,566
usual, with waiting for pauses,


664
00:24:20,816 --> 00:24:21,826
and later encoding


665
00:24:21,916 --> 00:24:26,076
pause-dependent work.


666
00:24:26,296 --> 00:24:28,296
So now we have a mechanism to


667
00:24:28,296 --> 00:24:30,306
synchronize different workloads


668
00:24:30,306 --> 00:24:31,146
between different GPUs.


669
00:24:31,146 --> 00:24:35,156
But as you can see, our


670
00:24:35,156 --> 00:24:37,426
secondary GPU is still a little


671
00:24:37,426 --> 00:24:38,176
bit idle.


672
00:24:38,226 --> 00:24:40,506
That's because in this example


673
00:24:40,926 --> 00:24:43,886
we decided to push through it,


674
00:24:44,316 --> 00:24:45,846
pause dependent workloads that


675
00:24:45,846 --> 00:24:47,396
have dependency with pause


676
00:24:47,396 --> 00:24:48,856
dependent ones.


677
00:24:49,346 --> 00:24:49,786
Excuse me.


678
00:24:50,796 --> 00:24:52,156
But of course there are types of


679
00:24:52,206 --> 00:24:53,396
workloads that have no


680
00:24:53,396 --> 00:24:55,086
dependencies, and they can


681
00:24:55,216 --> 00:24:57,496
happen at lower frequencies, the


682
00:24:57,496 --> 00:24:58,806
frequency of the headset.


683
00:24:59,576 --> 00:25:01,076
One example of such workloads


684
00:25:01,196 --> 00:25:03,456
can be, for example, simulation


685
00:25:03,456 --> 00:25:05,076
of physically based accurate


686
00:25:05,076 --> 00:25:07,076
[inaudible] or anything else


687
00:25:07,766 --> 00:25:11,236
that requires a lot of time to


688
00:25:11,976 --> 00:25:12,826
be updated.


689
00:25:13,326 --> 00:25:15,846
Such workload can happen in the


690
00:25:15,846 --> 00:25:17,316
background, completely


691
00:25:17,316 --> 00:25:18,846
asynchronously from rendering


692
00:25:18,846 --> 00:25:20,596
frames, and each time it's


693
00:25:20,596 --> 00:25:23,336
ready, its results will be sent


694
00:25:23,456 --> 00:25:24,536
to primary GPU.


695
00:25:25,316 --> 00:25:27,296
It's marked here with gray color


696
00:25:27,626 --> 00:25:28,996
to indicate that it's not


697
00:25:28,996 --> 00:25:31,036
related to any particular frame.


698
00:25:32,516 --> 00:25:33,616
So, of course there are


699
00:25:33,616 --> 00:25:36,016
different GPUs with different


700
00:25:36,066 --> 00:25:38,086
performance characteristics, and


701
00:25:38,086 --> 00:25:38,976
they will have different


702
00:25:38,976 --> 00:25:40,146
bandwidth connections.


703
00:25:40,606 --> 00:25:42,966
And your application will have


704
00:25:43,086 --> 00:25:44,976
different workloads in a single


705
00:25:44,976 --> 00:25:47,096
frame with different relations


706
00:25:47,176 --> 00:25:47,876
between them.


707
00:25:48,986 --> 00:25:51,676
So you will need to design a way


708
00:25:51,786 --> 00:25:53,446
to distribute this workload on


709
00:25:53,446 --> 00:25:56,406
your own, but saying all that,


710
00:25:56,406 --> 00:25:58,166
it's important to start thinking


711
00:25:58,166 --> 00:26:00,076
about this GPU workload


712
00:26:00,116 --> 00:26:02,106
distribution, as multi-GPU


713
00:26:02,106 --> 00:26:04,026
configuration are becoming


714
00:26:04,136 --> 00:26:05,956
common on Apple platforms.


715
00:26:07,956 --> 00:26:10,266
So let's summarize everything


716
00:26:10,266 --> 00:26:11,326
that we've learned in this


717
00:26:11,416 --> 00:26:11,866
section.


718
00:26:12,496 --> 00:26:13,426
We showed multi-thread


719
00:26:13,426 --> 00:26:15,986
application to take full benefit


720
00:26:16,076 --> 00:26:17,396
of all CPU codes.


721
00:26:17,466 --> 00:26:20,556
And split your command buffers,


722
00:26:20,786 --> 00:26:22,666
to ensure that GPU is not idle.


723
00:26:23,756 --> 00:26:25,776
When doing that, if possible,


724
00:26:26,066 --> 00:26:26,916
try to separate


725
00:26:27,046 --> 00:26:28,396
pause-independent from


726
00:26:28,466 --> 00:26:31,036
pause-dependent workloads, to be


727
00:26:31,036 --> 00:26:32,946
able to encode this work as soon


728
00:26:32,946 --> 00:26:35,746
as possible, and even further,


729
00:26:36,206 --> 00:26:38,086
splitting workloads by frequency


730
00:26:38,086 --> 00:26:41,126
of update so if your application


731
00:26:41,126 --> 00:26:42,586
will execute on multi-GPU


732
00:26:42,586 --> 00:26:44,486
configuration, you can easily


733
00:26:44,486 --> 00:26:47,576
distribute it across those GPUs.


734
00:26:48,126 --> 00:26:50,726
And while doing that, ensure


735
00:26:51,136 --> 00:26:53,296
that you drive each GPU with


736
00:26:53,296 --> 00:26:55,566
separate rendering threads to


737
00:26:55,566 --> 00:26:57,446
ensure that they all execute


738
00:26:57,446 --> 00:26:58,206
asynchronously.


739
00:26:59,116 --> 00:27:01,126
Now, you switch to reducing fill


740
00:27:01,666 --> 00:27:01,766
rate.


741
00:27:03,186 --> 00:27:04,986
Vive Pro introduces new


742
00:27:05,046 --> 00:27:06,836
challenges for VR application


743
00:27:06,836 --> 00:27:07,466
developers.


744
00:27:08,026 --> 00:27:09,516
To better understand scale of


745
00:27:09,516 --> 00:27:11,186
the problem, we will compare


746
00:27:11,186 --> 00:27:12,676
different medium fill rates.


747
00:27:12,676 --> 00:27:15,756
So, for example, application


748
00:27:15,756 --> 00:27:18,556
rendering in default scaling


749
00:27:18,616 --> 00:27:21,966
rate to Vive headset, produces


750
00:27:22,266 --> 00:27:27,776
436 megapixels per second.


751
00:27:27,776 --> 00:27:29,766
And most advanced [inaudible]


752
00:27:30,236 --> 00:27:32,776
against that [inaudible] HD TVs


753
00:27:32,776 --> 00:27:39,546
have fill rate of 475 megapixels


754
00:27:39,606 --> 00:27:40,156
per second.


755
00:27:40,916 --> 00:27:43,806
Those numbers are already so big


756
00:27:44,136 --> 00:27:45,416
that game developers use


757
00:27:45,476 --> 00:27:47,056
different tweaks to reduce this


758
00:27:47,056 --> 00:27:47,486
fill rate.


759
00:27:48,246 --> 00:27:49,856
Now, let's see how Vive Pro


760
00:27:49,856 --> 00:27:51,226
compares to those numbers.


761
00:27:53,046 --> 00:27:55,116
Vive Pro has a normal fill rate


762
00:27:55,116 --> 00:27:58,676
of 775 megapixels per second,


763
00:27:59,146 --> 00:28:00,826
and if you add to that four


764
00:28:00,826 --> 00:28:02,976
times multi-sampling [inaudible]


765
00:28:03,206 --> 00:28:05,186
or bigger scaling rate, this


766
00:28:05,186 --> 00:28:06,486
number will grow even more.


767
00:28:06,486 --> 00:28:09,786
That is why reducing fill rate


768
00:28:09,786 --> 00:28:11,206
is so important.


769
00:28:12,146 --> 00:28:13,406
There are multiple techniques


770
00:28:13,406 --> 00:28:14,956
there and new techniques are


771
00:28:14,956 --> 00:28:15,826
made every day.


772
00:28:16,316 --> 00:28:19,046
So I encourage you to try them


773
00:28:19,046 --> 00:28:22,106
all, but today we will focus


774
00:28:22,106 --> 00:28:24,506
only on a few still as they are


775
00:28:24,506 --> 00:28:27,746
the simplest to implement and


776
00:28:27,746 --> 00:28:29,836
bring nice performance gains.


777
00:28:30,356 --> 00:28:31,986
So we will start with clipping


778
00:28:31,986 --> 00:28:33,056
invisible pixels.


779
00:28:33,946 --> 00:28:36,186
Here, you can see image rendered


780
00:28:36,186 --> 00:28:37,076
for left eye.


781
00:28:38,526 --> 00:28:40,566
But due to the nature of the


782
00:28:40,566 --> 00:28:44,766
lens work, about 20% of those


783
00:28:44,896 --> 00:28:47,646
pixels are lost after compositor


784
00:28:47,726 --> 00:28:49,506
performs its distortion


785
00:28:49,506 --> 00:28:50,156
correction.


786
00:28:50,786 --> 00:28:52,166
So on the right, you can see


787
00:28:52,216 --> 00:28:53,796
image that will be displayed on


788
00:28:53,796 --> 00:28:55,746
a panel in a headset before it


789
00:28:55,746 --> 00:28:56,706
goes through the lens.


790
00:28:58,746 --> 00:29:00,896
So, the simplest way to reduce


791
00:29:00,896 --> 00:29:03,836
our fill rate is to prevent our


792
00:29:03,836 --> 00:29:05,426
application from rendering those


793
00:29:05,486 --> 00:29:07,066
pixels that won't be visible


794
00:29:07,066 --> 00:29:09,616
anyway, and you can do that


795
00:29:09,616 --> 00:29:12,176
easily by using SteamVR Stencil


796
00:29:12,176 --> 00:29:12,946
Mask.


797
00:29:14,416 --> 00:29:18,836
So we've just saved 20% of our


798
00:29:18,836 --> 00:29:20,406
fill rate by applying this


799
00:29:20,406 --> 00:29:22,626
simple mask, and reduce our Vive


800
00:29:22,966 --> 00:29:26,806
Pro fill rate to 620 megapixels.


801
00:29:29,286 --> 00:29:32,236
Now, we will analyze implication


802
00:29:32,276 --> 00:29:33,776
of this lens distortion


803
00:29:33,776 --> 00:29:35,326
correction in more detail.


804
00:29:36,336 --> 00:29:40,836
We will divide our field of view


805
00:29:40,836 --> 00:29:42,936
into nine sections.


806
00:29:43,886 --> 00:29:45,986
Central section has field of


807
00:29:45,986 --> 00:29:48,396
view of 80 degrees horizontally


808
00:29:48,456 --> 00:29:51,276
by 80 degrees vertically, and we


809
00:29:51,276 --> 00:29:53,206
have surrounding sections on the


810
00:29:53,266 --> 00:29:54,396
edges and corners.


811
00:29:55,306 --> 00:29:57,596
We've color tinted them to


812
00:29:57,596 --> 00:29:58,956
better visualize the


813
00:29:58,956 --> 00:30:00,726
contribution to final image.


814
00:30:01,306 --> 00:30:05,196
So as you can see, corners are


815
00:30:05,196 --> 00:30:08,236
almost completely invisible and


816
00:30:08,506 --> 00:30:11,786
edges have matched less


817
00:30:11,786 --> 00:30:13,596
contribution to the image than


818
00:30:13,626 --> 00:30:14,696
in the original one.


819
00:30:15,486 --> 00:30:17,996
In fact, if you see this image


820
00:30:17,996 --> 00:30:19,996
in the headset, you wouldn't be


821
00:30:20,066 --> 00:30:22,616
able to look directly at the red


822
00:30:22,726 --> 00:30:23,346
sections.


823
00:30:24,126 --> 00:30:25,846
The only way to see them would


824
00:30:25,846 --> 00:30:27,366
be with your peripheral vision.


825
00:30:28,886 --> 00:30:31,666
So this gives us great hint.


826
00:30:33,306 --> 00:30:35,186
We can render those edge and


827
00:30:35,236 --> 00:30:37,626
corner sections and a reduced


828
00:30:37,626 --> 00:30:40,086
fill rate, as they are mostly


829
00:30:40,086 --> 00:30:41,046
invisible anyway.


830
00:30:42,276 --> 00:30:45,056
We render the central section as


831
00:30:45,056 --> 00:30:45,956
we did before.


832
00:30:47,296 --> 00:30:49,046
But then we will render vertical


833
00:30:49,046 --> 00:30:51,856
edges with half of the width and


834
00:30:51,906 --> 00:30:53,946
horizontal sections with half of


835
00:30:53,976 --> 00:30:54,686
the height.


836
00:30:54,686 --> 00:30:56,966
And finally, we will render


837
00:30:57,026 --> 00:31:00,306
corner edges at one-fourth of


838
00:31:00,306 --> 00:31:01,066
the resolution.


839
00:31:03,076 --> 00:31:05,166
Once our expensive rendering


840
00:31:05,246 --> 00:31:06,976
pass is complete, we will


841
00:31:06,976 --> 00:31:09,606
perform cheap upscaling pass


842
00:31:09,606 --> 00:31:11,416
that will stretch those regions


843
00:31:11,646 --> 00:31:13,276
back to the resolution at which


844
00:31:13,276 --> 00:31:14,616
they need to be submitted to


845
00:31:14,616 --> 00:31:15,356
compositor.


846
00:31:16,386 --> 00:31:18,186
So you are wondering how much


847
00:31:18,226 --> 00:31:20,166
we've gained by doing that.


848
00:31:21,316 --> 00:31:23,576
In case of 80 by 80 degree


849
00:31:23,576 --> 00:31:25,546
central region, we reduced our


850
00:31:25,546 --> 00:31:27,556
fill rate all the way down to


851
00:31:27,556 --> 00:31:30,586
491 megapixels per second.


852
00:31:31,736 --> 00:31:32,936
But you remember that we just


853
00:31:32,996 --> 00:31:34,666
talked about clipping invisible


854
00:31:34,666 --> 00:31:36,866
pixels, so let's combine those


855
00:31:36,946 --> 00:31:38,036
two techniques together.


856
00:31:40,016 --> 00:31:42,456
By clipping pixels combined with


857
00:31:42,456 --> 00:31:44,016
multi-resolution shading, you


858
00:31:44,016 --> 00:31:45,696
can reduce your fill rate even


859
00:31:45,696 --> 00:31:48,796
further to 456 megapixels per


860
00:31:48,796 --> 00:31:50,636
second, and that is not a random


861
00:31:50,636 --> 00:31:50,996
number.


862
00:31:51,696 --> 00:31:54,196
In fact, that's a default fill


863
00:31:54,196 --> 00:31:57,456
rate of Vive headset, so by just


864
00:31:57,456 --> 00:31:59,176
using those two optimization


865
00:31:59,176 --> 00:32:01,246
techniques, your application can


866
00:32:01,246 --> 00:32:03,536
render to Vive Pro with much


867
00:32:03,536 --> 00:32:05,796
higher resolution using exactly


868
00:32:05,796 --> 00:32:08,386
the same GPU as it did when


869
00:32:08,386 --> 00:32:09,796
rendering to Vive headset.


870
00:32:10,816 --> 00:32:12,486
Of course, you can use those


871
00:32:12,556 --> 00:32:13,626
techniques when rendering to


872
00:32:13,626 --> 00:32:15,346
Vive as well, which will allow


873
00:32:15,346 --> 00:32:17,306
you to bring visualize of your


874
00:32:17,566 --> 00:32:19,386
application even further and


875
00:32:20,106 --> 00:32:21,046
make it prettier.


876
00:32:21,626 --> 00:32:25,306
There is one caveat here.


877
00:32:25,676 --> 00:32:27,166
Multi-resolution shading


878
00:32:27,296 --> 00:32:30,356
requires few render passes, so


879
00:32:30,356 --> 00:32:34,006
it will increase your workload


880
00:32:34,126 --> 00:32:36,356
on geometric [inaudible], but


881
00:32:36,356 --> 00:32:38,466
you can easily mitigate that by


882
00:32:38,516 --> 00:32:39,996
just reducing your central


883
00:32:39,996 --> 00:32:41,746
vision by a few degrees.


884
00:32:42,556 --> 00:32:44,036
Here, by just reducing our


885
00:32:44,036 --> 00:32:45,996
central vision by 10 degrees,


886
00:32:46,486 --> 00:32:48,026
we've reduced fill rate all the


887
00:32:48,026 --> 00:32:50,346
way to 382 megapixels per


888
00:32:50,346 --> 00:32:50,826
second.


889
00:32:51,546 --> 00:32:53,956
And if your geometry workload is


890
00:32:53,956 --> 00:32:56,606
really high, you can go further,


891
00:32:56,696 --> 00:32:59,146
and experiment with lower fill


892
00:32:59,146 --> 00:33:02,116
rate, lower regions, that will


893
00:33:02,446 --> 00:33:03,966
reduce fill rate even more.


894
00:33:04,846 --> 00:33:07,206
In case of 55 by 55 degrees


895
00:33:07,206 --> 00:33:10,446
central region, 80% of your


896
00:33:11,266 --> 00:33:12,536
[inaudible] eye movement will be


897
00:33:12,536 --> 00:33:15,546
still inside this region, but


898
00:33:15,546 --> 00:33:17,426
we've reduced our fill rate by


899
00:33:17,426 --> 00:33:20,016
more than half, to 360


900
00:33:20,146 --> 00:33:23,476
megapixels per second.


901
00:33:23,696 --> 00:33:25,116
So of course there are different


902
00:33:25,116 --> 00:33:26,316
ways to implement


903
00:33:26,366 --> 00:33:29,446
multi-resolution shading.


904
00:33:30,466 --> 00:33:32,326
And you will get different


905
00:33:32,416 --> 00:33:34,626
performance gains from that.


906
00:33:34,816 --> 00:33:36,366
So I encourage you to experiment


907
00:33:36,366 --> 00:33:37,966
with this technique and try what


908
00:33:38,006 --> 00:33:39,446
will work for you best.


909
00:33:41,546 --> 00:33:42,926
So let's summarize everything


910
00:33:42,926 --> 00:33:44,316
that we've learned during this


911
00:33:46,536 --> 00:33:46,986
session.


912
00:33:47,086 --> 00:33:48,676
We've just announced plug and


913
00:33:48,716 --> 00:33:50,446
play support for Vive Pro


914
00:33:50,486 --> 00:33:53,526
Headsets, and introduced new


915
00:33:53,526 --> 00:33:55,256
Metal 2 features that allow you


916
00:33:55,256 --> 00:33:57,306
now to develop even more


917
00:33:57,306 --> 00:33:58,916
advanced VR applications.


918
00:33:59,726 --> 00:34:01,486
And I encourage you to take


919
00:34:01,486 --> 00:34:03,246
advantage of multi-GPU


920
00:34:03,246 --> 00:34:05,006
configurations, as they are


921
00:34:05,006 --> 00:34:06,726
becoming common on other


922
00:34:06,816 --> 00:34:07,416
platforms.


923
00:34:08,076 --> 00:34:12,136
You can learn more about this


924
00:34:12,136 --> 00:34:14,106
session from this link, and I


925
00:34:14,106 --> 00:34:16,335
would like to invite all of you


926
00:34:16,525 --> 00:34:17,606
to meet with me and my


927
00:34:17,606 --> 00:34:19,616
colleagues during Metal 4 VR


928
00:34:19,616 --> 00:34:21,766
Lab, that will take place today


929
00:34:21,766 --> 00:34:24,136
at 12:00 p.m. in Technology Lab


930
00:34:24,136 --> 00:34:24,536
6.


931
00:34:24,626 --> 00:34:26,976
Thank you very much.


932
00:34:27,516 --> 00:34:30,500
[ Applause ]

