1
00:00:07,516 --> 00:00:19,286
[ Music ]


2
00:00:19,786 --> 00:00:22,776
>> Hi. I'm Henry Mason,
an engineer working


3
00:00:22,776 --> 00:00:24,366
on speech recognition for Siri.


4
00:00:25,486 --> 00:00:28,306
Today we're incredibly excited
to announce a brand new API


5
00:00:29,006 --> 00:00:31,376
which will let our speech
recognition solve problems


6
00:00:31,376 --> 00:00:32,386
for your apps too.


7
00:00:35,246 --> 00:00:37,636
A quick overview of what
speech recognition is.


8
00:00:38,286 --> 00:00:40,616
Speech recognition is
the automatic process


9
00:00:40,616 --> 00:00:43,486
of converting audio of
human speech into text.


10
00:00:44,226 --> 00:00:46,276
It depends on the
language of the speech.


11
00:00:46,536 --> 00:00:47,976
English will be recognized
differently


12
00:00:47,976 --> 00:00:49,136
than Chinese, for example.


13
00:00:50,196 --> 00:00:52,346
On iOS, most people
think of Siri


14
00:00:52,506 --> 00:00:55,376
but speech recognition is also
useful for many other tasks.


15
00:00:56,456 --> 00:00:58,776
Since Siri was released
with iPhone 4S,


16
00:00:59,166 --> 00:01:01,606
iOS has also featured
keyboard dictation.


17
00:01:02,776 --> 00:01:04,176
That little microphone
button next


18
00:01:04,176 --> 00:01:07,826
to your iOS keyboard spacebar
triggers speech recognition


19
00:01:07,826 --> 00:01:10,076
for any UI kit text input.


20
00:01:11,116 --> 00:01:13,726
Tens of thousands of apps
every day use this feature.


21
00:01:14,266 --> 00:01:16,176
In fact, about a third
of all requests come


22
00:01:16,176 --> 00:01:17,286
from third party apps.


23
00:01:18,356 --> 00:01:19,896
It's extremely easy to use.


24
00:01:20,236 --> 00:01:23,356
It handles audio recording
and recording interruption.


25
00:01:23,476 --> 00:01:25,216
It displays a user interface.


26
00:01:25,686 --> 00:01:28,106
It doesn't require you to write
anymore code than you would


27
00:01:28,106 --> 00:01:29,686
to support any text input.


28
00:01:30,666 --> 00:01:32,446
And it's been available
since iOS 5.


29
00:01:36,026 --> 00:01:38,326
But with its simplicity
comes many limitations.


30
00:01:40,116 --> 00:01:42,376
It often doesn't make sense
for your user interface


31
00:01:42,376 --> 00:01:43,576
to require a keyboard.


32
00:01:44,526 --> 00:01:47,146
You can't control when the
audio recording starts.


33
00:01:47,736 --> 00:01:49,876
There's no control over
which language is used.


34
00:01:49,996 --> 00:01:52,386
It just happens to use the
system's keyboard language.


35
00:01:53,166 --> 00:01:54,506
There isn't even a way to know


36
00:01:54,506 --> 00:01:56,136
if the dictation
button is available.


37
00:01:58,176 --> 00:02:00,596
The default audio recording
might not make sense


38
00:02:00,596 --> 00:02:03,596
for your use case and you
may want more information


39
00:02:03,596 --> 00:02:06,726
than just text.


40
00:02:07,166 --> 00:02:09,186
So now in iOS 10,


41
00:02:09,186 --> 00:02:11,346
we're introducing a
new speech framework.


42
00:02:14,286 --> 00:02:17,246
It uses the same underlying
technology that we use


43
00:02:17,296 --> 00:02:18,536
in Siri and Dictation.


44
00:02:19,576 --> 00:02:21,606
It provides fast
and accurate results


45
00:02:21,936 --> 00:02:24,166
which are transparently
customized to the user


46
00:02:24,666 --> 00:02:26,596
without you having to
collect any user data.


47
00:02:29,106 --> 00:02:31,576
The framework also
provides more information


48
00:02:33,356 --> 00:02:35,136
about recognition
than just text.


49
00:02:36,416 --> 00:02:39,066
For example, we also provide
alternative interpretations


50
00:02:39,066 --> 00:02:41,666
of what your users might
have said, confidence levels,


51
00:02:41,666 --> 00:02:42,866
and timing information.


52
00:02:44,166 --> 00:02:46,316
Audio for the API
can be provided


53
00:02:46,316 --> 00:02:47,856
from either pre-recorded files


54
00:02:47,966 --> 00:02:49,606
or a live source
like a microphone.


55
00:02:52,086 --> 00:02:55,616
iOS 10 supports over 50
languages and dialects


56
00:02:55,616 --> 00:02:56,976
from Arabic to Vietnamese.


57
00:02:57,206 --> 00:03:01,256
Any device which runs
iOS 10 is supported.


58
00:03:03,006 --> 00:03:06,326
The speech recognition API
typically does its heavy lifting


59
00:03:06,326 --> 00:03:09,376
on our big servers which
requires an internet connection.


60
00:03:10,296 --> 00:03:14,976
However, some newer devices do
support speech recognition all


61
00:03:14,976 --> 00:03:15,476
the time.


62
00:03:16,386 --> 00:03:18,906
We provide an availability
API to determine


63
00:03:18,906 --> 00:03:21,506
if a given language is
available at the moment.


64
00:03:21,506 --> 00:03:23,056
Use this rather than looking


65
00:03:23,056 --> 00:03:25,226
for internet connectivity
explicitly.


66
00:03:28,356 --> 00:03:30,856
Since speech recognition
requires transmitting the user's


67
00:03:30,856 --> 00:03:32,096
audio over the internet,


68
00:03:32,716 --> 00:03:34,796
the user must explicitly
provide permission


69
00:03:34,796 --> 00:03:37,766
to your application before
speech recognition can be used.


70
00:03:40,846 --> 00:03:42,196
There are four major steps


71
00:03:42,196 --> 00:03:44,206
to adopting speech
recognition in your app.


72
00:03:46,726 --> 00:03:48,756
First, provide a
usage description


73
00:03:48,756 --> 00:03:50,116
in your app's info.plist.


74
00:03:51,606 --> 00:03:54,636
For example, your
camera app, Phromage,


75
00:03:55,136 --> 00:03:57,306
might have used a
usage description


76
00:03:57,506 --> 00:04:01,536
for speech recognition
of this will allow you


77
00:04:01,736 --> 00:04:03,936
to take a photo just
by saying cheese.


78
00:04:05,966 --> 00:04:09,546
Second, request authorization
using the request authorization


79
00:04:09,546 --> 00:04:10,236
class method.


80
00:04:11,536 --> 00:04:14,146
The explanation you provided
earlier will be presented


81
00:04:14,426 --> 00:04:16,596
to the user in a
familiar dialogue.


82
00:04:16,596 --> 00:04:19,346
And the user will be able
to decide if they want


83
00:04:19,346 --> 00:04:21,495
to provide your app
to speech recognition.


84
00:04:23,196 --> 00:04:25,336
Next create a speech
recognition request.


85
00:04:27,126 --> 00:04:29,156
If you already have
recorded audio file,


86
00:04:29,636 --> 00:04:32,606
use the
SFSpeechURLRecognitionRequest


87
00:04:32,606 --> 00:04:33,086
class.


88
00:04:34,066 --> 00:04:34,946
Otherwise, you'll want


89
00:04:34,946 --> 00:04:38,476
to use the SFSpeechAudioBuffer
RecognitionRequest class.


90
00:04:40,836 --> 00:04:42,806
Finally hand the
recognition request


91
00:04:42,806 --> 00:04:45,446
to an SFSpeech recognizer
to begin recognition.


92
00:04:46,456 --> 00:04:49,066
You can optionally hold onto
the returned recognition task


93
00:04:49,336 --> 00:04:50,836
which can useful for monitoring


94
00:04:51,136 --> 00:04:52,856
and controlling recognition
progress.


95
00:04:56,356 --> 00:04:58,256
Let's see what this
all looks like in code.


96
00:04:59,286 --> 00:05:01,546
We'll assume we've already
updated our info.plist


97
00:05:01,546 --> 00:05:04,726
with an accurate description
of how will it be used.


98
00:05:05,066 --> 00:05:07,496
Our next step is to
request authorization.


99
00:05:08,486 --> 00:05:10,606
It may be a good idea
to wait to do this


100
00:05:10,606 --> 00:05:12,886
until the user has invoked
a feature of your app


101
00:05:12,886 --> 00:05:14,366
which depends on
speech recognition.


102
00:05:14,866 --> 00:05:20,036
The request authorization class
method takes the completion


103
00:05:20,036 --> 00:05:23,426
handler which doesn't guarantee
a particular execution context.


104
00:05:24,596 --> 00:05:26,976
Apps will typically want to
dispatch to the main queue


105
00:05:27,246 --> 00:05:29,046
if they're going to do
something like enable


106
00:05:29,046 --> 00:05:31,006
or disable a user
interface button.


107
00:05:33,636 --> 00:05:37,596
If your authorization handler
has given authorize status,


108
00:05:38,116 --> 00:05:39,896
you should be ready
to start recognition.


109
00:05:41,646 --> 00:05:44,296
If not, recognition won't
be available to your app.


110
00:05:45,586 --> 00:05:48,566
It's important to gracefully
disable necessary functionality


111
00:05:48,646 --> 00:05:50,106
when the user makes
this decision


112
00:05:50,886 --> 00:05:52,946
or when the device is
otherwise restricted


113
00:05:52,946 --> 00:05:54,506
from accessing speech
recognition.


114
00:05:55,496 --> 00:05:57,436
Authorization can
be changed later


115
00:05:57,466 --> 00:05:59,156
in the device's privacy
settings.


116
00:06:01,286 --> 00:06:04,066
Let's see what it looks like
to recognize a prerecorded


117
00:06:04,066 --> 00:06:04,796
audio file.


118
00:06:05,956 --> 00:06:07,946
We'll assume we already
have a file url.


119
00:06:09,886 --> 00:06:12,526
Recognition requires
a speech recognizer


120
00:06:12,716 --> 00:06:14,716
which only recognizes
a single language.


121
00:06:15,536 --> 00:06:19,826
The default initializer for
SFSpeechRecognizer is failable.


122
00:06:20,336 --> 00:06:23,936
So it'll return nil if the
locale is not supported.


123
00:06:24,906 --> 00:06:27,726
The default initializer uses
device's current locale.


124
00:06:29,866 --> 00:06:32,236
In this function, we'll just
return one in this case.


125
00:06:34,586 --> 00:06:36,836
While this speech
recognition may be supported,


126
00:06:37,016 --> 00:06:39,316
it may not be available,
perhaps due


127
00:06:39,316 --> 00:06:40,846
to having no internet
connectivity.


128
00:06:41,966 --> 00:06:45,636
Use the is available property on
your recognizer to monitor this.


129
00:06:48,916 --> 00:06:50,686
Now we create a recognition
request


130
00:06:50,686 --> 00:06:52,916
with the recorded
file's url and give


131
00:06:52,916 --> 00:06:56,976
that to the recognition task
method of the recognizer.


132
00:07:02,046 --> 00:07:03,906
This method takes
completion handler


133
00:07:03,956 --> 00:07:06,366
with two optional
arguments, result and error.


134
00:07:07,696 --> 00:07:10,706
If result is nil, that
means recognition has failed


135
00:07:10,706 --> 00:07:11,456
for some reason.


136
00:07:12,086 --> 00:07:14,106
Check the error parameter
for an explanation.


137
00:07:15,666 --> 00:07:18,896
Otherwise, we can read the
speech we recognize so far


138
00:07:19,306 --> 00:07:20,356
by looking at results.


139
00:07:21,706 --> 00:07:24,916
Note that the completion handler
may be called more than once


140
00:07:25,326 --> 00:07:27,136
as speech is recognized
incrementally.


141
00:07:28,256 --> 00:07:30,506
You can tell the
recognition is finished


142
00:07:30,506 --> 00:07:33,746
by checking the is final
property of the result.


143
00:07:34,656 --> 00:07:37,386
Here we'll just print the
text of the final recognition.


144
00:07:43,926 --> 00:07:46,876
Recognizing live audio from
the device's microphone is very


145
00:07:46,876 --> 00:07:49,396
similar but requires
a few changes.


146
00:07:50,486 --> 00:07:53,176
We'll make an audio buffer
recognition request instead.


147
00:07:53,926 --> 00:07:55,486
This allows us to
provide a sequence


148
00:07:55,486 --> 00:07:58,386
of in memory audio buffers
instead of a file on disc.


149
00:07:59,696 --> 00:08:03,316
Here we'll use AVAudioEngine to
get a stream of audio buffers.


150
00:08:04,946 --> 00:08:06,306
Then append them to the request.


151
00:08:07,456 --> 00:08:09,896
Note that it's totally okay
to append audio buffers


152
00:08:09,896 --> 00:08:11,906
to a recognition
request both before


153
00:08:11,906 --> 00:08:13,376
and after starting recognition.


154
00:08:17,346 --> 00:08:20,596
One difference here is that
we no longer ignore the return


155
00:08:20,596 --> 00:08:22,616
value of the recognition
task method.


156
00:08:23,466 --> 00:08:25,616
Instead we store it in
a variable property.


157
00:08:26,366 --> 00:08:27,406
We'll see why in a moment.


158
00:08:28,626 --> 00:08:32,696
When we're done recording,
we need to inform the request


159
00:08:32,696 --> 00:08:35,775
that no more audio is coming so
that it can finish recognition.


160
00:08:37,025 --> 00:08:39,046
Use the end audio
method to do this.


161
00:08:39,846 --> 00:08:42,076
But what if the user
cancels our recording


162
00:08:42,076 --> 00:08:43,885
or the audio recording
gets interrupted?


163
00:08:44,796 --> 00:08:47,236
In this case, we really
don't care about the results


164
00:08:47,416 --> 00:08:49,686
and we should free up any
resources still being used


165
00:08:49,686 --> 00:08:50,746
by speech recognition.


166
00:08:52,786 --> 00:08:54,996
Just cancel the recognition
task that we started --


167
00:08:55,196 --> 00:08:56,646
we stored when we
started recognition.


168
00:08:57,446 --> 00:08:59,856
This can also be done for
prerecorded audio recognition.


169
00:09:02,216 --> 00:09:04,516
Now just a quick talk
about some best practices.


170
00:09:07,276 --> 00:09:10,676
We're making speech recognition
available for free to all apps


171
00:09:11,256 --> 00:09:13,166
but we do have some
reasonable limits in place


172
00:09:13,166 --> 00:09:15,306
so that the service remains
available to everyone.


173
00:09:17,176 --> 00:09:19,426
Individual devices may
be limited in the amount


174
00:09:19,426 --> 00:09:21,416
of recognitions that can
be performed per day.


175
00:09:23,046 --> 00:09:25,216
Apps may also be
throttled globally


176
00:09:25,216 --> 00:09:26,886
on a request per day basis.


177
00:09:28,606 --> 00:09:32,986
Like other service backed
APIs, for example CLGO Coder,


178
00:09:33,656 --> 00:09:36,586
be prepared to handle network
and rate limiting failures.


179
00:09:38,326 --> 00:09:39,806
If you find that you're
routinely hitting your


180
00:09:39,806 --> 00:09:41,566
throttling limits,
please let us know.


181
00:09:42,576 --> 00:09:45,876
It's also important to be aware


182
00:09:45,876 --> 00:09:48,376
that speech recognition can
have a high cost in terms


183
00:09:48,376 --> 00:09:50,196
of battery drain
and network traffic.


184
00:09:52,336 --> 00:09:55,536
For iOS 10 we're starting with
a strict audio duration limit


185
00:09:55,536 --> 00:09:57,936
of about one minute
which is similar to that


186
00:09:57,936 --> 00:09:58,976
of keyboard dictation.


187
00:10:02,506 --> 00:10:04,576
A few words about
being transparent


188
00:10:04,576 --> 00:10:06,316
and respecting your
user's privacy.


189
00:10:07,676 --> 00:10:10,316
If you're recording the user's
speech, it's a great idea


190
00:10:10,316 --> 00:10:12,696
to make this crystal clear
in your user interface.


191
00:10:13,606 --> 00:10:16,706
Playing recording sounds and/or
displaying a visual recording


192
00:10:16,706 --> 00:10:18,536
indicator makes it
clear to your users


193
00:10:18,756 --> 00:10:19,866
that they're being recorded.


194
00:10:22,186 --> 00:10:24,726
Some speech is a bad
candidate for recognition.


195
00:10:25,556 --> 00:10:28,056
Passwords, health data,
financial information,


196
00:10:28,056 --> 00:10:30,086
and other sensitive
speech should not be given


197
00:10:30,086 --> 00:10:31,106
to speech recognition.


198
00:10:33,796 --> 00:10:36,396
Displaying speech as it
is recognized like Siri


199
00:10:36,396 --> 00:10:39,516
and Dictation do can also help
users understand what your app


200
00:10:39,516 --> 00:10:40,036
is doing.


201
00:10:40,966 --> 00:10:43,666
It can also be helpful for
users so that they can see


202
00:10:43,666 --> 00:10:45,146
when recognition errors occur.


203
00:10:47,896 --> 00:10:49,026
So developers.


204
00:10:49,456 --> 00:10:51,026
Your apps now have free access


205
00:10:51,026 --> 00:10:54,086
to high performance speech
recognition dozens of languages.


206
00:10:54,996 --> 00:10:56,826
But it's important to
gracefully handle cases


207
00:10:56,826 --> 00:10:57,756
where it's not available


208
00:10:57,756 --> 00:10:59,656
or the user doesn't
want your app to use it.


209
00:11:01,136 --> 00:11:02,966
Transparency is the best policy.


210
00:11:03,456 --> 00:11:04,396
Make it clear to the user


211
00:11:04,396 --> 00:11:06,246
when speech recognition
is being used.


212
00:11:07,916 --> 00:11:09,976
We're incredibly excited
to see what new uses


213
00:11:09,976 --> 00:11:11,946
for speech recognition
you come up with.


214
00:11:14,186 --> 00:11:16,416
For more information
and some sample code,


215
00:11:16,526 --> 00:11:18,096
check out this session's
webpage.


216
00:11:18,996 --> 00:11:21,396
You might also be interested
in some sessions on SiriKit.


217
00:11:21,956 --> 00:11:24,786
There's one on Wednesday and
the more advanced one later


218
00:11:24,786 --> 00:11:25,436
on Thursday.


219
00:11:26,706 --> 00:11:28,976
Thank you for your time
and have a great WWDC.

