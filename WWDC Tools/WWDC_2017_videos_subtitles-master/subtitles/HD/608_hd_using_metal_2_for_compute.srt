1
00:00:28,596 --> 00:00:29,576
>> Good afternoon everyone,


2
00:00:30,076 --> 00:00:32,006
welcome to our talk on using


3
00:00:32,006 --> 00:00:32,946
Metal 2 for Compute.


4
00:00:33,886 --> 00:00:35,296
My name is Anna Tikhonova.


5
00:00:35,296 --> 00:00:36,686
I'm an engineer on the GPU


6
00:00:36,686 --> 00:00:37,976
Software Team, so let's begin.


7
00:00:42,156 --> 00:00:44,046
The Metal 2 echo system is so


8
00:00:44,046 --> 00:00:45,836
much more than the Metal API and


9
00:00:45,836 --> 00:00:46,356
the language.


10
00:00:46,796 --> 00:00:48,946
We also have the GPU Tools and


11
00:00:48,946 --> 00:00:50,336
we have the MetalKit and Metal


12
00:00:50,336 --> 00:00:51,586
Performance Shaders frameworks.


13
00:00:53,006 --> 00:00:54,146
You might know Metal as this


14
00:00:54,496 --> 00:00:56,376
great technology for developing


15
00:00:56,376 --> 00:00:57,566
high-end games and graphics.


16
00:00:58,396 --> 00:00:59,616
But it can also be used for


17
00:00:59,616 --> 00:01:00,506
Compute processing.


18
00:01:01,626 --> 00:01:02,806
In fact, the Compute side of


19
00:01:02,806 --> 00:01:04,616
Metal is so powerful and


20
00:01:04,616 --> 00:01:06,526
flexible that the Metal


21
00:01:06,526 --> 00:01:08,196
Performance Shaders framework is


22
00:01:08,196 --> 00:01:09,326
built completely on top of


23
00:01:09,406 --> 00:01:09,756
Compute.


24
00:01:11,026 --> 00:01:12,486
And in this session, we'll talk


25
00:01:12,486 --> 00:01:14,076
about what's new in the Metal


26
00:01:14,076 --> 00:01:15,176
Performance Shaders framework.


27
00:01:17,956 --> 00:01:19,596
We introduced the Metal


28
00:01:19,596 --> 00:01:21,026
Performers Shaders framework, or


29
00:01:21,026 --> 00:01:22,756
MPS in 2015.


30
00:01:23,486 --> 00:01:24,546
And the videos of our past


31
00:01:24,546 --> 00:01:25,916
sessions are available on our


32
00:01:25,916 --> 00:01:28,906
developer website.


33
00:01:29,266 --> 00:01:30,686
MPS uses the compute power of


34
00:01:30,686 --> 00:01:33,446
the GPU to bring GPU accelerated


35
00:01:33,446 --> 00:01:33,816
primitives.


36
00:01:34,286 --> 00:01:35,886
For image processing, linear


37
00:01:35,926 --> 00:01:37,416
algebra and machine learning.


38
00:01:39,146 --> 00:01:40,416
The framework is optimized for


39
00:01:40,416 --> 00:01:42,336
iOS and we're happy to announce


40
00:01:42,336 --> 00:01:44,096
that this year we're also


41
00:01:44,096 --> 00:01:44,836
bringing MPS to the Mac.


42
00:01:45,516 --> 00:01:49,786
[ Applause ]


43
00:01:50,286 --> 00:01:50,716
Thank you.


44
00:01:51,926 --> 00:01:53,366
The entire feature set is


45
00:01:53,366 --> 00:01:55,616
available in both iOS and macOS.


46
00:01:55,616 --> 00:01:58,476
So let's begin with a quick


47
00:01:58,476 --> 00:02:00,336
update on our image processing


48
00:02:00,336 --> 00:02:00,686
support.


49
00:02:02,046 --> 00:02:03,866
So here's a list of all of the


50
00:02:03,866 --> 00:02:05,576
primitives for image processing


51
00:02:05,696 --> 00:02:07,486
that we had available in iOS 10.


52
00:02:08,106 --> 00:02:09,675
So there's Convolution, Gaussian


53
00:02:09,675 --> 00:02:11,586
Blur, Lanczos Resampling, just


54
00:02:11,586 --> 00:02:12,196
to name a few.


55
00:02:13,126 --> 00:02:14,726
They're all now available in


56
00:02:14,726 --> 00:02:14,996
macOS.


57
00:02:16,146 --> 00:02:17,656
And this year we're bringing you


58
00:02:17,656 --> 00:02:18,926
four new image processing


59
00:02:18,926 --> 00:02:19,336
primitives.


60
00:02:20,466 --> 00:02:21,816
The Image Keypoints primitive


61
00:02:22,206 --> 00:02:24,316
can be used -- is often used in


62
00:02:24,316 --> 00:02:26,256
computer vision algorithms such


63
00:02:26,256 --> 00:02:28,596
as image stabilization and


64
00:02:28,596 --> 00:02:29,926
Bilinear Rescale, Image


65
00:02:29,926 --> 00:02:31,726
Statistics, and Element-wise


66
00:02:31,726 --> 00:02:33,246
Arithmetic Operators, are


67
00:02:33,326 --> 00:02:34,826
commonly used to pre-process


68
00:02:34,826 --> 00:02:35,156
images.


69
00:02:35,466 --> 00:02:36,386
For example, in machine


70
00:02:36,386 --> 00:02:36,656
learning.


71
00:02:37,556 --> 00:02:38,926
And the arithmetic filters also


72
00:02:38,926 --> 00:02:40,446
support broadcasting operations.


73
00:02:41,256 --> 00:02:42,716
Which, for example, allow you to


74
00:02:42,716 --> 00:02:44,766
add a 2D image or the 1D image.


75
00:02:46,176 --> 00:02:48,406
So that's it for our very quick


76
00:02:48,406 --> 00:02:49,606
update on image processing.


77
00:02:49,986 --> 00:02:51,286
And now let's talk about the new


78
00:02:51,286 --> 00:02:52,386
Linear Algebra operations.


79
00:02:54,286 --> 00:02:55,686
Without support, Matrix


80
00:02:55,686 --> 00:02:57,436
Multiplication, Matrix Vector


81
00:02:57,436 --> 00:02:59,736
Multiplication, and Triangular


82
00:03:00,076 --> 00:03:01,866
Matrix Factorization and Linear


83
00:03:01,866 --> 00:03:02,306
Solvers.


84
00:03:05,356 --> 00:03:06,376
To support Linear Algebra


85
00:03:06,376 --> 00:03:09,256
operations, we now have multiple


86
00:03:09,256 --> 00:03:10,356
new data representations.


87
00:03:11,066 --> 00:03:13,076
First, we have the MPSVector


88
00:03:13,076 --> 00:03:15,186
object which interprets the data


89
00:03:15,186 --> 00:03:16,496
in a metal buffer as a


90
00:03:16,496 --> 00:03:17,416
one-dimensional array.


91
00:03:19,106 --> 00:03:21,506
And we have an MPSMatrix object


92
00:03:22,076 --> 00:03:23,276
which interprets the data in a


93
00:03:23,276 --> 00:03:24,886
metal buffer as a rectangular


94
00:03:24,886 --> 00:03:25,156
array.


95
00:03:25,886 --> 00:03:27,656
And MPS matrices are in role


96
00:03:27,656 --> 00:03:28,176
major order.


97
00:03:28,956 --> 00:03:30,166
And you can think of both


98
00:03:30,166 --> 00:03:32,956
MPSVectors and MPSMatrices as


99
00:03:33,456 --> 00:03:34,736
wrappers around user data


100
00:03:34,736 --> 00:03:35,096
buffers.


101
00:03:37,436 --> 00:03:39,296
And we also support a temporary


102
00:03:39,296 --> 00:03:40,926
variance of MPSMatrix.


103
00:03:42,296 --> 00:03:45,196
MPS images -- temporary images


104
00:03:45,196 --> 00:03:47,056
and MPSTemporaryMatrices are


105
00:03:47,056 --> 00:03:48,666
allocated from a Metal heap


106
00:03:48,906 --> 00:03:49,956
associated with a command


107
00:03:49,956 --> 00:03:50,216
buffer.


108
00:03:50,766 --> 00:03:51,856
And they are called temporary


109
00:03:52,226 --> 00:03:54,066
because their lifespan is


110
00:03:54,216 --> 00:03:55,996
limited to the lifetime of the


111
00:03:55,996 --> 00:03:56,516
command buffer.


112
00:03:57,546 --> 00:03:58,866
And we recommend you use


113
00:03:58,896 --> 00:04:00,176
temporary images and matrices


114
00:04:00,636 --> 00:04:02,526
for most of your intermediate


115
00:04:03,006 --> 00:04:03,206
storage.


116
00:04:04,316 --> 00:04:07,416
Both MPSVector and MPSMatrix


117
00:04:07,576 --> 00:04:09,116
support a number of input types.


118
00:04:09,646 --> 00:04:11,596
We support single-precision and


119
00:04:11,596 --> 00:04:14,236
half-precision input types and a


120
00:04:14,236 --> 00:04:15,266
floating-point input types.


121
00:04:15,756 --> 00:04:17,696
And 16-bits and 8-bit signed


122
00:04:17,986 --> 00:04:19,125
integer input types.


123
00:04:21,016 --> 00:04:22,136
And now let's take a look at how


124
00:04:22,136 --> 00:04:24,446
we can create an MPSVector of


125
00:04:24,536 --> 00:04:24,886
size N.


126
00:04:24,886 --> 00:04:26,856
So if you don't already have a


127
00:04:26,856 --> 00:04:28,346
Metal buffer, you need to create


128
00:04:28,346 --> 00:04:28,606
one.


129
00:04:29,666 --> 00:04:30,666
And then you need to create a


130
00:04:30,666 --> 00:04:31,686
descriptor for your vector.


131
00:04:32,526 --> 00:04:34,546
And note here that you specify


132
00:04:34,726 --> 00:04:36,086
the length of the vector.


133
00:04:36,666 --> 00:04:38,146
That's because the vector can be


134
00:04:38,146 --> 00:04:39,946
made from a portion of the


135
00:04:39,946 --> 00:04:40,926
original Metal buffer.


136
00:04:41,716 --> 00:04:43,236
And other related offsets can be


137
00:04:43,236 --> 00:04:44,556
set in a kernel that will use


138
00:04:44,556 --> 00:04:45,016
this vector.


139
00:04:45,996 --> 00:04:47,406
And then the last step is


140
00:04:47,466 --> 00:04:49,586
creating a vector from the


141
00:04:49,586 --> 00:04:50,906
buffer with a descriptor.


142
00:04:52,966 --> 00:04:53,976
And now let's take a look at how


143
00:04:53,976 --> 00:04:56,256
you can create an MPSMatrix with


144
00:04:56,326 --> 00:04:57,906
M rows and N columns.


145
00:04:59,516 --> 00:05:00,906
So it's very similar to the way


146
00:05:00,906 --> 00:05:02,916
you would create MPSVector, but


147
00:05:02,916 --> 00:05:04,006
there's just a few things we


148
00:05:04,006 --> 00:05:04,726
want to mention.


149
00:05:06,176 --> 00:05:08,256
We provide a convenient API that


150
00:05:08,256 --> 00:05:09,606
you can use to find the


151
00:05:09,606 --> 00:05:11,966
recommended bytes per row value


152
00:05:12,556 --> 00:05:13,756
for sizing your Metal buffers.


153
00:05:14,666 --> 00:05:15,716
And if you choose to use the


154
00:05:15,796 --> 00:05:17,246
API, this is how you would


155
00:05:17,246 --> 00:05:18,816
create a metal buffer with this


156
00:05:18,816 --> 00:05:19,596
recommended value.


157
00:05:20,596 --> 00:05:22,106
And using this API is completely


158
00:05:22,106 --> 00:05:24,416
optional, but recommended for


159
00:05:24,416 --> 00:05:25,136
better performance.


160
00:05:25,986 --> 00:05:27,156
And then the rest is simple.


161
00:05:28,256 --> 00:05:29,356
You create a descriptor for your


162
00:05:29,356 --> 00:05:30,886
matrix, and then you create a


163
00:05:30,886 --> 00:05:32,346
matrix with a descriptor.


164
00:05:34,936 --> 00:05:36,506
And now that we talked about the


165
00:05:36,506 --> 00:05:37,806
data presentations, now let's


166
00:05:37,806 --> 00:05:39,046
talk about the primitives.


167
00:05:39,896 --> 00:05:41,176
So for Matrix-Matrix and


168
00:05:41,176 --> 00:05:43,136
Matrix-Vector multiplication our


169
00:05:43,136 --> 00:05:44,586
API is modeled after the


170
00:05:44,586 --> 00:05:46,036
standard BLAS GEMM and GEMV


171
00:05:46,036 --> 00:05:46,766
interfaces.


172
00:05:47,646 --> 00:05:48,846
And for triangular matrix


173
00:05:48,846 --> 00:05:50,196
vectorization and linear


174
00:05:50,196 --> 00:05:52,216
solvers, our API is modeled


175
00:05:52,216 --> 00:05:53,246
after standard LAPACK


176
00:05:53,246 --> 00:05:54,916
decomposition and solve


177
00:05:54,916 --> 00:05:55,486
interfaces.


178
00:05:55,846 --> 00:05:57,036
So if you're familiar with those


179
00:05:57,036 --> 00:05:59,106
interfaces our API will look


180
00:05:59,106 --> 00:06:00,526
very familiar to you as well.


181
00:06:02,576 --> 00:06:04,216
And now let's take a look at a


182
00:06:04,216 --> 00:06:05,546
very simple code example.


183
00:06:05,836 --> 00:06:07,536
So we'll be doing matrix


184
00:06:07,536 --> 00:06:08,996
multiplication and computing


185
00:06:08,996 --> 00:06:10,426
just C = A times B.


186
00:06:10,426 --> 00:06:12,716
So first we need to create our


187
00:06:12,716 --> 00:06:14,476
matrices A, B and C.


188
00:06:14,706 --> 00:06:15,646
But I know you know how to do


189
00:06:15,646 --> 00:06:16,786
this, I showed you in a previous


190
00:06:16,836 --> 00:06:18,306
slide, so let's move on.


191
00:06:19,336 --> 00:06:21,116
Now we want to run matrix


192
00:06:21,116 --> 00:06:22,596
multiplication on the GPU.


193
00:06:23,886 --> 00:06:25,386
So first we do our usual Metal


194
00:06:25,466 --> 00:06:27,446
setup to getting a device, a


195
00:06:27,446 --> 00:06:29,076
command queue, and a command


196
00:06:29,076 --> 00:06:29,356
buffer.


197
00:06:29,356 --> 00:06:31,766
And then we need to create our


198
00:06:31,846 --> 00:06:33,186
matrix multiplication kernel.


199
00:06:33,766 --> 00:06:35,196
And note here that you specify


200
00:06:35,196 --> 00:06:36,296
the size of the result.


201
00:06:36,826 --> 00:06:38,216
That's because this kernel can


202
00:06:38,216 --> 00:06:39,896
operate on subregions of


203
00:06:39,896 --> 00:06:40,436
matrices.


204
00:06:41,066 --> 00:06:45,576
And then we encode this kernel


205
00:06:45,656 --> 00:06:47,056
to the GPU and tell it to start


206
00:06:47,056 --> 00:06:47,476
doing the work.


207
00:06:47,476 --> 00:06:51,036
And we already have sample code


208
00:06:51,036 --> 00:06:52,346
from Matrix multiplication


209
00:06:52,586 --> 00:06:53,906
available on our developer


210
00:06:53,906 --> 00:06:56,096
website, and the sample code for


211
00:06:56,096 --> 00:06:57,896
triangular matrix vectorization


212
00:06:58,206 --> 00:06:59,556
and solving a system of linear


213
00:06:59,556 --> 00:07:00,986
equations is coming very soon.


214
00:07:03,026 --> 00:07:05,526
So that's it for our -- for the


215
00:07:05,756 --> 00:07:07,026
linear algebra operations.


216
00:07:07,386 --> 00:07:08,996
Let's now move on to the next


217
00:07:08,996 --> 00:07:10,756
topic, which is Accelerating


218
00:07:10,756 --> 00:07:12,156
Machine Learning Primitives on


219
00:07:12,156 --> 00:07:12,636
the GPU.


220
00:07:14,116 --> 00:07:16,246
There are a number of


221
00:07:16,246 --> 00:07:17,906
machine-learning related talks


222
00:07:17,906 --> 00:07:19,156
at WWDC this year.


223
00:07:19,456 --> 00:07:20,346
And we are a part of the


224
00:07:20,346 --> 00:07:21,416
machine-learning community.


225
00:07:22,306 --> 00:07:23,586
And this slide shows the overall


226
00:07:23,586 --> 00:07:24,206
architecture.


227
00:07:25,086 --> 00:07:26,606
So as an application developer,


228
00:07:26,816 --> 00:07:27,876
you can add machine learning


229
00:07:27,876 --> 00:07:28,816
functionality to your


230
00:07:28,816 --> 00:07:30,956
applications by using high-level


231
00:07:30,996 --> 00:07:32,856
domain-specific frameworks such


232
00:07:32,856 --> 00:07:34,456
as division framework and the


233
00:07:34,456 --> 00:07:35,566
natural language processing


234
00:07:35,616 --> 00:07:37,496
framework, which rely on the


235
00:07:37,496 --> 00:07:38,366
Core ML framework.


236
00:07:39,216 --> 00:07:40,376
And the Core ML framework is


237
00:07:40,446 --> 00:07:42,316
powered by the accelerates


238
00:07:42,316 --> 00:07:44,076
framework BNNS primitives on the


239
00:07:44,076 --> 00:07:44,606
CPU.


240
00:07:45,066 --> 00:07:46,176
And by the machine learning --


241
00:07:47,066 --> 00:07:49,046
and by the Metal Performance


242
00:07:49,046 --> 00:07:51,946
Shaders framework on the GPU.


243
00:07:51,946 --> 00:07:52,706
But if you're writing an


244
00:07:52,706 --> 00:07:54,556
application that uses Metal,


245
00:07:54,906 --> 00:07:56,046
then you can use the MPS


246
00:07:56,046 --> 00:07:58,176
framework directly and I will


247
00:07:58,176 --> 00:07:59,386
show you how in this session.


248
00:08:01,666 --> 00:08:02,676
So let's start with what are we


249
00:08:02,736 --> 00:08:03,486
talking about here?


250
00:08:04,246 --> 00:08:05,116
What is deep learning?


251
00:08:05,366 --> 00:08:06,236
What is Machine learning?


252
00:08:07,686 --> 00:08:08,766
So imagine that this is you.


253
00:08:08,766 --> 00:08:11,436
And when you see an image, you


254
00:08:11,436 --> 00:08:13,076
know immediately what's depicted


255
00:08:13,076 --> 00:08:13,316
on it.


256
00:08:13,416 --> 00:08:13,956
It's a panda.


257
00:08:14,936 --> 00:08:16,686
But now think about all of the


258
00:08:16,686 --> 00:08:18,006
images on your iPhone.


259
00:08:18,596 --> 00:08:20,206
Or all of those pictures in your


260
00:08:20,206 --> 00:08:20,996
family albums.


261
00:08:21,606 --> 00:08:22,646
Or all of the images on the


262
00:08:22,646 --> 00:08:22,966
internet.


263
00:08:23,816 --> 00:08:27,096
No human can possibly -- can


264
00:08:27,096 --> 00:08:28,816
classify these many images.


265
00:08:29,086 --> 00:08:30,506
But deep-learning algorithms is


266
00:08:30,506 --> 00:08:32,056
designed specifically to do


267
00:08:32,056 --> 00:08:32,196
that.


268
00:08:33,186 --> 00:08:34,416
They can be used for sifting


269
00:08:34,416 --> 00:08:35,576
through large amounts of data


270
00:08:36,015 --> 00:08:37,866
and answering questions such as


271
00:08:37,996 --> 00:08:41,676
what is in this image?


272
00:08:42,236 --> 00:08:43,306
Deep learning algorithms have


273
00:08:43,405 --> 00:08:43,905
two phases.


274
00:08:44,206 --> 00:08:45,156
Training and inference.


275
00:08:45,426 --> 00:08:46,426
So let's talk about training


276
00:08:46,426 --> 00:08:46,736
first.


277
00:08:47,946 --> 00:08:49,246
And let's actually use an


278
00:08:49,246 --> 00:08:49,676
example.


279
00:08:49,676 --> 00:08:51,326
Let's train a system to classify


280
00:08:51,326 --> 00:08:51,716
images.


281
00:08:52,586 --> 00:08:53,536
So the training system to


282
00:08:53,536 --> 00:08:56,696
classify images, for example, if


283
00:08:56,696 --> 00:08:58,066
you want to have it recognize


284
00:08:58,126 --> 00:08:58,536
animals.


285
00:08:58,966 --> 00:09:00,516
Like, to have it recognize cats,


286
00:09:00,516 --> 00:09:02,126
you need to feed this system a


287
00:09:02,556 --> 00:09:04,196
large number of labeled images


288
00:09:04,196 --> 00:09:06,126
of cats and then rabbits, and


289
00:09:06,126 --> 00:09:07,336
all the other animals that you


290
00:09:07,336 --> 00:09:08,406
want your system to be able to


291
00:09:08,406 --> 00:09:08,866
recognize.


292
00:09:10,546 --> 00:09:12,316
And this training step is a


293
00:09:12,316 --> 00:09:13,916
one-time computationally


294
00:09:13,916 --> 00:09:16,476
expensive and labor-intensive


295
00:09:16,566 --> 00:09:16,786
step.


296
00:09:17,896 --> 00:09:19,076
And it's usually done offline.


297
00:09:19,696 --> 00:09:20,896
But the results of this training


298
00:09:20,896 --> 00:09:22,336
phase is trained parameters


299
00:09:23,306 --> 00:09:24,656
which are required for the next


300
00:09:24,656 --> 00:09:25,856
phase, the inference phase.


301
00:09:26,906 --> 00:09:28,096
This is when your system is


302
00:09:28,186 --> 00:09:30,156
presented with a new image that


303
00:09:30,156 --> 00:09:31,736
it has never seen before and it


304
00:09:31,736 --> 00:09:33,186
needs to classify, this is a


305
00:09:33,186 --> 00:09:33,396
cap.


306
00:09:35,126 --> 00:09:37,016
We provide view acceleration for


307
00:09:37,016 --> 00:09:38,306
the second phase; the inference


308
00:09:38,306 --> 00:09:38,526
phase.


309
00:09:39,096 --> 00:09:40,966
Specifically, last year we


310
00:09:40,966 --> 00:09:42,936
talked about the building blocks


311
00:09:43,056 --> 00:09:44,296
for building convolutional


312
00:09:44,296 --> 00:09:45,716
neural networks on the GPU for


313
00:09:45,716 --> 00:09:46,146
inference.


314
00:09:48,466 --> 00:09:50,176
So before we move onto any of


315
00:09:50,176 --> 00:09:51,966
the new features for machine


316
00:09:51,966 --> 00:09:52,826
learning that we brought you


317
00:09:52,856 --> 00:09:53,936
this year, we are going to


318
00:09:53,936 --> 00:09:55,136
review some of the core


319
00:09:55,136 --> 00:09:56,886
information that was covered in


320
00:09:56,886 --> 00:09:58,146
our last year's presentation.


321
00:09:58,736 --> 00:10:00,416
Such as, what are convolutional


322
00:10:00,416 --> 00:10:00,966
neural networks?


323
00:10:02,396 --> 00:10:04,326
And once we do that then we can


324
00:10:04,326 --> 00:10:06,056
talk about the new primitives


325
00:10:06,106 --> 00:10:06,836
that we've added for


326
00:10:06,836 --> 00:10:07,906
convolutional neural networks


327
00:10:07,966 --> 00:10:09,426
this year, and then we'll


328
00:10:09,426 --> 00:10:11,146
introduce the new, easy-to-use


329
00:10:11,516 --> 00:10:12,636
neural network graph API.


330
00:10:13,166 --> 00:10:14,746
And our last topic will be


331
00:10:14,746 --> 00:10:15,726
recurrent neural networks.


332
00:10:18,606 --> 00:10:20,976
So let's go into our recap.


333
00:10:21,106 --> 00:10:22,066
So what are convolutional neural


334
00:10:22,066 --> 00:10:22,366
networks?


335
00:10:24,446 --> 00:10:25,576
Convolutional neural networks


336
00:10:25,576 --> 00:10:27,426
are biologically inspired and


337
00:10:27,426 --> 00:10:28,836
designed to resemble the visual


338
00:10:28,836 --> 00:10:29,246
cortex.


339
00:10:29,796 --> 00:10:31,406
So let's think about how our


340
00:10:31,406 --> 00:10:33,076
brain processes visual inputs.


341
00:10:34,256 --> 00:10:35,636
The first hierarchy of neurons


342
00:10:35,736 --> 00:10:37,056
that receive information in the


343
00:10:37,056 --> 00:10:39,396
visual cortex is sensitive to


344
00:10:39,396 --> 00:10:40,786
specific edges and blobs of


345
00:10:40,886 --> 00:10:41,196
color.


346
00:10:42,366 --> 00:10:43,466
While the brain region's further


347
00:10:43,466 --> 00:10:45,966
down the visual pipeline respond


348
00:10:45,966 --> 00:10:47,596
to more complex structures such


349
00:10:47,596 --> 00:10:49,366
as faces of our friends or kinds


350
00:10:49,366 --> 00:10:50,166
of animals like cats.


351
00:10:50,996 --> 00:10:53,646
So in a similar way, CNNs are


352
00:10:53,646 --> 00:10:55,836
organized into a hierarchy of


353
00:10:55,836 --> 00:10:58,176
layers where high-level features


354
00:10:58,296 --> 00:10:59,836
are derived from low-level


355
00:10:59,836 --> 00:11:00,246
features.


356
00:11:01,156 --> 00:11:02,516
So the first few layers in your


357
00:11:02,516 --> 00:11:04,566
network respond to low-level


358
00:11:04,566 --> 00:11:06,766
features like edges and blobs of


359
00:11:06,826 --> 00:11:07,196
color.


360
00:11:07,886 --> 00:11:10,646
While subsequent layers respond


361
00:11:10,766 --> 00:11:12,516
to progressively more complex


362
00:11:12,616 --> 00:11:14,886
features such as faces.


363
00:11:16,106 --> 00:11:17,406
And I keep saying features.


364
00:11:17,846 --> 00:11:19,556
So think of a feature as a


365
00:11:19,556 --> 00:11:21,176
filter that filters your input


366
00:11:21,176 --> 00:11:22,706
data; that particular feature.


367
00:11:25,316 --> 00:11:26,906
And here's a list of all of the


368
00:11:26,976 --> 00:11:28,076
CNN primitives that we had


369
00:11:28,076 --> 00:11:29,356
available in iOS 10.


370
00:11:29,756 --> 00:11:31,806
And in this recap I will be just


371
00:11:31,806 --> 00:11:33,686
talking about the core


372
00:11:34,176 --> 00:11:35,146
convolution layer.


373
00:11:35,216 --> 00:11:36,426
The core building block of a


374
00:11:36,546 --> 00:11:36,756
CNN.


375
00:11:36,756 --> 00:11:39,046
And the rest of these primitives


376
00:11:39,106 --> 00:11:40,906
are covered in great detail in


377
00:11:40,906 --> 00:11:42,266
our presentation -- in our


378
00:11:42,266 --> 00:11:43,076
documentation.


379
00:11:43,196 --> 00:11:44,566
So Pooling, Fully-Connected and


380
00:11:44,616 --> 00:11:45,156
SoftMax.


381
00:11:45,746 --> 00:11:46,856
You can find information on


382
00:11:46,856 --> 00:11:47,056
those.


383
00:11:48,626 --> 00:11:50,386
So let's talk about the core


384
00:11:50,386 --> 00:11:51,186
building block.


385
00:11:52,596 --> 00:11:54,216
So the function of this core


386
00:11:54,216 --> 00:11:56,196
convolution layer is to


387
00:11:56,196 --> 00:11:57,766
recognize features in the input


388
00:11:57,766 --> 00:11:58,906
data and it's called a


389
00:11:58,906 --> 00:12:01,076
convolution layer because it


390
00:12:01,126 --> 00:12:02,576
performs a convolution on its


391
00:12:02,576 --> 00:12:02,846
input.


392
00:12:03,916 --> 00:12:05,246
So let's recall how regular


393
00:12:05,246 --> 00:12:06,076
convolution works.


394
00:12:06,906 --> 00:12:08,146
You have your inputs, your


395
00:12:08,186 --> 00:12:09,366
outputs and the filter.


396
00:12:10,366 --> 00:12:12,386
And to convole a filter with the


397
00:12:12,386 --> 00:12:14,506
input data you need to multiply


398
00:12:14,756 --> 00:12:16,626
each value in your filter with


399
00:12:16,626 --> 00:12:18,446
the value in the input data and


400
00:12:18,446 --> 00:12:19,686
combine that information to


401
00:12:19,686 --> 00:12:21,106
compute a single output value.


402
00:12:22,046 --> 00:12:23,536
And you do the same for the rest


403
00:12:23,636 --> 00:12:25,166
of the output pixels.


404
00:12:27,596 --> 00:12:29,856
And now the convolution layer is


405
00:12:29,856 --> 00:12:31,606
a generalization of regular


406
00:12:31,606 --> 00:12:32,226
convolution.


407
00:12:32,396 --> 00:12:34,666
It allows you to have multiple


408
00:12:34,666 --> 00:12:35,136
filters.


409
00:12:35,526 --> 00:12:37,536
So you have as many filters as


410
00:12:37,536 --> 00:12:38,916
you have output channels -- or


411
00:12:38,916 --> 00:12:39,816
16 in this case.


412
00:12:41,666 --> 00:12:43,046
And these are the filters which


413
00:12:43,046 --> 00:12:44,656
are going to be filtering input


414
00:12:44,656 --> 00:12:46,006
data for particular features.


415
00:12:47,726 --> 00:12:49,016
Now imagine that you're working


416
00:12:49,016 --> 00:12:50,266
with RGB data.


417
00:12:50,356 --> 00:12:52,186
So you actually have three


418
00:12:52,186 --> 00:12:53,266
channels in your input.


419
00:12:54,156 --> 00:12:56,276
And just because how CNNs work,


420
00:12:56,476 --> 00:12:58,816
this means you need three sets


421
00:12:58,886 --> 00:13:00,156
of 16 filters.


422
00:13:00,866 --> 00:13:02,576
One set for each input channel.


423
00:13:03,856 --> 00:13:05,916
And then these filters are


424
00:13:05,916 --> 00:13:07,296
applied to the input data


425
00:13:08,486 --> 00:13:09,036
separately.


426
00:13:09,036 --> 00:13:10,816
And then the final step combines


427
00:13:10,816 --> 00:13:12,386
all of this information to


428
00:13:12,386 --> 00:13:13,796
compute a single output pixel.


429
00:13:15,516 --> 00:13:17,156
So that's it for our recap of


430
00:13:17,156 --> 00:13:18,006
the convolution layer.


431
00:13:18,536 --> 00:13:19,526
Now let's talk about the new


432
00:13:19,576 --> 00:13:20,846
primitives we've added for


433
00:13:20,846 --> 00:13:22,026
convolutional neural networks.


434
00:13:22,116 --> 00:13:25,176
So as you can see, we've added


435
00:13:25,176 --> 00:13:25,686
quite a few.


436
00:13:27,736 --> 00:13:29,096
And I'll be talking about the


437
00:13:29,096 --> 00:13:31,476
ones highlighted in yellow, but


438
00:13:31,476 --> 00:13:33,206
the rest of them like L2Norm


439
00:13:33,256 --> 00:13:34,686
Pooling, Resampling,


440
00:13:34,686 --> 00:13:36,086
Up-sampling, they will all be


441
00:13:36,086 --> 00:13:37,396
covered in our documentation.


442
00:13:39,286 --> 00:13:40,986
So let's talk about updates to


443
00:13:40,986 --> 00:13:42,786
our core convolution layer.


444
00:13:43,916 --> 00:13:45,146
We used to support only single


445
00:13:45,186 --> 00:13:46,716
precision floating-point weight


446
00:13:46,716 --> 00:13:47,026
types.


447
00:13:47,466 --> 00:13:49,076
And now to help you reduce the


448
00:13:49,076 --> 00:13:51,126
memory footprint and to prove


449
00:13:51,126 --> 00:13:51,966
the performance of your


450
00:13:51,966 --> 00:13:52,326
networks.


451
00:13:52,876 --> 00:13:54,546
We also support half-precision


452
00:13:54,546 --> 00:13:56,466
floating points, 8-bit integer,


453
00:13:56,806 --> 00:13:58,076
and binary weight types.


454
00:13:59,496 --> 00:14:01,006
We used to support only standard


455
00:14:01,006 --> 00:14:02,676
convolution and now we also


456
00:14:02,736 --> 00:14:03,926
support binary and XNOR


457
00:14:03,926 --> 00:14:04,646
convolution.


458
00:14:04,986 --> 00:14:06,096
Dilated convolution.


459
00:14:06,096 --> 00:14:08,406
Sub-pixel convolution and


460
00:14:08,406 --> 00:14:09,366
convolution transpose


461
00:14:09,366 --> 00:14:09,996
operations.


462
00:14:11,056 --> 00:14:12,156
And many of these are


463
00:14:12,156 --> 00:14:13,716
orthogonal, so you can even have


464
00:14:14,006 --> 00:14:15,946
dilated sub-pixel convolution if


465
00:14:15,946 --> 00:14:16,276
you want.


466
00:14:17,706 --> 00:14:18,486
So let's go through them


467
00:14:18,486 --> 00:14:19,046
one-by-one.


468
00:14:20,806 --> 00:14:22,216
Binary and XNOR convolution


469
00:14:22,256 --> 00:14:24,276
perform the same exact operation


470
00:14:24,306 --> 00:14:26,336
as regular convolution but they


471
00:14:26,336 --> 00:14:28,016
do so with improved performance


472
00:14:28,476 --> 00:14:29,566
and great space savings.


473
00:14:30,016 --> 00:14:31,726
So in regular convolution, you


474
00:14:31,726 --> 00:14:33,546
may have floating point inputs


475
00:14:33,846 --> 00:14:35,166
and floating point weights.


476
00:14:36,036 --> 00:14:37,446
What binary convolution allow


477
00:14:37,516 --> 00:14:39,236
you to do is to use your


478
00:14:39,236 --> 00:14:41,266
full-sized input with binary


479
00:14:41,266 --> 00:14:41,486
weights.


480
00:14:42,416 --> 00:14:44,776
And for XNOR convolution the


481
00:14:44,776 --> 00:14:46,706
first thing that happens is that


482
00:14:47,226 --> 00:14:48,626
your input is first converted to


483
00:14:48,626 --> 00:14:50,826
binary so that both your inputs


484
00:14:51,176 --> 00:14:52,396
and the weights are binary.


485
00:14:53,476 --> 00:14:55,286
In regular convolution, the


486
00:14:55,286 --> 00:14:57,066
input has to be multiplied with


487
00:14:57,106 --> 00:14:57,446
the weights.


488
00:14:57,886 --> 00:14:59,876
And for XNOR convolution the


489
00:14:59,876 --> 00:15:01,806
separation becomes a simple XNOR


490
00:15:01,806 --> 00:15:02,336
operation.


491
00:15:04,746 --> 00:15:06,066
And now let's talk about dilated


492
00:15:06,066 --> 00:15:06,656
convolution.


493
00:15:07,626 --> 00:15:08,996
So we already know how regular


494
00:15:08,996 --> 00:15:09,786
convolution works.


495
00:15:10,486 --> 00:15:12,396
You need to apply a filter to


496
00:15:12,396 --> 00:15:13,656
the input data to compute a


497
00:15:13,656 --> 00:15:14,706
single output value.


498
00:15:17,526 --> 00:15:18,786
But say you're working on an


499
00:15:18,786 --> 00:15:21,736
algorithm that requires global


500
00:15:21,736 --> 00:15:24,846
integration of a wider context


501
00:15:25,216 --> 00:15:26,166
of your input data.


502
00:15:26,816 --> 00:15:28,466
So instead of a 3 by 3 kernel,


503
00:15:28,536 --> 00:15:30,496
you may be using a 5 by 5 kernel


504
00:15:31,736 --> 00:15:32,476
to look out further.


505
00:15:33,026 --> 00:15:33,666
But that's a lot more


506
00:15:33,666 --> 00:15:34,756
computationally expensive.


507
00:15:35,256 --> 00:15:37,266
What you can do instead is use


508
00:15:37,266 --> 00:15:39,046
dilated convolutions which


509
00:15:39,046 --> 00:15:43,206
allows you to -- which allows


510
00:15:43,206 --> 00:15:45,256
you to use dilation factors to


511
00:15:45,256 --> 00:15:46,836
introduce gaps into your


512
00:15:46,836 --> 00:15:48,836
convolution kernel so that


513
00:15:48,836 --> 00:15:50,576
you're still using just a 3 by 3


514
00:15:50,576 --> 00:15:52,676
kernel, but you can look out


515
00:15:52,676 --> 00:15:53,016
further.


516
00:15:54,996 --> 00:15:55,876
And now let's talk about


517
00:15:55,946 --> 00:15:57,266
subluxal convolution and


518
00:15:57,266 --> 00:15:58,846
convolution transpose primitive;


519
00:15:59,766 --> 00:16:01,266
very commonly used for image


520
00:16:01,266 --> 00:16:01,836
upscaling.


521
00:16:03,066 --> 00:16:04,126
And let's think about how


522
00:16:04,176 --> 00:16:05,366
upscaling usually works.


523
00:16:05,456 --> 00:16:07,456
So you have your input data and


524
00:16:07,516 --> 00:16:09,196
you want to upscale it by a


525
00:16:09,196 --> 00:16:09,956
factor of 2.


526
00:16:12,336 --> 00:16:13,376
So you won't -- you have some


527
00:16:13,376 --> 00:16:14,566
missing pixels to compute.


528
00:16:15,216 --> 00:16:16,536
And usually upscaling is the


529
00:16:16,536 --> 00:16:18,156
fixed operation with a constant


530
00:16:18,156 --> 00:16:18,606
filter.


531
00:16:18,766 --> 00:16:20,336
So for example how would a box


532
00:16:20,336 --> 00:16:22,336
filter help you to upscale this


533
00:16:22,386 --> 00:16:22,756
image?


534
00:16:23,416 --> 00:16:25,146
So the box filter, which is take


535
00:16:25,316 --> 00:16:28,006
the known pixels and copy the


536
00:16:28,006 --> 00:16:29,326
known data into the missing


537
00:16:29,326 --> 00:16:30,786
location to get you upscaled


538
00:16:30,786 --> 00:16:31,216
results.


539
00:16:33,326 --> 00:16:35,196
For sub-pixel convolution, your


540
00:16:35,196 --> 00:16:36,646
filters are not constant.


541
00:16:36,976 --> 00:16:38,226
Your filters are learned from


542
00:16:38,226 --> 00:16:38,626
the data.


543
00:16:38,766 --> 00:16:40,286
They are your trained parameters


544
00:16:40,716 --> 00:16:41,786
that you get from the training


545
00:16:41,786 --> 00:16:42,906
step where the system was


546
00:16:42,986 --> 00:16:45,106
trained to do this task; to do


547
00:16:45,106 --> 00:16:45,946
image upscaling.


548
00:16:47,086 --> 00:16:49,176
So for 2x upscaling you get 4


549
00:16:49,236 --> 00:16:49,686
filters.


550
00:16:49,806 --> 00:16:51,656
For 4x upscaling you get 16


551
00:16:51,656 --> 00:16:52,656
filters and so on.


552
00:16:53,566 --> 00:16:55,446
So for our 2x upscaling we get


553
00:16:55,446 --> 00:16:57,456
our 4 filters and we apply them


554
00:16:57,456 --> 00:16:58,166
to the input data.


555
00:16:58,166 --> 00:17:00,216
And then the output of that


556
00:17:00,216 --> 00:17:02,076
operation is reshuffled to get


557
00:17:02,076 --> 00:17:03,476
your final full-resolution


558
00:17:03,476 --> 00:17:03,856
image.


559
00:17:04,925 --> 00:17:06,445
And now let's talk about how the


560
00:17:06,445 --> 00:17:08,076
convolution transpose primitive


561
00:17:08,156 --> 00:17:09,536
can be used to upscale images.


562
00:17:10,435 --> 00:17:12,026
So we have our inputs and we


563
00:17:12,026 --> 00:17:13,465
still have to compute our


564
00:17:13,465 --> 00:17:14,146
missing data.


565
00:17:14,945 --> 00:17:16,665
So the way that this primitive


566
00:17:17,156 --> 00:17:18,616
computes the missing data is


567
00:17:18,616 --> 00:17:19,586
that it applies a kind of


568
00:17:19,586 --> 00:17:21,296
convolution pass to this


569
00:17:21,296 --> 00:17:23,165
intermediate result with gaps to


570
00:17:23,165 --> 00:17:24,596
compute each output pixel.


571
00:17:25,185 --> 00:17:27,316
So that's how you get your


572
00:17:27,356 --> 00:17:28,256
upscaled output.


573
00:17:31,136 --> 00:17:32,216
And now we're going to show you


574
00:17:32,216 --> 00:17:33,556
how you can use these new


575
00:17:33,556 --> 00:17:35,046
convolution primitives to


576
00:17:35,046 --> 00:17:36,626
implement a real-world network.


577
00:17:37,096 --> 00:17:38,606
So we took this colorization


578
00:17:38,606 --> 00:17:41,486
network that takes black and


579
00:17:41,486 --> 00:17:42,886
white images as input and


580
00:17:42,886 --> 00:17:44,356
produces colorized images.


581
00:17:44,356 --> 00:17:47,156
And this particular network uses


582
00:17:47,236 --> 00:17:48,426
the dilated convolution


583
00:17:48,426 --> 00:17:50,396
primitive to integrate wider


584
00:17:50,396 --> 00:17:52,296
global context quicker.


585
00:17:52,926 --> 00:17:54,756
And it uses the convolution


586
00:17:54,756 --> 00:17:56,726
transpose primitive to upscale


587
00:17:56,726 --> 00:17:57,776
the results of the network.


588
00:18:00,166 --> 00:18:01,276
And now let's look at this


589
00:18:01,666 --> 00:18:03,966
colorization network in action.


590
00:18:10,226 --> 00:18:11,466
So in this demo we have a


591
00:18:11,466 --> 00:18:12,886
collection of black and white


592
00:18:12,886 --> 00:18:14,046
images like this image of a


593
00:18:14,046 --> 00:18:14,406
lion.


594
00:18:15,046 --> 00:18:16,416
And as soon as I tap on this


595
00:18:16,416 --> 00:18:17,796
image, the colorization network


596
00:18:17,796 --> 00:18:20,046
will run right here live on the


597
00:18:20,046 --> 00:18:21,126
device, and we'll see a


598
00:18:21,126 --> 00:18:21,946
colorized image.


599
00:18:23,906 --> 00:18:25,456
And let's try another example


600
00:18:25,456 --> 00:18:27,046
for this beautiful snowy


601
00:18:27,046 --> 00:18:27,616
mountain.


602
00:18:28,836 --> 00:18:30,036
And now we see it in color.


603
00:18:31,586 --> 00:18:33,756
And this beautiful lovely image


604
00:18:33,756 --> 00:18:35,016
of a dad and a daughter playing


605
00:18:35,016 --> 00:18:35,366
guitar.


606
00:18:35,366 --> 00:18:37,386
And now you can see them playing


607
00:18:37,386 --> 00:18:38,026
guitar in color.


608
00:18:39,516 --> 00:18:40,896
And I really like this one, the


609
00:18:40,896 --> 00:18:42,306
brown bear walking in the


610
00:18:42,356 --> 00:18:42,696
forest.


611
00:18:42,696 --> 00:18:43,756
So I think this network does


612
00:18:43,786 --> 00:18:45,146
just a really wonderful job.


613
00:18:46,886 --> 00:18:48,726
Okay. So that's it for the live


614
00:18:48,726 --> 00:18:48,916
demo.


615
00:18:49,516 --> 00:18:54,686
[ Applause ]


616
00:18:55,186 --> 00:18:55,796
Thank you so much.


617
00:18:57,766 --> 00:18:59,216
So we've added all of these new


618
00:18:59,216 --> 00:19:01,456
convolution CNN primitives, but


619
00:19:01,456 --> 00:19:02,056
that's not all.


620
00:19:02,976 --> 00:19:04,666
We also went back and improved


621
00:19:04,666 --> 00:19:06,046
the performance of some of the


622
00:19:06,206 --> 00:19:07,936
core CNN kernels that were


623
00:19:07,936 --> 00:19:09,646
available to you in iOS 10.


624
00:19:10,406 --> 00:19:11,776
So this chart will show the


625
00:19:11,806 --> 00:19:13,846
performance of the Inception-v3


626
00:19:13,846 --> 00:19:15,386
network, which is a commonly


627
00:19:15,386 --> 00:19:16,936
used network for image


628
00:19:17,056 --> 00:19:17,546
recognition.


629
00:19:18,756 --> 00:19:20,396
So it shows the performance of


630
00:19:20,396 --> 00:19:22,196
this network in iOS 11.


631
00:19:22,196 --> 00:19:23,786
And as you can see, we're


632
00:19:23,786 --> 00:19:25,866
bringing you at least 20 percent


633
00:19:25,866 --> 00:19:27,546
performance improvement across


634
00:19:27,756 --> 00:19:28,696
different iOS hardware.


635
00:19:30,406 --> 00:19:33,786
And now let's talk about the new


636
00:19:33,786 --> 00:19:37,096
neural network graph API.


637
00:19:37,716 --> 00:19:40,036
the neural networks are commonly


638
00:19:40,036 --> 00:19:41,416
described using a graph


639
00:19:41,416 --> 00:19:42,476
abstraction like this


640
00:19:42,476 --> 00:19:43,506
visualization of the


641
00:19:43,506 --> 00:19:44,616
Inception-v3 network.


642
00:19:44,616 --> 00:19:46,696
And we're now allowing to do


643
00:19:46,696 --> 00:19:48,706
just this using the new graph


644
00:19:48,706 --> 00:19:48,966
API.


645
00:19:50,446 --> 00:19:51,556
So let's zoom in on one of these


646
00:19:51,556 --> 00:19:53,186
inception modules.


647
00:19:54,676 --> 00:19:56,496
You have filter nodes which


648
00:19:56,496 --> 00:19:58,176
describe the operations that you


649
00:19:58,176 --> 00:19:59,216
can perform on your data.


650
00:19:59,526 --> 00:20:01,146
Such as convolution, pooling,


651
00:20:01,146 --> 00:20:01,566
etcetera.


652
00:20:02,556 --> 00:20:04,316
And you have image nodes which


653
00:20:04,316 --> 00:20:05,736
describe how the data flows


654
00:20:05,786 --> 00:20:06,546
between these different


655
00:20:06,546 --> 00:20:07,096
operations.


656
00:20:07,826 --> 00:20:11,306
So why did we add this new graph


657
00:20:11,306 --> 00:20:11,556
API?


658
00:20:11,926 --> 00:20:13,156
Well because it's easy to use.


659
00:20:13,686 --> 00:20:14,656
You get this compact


660
00:20:14,656 --> 00:20:16,116
representation of your entire


661
00:20:16,116 --> 00:20:18,326
network and you can save it to


662
00:20:18,326 --> 00:20:20,356
disk and restore it, and that


663
00:20:20,466 --> 00:20:21,546
works across platforms.


664
00:20:23,166 --> 00:20:24,426
You only need to initialize the


665
00:20:24,426 --> 00:20:26,366
graph once and then you can


666
00:20:26,366 --> 00:20:27,716
reuse it for multiple input


667
00:20:27,716 --> 00:20:28,106
images.


668
00:20:29,516 --> 00:20:31,476
And you can execute the entire


669
00:20:31,476 --> 00:20:34,056
graph on the GPU with a single


670
00:20:34,056 --> 00:20:34,346
call.


671
00:20:36,276 --> 00:20:37,536
There are no intermediate images


672
00:20:37,536 --> 00:20:39,196
for you to manage, you just need


673
00:20:39,196 --> 00:20:40,756
to take care of your input and


674
00:20:40,756 --> 00:20:41,036
output.


675
00:20:42,146 --> 00:20:45,016
Internally we use Metal heaps to


676
00:20:45,016 --> 00:20:46,796
make sure that the memory


677
00:20:46,796 --> 00:20:47,916
footprint of all your


678
00:20:47,916 --> 00:20:49,506
intermediate images is as small


679
00:20:49,506 --> 00:20:50,126
as possible.


680
00:20:50,606 --> 00:20:51,296
For example, for the


681
00:20:51,296 --> 00:20:53,376
Inception-v3 network this means


682
00:20:53,826 --> 00:20:56,856
5x memory savings and 10x viewer


683
00:20:56,856 --> 00:20:58,496
allocations, which I think is


684
00:20:58,556 --> 00:20:59,256
pretty impressive.


685
00:21:00,836 --> 00:21:02,926
So as I said, the graph does all


686
00:21:02,926 --> 00:21:03,956
the groundwork for you.


687
00:21:04,316 --> 00:21:05,646
It takes care of creating


688
00:21:05,846 --> 00:21:06,846
intermediate images.


689
00:21:06,996 --> 00:21:08,706
It takes care of sizing them.


690
00:21:09,296 --> 00:21:11,086
It also -- it even sizes your


691
00:21:11,086 --> 00:21:11,366
outputs.


692
00:21:11,786 --> 00:21:12,766
It takes care of the padding


693
00:21:12,766 --> 00:21:13,406
policies.


694
00:21:13,796 --> 00:21:15,016
It takes care of censoring.


695
00:21:15,426 --> 00:21:17,516
So in short, it's a lot less


696
00:21:17,566 --> 00:21:19,406
code for you to write and a lot


697
00:21:19,486 --> 00:21:20,746
fewer bugs for you to write as


698
00:21:20,746 --> 00:21:20,956
well.


699
00:21:21,956 --> 00:21:24,066
And when I say less code, I mean


700
00:21:24,596 --> 00:21:25,376
a lot less code.


701
00:21:26,206 --> 00:21:27,906
So last year we released this


702
00:21:27,996 --> 00:21:30,726
Metal recognition sample that


703
00:21:30,726 --> 00:21:32,036
uses the Inception-v3 network


704
00:21:32,036 --> 00:21:33,286
for image recognition.


705
00:21:34,326 --> 00:21:36,026
And we took that sample and


706
00:21:36,026 --> 00:21:37,576
converted it to use the new


707
00:21:37,806 --> 00:21:40,036
graph API and found that we had


708
00:21:40,036 --> 00:21:41,956
to write four times less code.


709
00:21:42,356 --> 00:21:43,956
And that's pretty much the same


710
00:21:43,956 --> 00:21:45,846
number of lines as Python code


711
00:21:46,226 --> 00:21:47,916
you would have to write in the


712
00:21:47,916 --> 00:21:48,886
open-source sensor flow


713
00:21:48,886 --> 00:21:50,436
framework to implement the same


714
00:21:50,436 --> 00:21:50,846
network.


715
00:21:51,476 --> 00:21:52,956
And we just want to mention that


716
00:21:52,956 --> 00:21:54,046
we will be releasing this


717
00:21:54,086 --> 00:21:57,196
updated sample code -- updated


718
00:21:57,196 --> 00:21:58,346
example as sample code.


719
00:21:59,026 --> 00:22:01,796
And now having all this


720
00:22:01,796 --> 00:22:03,276
information about your entire


721
00:22:03,276 --> 00:22:06,116
network allows us to deliver the


722
00:22:06,116 --> 00:22:07,956
best performance across


723
00:22:08,106 --> 00:22:08,866
different views.


724
00:22:09,336 --> 00:22:10,946
We make it easy for your to


725
00:22:10,946 --> 00:22:12,766
parallelize between the CPU and


726
00:22:12,766 --> 00:22:13,206
the GPU.


727
00:22:13,976 --> 00:22:15,876
so as the graph is executing --


728
00:22:16,326 --> 00:22:18,076
as the GPU is executing the


729
00:22:18,076 --> 00:22:19,986
graph of one input image, the


730
00:22:19,986 --> 00:22:21,686
CPU can already prepare to


731
00:22:21,686 --> 00:22:22,776
execute the graph for a


732
00:22:22,776 --> 00:22:23,716
different input image.


733
00:22:25,176 --> 00:22:26,606
We can also fuse graph nodes


734
00:22:26,676 --> 00:22:28,446
together like the convolution


735
00:22:28,856 --> 00:22:29,966
and neuron nodes.


736
00:22:31,856 --> 00:22:33,746
And we can execute graph nodes


737
00:22:33,746 --> 00:22:34,386
concurrently.


738
00:22:34,386 --> 00:22:36,256
So if we look at this inception


739
00:22:36,256 --> 00:22:38,056
module again, you can see that


740
00:22:38,056 --> 00:22:39,886
there are multiple rows of these


741
00:22:39,886 --> 00:22:41,386
nodes that can be executed


742
00:22:41,456 --> 00:22:43,036
completely independently of each


743
00:22:43,036 --> 00:22:43,236
other.


744
00:22:44,176 --> 00:22:45,486
And of course the output of


745
00:22:45,486 --> 00:22:47,326
these independent executions


746
00:22:47,926 --> 00:22:49,216
need to be concatenated via


747
00:22:49,216 --> 00:22:50,216
concatenation nodes.


748
00:22:51,206 --> 00:22:52,656
And the graph is smart enough to


749
00:22:52,656 --> 00:22:54,276
optimize those away as well.


750
00:22:54,886 --> 00:22:57,706
And now let's take a look at how


751
00:22:57,706 --> 00:22:59,586
you can use the new graph API.


752
00:23:00,296 --> 00:23:02,176
So this is the code for creating


753
00:23:02,176 --> 00:23:04,126
a convolution node using the


754
00:23:04,126 --> 00:23:04,646
graph API.


755
00:23:05,826 --> 00:23:07,256
So it takes an image as source


756
00:23:08,226 --> 00:23:09,486
and it also has weights.


757
00:23:09,486 --> 00:23:10,926
So let's talk about weights for


758
00:23:10,926 --> 00:23:11,176
a minute.


759
00:23:13,056 --> 00:23:14,256
Neural networks keep growing


760
00:23:14,256 --> 00:23:15,396
larger and larger in size.


761
00:23:16,136 --> 00:23:17,736
And if you have many convolution


762
00:23:17,736 --> 00:23:19,346
nodes in your networks, that


763
00:23:19,346 --> 00:23:21,436
means that the overall size of


764
00:23:21,496 --> 00:23:22,466
the weights for your entire


765
00:23:22,466 --> 00:23:23,616
network could be quite


766
00:23:23,616 --> 00:23:24,246
considerable.


767
00:23:25,216 --> 00:23:27,016
And to help with that we've


768
00:23:27,016 --> 00:23:30,126
added a convolution data source


769
00:23:30,186 --> 00:23:31,296
protocol that you can implement


770
00:23:31,656 --> 00:23:33,186
and it provides just in time


771
00:23:33,606 --> 00:23:35,036
loading and purging of weights


772
00:23:35,076 --> 00:23:35,276
data.


773
00:23:36,296 --> 00:23:40,046
So the idea is that the weights


774
00:23:40,046 --> 00:23:41,546
for your entire network do not


775
00:23:41,546 --> 00:23:43,196
have to be loaded in memory all


776
00:23:43,196 --> 00:23:44,086
at the same time.


777
00:23:44,626 --> 00:23:45,956
They also do not have to be


778
00:23:45,956 --> 00:23:46,886
loaded in advance.


779
00:23:48,396 --> 00:23:49,656
To help minimize the memory


780
00:23:49,656 --> 00:23:51,556
footprint, when we initialize


781
00:23:51,556 --> 00:23:53,136
the graph and we process a


782
00:23:53,136 --> 00:23:54,516
particular convolution layer,


783
00:23:55,096 --> 00:23:56,126
we'll load the weights for that


784
00:23:56,126 --> 00:23:57,726
convolution layer and then we


785
00:23:57,856 --> 00:23:59,486
purge them before we move on to


786
00:23:59,486 --> 00:24:00,726
the next convolution layer.


787
00:24:02,226 --> 00:24:03,586
What you have to do is to


788
00:24:03,586 --> 00:24:05,146
implement this initialization


789
00:24:05,146 --> 00:24:06,916
method which just knows where


790
00:24:06,916 --> 00:24:08,376
the data is but it doesn't


791
00:24:08,376 --> 00:24:09,086
actually load it.


792
00:24:10,096 --> 00:24:11,256
And then when the graph calls


793
00:24:11,256 --> 00:24:13,266
the load function that alerts


794
00:24:13,266 --> 00:24:14,846
you that the weights need to be


795
00:24:14,846 --> 00:24:15,236
loaded.


796
00:24:15,366 --> 00:24:16,546
And then when the purge function


797
00:24:16,546 --> 00:24:18,166
is called by the graph then you


798
00:24:18,166 --> 00:24:19,316
can release the weights.


799
00:24:21,586 --> 00:24:22,526
And now let's build a graph.


800
00:24:23,446 --> 00:24:24,926
So here we're implementing this


801
00:24:24,926 --> 00:24:26,116
makeGraph function.


802
00:24:26,596 --> 00:24:28,366
And on the left you can see all


803
00:24:28,366 --> 00:24:29,636
the nodes that make up our


804
00:24:29,636 --> 00:24:30,826
network that we need to build.


805
00:24:31,256 --> 00:24:32,926
So then we create the nodes.


806
00:24:33,226 --> 00:24:34,446
So we create the convolution


807
00:24:34,446 --> 00:24:34,836
node.


808
00:24:35,016 --> 00:24:35,616
The pooling node.


809
00:24:35,616 --> 00:24:37,456
And then the rest of the nodes.


810
00:24:37,756 --> 00:24:38,606
So we have the nodes.


811
00:24:38,606 --> 00:24:40,226
How do we connect them into a


812
00:24:40,226 --> 00:24:40,446
graph?


813
00:24:41,646 --> 00:24:43,186
So we just take the result image


814
00:24:43,186 --> 00:24:44,986
of one node and pass it as a


815
00:24:45,066 --> 00:24:46,516
source image to the next node.


816
00:24:46,516 --> 00:24:48,106
And then we have our graph.


817
00:24:49,736 --> 00:24:51,236
And now let's run it on the GPU.


818
00:24:51,906 --> 00:24:54,066
So first we do our usual Metal


819
00:24:54,066 --> 00:24:54,456
setup.


820
00:24:54,886 --> 00:24:56,016
We initialize the graph.


821
00:24:56,676 --> 00:24:58,166
We take care of our input data


822
00:24:58,866 --> 00:25:01,066
and then we encode the graph to


823
00:25:01,066 --> 00:25:01,506
the GPU.


824
00:25:02,276 --> 00:25:04,026
And the data in the output image


825
00:25:04,556 --> 00:25:06,546
will be -- the output image will


826
00:25:06,546 --> 00:25:08,466
be populated with data when the


827
00:25:08,466 --> 00:25:09,576
command buffer completes.


828
00:25:10,086 --> 00:25:11,526
And then we have an option to


829
00:25:11,526 --> 00:25:12,996
wait for the GPU to finish.


830
00:25:13,406 --> 00:25:14,686
But we don't want you to do


831
00:25:14,686 --> 00:25:14,956
that.


832
00:25:15,886 --> 00:25:17,506
When this happens the CPU is


833
00:25:17,506 --> 00:25:19,246
waiting for the GPU to finish


834
00:25:19,836 --> 00:25:21,486
before it can start encoding the


835
00:25:21,486 --> 00:25:22,846
next run of the graph.


836
00:25:23,486 --> 00:25:25,256
And this introduces bubbles into


837
00:25:25,256 --> 00:25:26,266
your pipeline, which can


838
00:25:26,526 --> 00:25:28,036
adversely affect performance.


839
00:25:29,816 --> 00:25:30,686
So what we want you to do


840
00:25:30,686 --> 00:25:32,606
instead is to use the new


841
00:25:32,606 --> 00:25:34,596
asynchronous executeAsync API.


842
00:25:35,426 --> 00:25:37,896
So with this API your Metal


843
00:25:37,966 --> 00:25:39,436
setup is even smaller.


844
00:25:39,626 --> 00:25:41,076
So you just need to get the


845
00:25:41,076 --> 00:25:41,736
Metal device.


846
00:25:42,136 --> 00:25:43,096
Then you still need to


847
00:25:43,096 --> 00:25:44,016
initialize your graph.


848
00:25:44,056 --> 00:25:46,426
Prepare the input data and then


849
00:25:46,426 --> 00:25:47,856
you executeAsync call.


850
00:25:49,586 --> 00:25:52,376
It returns immediately and then


851
00:25:52,376 --> 00:25:54,216
the output image will be ready


852
00:25:55,196 --> 00:25:56,236
when this code inside the


853
00:25:56,236 --> 00:25:57,086
closure executes.


854
00:25:57,796 --> 00:25:59,056
But in the meantime, you don't


855
00:25:59,056 --> 00:26:00,136
have to wait for the GPU to


856
00:26:00,306 --> 00:26:02,056
finish, you can already proceed


857
00:26:02,056 --> 00:26:03,676
with a coding and new GPU task.


858
00:26:04,396 --> 00:26:07,236
And this way the CPU and the GPU


859
00:26:07,236 --> 00:26:08,846
are executing concurrently.


860
00:26:09,406 --> 00:26:10,316
There are no bubbles in your


861
00:26:10,316 --> 00:26:12,756
pipeline and they're both


862
00:26:12,756 --> 00:26:14,376
utilized to full capacity.


863
00:26:16,756 --> 00:26:18,996
Okay. And now I will do a live


864
00:26:18,996 --> 00:26:21,106
demo that demonstrates the


865
00:26:21,146 --> 00:26:22,846
performance difference between


866
00:26:22,966 --> 00:26:24,526
the synchronous and asynchronous


867
00:26:24,526 --> 00:26:24,786
APIs.


868
00:26:24,786 --> 00:26:27,576
And this demo will be using the


869
00:26:27,576 --> 00:26:29,576
Inception-v3 network for image


870
00:26:29,576 --> 00:26:30,116
recognition.


871
00:26:30,516 --> 00:26:30,806
All right.


872
00:26:31,136 --> 00:26:32,726
So I will be starting with


873
00:26:32,726 --> 00:26:34,416
synchronous API and here we're


874
00:26:34,416 --> 00:26:35,706
detecting a water bottle.


875
00:26:35,706 --> 00:26:38,276
And we're getting about 50


876
00:26:38,276 --> 00:26:41,756
milliseconds per second per


877
00:26:41,756 --> 00:26:42,596
image on average.


878
00:26:42,826 --> 00:26:44,066
And now I will switch to the


879
00:26:44,066 --> 00:26:44,956
asynchronous API.


880
00:26:44,956 --> 00:26:47,586
And now we're getting about 36


881
00:26:47,586 --> 00:26:49,546
milliseconds per image on


882
00:26:49,546 --> 00:26:49,936
average.


883
00:26:49,936 --> 00:26:51,656
So that's pretty good


884
00:26:51,686 --> 00:26:52,666
performance improvement.


885
00:26:54,776 --> 00:26:55,066
All right.


886
00:26:55,196 --> 00:26:56,436
So that's it for the live demo.


887
00:26:58,516 --> 00:27:04,016
[ Applause ]


888
00:27:04,516 --> 00:27:04,876
Thank you.


889
00:27:06,566 --> 00:27:07,656
Okay. Now that we've talked


890
00:27:07,656 --> 00:27:08,626
about the new neural network


891
00:27:08,626 --> 00:27:10,926
graph API and I showed you how


892
00:27:10,956 --> 00:27:12,716
easy it is to use and what great


893
00:27:12,716 --> 00:27:14,016
performance you can achieve with


894
00:27:14,016 --> 00:27:16,086
it, let's now switch gears and


895
00:27:16,086 --> 00:27:17,166
talk about recurrent neural


896
00:27:17,166 --> 00:27:17,566
networks.


897
00:27:19,416 --> 00:27:20,386
So what are recurrent neural


898
00:27:20,386 --> 00:27:20,786
networks?


899
00:27:23,406 --> 00:27:25,456
So one disadvantage of CNNs is


900
00:27:25,456 --> 00:27:27,276
their inability to remember


901
00:27:27,306 --> 00:27:28,326
anything that happened in the


902
00:27:28,326 --> 00:27:28,466
past.


903
00:27:29,456 --> 00:27:31,226
They can take one image as input


904
00:27:31,896 --> 00:27:33,926
and generate a single output


905
00:27:34,446 --> 00:27:36,316
such as the set of probabilities


906
00:27:36,356 --> 00:27:37,396
of what is depicted in the


907
00:27:37,396 --> 00:27:37,836
image.


908
00:27:39,056 --> 00:27:41,016
RNNs on the other hand have


909
00:27:41,016 --> 00:27:41,446
memory.


910
00:27:42,346 --> 00:27:43,466
And they're good at operating on


911
00:27:43,546 --> 00:27:44,066
sequences.


912
00:27:44,536 --> 00:27:48,256
So they can take one input such


913
00:27:48,256 --> 00:27:49,576
as a set of probabilities of


914
00:27:49,636 --> 00:27:51,016
what is depicted in the image


915
00:27:51,616 --> 00:27:52,966
and generate a sequence of


916
00:27:53,036 --> 00:27:53,366
outputs.


917
00:27:53,366 --> 00:27:56,196
So a sequence of words that make


918
00:27:56,196 --> 00:27:57,586
up a caption for this image.


919
00:27:59,416 --> 00:28:01,716
They can also take a sequence of


920
00:28:01,806 --> 00:28:03,506
inputs such as a sentence in


921
00:28:03,506 --> 00:28:06,626
English and generate a sequence


922
00:28:06,626 --> 00:28:08,506
of outputs such as the same


923
00:28:08,666 --> 00:28:09,946
sentence translated to a


924
00:28:09,946 --> 00:28:12,376
different language like Russian


925
00:28:12,466 --> 00:28:13,056
or Finnish.


926
00:28:13,366 --> 00:28:16,356
And we support a number of


927
00:28:16,356 --> 00:28:17,756
different of variants of RNNs.


928
00:28:18,586 --> 00:28:20,146
The single gate RNN, the long


929
00:28:20,146 --> 00:28:22,156
short-term memory RNN or LSTM,


930
00:28:22,596 --> 00:28:24,586
and multiple variants of LSTMs.


931
00:28:24,866 --> 00:28:26,336
The GRU and the MGU.


932
00:28:27,766 --> 00:28:29,336
So let's talk about the simplest


933
00:28:29,366 --> 00:28:31,326
kind of RNN, the single gate


934
00:28:31,326 --> 00:28:31,526
RNN.


935
00:28:33,666 --> 00:28:34,966
the single gate RNN has a


936
00:28:34,966 --> 00:28:37,006
recurrent unit which enables the


937
00:28:37,066 --> 00:28:38,676
previous output over RNN to


938
00:28:38,996 --> 00:28:40,346
affect the output of the


939
00:28:40,406 --> 00:28:41,846
subsequent iterations of the


940
00:28:41,846 --> 00:28:42,406
same RNN.


941
00:28:43,606 --> 00:28:45,496
But the single gate RNNs are not


942
00:28:45,546 --> 00:28:47,466
powerful enough to carry on


943
00:28:47,466 --> 00:28:48,746
important information for many


944
00:28:48,746 --> 00:28:49,286
iterations.


945
00:28:50,136 --> 00:28:51,676
Because the current output of an


946
00:28:51,786 --> 00:28:53,976
RNN -- of the single gate RNN is


947
00:28:53,976 --> 00:28:54,996
also its current state.


948
00:28:54,996 --> 00:28:55,976
There's nothing else there.


949
00:28:57,146 --> 00:28:59,426
The solution to this is the long


950
00:28:59,476 --> 00:29:01,596
short-term memory RNN or LSTM.


951
00:29:02,376 --> 00:29:03,906
It's built from single gate RNNs


952
00:29:03,906 --> 00:29:06,246
and it has an internal memory


953
00:29:06,246 --> 00:29:06,506
cell.


954
00:29:07,436 --> 00:29:08,856
And a certain combination of


955
00:29:08,956 --> 00:29:10,596
gates control how the


956
00:29:10,596 --> 00:29:13,106
information flows inside LSTM.


957
00:29:13,436 --> 00:29:15,016
And what is stored and not


958
00:29:15,136 --> 00:29:16,266
stored in the memory cell.


959
00:29:16,846 --> 00:29:19,656
So let's take a look at the


960
00:29:19,656 --> 00:29:21,316
architecture of LSTM in more


961
00:29:21,316 --> 00:29:21,776
detail.


962
00:29:22,206 --> 00:29:25,246
As I said, the most important


963
00:29:25,356 --> 00:29:27,846
entity inside LSTM is the memory


964
00:29:27,886 --> 00:29:30,406
cell which is updated in every


965
00:29:30,476 --> 00:29:31,536
duration of LSTM.


966
00:29:31,536 --> 00:29:33,426
So you can think of each


967
00:29:33,846 --> 00:29:35,246
iteration of LSTM is this


968
00:29:35,306 --> 00:29:37,456
transition between the old and


969
00:29:37,456 --> 00:29:38,016
new memory.


970
00:29:38,676 --> 00:29:40,786
And now let's talk about the


971
00:29:40,816 --> 00:29:41,036
gates.


972
00:29:41,566 --> 00:29:43,376
So first there is a forget gate


973
00:29:44,216 --> 00:29:45,876
which decides what to keep and


974
00:29:45,876 --> 00:29:47,206
what not to keep from old


975
00:29:47,206 --> 00:29:47,566
memory.


976
00:29:48,966 --> 00:29:50,456
And then there are the inputs


977
00:29:50,456 --> 00:29:51,836
and the cell gates and their


978
00:29:51,836 --> 00:29:53,996
combined contribution determines


979
00:29:54,066 --> 00:29:55,786
what from the current input will


980
00:29:55,786 --> 00:29:56,936
affect the new memory.


981
00:29:56,936 --> 00:29:59,136
And then the combination of all


982
00:29:59,136 --> 00:30:00,866
of these three gates is combined


983
00:30:01,226 --> 00:30:04,596
to update the memory cell.


984
00:30:05,696 --> 00:30:07,576
And finally, there is the output


985
00:30:07,626 --> 00:30:09,656
gate which determines what from


986
00:30:09,656 --> 00:30:11,976
the previous inputs the -- the


987
00:30:12,476 --> 00:30:14,076
previous output, the current


988
00:30:14,076 --> 00:30:16,126
inputs and the new memory will


989
00:30:16,126 --> 00:30:17,936
affect the output of LSTM.


990
00:30:19,196 --> 00:30:20,626
So now that you know what LSTM


991
00:30:20,626 --> 00:30:22,206
is made up of, let's take a look


992
00:30:22,206 --> 00:30:24,006
at how you can create one using


993
00:30:24,006 --> 00:30:24,576
our framework.


994
00:30:25,616 --> 00:30:27,536
So first you create a descriptor


995
00:30:27,756 --> 00:30:28,576
for the LSTM.


996
00:30:29,146 --> 00:30:31,306
And then you need to initialize


997
00:30:31,526 --> 00:30:31,876
the gates.


998
00:30:32,436 --> 00:30:33,676
So what controls the gates?


999
00:30:33,676 --> 00:30:35,146
So what controls the gates with


1000
00:30:35,306 --> 00:30:36,156
-- what controls how they


1001
00:30:36,156 --> 00:30:37,756
operate is the trained


1002
00:30:37,756 --> 00:30:38,386
parameters.


1003
00:30:39,146 --> 00:30:40,156
The ones that come from the


1004
00:30:40,156 --> 00:30:41,726
training step where you train a


1005
00:30:41,726 --> 00:30:43,526
system to do a particular task.


1006
00:30:45,566 --> 00:30:47,496
And there are multiple gates for


1007
00:30:47,496 --> 00:30:48,586
you to initialize as you can


1008
00:30:48,646 --> 00:30:49,856
see, but we're only showing two


1009
00:30:49,856 --> 00:30:52,356
initializations just to be


1010
00:30:52,356 --> 00:30:52,726
brief.


1011
00:30:53,206 --> 00:30:54,336
And as you can see, we're also


1012
00:30:54,336 --> 00:30:56,046
using a data source provider.


1013
00:30:56,046 --> 00:30:57,446
The same one I showed you before


1014
00:30:57,556 --> 00:30:58,606
to initialize the weights.


1015
00:30:59,656 --> 00:31:01,536
And the next step is to create


1016
00:31:01,536 --> 00:31:03,606
our LSTM layer and now we want


1017
00:31:03,606 --> 00:31:04,876
to run it on the GPU.


1018
00:31:06,586 --> 00:31:08,646
So we need to create our arrays


1019
00:31:08,646 --> 00:31:10,976
that will hold the input and


1020
00:31:10,976 --> 00:31:13,236
output for the sequence of the


1021
00:31:13,236 --> 00:31:14,266
LSTM executions.


1022
00:31:14,886 --> 00:31:16,076
And then we encode the sequence


1023
00:31:16,076 --> 00:31:16,716
to the GPU.


1024
00:31:17,416 --> 00:31:19,446
And here we're showing you the


1025
00:31:19,666 --> 00:31:21,546
-- a matrix-based RNN, but we


1026
00:31:21,546 --> 00:31:22,646
just want to mention that we


1027
00:31:22,686 --> 00:31:25,726
also support RNNs that operate


1028
00:31:25,726 --> 00:31:27,636
on MPS images via convolutions.


1029
00:31:30,176 --> 00:31:31,216
And now let's take a look at an


1030
00:31:31,216 --> 00:31:32,016
actual example.


1031
00:31:32,596 --> 00:31:34,366
So we'll use image captioning as


1032
00:31:34,366 --> 00:31:35,706
an example of using LSTM.


1033
00:31:36,726 --> 00:31:38,566
So as you recall, I told you


1034
00:31:38,896 --> 00:31:40,646
that deep learning algorithms


1035
00:31:40,646 --> 00:31:42,076
have two phases.


1036
00:31:42,346 --> 00:31:43,316
The training phase and the


1037
00:31:43,316 --> 00:31:44,026
inference phase.


1038
00:31:44,816 --> 00:31:46,816
So to train a system to caption


1039
00:31:46,816 --> 00:31:49,196
images you need to feed it a


1040
00:31:49,196 --> 00:31:51,056
large number of images with


1041
00:31:51,056 --> 00:31:52,356
human-generated captions.


1042
00:31:53,836 --> 00:31:56,606
So what does this system have?


1043
00:31:56,656 --> 00:31:57,686
Like what is it made out of?


1044
00:31:58,536 --> 00:32:02,016
So this system has a CNN and a


1045
00:32:02,016 --> 00:32:03,906
RNN working together to generate


1046
00:32:03,906 --> 00:32:04,346
captions.


1047
00:32:04,746 --> 00:32:07,316
The CNN is used to figure out


1048
00:32:07,316 --> 00:32:09,316
what's depicted in the image and


1049
00:32:09,316 --> 00:32:10,886
then the RNN is used to generate


1050
00:32:10,956 --> 00:32:11,806
the actual caption.


1051
00:32:13,256 --> 00:32:15,076
And the output of that process


1052
00:32:15,156 --> 00:32:17,006
is the trained parameters which


1053
00:32:17,006 --> 00:32:19,036
are required for the next step,


1054
00:32:20,186 --> 00:32:20,886
the inference step.


1055
00:32:21,676 --> 00:32:25,606
So in the inference phase, the


1056
00:32:25,656 --> 00:32:27,766
trained parameters control both


1057
00:32:27,766 --> 00:32:29,826
the operation of the CNN layers


1058
00:32:29,826 --> 00:32:31,866
and the operation of the RNN


1059
00:32:31,866 --> 00:32:32,146
gates.


1060
00:32:33,206 --> 00:32:37,276
And then for each image it's


1061
00:32:37,336 --> 00:32:39,166
processed by both the CNN and


1062
00:32:39,446 --> 00:32:41,326
the RNN to generate a caption.


1063
00:32:42,216 --> 00:32:43,246
So we already know a good


1064
00:32:43,246 --> 00:32:44,636
network for figuring out what's


1065
00:32:44,706 --> 00:32:45,816
depicted in the image.


1066
00:32:46,086 --> 00:32:47,506
It's the Inception-v3 network,


1067
00:32:47,876 --> 00:32:48,626
so we'll use that.


1068
00:32:49,186 --> 00:32:50,456
And we just talked about LSTMs,


1069
00:32:50,456 --> 00:32:52,236
so let's use that to generate


1070
00:32:52,446 --> 00:32:53,036
our caption.


1071
00:32:53,996 --> 00:32:56,456
And the caption generation phase


1072
00:32:57,266 --> 00:32:58,176
-- the caption generation


1073
00:32:58,176 --> 00:32:59,726
process also has two phases.


1074
00:33:00,006 --> 00:33:02,206
So first we have the LSTM


1075
00:33:02,486 --> 00:33:03,646
initialization phase.


1076
00:33:04,846 --> 00:33:06,126
So we run our Inception-v3


1077
00:33:06,126 --> 00:33:08,616
network and we actually run all


1078
00:33:08,616 --> 00:33:10,496
of the layers except the very


1079
00:33:10,496 --> 00:33:11,986
last SoftMax layer.


1080
00:33:12,236 --> 00:33:13,386
And the output of that is a


1081
00:33:13,386 --> 00:33:15,016
feature vector which has


1082
00:33:15,016 --> 00:33:16,196
information about what is


1083
00:33:16,196 --> 00:33:17,226
depicted in the image.


1084
00:33:17,906 --> 00:33:19,016
And then we take that feature


1085
00:33:19,016 --> 00:33:20,566
vector and convert it to a


1086
00:33:20,566 --> 00:33:23,246
compact representation that's


1087
00:33:23,246 --> 00:33:24,286
required by LSTM.


1088
00:33:24,286 --> 00:33:26,596
And then run that through LSTM


1089
00:33:26,946 --> 00:33:27,746
to initialize it.


1090
00:33:28,736 --> 00:33:30,466
And then once we have our


1091
00:33:30,466 --> 00:33:32,436
initialized LSTM, then we're


1092
00:33:32,436 --> 00:33:33,586
ready for the next phase.


1093
00:33:34,606 --> 00:33:36,066
Our actual caption generation


1094
00:33:36,066 --> 00:33:36,376
phase.


1095
00:33:38,116 --> 00:33:39,676
And we start this process by


1096
00:33:39,676 --> 00:33:41,366
passing in a special sentence


1097
00:33:41,436 --> 00:33:44,026
start ID token to our LSTM.


1098
00:33:44,236 --> 00:33:45,046
And the output of that


1099
00:33:45,046 --> 00:33:46,756
operations is a sequence of


1100
00:33:46,756 --> 00:33:50,006
words which are, you know, the


1101
00:33:50,006 --> 00:33:51,276
words that are connected to what


1102
00:33:51,276 --> 00:33:52,666
is depicted in the image.


1103
00:33:53,526 --> 00:33:55,806
And then we pass those words to


1104
00:33:55,806 --> 00:33:57,226
a SoftMax layer which computes


1105
00:33:57,226 --> 00:33:58,796
probabilities for these words.


1106
00:33:59,326 --> 00:34:01,176
And we pick the three best ones.


1107
00:34:01,326 --> 00:34:03,276
And these three best words are


1108
00:34:03,276 --> 00:34:05,816
also our one-word partial


1109
00:34:05,816 --> 00:34:07,546
captions for a particular image.


1110
00:34:08,126 --> 00:34:09,726
So we take those words and pass


1111
00:34:09,806 --> 00:34:11,856
them to the next situation of


1112
00:34:11,946 --> 00:34:15,045
LSTM which function is to now


1113
00:34:15,096 --> 00:34:16,886
come up with three best two-word


1114
00:34:16,916 --> 00:34:19,216
captions for our image and so


1115
00:34:19,216 --> 00:34:19,386
on.


1116
00:34:19,466 --> 00:34:21,416
We execute for N iterations


1117
00:34:21,886 --> 00:34:23,076
until we reach a stopping


1118
00:34:23,076 --> 00:34:25,596
condition, which is when we


1119
00:34:25,626 --> 00:34:27,116
either reach the maximum number


1120
00:34:27,116 --> 00:34:28,966
of words that we want to be in


1121
00:34:28,966 --> 00:34:30,856
our caption, or when the


1122
00:34:30,856 --> 00:34:32,136
probabilities evolving the newly


1123
00:34:32,136 --> 00:34:33,906
generating captions drop to 0.


1124
00:34:34,985 --> 00:34:35,996
So I know this is still pretty


1125
00:34:35,996 --> 00:34:36,505
abstract.


1126
00:34:36,846 --> 00:34:39,005
So let's look at the output of


1127
00:34:39,386 --> 00:34:42,266
LSTM -- of an actual output of


1128
00:34:42,326 --> 00:34:44,556
LSTM for several iterations for


1129
00:34:44,556 --> 00:34:45,545
a particular image.


1130
00:34:46,216 --> 00:34:49,786
So in this image we have, you


1131
00:34:49,786 --> 00:34:51,636
know, our surfer riding a wave.


1132
00:34:51,636 --> 00:34:53,146
And we want to compute the top


1133
00:34:53,146 --> 00:34:54,666
three captions for this image.


1134
00:34:55,696 --> 00:34:57,286
And in the first iteration of


1135
00:34:57,366 --> 00:34:59,936
LSTM we generate three best


1136
00:34:59,936 --> 00:35:00,306
words.


1137
00:35:02,336 --> 00:35:04,446
So -- which are our best


1138
00:35:04,446 --> 00:35:05,686
one-word captions for this


1139
00:35:05,686 --> 00:35:05,996
image.


1140
00:35:06,506 --> 00:35:07,596
So "man", "a", and "the".


1141
00:35:08,316 --> 00:35:10,596
And the word "a" has the highest


1142
00:35:10,596 --> 00:35:11,186
probability.


1143
00:35:11,936 --> 00:35:13,546
So we take these three words and


1144
00:35:13,546 --> 00:35:15,196
we pass them to the next


1145
00:35:15,196 --> 00:35:16,436
iteration of LSTM.


1146
00:35:17,166 --> 00:35:19,356
And in this iteration, for every


1147
00:35:19,356 --> 00:35:21,416
one of these three starter


1148
00:35:22,436 --> 00:35:24,416
words, LSTM generates three new


1149
00:35:24,416 --> 00:35:26,026
words that have the highest


1150
00:35:26,026 --> 00:35:28,496
probability of following each


1151
00:35:28,496 --> 00:35:29,606
one of these starter words.


1152
00:35:30,666 --> 00:35:31,966
Right? So we have three new


1153
00:35:31,966 --> 00:35:33,556
words to follow the word "man".


1154
00:35:33,946 --> 00:35:35,256
Three new words to follow the


1155
00:35:35,256 --> 00:35:37,766
word "a", and three new words to


1156
00:35:37,826 --> 00:35:38,976
follow the word "the".


1157
00:35:40,686 --> 00:35:42,296
And now as you can see, each one


1158
00:35:42,296 --> 00:35:44,486
of these two-word captions also


1159
00:35:44,486 --> 00:35:45,486
have a probability.


1160
00:35:46,026 --> 00:35:47,866
And because the word "a" had


1161
00:35:47,936 --> 00:35:49,286
such a high probability in the


1162
00:35:49,286 --> 00:35:53,366
first iteration, then the


1163
00:35:53,366 --> 00:35:54,646
captions that start with the


1164
00:35:54,646 --> 00:35:56,426
word "a" in the second iteration


1165
00:35:56,476 --> 00:35:58,156
also end up having the highest


1166
00:35:58,156 --> 00:35:58,796
probability.


1167
00:35:58,896 --> 00:36:00,916
Why? Because the probability of


1168
00:36:00,916 --> 00:36:02,506
a two-word caption is just a


1169
00:36:02,536 --> 00:36:04,516
product of the probabilities of


1170
00:36:04,516 --> 00:36:05,706
the words that make up that


1171
00:36:05,706 --> 00:36:06,006
caption.


1172
00:36:07,116 --> 00:36:08,886
So that's how we get these three


1173
00:36:08,886 --> 00:36:09,396
best ones.


1174
00:36:09,696 --> 00:36:11,226
And then we take them and we


1175
00:36:11,226 --> 00:36:12,836
move on to the next iteration.


1176
00:36:13,186 --> 00:36:14,406
And in the next iteration we


1177
00:36:14,406 --> 00:36:15,936
just add one more word to our


1178
00:36:15,936 --> 00:36:17,536
captions so that we have


1179
00:36:17,846 --> 00:36:18,806
three-word captions.


1180
00:36:19,206 --> 00:36:19,996
And then we compute the


1181
00:36:19,996 --> 00:36:21,836
probabilities of those captions


1182
00:36:21,836 --> 00:36:22,886
and pick the best three.


1183
00:36:23,936 --> 00:36:25,106
And we move on to the next


1184
00:36:25,106 --> 00:36:27,026
iteration where we just end up


1185
00:36:27,026 --> 00:36:28,746
adding one more word to our


1186
00:36:28,746 --> 00:36:29,186
caption.


1187
00:36:29,186 --> 00:36:30,566
So we have four-word captions.


1188
00:36:30,976 --> 00:36:31,876
And then we compute the


1189
00:36:31,926 --> 00:36:33,186
probabilities of all these


1190
00:36:33,236 --> 00:36:34,546
captions and pick the best


1191
00:36:34,606 --> 00:36:34,816
three.


1192
00:36:36,176 --> 00:36:37,306
And so on -- I think you get the


1193
00:36:37,306 --> 00:36:37,646
idea.


1194
00:36:37,866 --> 00:36:38,936
Let's just skip to the end.


1195
00:36:39,296 --> 00:36:41,416
So in the end, we get our three


1196
00:36:41,416 --> 00:36:43,506
top captions for this particular


1197
00:36:43,506 --> 00:36:43,936
image.


1198
00:36:43,936 --> 00:36:45,906
And the best one is a man riding


1199
00:36:45,906 --> 00:36:47,426
a wave on top of a surfboard,


1200
00:36:47,656 --> 00:36:48,616
which I think it's pretty close.


1201
00:36:50,966 --> 00:36:52,346
So -- [applause] and let's now


1202
00:36:52,346 --> 00:36:52,726
do a demo.


1203
00:36:53,508 --> 00:36:55,508
[ Applause ]


1204
00:36:58,476 --> 00:37:00,116
So now we'll do a demo of this


1205
00:37:00,606 --> 00:37:02,206
-- of the captioning network.


1206
00:37:03,346 --> 00:37:04,856
So we have a collection of


1207
00:37:04,856 --> 00:37:07,156
images here, and as soon as I


1208
00:37:07,156 --> 00:37:09,036
tap on an image then the CNN


1209
00:37:09,136 --> 00:37:10,826
will run to determine what is


1210
00:37:10,826 --> 00:37:12,526
depicted in the image.


1211
00:37:12,526 --> 00:37:14,046
And then the RNN will run to


1212
00:37:14,046 --> 00:37:15,226
generate the actual caption.


1213
00:37:15,356 --> 00:37:16,036
So let's try it out.


1214
00:37:17,826 --> 00:37:19,446
>> A man riding a wave on top of


1215
00:37:19,446 --> 00:37:19,766
a surfboard.


1216
00:37:19,766 --> 00:37:22,356
>> So we already know this.


1217
00:37:23,526 --> 00:37:24,566
Now let's try another image.


1218
00:37:24,996 --> 00:37:26,536
>> An old truck is parked in the


1219
00:37:26,536 --> 00:37:27,106
field.


1220
00:37:27,626 --> 00:37:28,936
>> So the network actually knows


1221
00:37:28,976 --> 00:37:30,356
that it's an old truck and that


1222
00:37:30,356 --> 00:37:31,786
it's parked and not moving,


1223
00:37:31,966 --> 00:37:33,336
which I think is pretty


1224
00:37:33,556 --> 00:37:34,156
impressive.


1225
00:37:34,786 --> 00:37:35,656
Now let's try one more.


1226
00:37:37,026 --> 00:37:38,496
>> A black and white dog laying


1227
00:37:38,496 --> 00:37:39,086
in the grass.


1228
00:37:39,796 --> 00:37:40,896
>> So the network knows that


1229
00:37:40,896 --> 00:37:42,176
it's a black and white dog and


1230
00:37:42,176 --> 00:37:43,636
that it's laying in the grass,


1231
00:37:44,326 --> 00:37:45,106
not running.


1232
00:37:45,236 --> 00:37:46,356
Not walking.


1233
00:37:46,726 --> 00:37:47,876
Not sitting.


1234
00:37:48,336 --> 00:37:50,266
Laying in the grass.


1235
00:37:50,766 --> 00:37:52,796
So pretty cool.


1236
00:37:53,516 --> 00:37:57,786
[ Applause ]


1237
00:37:58,286 --> 00:37:58,726
Thank you.


1238
00:37:59,306 --> 00:38:01,166
And on this note, let's go to


1239
00:38:01,236 --> 00:38:01,586
the summary.


1240
00:38:02,066 --> 00:38:03,536
So in this session we talked


1241
00:38:03,536 --> 00:38:05,246
about all of the new primitives


1242
00:38:05,276 --> 00:38:07,006
that we added to the MPS


1243
00:38:07,336 --> 00:38:08,206
framework this year.


1244
00:38:08,586 --> 00:38:10,236
We've expanded our support for


1245
00:38:10,236 --> 00:38:12,386
image processing primitives and


1246
00:38:12,786 --> 00:38:13,736
for convolutional neural


1247
00:38:13,736 --> 00:38:14,136
networks.


1248
00:38:15,006 --> 00:38:16,596
And we've added support for


1249
00:38:16,596 --> 00:38:18,886
linear algebra and recurrent


1250
00:38:18,886 --> 00:38:22,226
neural networks.


1251
00:38:23,096 --> 00:38:24,666
the framework is optimized for


1252
00:38:24,666 --> 00:38:26,146
iOS, as I told you, and now


1253
00:38:26,596 --> 00:38:29,226
these primitives are also all


1254
00:38:29,226 --> 00:38:30,016
available on the Mac.


1255
00:38:31,116 --> 00:38:32,416
We also talked about the new


1256
00:38:32,416 --> 00:38:34,676
neural network graph API and we


1257
00:38:34,676 --> 00:38:36,406
showed you how easy it is to


1258
00:38:36,406 --> 00:38:38,336
use, to build and execute your


1259
00:38:38,336 --> 00:38:39,666
networks on the GPU.


1260
00:38:40,426 --> 00:38:42,186
And that it makes it possible


1261
00:38:42,186 --> 00:38:43,586
for us to deliver the best


1262
00:38:43,666 --> 00:38:45,346
performance for your networks


1263
00:38:45,346 --> 00:38:46,336
across the different GPUs.


1264
00:38:46,336 --> 00:38:49,776
And we would love to see you use


1265
00:38:49,776 --> 00:38:51,976
all of this new functionality to


1266
00:38:51,976 --> 00:38:53,526
create a really great apps and


1267
00:38:53,526 --> 00:38:55,836
tell us about it.


1268
00:38:56,116 --> 00:38:57,296
So please check out the related


1269
00:38:57,296 --> 00:38:58,856
Metal 2 sessions and the


1270
00:38:58,856 --> 00:39:01,186
sessions on the core ML and


1271
00:39:01,676 --> 00:39:03,006
Accelerate and Vision


1272
00:39:03,006 --> 00:39:03,486
Frameworks.


1273
00:39:04,716 --> 00:39:06,036
And for more information about


1274
00:39:06,096 --> 00:39:07,636
this session and for links of


1275
00:39:07,706 --> 00:39:09,936
sample code, please check out


1276
00:39:09,986 --> 00:39:11,426
this link on our developer


1277
00:39:11,426 --> 00:39:14,276
website and thank you so much


1278
00:39:14,276 --> 00:39:15,636
for coming, and have a great


1279
00:39:15,636 --> 00:39:15,846
WWDC.


1280
00:39:16,516 --> 00:39:20,500
[ Applause ]

