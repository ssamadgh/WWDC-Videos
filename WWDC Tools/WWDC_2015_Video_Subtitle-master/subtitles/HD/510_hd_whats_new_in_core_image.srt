1
00:00:24,031 --> 00:00:26,031
[ Applause ]


2
00:00:26,046 --> 00:00:26,586
>> DAVID HAYWARD: Good
morning, everyone,


3
00:00:26,586 --> 00:00:29,416
my name is David Hayward, and
it's my privilege today to talk


4
00:00:29,416 --> 00:00:32,415
about what's new in
Core Image on iOS 9


5
00:00:32,576 --> 00:00:34,496
and Mac OS X El Capitan.


6
00:00:35,136 --> 00:00:38,166
So to start off, we will be
covering several things today.


7
00:00:38,276 --> 00:00:40,576
First off I will give a brief
introduction to Core Image


8
00:00:40,676 --> 00:00:42,736
for those new to the subject.


9
00:00:42,866 --> 00:00:44,356
I recommend that you go back


10
00:00:44,356 --> 00:00:46,236
and see our presentations
from last year.


11
00:00:46,316 --> 00:00:48,196
In particular, there was
a great discussion on how


12
00:00:48,196 --> 00:00:49,926
to write kernels in Core Image.


13
00:00:50,686 --> 00:00:52,936
Next, we'll be talking
about what's new


14
00:00:52,936 --> 00:00:53,876
in Core Image this year.


15
00:00:53,926 --> 00:00:55,456
We have a lot of stuff
to talk about here.


16
00:00:55,876 --> 00:00:58,736
And the other third of our
discussion today will be talking


17
00:00:58,736 --> 00:01:00,786
about using Core
Image and bridging it


18
00:01:00,786 --> 00:01:04,855
with other graphics
frameworks on our platforms.


19
00:01:05,476 --> 00:01:08,956
First, an introduction
to Core Image.


20
00:01:09,456 --> 00:01:13,306
In concept, the idea of Core
Image is you can apply filters


21
00:01:13,306 --> 00:01:13,906
to images.


22
00:01:14,436 --> 00:01:17,086
In a simple example, you can
start with an input image


23
00:01:17,086 --> 00:01:19,996
and apply a filter to do a
color effect such as sepia tone,


24
00:01:20,726 --> 00:01:21,756
but if you don't like the color


25
00:01:21,756 --> 00:01:24,666
of sepia tone you can
apply another color effect


26
00:01:24,766 --> 00:01:27,256
to change the hue to make it
more of a blue-toned image.


27
00:01:27,816 --> 00:01:29,996
You can also use Core
Image to apply effects


28
00:01:29,996 --> 00:01:32,166
such as geometry-distorting
events.


29
00:01:32,436 --> 00:01:36,416
In this example, we are just
using a simple transform to zoom


30
00:01:36,416 --> 00:01:37,576
in on a portion of the image.


31
00:01:38,466 --> 00:01:40,716
You can think of there
being an intermediate image


32
00:01:40,806 --> 00:01:41,796
between every filter.


33
00:01:42,416 --> 00:01:45,396
However, the way we
implement filters,


34
00:01:45,396 --> 00:01:47,056
they are actually very
lightweight objects


35
00:01:47,056 --> 00:01:48,876
that take very little
time to create,


36
00:01:49,336 --> 00:01:51,366
and there are not necessarily
intermediate buffers


37
00:01:51,366 --> 00:01:52,706
in between them.


38
00:01:52,786 --> 00:01:54,956
Another important
concept is that associated


39
00:01:54,956 --> 00:01:57,216
with each filter is
one or more kernels.


40
00:01:57,636 --> 00:02:01,306
CI kernels are small subroutines
that apply the effect


41
00:02:01,306 --> 00:02:03,106
that that kernel is
trying to achieve.


42
00:02:04,586 --> 00:02:05,576
One of the other key features


43
00:02:05,576 --> 00:02:12,116
of Core Image is we concatenate
these kernels into a program


44
00:02:12,456 --> 00:02:15,576
as much as possible to minimize
the use of intermediate buffers


45
00:02:15,576 --> 00:02:18,706
and improve performance.


46
00:02:19,786 --> 00:02:22,696
Another key feature in Core
Image is what we call region


47
00:02:22,696 --> 00:02:23,586
of interest support.


48
00:02:24,146 --> 00:02:26,746
The idea is that if you are only
rendering a portion of an image,


49
00:02:27,376 --> 00:02:29,286
either because you are
zoomed in on a large image


50
00:02:29,286 --> 00:02:31,466
or because it's being
rendered out in tiles,


51
00:02:32,556 --> 00:02:35,666
we can ask each filter
that's been rendered how much


52
00:02:35,666 --> 00:02:38,896
of the input image it needs, and
from that we can calculate back


53
00:02:38,896 --> 00:02:41,316
to the source image the
exact region that's needed


54
00:02:41,546 --> 00:02:45,136
of that image in order to
produce the desired output.


55
00:02:45,286 --> 00:02:48,876
This is another great feature
of Core Image that allows us


56
00:02:48,876 --> 00:02:50,336
to get good performance
especially


57
00:02:50,336 --> 00:02:51,686
when working on large images.


58
00:02:53,136 --> 00:02:56,046
There are four main
classes you need to be aware


59
00:02:56,046 --> 00:02:57,326
of when you are using
Core Image.


60
00:02:57,716 --> 00:02:59,066
The first is the CI kernel.


61
00:02:59,066 --> 00:03:00,646
I mentioned this earlier.


62
00:03:01,046 --> 00:03:03,366
This represents the
program or routine written


63
00:03:03,366 --> 00:03:05,486
in Core Image's kernel language.


64
00:03:05,996 --> 00:03:09,456
The second key class is
the filter, the CI filter.


65
00:03:09,816 --> 00:03:13,326
This is a mutable object, and
it can have multiple inputs,


66
00:03:13,366 --> 00:03:16,596
and these input parameters
can be either numbers


67
00:03:16,596 --> 00:03:18,046
or vectors or other images.


68
00:03:19,416 --> 00:03:23,106
The filter uses one or
more kernels in order


69
00:03:23,106 --> 00:03:26,156
to create an output image
based on the current state


70
00:03:26,156 --> 00:03:27,236
of the input parameters.


71
00:03:27,916 --> 00:03:30,346
A CI image is an
immutable object


72
00:03:30,746 --> 00:03:33,696
that represents the recipe
to produce that image based


73
00:03:33,696 --> 00:03:35,946
on the previous kernels
that have been applied.


74
00:03:37,446 --> 00:03:40,126
And lastly, there is
the CIContext object.


75
00:03:40,356 --> 00:03:41,826
And this is a more
heavyweight object.


76
00:03:41,826 --> 00:03:44,436
This is the object through
which Core Image will render.


77
00:03:44,936 --> 00:03:48,376
You want to avoid creating these
too often in your application,


78
00:03:48,416 --> 00:03:50,356
so if you are doing
fast animation you want


79
00:03:50,356 --> 00:03:51,226
to do this just once.


80
00:03:51,836 --> 00:03:52,436
And the great thing


81
00:03:52,436 --> 00:03:54,546
about CIContext is
they can be implemented


82
00:03:54,546 --> 00:03:57,736
on various different back-end
renderers in our system.


83
00:03:58,326 --> 00:04:02,786
So the next thing I would
like to talk about now


84
00:04:02,786 --> 00:04:04,516
that the introduction
is behind us is to talk


85
00:04:04,516 --> 00:04:06,456
about what's new this
year in Core Image.


86
00:04:07,716 --> 00:04:09,536
We have several things
to talk about today.


87
00:04:10,136 --> 00:04:13,786
We will be talking about Metal,
talking about new filters,


88
00:04:14,276 --> 00:04:18,375
new detector, color management
support, and some improvements


89
00:04:18,375 --> 00:04:20,836
to the kernel classes
and languages.


90
00:04:21,815 --> 00:04:24,576
And most important thing I want
to talk about is that we now


91
00:04:24,936 --> 00:04:28,096
in Core Image have a unified
implementation across both


92
00:04:28,096 --> 00:04:32,136
of our platforms, so for the
most part unless we mention


93
00:04:32,136 --> 00:04:34,786
otherwise the behavior of Core
Image is completely consistent


94
00:04:35,036 --> 00:04:37,296
and equivalent between
iOS and OS X.


95
00:04:37,296 --> 00:04:41,036
And this is a great feature for
developers to be able to rely


96
00:04:41,036 --> 00:04:42,166
on this consistent behavior.


97
00:04:43,036 --> 00:04:46,126
This can be little
things such as the fact


98
00:04:46,126 --> 00:04:47,876
that when you include
the Core Image header,


99
00:04:48,166 --> 00:04:50,616
you can include Core
Image, Core Image.H,


100
00:04:50,816 --> 00:04:52,276
regardless what platform
you are.


101
00:04:52,336 --> 00:04:53,326
So it makes it a lot easier


102
00:04:53,326 --> 00:04:56,266
if you're doing cross-platform
code.


103
00:04:56,626 --> 00:05:03,976
We have now got API parity
between the two platforms.


104
00:05:03,976 --> 00:05:07,316
So one of the major
things we want to talk


105
00:05:07,316 --> 00:05:09,686
about today is Core
Image support for Metal.


106
00:05:09,746 --> 00:05:12,526
And we will be talking this
in much more detail later


107
00:05:12,526 --> 00:05:13,506
on in the presentation,


108
00:05:13,836 --> 00:05:15,346
but I want to give you
the highlights right now.


109
00:05:15,886 --> 00:05:17,446
The key things to think of is


110
00:05:17,446 --> 00:05:21,256
that Metal Textures can now be
given as an input to Core Image,


111
00:05:21,806 --> 00:05:24,126
and also Metal Textures can
be the output of Core Image.


112
00:05:25,146 --> 00:05:28,146
And internally, Core
Image context will be able


113
00:05:28,146 --> 00:05:30,426
to use Metal as their
internal renderer.


114
00:05:31,226 --> 00:05:32,986
What that means is if
you have written a kernel


115
00:05:32,986 --> 00:05:35,966
in CI's kernel language, it
will be automatically translated


116
00:05:36,406 --> 00:05:38,336
on the fly to Metal language.


117
00:05:40,256 --> 00:05:42,696
Another thing to keep in mind
is some of our built-in filters,


118
00:05:42,696 --> 00:05:46,706
notably Gaussian and convolution
filters, are now built on top


119
00:05:46,706 --> 00:05:48,126
of Metal performance
shaders in order


120
00:05:48,126 --> 00:05:50,496
to get the best possible
performance on the diversity


121
00:05:50,496 --> 00:05:51,726
of platforms that are supported.


122
00:05:53,636 --> 00:05:56,606
A little bit about filters.


123
00:05:57,396 --> 00:05:58,796
Again, as I mentioned before,


124
00:05:58,796 --> 00:06:01,016
we now have a unified
implementation of Core Image.


125
00:06:01,016 --> 00:06:04,366
This means we have 200 buil-
in filters on both platforms.


126
00:06:04,906 --> 00:06:07,876
That means we have
brought a lot of filters


127
00:06:08,096 --> 00:06:10,736
to the iOS implementation
of Core Image.


128
00:06:10,886 --> 00:06:12,736
Over 40 have been
added in this release.


129
00:06:13,126 --> 00:06:14,846
These fall into different
categories.


130
00:06:14,846 --> 00:06:18,406
There are fun filters like
comic effect, and CMYK Halftone,


131
00:06:18,406 --> 00:06:20,346
and Droste, and Page Curl.


132
00:06:20,346 --> 00:06:22,846
There are also some convolutions
filters that are useful,


133
00:06:22,846 --> 00:06:27,336
such as median filters, edge
detection, and noise reduction.


134
00:06:27,916 --> 00:06:31,866
Also we have reduction
filters, which are useful


135
00:06:31,866 --> 00:06:34,586
for image analysis
such as Area Maximum


136
00:06:34,586 --> 00:06:36,106
or Column Averaging an image.


137
00:06:37,476 --> 00:06:40,056
In order to give you a
little taste of this,


138
00:06:40,056 --> 00:06:42,576
I want to show you the
latest version of one


139
00:06:42,576 --> 00:06:44,916
of our sample applications
called Core Image FunHouse,


140
00:06:45,596 --> 00:06:48,336
and we try to update
this every year.


141
00:06:49,266 --> 00:06:52,336
One of the things we have now
is now that we have 200 filters,


142
00:06:52,926 --> 00:06:55,936
when you bring up the Filters
pop-up in this application,


143
00:06:55,936 --> 00:06:58,156
we have now broken them
up into categories.


144
00:06:58,556 --> 00:07:00,246
You can also see
highlighted in red all


145
00:07:00,246 --> 00:07:03,306
of the new filters we have
added, and there is now an API


146
00:07:03,306 --> 00:07:05,966
for you to determine
what release a filter was


147
00:07:05,966 --> 00:07:06,486
included in.


148
00:07:07,816 --> 00:07:11,396
And this is, of course, showing
the CMYK Halftone effect getting


149
00:07:11,396 --> 00:07:14,626
good performance on an
iPad at Retina resolution.


150
00:07:14,936 --> 00:07:22,366
There are two new filters
we have added to Core Image


151
00:07:22,366 --> 00:07:24,236
on both platforms
by popular request.


152
00:07:24,586 --> 00:07:27,756
These are filters for
generating bar codes.


153
00:07:27,876 --> 00:07:31,226
So the input in this case
to a filter is not a number


154
00:07:31,226 --> 00:07:33,246
or another image,
but a text string.


155
00:07:34,006 --> 00:07:38,296
And we have added these two
for generating PDF417 bar codes


156
00:07:38,336 --> 00:07:42,176
and code 128 bar codes.


157
00:07:43,606 --> 00:07:46,526
Another feature of Core
Image is what we call our CI


158
00:07:46,606 --> 00:07:47,616
detector classes.


159
00:07:47,876 --> 00:07:49,906
These are our classes we
have released in the past


160
00:07:49,906 --> 00:07:52,926
for doing things like
detecting faces in an image,


161
00:07:53,416 --> 00:07:55,346
detecting QR codes in an image,


162
00:07:55,796 --> 00:07:57,386
or detecting rectangles
in an image.


163
00:07:57,656 --> 00:07:58,846
And we have a new one this year,


164
00:07:58,846 --> 00:08:01,596
which is for detecting
areas of text in an image.


165
00:08:02,036 --> 00:08:05,006
The idea in the filter is to
locate areas that are likely


166
00:08:05,006 --> 00:08:06,876
to contain upright text.


167
00:08:07,636 --> 00:08:10,666
So let me give a brief demo
of this running on an iPad.


168
00:08:11,006 --> 00:08:13,756
We have hooked this up to the
Core Image FunHouse application.


169
00:08:14,186 --> 00:08:17,786
So we have an old box that was
on my shelf, and if we turn


170
00:08:17,786 --> 00:08:20,756
on the text detector it's
locating the upright text,


171
00:08:21,236 --> 00:08:24,236
both the runs of text and
the individual characters.


172
00:08:24,776 --> 00:08:27,786
And as we zoom in and
rotate the camera,


173
00:08:28,146 --> 00:08:30,786
the upright text will
detect some of the text


174
00:08:30,786 --> 00:08:33,395
that was at an angle as well.


175
00:08:34,426 --> 00:08:36,236
So that's our new text
detector so I'm excited


176
00:08:36,236 --> 00:08:39,385
to see what developers
come up with to use that.


177
00:08:42,196 --> 00:08:43,736
Another thing we
have brought now


178
00:08:43,736 --> 00:08:46,216
with our unified implementation
of Core Image is now


179
00:08:46,216 --> 00:08:48,436
on iOS we have the
great functionality


180
00:08:48,436 --> 00:08:49,876
of automatic color management.


181
00:08:50,146 --> 00:08:53,256
This has been available on
OS X ever since the beginning


182
00:08:53,256 --> 00:08:56,476
of Core Image, but now we
have this on iOS as well.


183
00:08:57,096 --> 00:09:01,266
What this means is that Core
Image now supports ICC-based


184
00:09:02,396 --> 00:09:04,936
CGColorSpaceRefs fully.


185
00:09:05,846 --> 00:09:09,236
And these can be used on input
images or output images or even


186
00:09:09,236 --> 00:09:10,656
as a working space
in Core Image.


187
00:09:11,856 --> 00:09:14,056
This is through great
work that's been done


188
00:09:14,356 --> 00:09:19,466
to support ColorSync
on iOS as well.


189
00:09:21,066 --> 00:09:22,176
What this means to users is


190
00:09:22,176 --> 00:09:25,406
that automatically you will
get correct rendering of TIFFs


191
00:09:25,406 --> 00:09:27,756
or JPGs that are tagged
with color spaces.


192
00:09:28,426 --> 00:09:29,826
Many images are tagged with sRGB


193
00:09:29,826 --> 00:09:34,086
and those have been rendered
correctly on previous versions


194
00:09:34,126 --> 00:09:38,976
of iOS, but now if you have an
image tagged with a color space


195
00:09:39,156 --> 00:09:41,126
that is not sRGB, you
get the correct behavior.


196
00:09:41,526 --> 00:09:43,766
Here is an example
of an image tagged


197
00:09:43,766 --> 00:09:45,076
with the Pro Photo color space.


198
00:09:45,076 --> 00:09:48,586
The red bench in the
background is desaturated,


199
00:09:48,586 --> 00:09:50,186
and skin tones look poor.


200
00:09:50,686 --> 00:09:56,296
When you correctly see the
embedded ICC profile on this,


201
00:09:56,456 --> 00:09:57,686
the image is rendered correctly.


202
00:09:58,856 --> 00:10:03,936
And this you get
automatically in Core Image.


203
00:10:04,086 --> 00:10:07,766
We also have some new
support for CI kernel classes


204
00:10:07,766 --> 00:10:09,406
that is now available on OS X


205
00:10:09,406 --> 00:10:11,146
like it has been
on iOS in the past.


206
00:10:11,196 --> 00:10:13,446
This is another benefit of
our unified implementation.


207
00:10:14,626 --> 00:10:17,866
For example, we have two
classes called CI color kernel


208
00:10:17,866 --> 00:10:19,026
and CI warp kernels.


209
00:10:19,336 --> 00:10:22,096
The idea behind these classes is
to make it much easier for you


210
00:10:22,096 --> 00:10:23,956
to write the most
common basic filters.


211
00:10:24,806 --> 00:10:26,526
Traditionally on
OS X, if you wanted


212
00:10:26,526 --> 00:10:27,916
to write a simple blend filter


213
00:10:27,986 --> 00:10:32,306
to blend three images given
a mask, you would have


214
00:10:32,786 --> 00:10:34,466
to write several lines
of code to sample


215
00:10:34,826 --> 00:10:37,966
from the sampler correctly,
and then you would do the math


216
00:10:38,576 --> 00:10:39,796
to combine the three images.


217
00:10:40,736 --> 00:10:42,606
If you use CI color
kernel classes,


218
00:10:43,006 --> 00:10:44,246
the code becomes much simpler.


219
00:10:44,786 --> 00:10:48,386
So now the input to the kernel
is a sampler, underscore,


220
00:10:48,386 --> 00:10:51,306
underscore sample
parameter, and the code


221
00:10:51,306 --> 00:10:54,266
for the kernel is just the math
for mixing the three results.


222
00:10:54,806 --> 00:10:56,766
So this is a great
thing for developers.


223
00:10:56,766 --> 00:10:57,456
It makes it easier.


224
00:10:57,456 --> 00:11:01,336
It also makes the job for
Core Image easier to simplify


225
00:11:01,906 --> 00:11:04,036
and concatenate programs.


226
00:11:07,146 --> 00:11:09,096
We also have a lot
of improvements


227
00:11:09,096 --> 00:11:12,006
to the CI kernel language
are available on OS X.


228
00:11:12,596 --> 00:11:16,086
Our unified implementation when
we compile CI kernel language


229
00:11:16,086 --> 00:11:18,976
into the destination
context language,


230
00:11:19,916 --> 00:11:22,356
we do that using LLVM
technology at Apple.


231
00:11:22,406 --> 00:11:26,776
And what this this has given us
is new features in our language,


232
00:11:27,376 --> 00:11:30,226
such as If, For, and While, that
were not previously available.


233
00:11:31,456 --> 00:11:33,696
CI kernels in existing
apps should work fine.


234
00:11:34,096 --> 00:11:36,616
However, with the new compiler
we have stricter warnings


235
00:11:36,616 --> 00:11:39,596
so if your app is linked
against El Capitan or later,


236
00:11:40,396 --> 00:11:42,306
keep a look out for
any compiler warnings.


237
00:11:42,966 --> 00:11:47,256
As an example of this, here is
a simple example of a kernel


238
00:11:47,256 --> 00:11:51,406
that wasn't possible before,
on OS X using kernel language,


239
00:11:51,766 --> 00:11:54,536
and that's because this
particular filter has an input


240
00:11:54,536 --> 00:11:55,926
parameter, which is a count.


241
00:11:56,566 --> 00:11:58,646
And we want to have a For
loop inside this kernel


242
00:11:58,946 --> 00:12:00,676
that loops based on
that count variable.


243
00:12:01,466 --> 00:12:03,546
In this particular
example, we're trying


244
00:12:03,546 --> 00:12:06,026
to do a motion blur along
a vector for n points.


245
00:12:06,636 --> 00:12:08,526
And this is now a
trivial kernel to write.


246
00:12:09,446 --> 00:12:12,336
You can get fancier,
and you can have a


247
00:12:12,336 --> 00:12:13,766
For loop with an early exit.


248
00:12:14,596 --> 00:12:17,346
In this case, we are sampling
from that image until we get


249
00:12:17,346 --> 00:12:21,566
to an area of the image that is
not opaque, and then we break


250
00:12:21,566 --> 00:12:24,756
out of the For loop and
return the average color


251
00:12:24,756 --> 00:12:29,756
of only the colors
that are in the image.


252
00:12:29,916 --> 00:12:32,306
So one thing to keep in mind is


253
00:12:32,306 --> 00:12:34,256
with our kernel language is
what are our overall goals


254
00:12:34,256 --> 00:12:35,046
of this language are.


255
00:12:35,476 --> 00:12:38,566
What we want to do is enable
you to write kernels once


256
00:12:38,566 --> 00:12:42,766
and have them run
regardless of the device


257
00:12:42,766 --> 00:12:44,046
that your kernels
are running on.


258
00:12:44,546 --> 00:12:46,866
So it will run independent of
what system you are running on,


259
00:12:46,866 --> 00:12:50,836
whether iOS or OS X, what
size your input image is.


260
00:12:51,156 --> 00:12:54,916
The CI kernel language has
support for destination core


261
00:12:54,916 --> 00:12:57,536
to sampler transforms so we
can support automatic tiling


262
00:12:57,536 --> 00:12:58,086
of images.


263
00:12:58,696 --> 00:13:02,136
And the CI kernel
language works independent


264
00:13:02,186 --> 00:13:03,656
of what our back-end
renderer is,


265
00:13:03,906 --> 00:13:07,176
so whether we are using
Metal, or OpenCL, or OpenGL,


266
00:13:07,176 --> 00:13:10,436
or OpenGL ES, you can
write your algorithms


267
00:13:10,606 --> 00:13:11,986
in the CI kernel language once.


268
00:13:16,056 --> 00:13:18,066
So that's the highlights
of what's new


269
00:13:18,066 --> 00:13:19,366
in Core Image this year.


270
00:13:20,056 --> 00:13:22,096
The next subject we would
like to talk about is how


271
00:13:22,096 --> 00:13:23,856
to bridge Core Image
with other frameworks.


272
00:13:24,056 --> 00:13:26,016
Specifically, some of the wealth


273
00:13:26,016 --> 00:13:28,086
of other great graphics
frameworks available


274
00:13:28,086 --> 00:13:28,746
on our platform.


275
00:13:30,406 --> 00:13:35,236
We have great imaging
frameworks on our platform


276
00:13:35,236 --> 00:13:39,276
such as Core Animation,
SceneKit, SpriteKit, Metal,


277
00:13:39,746 --> 00:13:43,816
AV Foundation, IOSurfaces,
and various View classes.


278
00:13:44,306 --> 00:13:46,046
We spent a lot of
time this year trying


279
00:13:46,046 --> 00:13:49,656
to make these work right
together with Core Image.


280
00:13:49,996 --> 00:13:51,806
So to start that discussion,


281
00:13:51,926 --> 00:13:55,126
I would like to introduce
Tony Chu, who will be talking


282
00:13:55,126 --> 00:13:56,916
about Core Image and
Metal in more detail.


283
00:13:58,516 --> 00:14:04,606
[ Applause ]


284
00:14:05,106 --> 00:14:05,726
>> TONY CHU: Thank you, David.


285
00:14:06,106 --> 00:14:08,756
Good morning, my name
is Tony and I would


286
00:14:08,756 --> 00:14:10,866
like to first tell you
about a little bit more


287
00:14:10,866 --> 00:14:12,176
about Core Image and Metal.


288
00:14:12,176 --> 00:14:17,156
So as David mentioned earlier,
this year we have added support


289
00:14:17,156 --> 00:14:18,886
for rendering with
Metal in Core Image,


290
00:14:19,386 --> 00:14:21,906
and one of the reasons
we did that is to add


291
00:14:21,906 --> 00:14:24,266
to our extensive suite
of supported image types


292
00:14:24,746 --> 00:14:28,446
such as IOSurface and CGImag,e
all of which can be used


293
00:14:28,446 --> 00:14:32,226
as inputs or outputs to a CI
filter regardless of the type


294
00:14:32,226 --> 00:14:33,246
of CIContext you have.


295
00:14:34,116 --> 00:14:37,746
But if you have an OpenGL-based
CIContext, you can also render


296
00:14:37,746 --> 00:14:39,646
to and from OpenGL textures.


297
00:14:40,986 --> 00:14:43,636
And now this year if you
have a Metal-based CIContext,


298
00:14:43,876 --> 00:14:45,916
you can also render to
and from Metal Textures.


299
00:14:46,656 --> 00:14:48,996
Previously, without this
support you would have had


300
00:14:48,996 --> 00:14:51,696
to convert a Metal Texture to
one of the existing image types,


301
00:14:52,166 --> 00:14:55,366
which would have likely
incurred an expensive data copy


302
00:14:55,366 --> 00:14:56,906
between the CPU and GPU.


303
00:14:57,556 --> 00:14:59,496
With proper support
we can render to


304
00:14:59,496 --> 00:15:01,276
and from these resources
very efficiently.


305
00:15:01,786 --> 00:15:06,246
Let's take a look at some of
the new APIs we have available


306
00:15:06,246 --> 00:15:08,176
for Metal support in Core Image.


307
00:15:09,016 --> 00:15:12,546
The first is an API that allows
you to initialize a CI image


308
00:15:12,916 --> 00:15:18,126
with an input Metal Texture as
well as an optional dictionary


309
00:15:18,126 --> 00:15:20,546
where you can specify things
such as the color space


310
00:15:21,116 --> 00:15:22,576
that that texture
it tagged with.


311
00:15:23,216 --> 00:15:25,096
That's an example of
one of the advantages


312
00:15:25,096 --> 00:15:27,426
of using a higher-level
framework, such as Core Image,


313
00:15:27,826 --> 00:15:29,226
is that it will take
care of details


314
00:15:29,226 --> 00:15:32,016
such as color management
automatically for you.


315
00:15:33,936 --> 00:15:35,706
Then, in order to do rendering


316
00:15:35,706 --> 00:15:37,986
with these Metal-based
resources, you will want


317
00:15:37,986 --> 00:15:41,836
to create a new CIContext that
is a Metal-based CIContext


318
00:15:42,166 --> 00:15:45,066
by giving it the Metal device
your application is using.


319
00:15:45,066 --> 00:15:49,286
And, again, you can specify an
options dictionary for things


320
00:15:49,286 --> 00:15:52,896
such as working color
space or working floor mat


321
00:15:52,896 --> 00:15:56,886
for intermediate buffers or
even, you can even declare


322
00:15:56,886 --> 00:16:03,336
that you want to use
a low-priority GPU.


323
00:16:03,546 --> 00:16:06,636
In any case, with this
new Metal-based CIContext,


324
00:16:06,816 --> 00:16:09,736
we have the new render
API that allows you


325
00:16:09,736 --> 00:16:12,966
to render any CI image to
an output Metal Texture.


326
00:16:14,006 --> 00:16:15,786
And one of the nice
features I want to call


327
00:16:15,786 --> 00:16:17,826
out about this API
is the ability


328
00:16:17,826 --> 00:16:19,586
to specify optional
command buffer.


329
00:16:21,126 --> 00:16:23,816
If you want things nice and
simple, you can specify nil


330
00:16:23,816 --> 00:16:26,736
and in that case Core Image
will create one internally


331
00:16:27,286 --> 00:16:29,126
and code all the
necessary commands to it


332
00:16:29,126 --> 00:16:31,076
and then commit it
before returning,


333
00:16:31,666 --> 00:16:33,056
which will then effectively
schedule


334
00:16:33,056 --> 00:16:34,646
that render call on the GPU.


335
00:16:35,166 --> 00:16:39,266
But you can also provide a
command buffer to that call,


336
00:16:39,266 --> 00:16:42,596
and in that case Core Image will
merely encode commands to it


337
00:16:42,916 --> 00:16:44,876
and return without
committing it.


338
00:16:45,366 --> 00:16:48,406
What that gives you is full
control on how you want


339
00:16:48,406 --> 00:16:53,486
to schedule your command buffer
for rendering on the GPU as well


340
00:16:53,486 --> 00:16:56,486
as the flexibility to
insert CI filters anywhere


341
00:16:56,486 --> 00:16:57,446
into a command buffer.


342
00:16:57,586 --> 00:17:00,886
So let me explain that in
a little bit more detail.


343
00:17:00,886 --> 00:17:03,376
For those who are new to Metal,


344
00:17:03,376 --> 00:17:05,976
rendering with Metal basically
involves encoding a series


345
00:17:05,976 --> 00:17:07,726
of render commands
to a command buffer.


346
00:17:08,576 --> 00:17:10,715
In this case, we have
two sets of commands.


347
00:17:10,715 --> 00:17:16,185
And with that new API that we
just saw, you can now insert


348
00:17:16,185 --> 00:17:19,736
that CI filter anywhere into
this command buffer such as


349
00:17:19,736 --> 00:17:23,886
at the very beginning or at
the very end, or even right


350
00:17:24,106 --> 00:17:26,156
in the very middle between
these two render commands.


351
00:17:26,856 --> 00:17:29,156
So you can imagine this might
be a situation where you want


352
00:17:29,156 --> 00:17:32,206
to do some draw, cause,
and render to some texture


353
00:17:32,206 --> 00:17:35,656
and then feed the texture
into a series of CI filters,


354
00:17:36,696 --> 00:17:38,176
generate some output
texture from that,


355
00:17:38,176 --> 00:17:40,106
and do more rendering with it.


356
00:17:42,016 --> 00:17:46,116
And internally, Core Image will
then encode all the commands


357
00:17:46,256 --> 00:17:48,466
for each filter you may
have in your image graph.


358
00:17:49,046 --> 00:17:52,306
And in fact, as David
mentioned earlier,


359
00:17:52,796 --> 00:17:55,096
some of our built-in filters
will use Metal performance


360
00:17:55,096 --> 00:17:56,586
shaders to take advantage


361
00:17:56,586 --> 00:17:59,516
of these highly optimized
shaders specifically tuned


362
00:17:59,516 --> 00:18:00,766
for Metal-capable devices.


363
00:18:01,436 --> 00:18:06,346
And lastly, I want to
mention that this type


364
00:18:06,346 --> 00:18:09,116
of calling convention lends
itself perfectly to be able


365
00:18:09,116 --> 00:18:12,006
to use CI to render
directly to a MetalKit view.


366
00:18:12,566 --> 00:18:15,176
So to explain that further,


367
00:18:15,176 --> 00:18:16,616
I would like to show
you sample code.


368
00:18:17,926 --> 00:18:20,656
So this is sample code that you
would have to write if you were


369
00:18:20,656 --> 00:18:23,256
to create a new application
based


370
00:18:23,256 --> 00:18:25,406
on the new MetalKit framework.


371
00:18:26,216 --> 00:18:28,746
The first thing you need to
do is to do a couple of things


372
00:18:28,906 --> 00:18:30,646
in this when you want
to set up the view.


373
00:18:31,516 --> 00:18:32,866
The first key thing is


374
00:18:33,296 --> 00:18:37,136
to specify the Frame Buffer Only
property on that view to False,


375
00:18:37,646 --> 00:18:40,146
which will allow Core Image
to use Metal compute shaders


376
00:18:40,176 --> 00:18:42,316
to render to that
view's output texture.


377
00:18:42,746 --> 00:18:47,476
The next thing you want to do
is initialize the CIContext


378
00:18:47,476 --> 00:18:48,196
with a Metal device.


379
00:18:48,806 --> 00:18:51,656
You want to do that here because
initializing a CIContext is


380
00:18:51,656 --> 00:18:53,936
something you only want to
do once in an application.


381
00:18:56,616 --> 00:18:59,436
Then, in the Draw and
View Delegate function,


382
00:18:59,946 --> 00:19:01,936
this is the code you would
need to write in order


383
00:19:01,936 --> 00:19:03,546
to render some CI
filters through that view.


384
00:19:03,796 --> 00:19:05,716
So let me step you
through this line by line.


385
00:19:06,706 --> 00:19:08,686
First thing is, you
create a command buffer


386
00:19:08,686 --> 00:19:11,446
that will eventually be used
to present the drawable with.


387
00:19:11,446 --> 00:19:16,196
Then we are going to
initialize a CI image


388
00:19:16,286 --> 00:19:18,336
with some given input
Metal Texture.


389
00:19:18,656 --> 00:19:22,686
Now, this CI image could come
by other means, for example,


390
00:19:22,686 --> 00:19:25,116
some of the other image types
we have, like a CGImage.


391
00:19:25,116 --> 00:19:28,956
But in this case we will just
show you how to use our new API.


392
00:19:28,956 --> 00:19:32,096
But then once you
have a CI image,


393
00:19:32,206 --> 00:19:36,276
you can chain together a
series of CI filters to it.


394
00:19:36,276 --> 00:19:39,456
In this case, we will apply
a CI Gaussian Blur filter.


395
00:19:41,856 --> 00:19:44,986
Then, once you have your CI
image that you want to render,


396
00:19:45,496 --> 00:19:48,596
you want to grab the
texture currently bound


397
00:19:48,636 --> 00:19:52,666
to that view's current
drawable and render the CI image


398
00:19:52,746 --> 00:19:56,226
to that texture with the command
buffer we want to use here.


399
00:19:57,856 --> 00:20:00,236
Then finally, once we have
encoded the render commands


400
00:20:00,236 --> 00:20:02,716
there is one more Metal
command that you need to insert


401
00:20:02,716 --> 00:20:03,496
at the command buffer,


402
00:20:03,756 --> 00:20:05,826
and that's to present the
view's current drawable.


403
00:20:05,826 --> 00:20:08,326
And then you just call
Commit on that buffer.


404
00:20:09,676 --> 00:20:11,096
That is how easy it is


405
00:20:11,096 --> 00:20:13,076
to integrate some
Core Image filters


406
00:20:13,226 --> 00:20:14,816
into a MetalKit application.


407
00:20:14,816 --> 00:20:19,996
So next I would like to talk


408
00:20:19,996 --> 00:20:22,286
about bridging Core
Image and AV Foundation.


409
00:20:22,906 --> 00:20:26,946
So with the latest changes
we have this year in both


410
00:20:26,946 --> 00:20:30,666
of these frameworks, it is now
easy to add Core Image filters


411
00:20:30,666 --> 00:20:31,876
to your AV Foundation app,


412
00:20:32,936 --> 00:20:35,836
and that's because Core Image
is now conveniently integrated


413
00:20:36,036 --> 00:20:38,106
with the AVVideoComposition
class.


414
00:20:38,786 --> 00:20:42,276
And by default you would get
automatic color management,


415
00:20:42,276 --> 00:20:44,926
but if you don't need
it, you can disable it.


416
00:20:46,036 --> 00:20:48,716
So we will take a look at
a couple of examples on how


417
00:20:48,716 --> 00:20:50,346
to apply CI filters to videos.


418
00:20:50,916 --> 00:20:52,706
First in the context
of exporting the video


419
00:20:52,776 --> 00:20:54,846
and next during live
playback of a video.


420
00:20:55,426 --> 00:20:59,916
So for the purpose of
demonstrating these examples,


421
00:21:00,096 --> 00:21:01,366
we are going to use a filter


422
00:21:01,366 --> 00:21:03,566
that we showed you a
couple of years ago at WWDC.


423
00:21:03,566 --> 00:21:08,066
And this is a filter where for
every frame of the video image,


424
00:21:08,066 --> 00:21:12,866
we are going to first apply a
sepia tone filter to it along


425
00:21:12,866 --> 00:21:19,126
with some random noise, and then
finally some vertical scratches


426
00:21:19,126 --> 00:21:21,906
overlaid on top of it.


427
00:21:21,906 --> 00:21:27,756
For those who may recall,
this is the old film filter


428
00:21:28,346 --> 00:21:31,586
that we showed you from a
couple of years ago at WWDC.


429
00:21:31,736 --> 00:21:34,186
This filter is very
straightforward,


430
00:21:34,186 --> 00:21:37,746
all it takes is a single
input image as well


431
00:21:37,746 --> 00:21:39,866
as an input time parameter,
which will allow you


432
00:21:39,866 --> 00:21:42,656
to apply the effect to the
video in a repeatable way


433
00:21:42,946 --> 00:21:44,166
with deterministic results.


434
00:21:44,886 --> 00:21:49,896
So let's get back to how we
would apply this filter during


435
00:21:49,896 --> 00:21:50,876
exporting of that video.


436
00:21:52,266 --> 00:21:55,246
What you need to do first is
create a filtered composition,


437
00:21:55,866 --> 00:21:59,796
giving it the AV asset that
you want to export as well


438
00:21:59,796 --> 00:22:04,666
as a callback block in which
you can specify a filter recipe


439
00:22:05,196 --> 00:22:09,586
to be applied as each frame of
the video is being rendered.


440
00:22:10,486 --> 00:22:13,246
And from this callback block,
we will get a request object


441
00:22:13,246 --> 00:22:16,326
as an input parameter from which
you can get the composition time


442
00:22:16,696 --> 00:22:18,076
as well as the source image


443
00:22:18,306 --> 00:22:20,396
for chaining together
your CI filters.


444
00:22:21,036 --> 00:22:26,006
And then once you have
your filtered CI image,


445
00:22:26,396 --> 00:22:28,836
you then call Finish With
Image on the request object.


446
00:22:29,456 --> 00:22:31,816
You can pass in a nil
context to that call,


447
00:22:32,186 --> 00:22:36,646
and the AVVideoComposition would
create a CIContext by default,


448
00:22:37,326 --> 00:22:38,476
which, as I mentioned earlier,


449
00:22:38,476 --> 00:22:40,016
will get automatic
color management.


450
00:22:41,006 --> 00:22:42,776
If you want to disable
that, all you need


451
00:22:42,776 --> 00:22:44,736
to do is create a
CIContext on your own


452
00:22:44,736 --> 00:22:48,746
and specify a null color
working space and pass that into


453
00:22:48,746 --> 00:22:50,446
that Finish With Image call.


454
00:22:50,986 --> 00:22:57,106
Now, that filter we just showed
you is a pretty simple filter


455
00:22:57,676 --> 00:23:00,446
that has no convolution
filters involved.


456
00:23:00,996 --> 00:23:03,226
But in the case where you
do have convolution filters,


457
00:23:03,546 --> 00:23:07,456
one thing you want to watch out
for is an undesirable result


458
00:23:08,006 --> 00:23:11,056
where you get clear
pixels bleeding


459
00:23:11,056 --> 00:23:12,806
into the edges of that image.


460
00:23:13,816 --> 00:23:17,346
To fix that, we have a pretty
simple recipe that we use


461
00:23:17,576 --> 00:23:19,436
in a lot of cases
such as that one.


462
00:23:20,396 --> 00:23:23,486
The first thing you want to
do is with the source image


463
00:23:23,486 --> 00:23:26,026
that you have, that you want
to apply the convolution filter


464
00:23:26,026 --> 00:23:28,676
to it, you want to call
image by clamping to extent.


465
00:23:28,676 --> 00:23:32,926
It will edge replicate all
of the pixels along the edge


466
00:23:32,926 --> 00:23:34,306
of that image to infinity.


467
00:23:34,306 --> 00:23:37,726
And by doing that, you will
no longer have the problem


468
00:23:37,726 --> 00:23:42,056
of clear pixels bleeding
into the image


469
00:23:42,056 --> 00:23:44,446
as you are applying the filter.


470
00:23:44,586 --> 00:23:47,436
Because by doing that you end
up with an infinite image,


471
00:23:47,806 --> 00:23:51,486
at the very end of applying the
filter you want to add image


472
00:23:51,486 --> 00:23:54,176
by cropping the rect in
order to crop that image back


473
00:23:54,176 --> 00:23:55,636
into the source image's extent.


474
00:23:57,256 --> 00:24:01,626
By applying that simple recipe,
you will get a much cleaner look


475
00:24:02,056 --> 00:24:08,556
with nice, crisp, sharp
borders on the edge.


476
00:24:08,766 --> 00:24:11,126
So once we have that
AVVideoComposition,


477
00:24:12,106 --> 00:24:15,636
if we want to create an
export session in order


478
00:24:15,636 --> 00:24:17,996
to export a video, and you do


479
00:24:17,996 --> 00:24:19,916
that by creating this
AV export session


480
00:24:19,946 --> 00:24:23,746
and specify an output URL
location to which you want


481
00:24:23,746 --> 00:24:26,266
to export as well as
the video composition


482
00:24:26,266 --> 00:24:27,106
that we just created.


483
00:24:28,156 --> 00:24:30,316
And one thing to keep in
mind is you might want to --


484
00:24:31,016 --> 00:24:34,286
you want to call Remove Item
at URL to remove any item


485
00:24:34,286 --> 00:24:36,366
that might already exist
at that output location.


486
00:24:37,226 --> 00:24:38,156
Once you have done that,


487
00:24:38,156 --> 00:24:40,426
then you can call
Export Asynchronously


488
00:24:40,426 --> 00:24:43,486
on that export session, which
will then kick off a process


489
00:24:44,216 --> 00:24:47,976
to export that video and
apply all of the CI filters


490
00:24:48,016 --> 00:24:49,486
to every single frame
of your video.


491
00:24:50,376 --> 00:24:53,376
If you wanted to update
some progress on your UI


492
00:24:53,376 --> 00:24:56,296
to show the progress
of that export,


493
00:24:56,516 --> 00:24:59,876
you can use the Composition Time
parameter in your callback block


494
00:25:00,016 --> 00:25:01,936
to update such a UI element.


495
00:25:06,046 --> 00:25:07,836
So now that was exporting.


496
00:25:08,156 --> 00:25:11,116
For playing back an AV asset,
the code that you would need


497
00:25:11,116 --> 00:25:12,886
to write is actually
very similar.


498
00:25:13,316 --> 00:25:16,126
Creating the video composition
is exactly the same code we


499
00:25:16,126 --> 00:25:16,736
saw earlier.


500
00:25:17,466 --> 00:25:19,976
The only difference
now is instead


501
00:25:20,016 --> 00:25:21,296
of creating an export session,


502
00:25:21,726 --> 00:25:23,376
you need to create
an AVPlayerItem


503
00:25:23,456 --> 00:25:25,316
with that AV asset along


504
00:25:25,316 --> 00:25:27,476
with the video composition
we just created


505
00:25:28,396 --> 00:25:30,866
and then create an AVPlayer
with that player item


506
00:25:30,866 --> 00:25:33,116
and then call Play
on your player.


507
00:25:33,726 --> 00:25:39,716
So I'm going to now show you
a video of how we are applying


508
00:25:39,716 --> 00:25:44,386
that old film filter to an
AV asset during playback.


509
00:25:48,146 --> 00:25:50,006
So one thing to notice here is


510
00:25:50,006 --> 00:25:51,706
as you are scrubbing
back this video,


511
00:25:51,986 --> 00:25:53,986
you can see the same
effect being applied


512
00:25:53,986 --> 00:25:56,996
in a repeatable way with
deterministic results.


513
00:25:58,016 --> 00:25:59,466
So that is Core Image


514
00:25:59,466 --> 00:26:02,726
and AV Foundation interoperating
together very efficiently.


515
00:26:03,216 --> 00:26:07,256
Next, I would like
to call up Alex


516
00:26:07,256 --> 00:26:09,266
to tell you a little bit more
about Core Image providers.


517
00:26:09,896 --> 00:26:10,126
Thank you.


518
00:26:11,516 --> 00:26:14,996
[ Applause ]


519
00:26:15,496 --> 00:26:15,916
>> ALEXANDRE NAAMAN:
Thank you, Tony.


520
00:26:17,236 --> 00:26:19,906
My name is Alexandre
Naaman and I will talk


521
00:26:19,906 --> 00:26:21,446
about Core Image providers
and then we will talk


522
00:26:21,446 --> 00:26:24,136
about more APIs we have
on the system and STKs


523
00:26:24,136 --> 00:26:26,116
and how they can
work with Core Image


524
00:26:26,366 --> 00:26:27,796
to create interesting
applications.


525
00:26:29,486 --> 00:26:31,286
Let's start with
CIImageProvider,


526
00:26:31,626 --> 00:26:33,706
which is a category
we had on CI image


527
00:26:33,706 --> 00:26:37,616
that existed previously just on
OS X but now exists also on iOS


528
00:26:37,686 --> 00:26:39,896
as part of our unified
implementation.


529
00:26:40,406 --> 00:26:44,246
It's a great way for you
to bring input images


530
00:26:44,246 --> 00:26:48,186
into your system that wouldn't
be able to be done otherwise.


531
00:26:48,186 --> 00:26:50,996
So, for example, if
you had a file format


532
00:26:50,996 --> 00:26:52,256
that wasn't supported
and you wanted


533
00:26:52,256 --> 00:26:55,006
to somehow create a CI image
that was based on that,


534
00:26:55,676 --> 00:26:58,546
or if you had data that was
streaming from some site


535
00:26:58,546 --> 00:27:01,006
and you wanted to
create a CI image,


536
00:27:01,006 --> 00:27:02,206
you could use a CIImageProvider.


537
00:27:03,336 --> 00:27:05,286
They are implemented
via callbacks.


538
00:27:06,176 --> 00:27:10,336
It's all done lazily, and we
will call you and tell you


539
00:27:10,336 --> 00:27:13,486
when we need to fill in the data
and you get automatic tiling


540
00:27:13,486 --> 00:27:15,956
and we handle the purgability
and caching for you.


541
00:27:16,706 --> 00:27:19,416
Let's take a look
at how this is done.


542
00:27:19,416 --> 00:27:20,976
First things first, you
will create your own class.


543
00:27:20,976 --> 00:27:23,516
In this case, we will create
one called tile provider.


544
00:27:24,686 --> 00:27:27,456
And then we create a CI image
with that tile provider,


545
00:27:27,926 --> 00:27:30,446
and in addition to that,
we give it the size


546
00:27:30,666 --> 00:27:33,466
of the image we are trying to
create, whatever format we would


547
00:27:33,466 --> 00:27:37,066
like to use to create for this
image, a color space optionally,


548
00:27:37,346 --> 00:27:40,226
and in this case we
will give the tile size


549
00:27:40,226 --> 00:27:42,786
in the options dictionary.


550
00:27:44,466 --> 00:27:45,866
Now, in order to use this,


551
00:27:46,846 --> 00:27:50,016
all we have to do is implement
a method called Provide Image


552
00:27:50,016 --> 00:27:52,476
Data, and Core Image
will call you and ask you


553
00:27:52,476 --> 00:27:53,846
to fill in this information.


554
00:27:55,066 --> 00:27:56,976
And you have to fill
in that data pointer


555
00:27:57,216 --> 00:28:01,616
with a given row bytes value,
a certain location in X and Y,


556
00:28:01,726 --> 00:28:04,216
width and height, and you
can tag some user info


557
00:28:04,216 --> 00:28:04,896
if you would like as well.


558
00:28:05,346 --> 00:28:06,926
That's all you need
to do in order


559
00:28:06,926 --> 00:28:09,926
to implement your
own image providers.


560
00:28:09,926 --> 00:28:15,446
Now, let's talk about
the various view classes


561
00:28:15,516 --> 00:28:17,036
that we have that you can use


562
00:28:17,036 --> 00:28:19,016
with Core Image on
both iOS and OS X.


563
00:28:20,206 --> 00:28:23,316
So we have a broad spectrum
of support for rendering


564
00:28:23,316 --> 00:28:25,086
with Core Image on
a system ranging


565
00:28:25,086 --> 00:28:29,166
from the very high level,
such as UIImageView,


566
00:28:29,476 --> 00:28:30,686
which makes it very easy


567
00:28:30,996 --> 00:28:32,766
to render an image
that's been applied


568
00:28:32,766 --> 00:28:33,636
with a Core Image effect.


569
00:28:34,076 --> 00:28:36,096
And going to the
much more low-level


570
00:28:36,556 --> 00:28:40,826
and potentially higher
performance APIs such as GLKView


571
00:28:40,826 --> 00:28:45,406
or MTK view, which give
you more fine-grain control


572
00:28:45,406 --> 00:28:50,036
over what you are doing.


573
00:28:50,206 --> 00:28:51,886
So let's take a look
at UIImageView.


574
00:28:53,256 --> 00:28:55,376
UIImageView is probably
the simplest way


575
00:28:55,376 --> 00:28:59,616
to display a CI image on iOS,
and all you have to do is


576
00:28:59,916 --> 00:29:03,386
on your UIImageView, set the
Image property to a UI image --


577
00:29:03,936 --> 00:29:06,226
in this case, one
based on a CI image.


578
00:29:07,196 --> 00:29:10,196
The problem is, although
this is very easy to use,


579
00:29:10,826 --> 00:29:12,826
it's not the most
high-performance method


580
00:29:12,826 --> 00:29:13,386
of doing so.


581
00:29:13,976 --> 00:29:18,826
So what ends up happening is
we have to render that back


582
00:29:18,826 --> 00:29:21,206
onto the CPU and send
it back up to the GPU,


583
00:29:21,206 --> 00:29:22,836
so it's not as efficient
as possible.


584
00:29:22,836 --> 00:29:24,776
And if we take a look
at a simple example,


585
00:29:24,776 --> 00:29:27,996
in this case we will run
a pixelate filter using


586
00:29:28,116 --> 00:29:29,536
a UIImageView.


587
00:29:29,976 --> 00:29:32,976
We see that we get about
20 frames per second


588
00:29:32,976 --> 00:29:38,166
on a Retina-sized image with
this effect being applied.


589
00:29:40,016 --> 00:29:44,966
Now, if we switch to
an OpenGL ES-based view


590
00:29:45,726 --> 00:29:48,246
and apply the same filter,


591
00:29:49,266 --> 00:29:52,426
we can see that now we
get 48 frames per second.


592
00:29:53,096 --> 00:29:58,656
And if we go one step further
and do a Metal-based view,


593
00:30:00,026 --> 00:30:02,966
we get slight improvement here.


594
00:30:02,966 --> 00:30:04,316
We get 52 frames per second.


595
00:30:04,686 --> 00:30:07,436
And although this isn't great
we are only applying one filter,


596
00:30:07,436 --> 00:30:11,086
and so the advantages that
we get aren't as noticeable


597
00:30:11,086 --> 00:30:14,126
as we would get if we had
many filters being applied


598
00:30:14,776 --> 00:30:17,106
or if we had a bunch
of smaller renders.


599
00:30:18,196 --> 00:30:19,276
But the basic idea is there.


600
00:30:20,936 --> 00:30:23,786
So now, let's take a look at
Core Image and Core Animation


601
00:30:23,786 --> 00:30:25,436
and how we can make
those work well together.


602
00:30:27,116 --> 00:30:28,766
This is one of the few instances


603
00:30:28,766 --> 00:30:31,536
where we do have differences
between iOS and OS X.


604
00:30:31,666 --> 00:30:36,126
On OS X, we can just apply,
we just have to do two things


605
00:30:36,576 --> 00:30:39,266
in order to use Core Image
and Core Animation together.


606
00:30:39,676 --> 00:30:43,646
First things first, on
your NSview, all you need


607
00:30:43,646 --> 00:30:48,296
to do is say view.layer uses
Core Image filters and set


608
00:30:48,296 --> 00:30:51,666
that to True and then
optionally specify an array


609
00:30:51,666 --> 00:30:53,246
of filters you would
like to be applied


610
00:30:53,386 --> 00:30:55,106
to whatever layer you have.


611
00:30:55,156 --> 00:30:58,956
And that's really
all you need to do.


612
00:30:59,886 --> 00:31:03,396
On iOS, we don't
have that support,


613
00:31:03,396 --> 00:31:06,126
so instead what you could
do is OpenGL directly.


614
00:31:07,646 --> 00:31:10,726
You can do that either
by deriving from GLKView


615
00:31:11,126 --> 00:31:14,346
or by creating a
UIView and ensuring


616
00:31:14,346 --> 00:31:17,026
that you override the
layer class method


617
00:31:17,246 --> 00:31:19,346
and returning CA
Eagle layer.self.


618
00:31:19,346 --> 00:31:24,346
And when you do that, then
you get a GL ES-based object


619
00:31:24,346 --> 00:31:26,306
that you can use to
create your CIContext.


620
00:31:26,376 --> 00:31:28,416
And that will ensure that
you get optimal performance.


621
00:31:28,906 --> 00:31:31,446
All of that is great, but
one thing you need to keep


622
00:31:31,446 --> 00:31:33,996
into mind is that if you want
to get great performance,


623
00:31:33,996 --> 00:31:35,676
it's not just a question
of using the best API,


624
00:31:35,736 --> 00:31:37,086
but using it efficiently.


625
00:31:37,316 --> 00:31:39,876
And in this case, the number one
thing you have to remember is


626
00:31:39,916 --> 00:31:42,076
to only create your
CIContext once because that's


627
00:31:42,076 --> 00:31:46,146
where the caching takes place
and a bunch of state is held.


628
00:31:46,696 --> 00:31:51,126
So keep that in mind when you're
using the lower-level APIs.


629
00:31:51,126 --> 00:31:56,736
Now, I would like to talk
about Core Image on IOSurface.


630
00:31:57,976 --> 00:32:01,736
Internally, within the
Core Image implementation,


631
00:32:01,736 --> 00:32:03,216
we use IOSurface extensively.


632
00:32:03,716 --> 00:32:06,236
We love it as an API because
it provides us with a bunch


633
00:32:06,236 --> 00:32:07,546
of functionality
that doesn't exist


634
00:32:07,546 --> 00:32:10,986
with any other API
on the system.


635
00:32:10,986 --> 00:32:13,416
So mainly we have
great purgability,


636
00:32:13,756 --> 00:32:15,906
some locking semantics so
we can get data in and out


637
00:32:15,906 --> 00:32:19,306
of IOSurfaces, and it's a great
way to move data between CPU


638
00:32:19,306 --> 00:32:20,566
and GPU and vice versa.


639
00:32:21,406 --> 00:32:24,266
We have an incredibly
broad spectrum of support


640
00:32:24,266 --> 00:32:26,456
for different formats,
we think probably some


641
00:32:26,456 --> 00:32:27,636
of the best on the
entire system.


642
00:32:27,636 --> 00:32:31,756
For example, we have 420, 444,


643
00:32:31,756 --> 00:32:34,296
RGBA half float,
and many others.


644
00:32:34,296 --> 00:32:37,396
Now, on iOS, as a
developer it's difficult


645
00:32:37,396 --> 00:32:40,806
to use IOSurface directly,
but you can inform Core Image


646
00:32:40,806 --> 00:32:42,336
that you would like
to use IOSurface


647
00:32:42,966 --> 00:32:44,396
by creating Pixel Buffers


648
00:32:45,756 --> 00:32:49,036
that have the KCV pixel
buffer IOSurface property


649
00:32:49,036 --> 00:32:49,966
key specified.


650
00:32:50,846 --> 00:32:52,986
When you do that, so if
you create a CV Image


651
00:32:53,276 --> 00:32:55,796
from a CV Pixel Buffer
that has this key,


652
00:32:56,706 --> 00:32:58,036
what ends up happening
internally is


653
00:32:58,316 --> 00:33:02,166
that Core Image knows that it's
an IOSurface-backed CV Pixel


654
00:33:02,166 --> 00:33:04,256
Buffer and we can render
as efficiently as possible.


655
00:33:04,936 --> 00:33:06,646
So, this is something to
keep in mind if you want


656
00:33:06,646 --> 00:33:08,336
to get all the benefits
of IOSurface on iOS.


657
00:33:08,336 --> 00:33:13,066
Next I'd like to talk
about a few other APIs.


658
00:33:13,066 --> 00:33:16,236
We will go over examples of how
we can actually use Core Image


659
00:33:16,616 --> 00:33:21,236
and STKs together to create
sample applications very simply.


660
00:33:21,236 --> 00:33:23,356
So we are going to start
off with SpriteKit.


661
00:33:24,436 --> 00:33:29,796
If we start in XCode and
create a new application,


662
00:33:30,176 --> 00:33:34,266
we choose Game, and
then we will choose


663
00:33:34,266 --> 00:33:36,526
as a game technology SpriteKit.


664
00:33:36,966 --> 00:33:41,996
And we just build and run,
we will get this application,


665
00:33:41,996 --> 00:33:46,406
which as you tap on the screen
you get new ships showing up,


666
00:33:46,796 --> 00:33:51,706
and you can see here we are
getting 60 frames a second.


667
00:33:51,706 --> 00:33:54,626
We can now with a
very small amount


668
00:33:54,626 --> 00:33:58,306
of code add Core Image
to this application.


669
00:33:58,646 --> 00:33:59,716
So in this case, we will go


670
00:33:59,716 --> 00:34:01,986
and modify the Touches
Began method inside


671
00:34:01,986 --> 00:34:06,086
of GameScene.swift, and
initially what was happening was


672
00:34:06,386 --> 00:34:09,676
for every tap it was adding
that sprite to the root node.


673
00:34:09,676 --> 00:34:15,565
We will modify that a bit, and
we will use an SK effect node.


674
00:34:15,565 --> 00:34:18,856
An SK effect node renders the
entire contexts into a buffer,


675
00:34:19,396 --> 00:34:21,826
which you can then apply
a series of filters to.


676
00:34:23,226 --> 00:34:24,716
So we put an SK effect node.


677
00:34:25,065 --> 00:34:27,286
Instead of adding our sprite
that we had earlier to the root,


678
00:34:27,286 --> 00:34:28,676
we will add it to the effect.


679
00:34:29,726 --> 00:34:33,556
We will say we want to enable
some effects, we are going


680
00:34:33,556 --> 00:34:35,536
to create a filter, in
this case we are going


681
00:34:35,536 --> 00:34:36,565
to use a pixelate filter,


682
00:34:37,315 --> 00:34:39,025
which is the same one
we were viewing earlier.


683
00:34:39,826 --> 00:34:43,716
And then we will add
that effect to the root.


684
00:34:43,716 --> 00:34:44,786
That is all we need to do.


685
00:34:45,266 --> 00:34:47,255
This is the exact code you
would write if you wanted


686
00:34:47,255 --> 00:34:51,505
to add Core Image to a
SpriteKit application.


687
00:34:51,856 --> 00:34:54,226
And if we now run that exact
same sample that we had


688
00:34:54,866 --> 00:34:59,706
and start tapping away, we get
beautifully pixelated sprites


689
00:34:59,856 --> 00:35:01,846
in our application and running


690
00:35:01,846 --> 00:35:03,536
at pretty much the
same frame rate.


691
00:35:03,536 --> 00:35:08,296
Now, let's talk a little
bit about SceneKit.


692
00:35:08,906 --> 00:35:09,686
Same idea.


693
00:35:10,556 --> 00:35:13,636
We will create an
application from Start,


694
00:35:13,636 --> 00:35:16,486
and we will choose SceneKit
as a game technology.


695
00:35:16,926 --> 00:35:20,716
If we just build and run this
app, we get this spaceship


696
00:35:20,716 --> 00:35:24,556
that just rotates around
at interactive rates.


697
00:35:24,976 --> 00:35:29,176
Now, if we want to add Core
Image to this application,


698
00:35:29,176 --> 00:35:31,826
all we have to do is go to
the View Did Load method


699
00:35:31,876 --> 00:35:35,806
in GameViewController.swift,
find the ship,


700
00:35:36,566 --> 00:35:40,546
which is aligned
in the sample code.


701
00:35:41,916 --> 00:35:44,646
We then create, once
again, the pixelated filter,


702
00:35:45,606 --> 00:35:48,556
and we specify an optional
array of filters to the ship.


703
00:35:49,476 --> 00:35:54,766
If we do that and run, we get
a beautifully pixelated ship.


704
00:35:55,616 --> 00:35:59,396
So you can apply this to any
node in your scene, and, again,


705
00:35:59,396 --> 00:36:01,956
we get great frame rate.


706
00:36:02,086 --> 00:36:03,156
One of the big advantages


707
00:36:03,526 --> 00:36:08,496
of using SceneKit alongside
Core Image is you can animate


708
00:36:08,496 --> 00:36:10,176
properties with Core Animation.


709
00:36:11,496 --> 00:36:13,176
So in this case,
we are just going


710
00:36:13,176 --> 00:36:15,806
to create a CA basic
animation, and we are going


711
00:36:15,806 --> 00:36:17,526
to animate the input
scale, so we are going


712
00:36:17,526 --> 00:36:21,676
to get a varying scale pixelate
effect that will be applied


713
00:36:21,676 --> 00:36:23,926
over time going from
a value of 0 to 50.


714
00:36:24,586 --> 00:36:28,106
It will ease in and ease out
over the course of two seconds.


715
00:36:28,666 --> 00:36:33,506
If we add this code,
we then get our ship


716
00:36:34,756 --> 00:36:40,826
with a beautifully
animated pixelate effect.


717
00:36:41,576 --> 00:36:44,526
And, again, great frame rates.


718
00:36:45,176 --> 00:36:49,876
Now, this doesn't necessarily
have to be applied to one node.


719
00:36:49,876 --> 00:36:51,806
You can apply it to
your entire scene.


720
00:36:51,806 --> 00:36:53,116
Here we have a sample
that we shipped


721
00:36:53,186 --> 00:36:55,726
that you can download
called Bananas,


722
00:36:56,466 --> 00:36:59,006
where we have applied the same
effect along with animation


723
00:36:59,986 --> 00:37:04,066
and we are changing the pixelate
scale in real time here.


724
00:37:04,916 --> 00:37:07,396
I was surprised that I
could play this game better


725
00:37:07,396 --> 00:37:12,226
when it was pixelated
than when it was full-res.


726
00:37:12,396 --> 00:37:15,116
But you could use this
not just to create games


727
00:37:15,116 --> 00:37:18,336
but to also apply an effect
at the end of the game,


728
00:37:18,336 --> 00:37:23,356
for example, or if you wanted
to have different versions


729
00:37:23,356 --> 00:37:24,466
of your assets rendered


730
00:37:24,836 --> 00:37:26,316
with different image
processing effects,


731
00:37:26,316 --> 00:37:28,376
you could use Core Image


732
00:37:28,376 --> 00:37:31,856
with these APIs together
and it works great.


733
00:37:32,446 --> 00:37:36,796
So, so far today we have seen
a bunch of stuff about how


734
00:37:36,796 --> 00:37:39,476
to use Core Image with
Metal, AV Foundation,


735
00:37:40,406 --> 00:37:42,496
why IOSurface is
so important to us.


736
00:37:42,496 --> 00:37:45,056
The easy way to use UIImageView


737
00:37:45,056 --> 00:37:47,736
if you are only creating
an image once


738
00:37:47,736 --> 00:37:49,576
and don't need to
constantly update.


739
00:37:49,576 --> 00:37:51,876
It's a great way to
apply an effect once.


740
00:37:52,876 --> 00:37:55,736
We showed you how to use Core
Animation as well, how to bring


741
00:37:55,736 --> 00:37:57,766
in custom data with
CIImageProvider,


742
00:37:57,766 --> 00:38:01,826
and how to use it in the context
of games or other applications


743
00:38:02,106 --> 00:38:08,786
that you can create very simply
with SceneKit or SpriteKit.


744
00:38:09,216 --> 00:38:10,966
For additional information,
we have a bunch


745
00:38:10,966 --> 00:38:14,276
of resources available online
at developer.apple.com,


746
00:38:15,126 --> 00:38:18,436
and for any additional inquiries
you can contact Stephen Chick


747
00:38:18,436 --> 00:38:20,796
at chick@apple.com.


748
00:38:22,036 --> 00:38:25,696
There are other sessions you
may be interested in going to,


749
00:38:25,696 --> 00:38:27,876
including the Editing
Movies in AV Foundation


750
00:38:28,346 --> 00:38:31,996
which took place a few days
ago but you can look at online


751
00:38:31,996 --> 00:38:35,466
and What's New in Metal Part 2
that also took place yesterday.


752
00:38:36,336 --> 00:38:40,456
And on that note, I would like
to thank you all for coming


753
00:38:40,526 --> 00:38:43,116
and I hope you enjoy using Core
Image in your applications,


754
00:38:43,176 --> 00:38:44,596
and I hope you enjoy the
rest of the conference.


755
00:38:44,596 --> 00:38:45,256
Thank you very much!


756
00:38:47,516 --> 00:38:58,360
[ Applause ]

