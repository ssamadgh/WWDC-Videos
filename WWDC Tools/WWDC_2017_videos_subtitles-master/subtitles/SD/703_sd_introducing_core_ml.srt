1
00:00:06,516 --> 00:00:16,500
[ Crowd Sounds ]


2
00:00:23,516 --> 00:00:27,046
[ Applause ]


3
00:00:27,546 --> 00:00:30,546
>> Wow. It's very exciting to be


4
00:00:30,546 --> 00:00:30,676
here.


5
00:00:31,606 --> 00:00:32,786
I'm Gaurav.


6
00:00:32,786 --> 00:00:35,046
And today we are going to


7
00:00:35,776 --> 00:00:37,466
discuss machine learning.


8
00:00:37,466 --> 00:00:38,686
Machine learning is a very


9
00:00:38,686 --> 00:00:39,866
powerful technology.


10
00:00:40,516 --> 00:00:42,066
And together with the power of


11
00:00:42,066 --> 00:00:44,656
our devices, you can create some


12
00:00:44,656 --> 00:00:46,556
really amazing experiences.


13
00:00:47,896 --> 00:00:49,946
For example, you can do things


14
00:00:49,946 --> 00:00:51,746
like real-time image recognition


15
00:00:52,346 --> 00:00:53,846
and content creation.


16
00:00:54,926 --> 00:00:56,366
In the next 40 minutes, we are


17
00:00:56,366 --> 00:00:57,576
going to tell you all about


18
00:00:57,576 --> 00:01:00,126
those experiences and how Apple


19
00:01:00,126 --> 00:01:01,866
is making it easy for you to


20
00:01:02,076 --> 00:01:03,846
integrate machine learning in


21
00:01:03,846 --> 00:01:04,965
your app.


22
00:01:06,156 --> 00:01:07,916
In particular, we will be


23
00:01:07,916 --> 00:01:10,366
focusing on Core ML, our machine


24
00:01:10,366 --> 00:01:11,066
learning framework.


25
00:01:14,946 --> 00:01:17,026
At Apple, we are using machine


26
00:01:17,026 --> 00:01:18,786
learning extensively.


27
00:01:19,796 --> 00:01:22,586
In our photos app, we use it for


28
00:01:22,586 --> 00:01:24,096
people recognition, scene


29
00:01:24,096 --> 00:01:24,886
recognition.


30
00:01:28,796 --> 00:01:31,106
In our keyboard app, we use it


31
00:01:31,106 --> 00:01:32,496
for the next word prediction,


32
00:01:33,006 --> 00:01:34,146
smart responses.


33
00:01:35,236 --> 00:01:37,006
We use it even in our watch to


34
00:01:37,006 --> 00:01:38,366
do smart responses and


35
00:01:38,366 --> 00:01:39,736
handwriting recognition.


36
00:01:40,926 --> 00:01:43,466
And I'm sure you guys also want


37
00:01:43,466 --> 00:01:45,966
to create similar experiences in


38
00:01:45,966 --> 00:01:47,306
your app.


39
00:01:48,206 --> 00:01:51,346
For example, some of you might


40
00:01:51,346 --> 00:01:52,646
like to do real-time image


41
00:01:52,646 --> 00:01:53,446
recognition.


42
00:01:53,996 --> 00:01:56,856
In this way, you can tell your


43
00:01:56,856 --> 00:01:59,046
users, provide them more


44
00:01:59,046 --> 00:02:00,636
contextual information about


45
00:02:00,636 --> 00:02:02,246
they are seeing, for example


46
00:02:02,246 --> 00:02:03,536
whether they are eating a hotdog


47
00:02:03,536 --> 00:02:03,956
or not.


48
00:02:04,516 --> 00:02:07,816
[ Laughter ]


49
00:02:08,316 --> 00:02:11,656
We want you to enable all of


50
00:02:11,656 --> 00:02:13,956
these great facility in your


51
00:02:13,956 --> 00:02:14,236
app.


52
00:02:15,076 --> 00:02:17,176
And, hence, we are introducing


53
00:02:17,266 --> 00:02:18,996
machine learning frameworks just


54
00:02:18,996 --> 00:02:20,476
for you guys to enable all the


55
00:02:20,476 --> 00:02:21,116
awesome things.


56
00:02:21,536 --> 00:02:22,586
You can clap.


57
00:02:23,516 --> 00:02:27,916
[ Applause ]


58
00:02:28,416 --> 00:02:30,276
But before we clap, we should


59
00:02:30,276 --> 00:02:31,686
understand why do we even need


60
00:02:31,686 --> 00:02:32,816
machine learning in the first


61
00:02:32,816 --> 00:02:33,096
place?


62
00:02:33,616 --> 00:02:38,826
And when do you need machine


63
00:02:38,886 --> 00:02:39,036
learning?


64
00:02:39,036 --> 00:02:42,486
So let's just say I'm a florist.


65
00:02:42,986 --> 00:02:44,926
And all I want to show is all


66
00:02:44,926 --> 00:02:46,416
the images of roses in user's


67
00:02:46,416 --> 00:02:47,886
photo library.


68
00:02:48,596 --> 00:02:50,386
This seems like a simple task,


69
00:02:50,386 --> 00:02:51,876
so I can do some programming.


70
00:02:53,196 --> 00:02:56,006
Perhaps, I'll start with the


71
00:02:56,006 --> 00:02:56,756
color.


72
00:02:57,346 --> 00:02:59,406
I say if the dominant color in


73
00:02:59,406 --> 00:03:00,766
the picture is red maybe it's a


74
00:03:00,766 --> 00:03:01,026
rose.


75
00:03:02,576 --> 00:03:03,966
The problem is there are many


76
00:03:03,966 --> 00:03:05,386
rose that are white in nature,


77
00:03:05,386 --> 00:03:07,876
white in color, or yellow in


78
00:03:09,076 --> 00:03:09,216
color.


79
00:03:09,456 --> 00:03:11,916
So I will actually go forward


80
00:03:11,916 --> 00:03:13,996
and start describing the shape.


81
00:03:13,996 --> 00:03:17,716
And soon I will realize that


82
00:03:17,716 --> 00:03:20,016
it's very difficult to write


83
00:03:20,356 --> 00:03:23,866
even such a simple program


84
00:03:25,656 --> 00:03:27,016
programmatically.


85
00:03:27,016 --> 00:03:28,276
Hence, [inaudible] to machine


86
00:03:28,276 --> 00:03:29,216
learning for our help.


87
00:03:30,546 --> 00:03:32,116
Rather than describing how a


88
00:03:32,116 --> 00:03:33,336
rose looks like


89
00:03:33,376 --> 00:03:34,966
programmatically, we will


90
00:03:34,966 --> 00:03:37,816
describe a rose empirically.


91
00:03:38,176 --> 00:03:39,026
We will learn that


92
00:03:39,026 --> 00:03:40,466
representation of rose


93
00:03:40,726 --> 00:03:41,386
empirically.


94
00:03:42,036 --> 00:03:43,896
Machine learning actually has


95
00:03:43,896 --> 00:03:44,416
two steps.


96
00:03:45,146 --> 00:03:46,406
One, training.


97
00:03:48,116 --> 00:03:50,096
So what you will do in this


98
00:03:50,096 --> 00:03:51,376
case, off-line you will collect


99
00:03:51,376 --> 00:03:53,056
images of roses, lilies,


100
00:03:53,666 --> 00:03:56,086
sunflowers and label them.


101
00:03:56,086 --> 00:03:59,036
You will run it past through a


102
00:03:59,096 --> 00:04:01,156
learning algorithm.


103
00:04:01,156 --> 00:04:05,506
And you will get what we call as


104
00:04:05,546 --> 00:04:06,036
a model.


105
00:04:07,416 --> 00:04:10,396
This model is an empirical


106
00:04:10,396 --> 00:04:12,616
representation of how a rose


107
00:04:13,286 --> 00:04:14,896
look like.


108
00:04:15,116 --> 00:04:17,055
Almost all of this stuff happens


109
00:04:17,156 --> 00:04:17,656
off-line.


110
00:04:18,446 --> 00:04:21,196
And there is a big [inaudible]


111
00:04:21,196 --> 00:04:23,426
on the Internet around training.


112
00:04:23,906 --> 00:04:25,346
Training is a complex subject.


113
00:04:27,236 --> 00:04:28,846
Now once you have the model, you


114
00:04:28,846 --> 00:04:30,026
want to use this model.


115
00:04:30,026 --> 00:04:33,976
So what you will do, you will


116
00:04:33,976 --> 00:04:36,236
take a picture of a rose, embed


117
00:04:36,286 --> 00:04:38,056
this model in your app, you run


118
00:04:38,136 --> 00:04:41,706
it past through your model, and


119
00:04:42,376 --> 00:04:44,546
you will get the label and the


120
00:04:44,546 --> 00:04:44,996
confidence.


121
00:04:45,806 --> 00:04:48,436
This step is known as inference.


122
00:04:49,726 --> 00:04:52,036
Training is very complex, but


123
00:04:52,336 --> 00:04:53,886
inference is also getting very,


124
00:04:53,886 --> 00:04:55,636
very challenging nowadays.


125
00:04:58,686 --> 00:05:00,526
For example, if you want to


126
00:05:00,526 --> 00:05:02,516
describe a new, deep neural


127
00:05:02,516 --> 00:05:05,336
network programmatically in your


128
00:05:05,336 --> 00:05:06,336
code, you have to write


129
00:05:06,336 --> 00:05:07,726
thousands of lines of code just


130
00:05:07,726 --> 00:05:08,686
to describe the network.


131
00:05:09,336 --> 00:05:12,146
It can be [inaudible] tedious


132
00:05:12,146 --> 00:05:15,866
and perhaps you can face your


133
00:05:15,866 --> 00:05:17,946
biggest challenge in proving the


134
00:05:17,946 --> 00:05:21,436
correctness of such a program.


135
00:05:21,436 --> 00:05:23,826
Once you make sure that the


136
00:05:23,826 --> 00:05:26,636
program is correct, you have to


137
00:05:26,636 --> 00:05:28,656
make sure that you're getting


138
00:05:28,656 --> 00:05:30,176
the best performance possible.


139
00:05:31,996 --> 00:05:33,886
Finally, you also have to make


140
00:05:33,886 --> 00:05:35,496
sure that there is energy


141
00:05:35,496 --> 00:05:36,686
efficiency involved here.


142
00:05:37,216 --> 00:05:40,116
These tasks can be very, very


143
00:05:40,116 --> 00:05:43,216
challenging and can bog you down


144
00:05:43,596 --> 00:05:44,246
completely.


145
00:05:44,816 --> 00:05:48,046
At Apple, we are using on-device


146
00:05:48,046 --> 00:05:49,306
machine learning for several


147
00:05:49,306 --> 00:05:49,996
years now.


148
00:05:50,536 --> 00:05:53,536
And we have solved these


149
00:05:53,536 --> 00:05:54,046
challenges.


150
00:05:54,126 --> 00:05:55,386
We have faced these challenges


151
00:05:56,016 --> 00:05:56,716
and solved them.


152
00:05:57,286 --> 00:05:58,466
And we really don't want you to


153
00:05:58,466 --> 00:05:59,096
solve them.


154
00:05:59,096 --> 00:06:00,066
We will solve them for you.


155
00:06:00,306 --> 00:06:01,746
We want you to focus only on the


156
00:06:01,746 --> 00:06:02,596
experience.


157
00:06:02,596 --> 00:06:03,166
We will give you the


158
00:06:03,166 --> 00:06:04,476
implementation, the best


159
00:06:04,476 --> 00:06:05,776
implementation possible.


160
00:06:06,536 --> 00:06:09,136
And, hence, we are introducing


161
00:06:09,536 --> 00:06:11,216
machine learning frameworks for


162
00:06:11,216 --> 00:06:13,706
you in which case we take care


163
00:06:13,706 --> 00:06:15,516
of all the challenges, and you


164
00:06:15,586 --> 00:06:17,016
take care of all the experience


165
00:06:17,296 --> 00:06:17,656
in your app.


166
00:06:18,786 --> 00:06:21,426
By doing so, we are going to


167
00:06:21,426 --> 00:06:22,686
follow a layered approach.


168
00:06:23,376 --> 00:06:26,966
At the top is your app.


169
00:06:29,256 --> 00:06:31,016
We are going to introduce new


170
00:06:31,016 --> 00:06:32,746
domain specific framework, in


171
00:06:32,746 --> 00:06:33,686
particular, Vision.


172
00:06:34,876 --> 00:06:36,496
The Vision framework will


173
00:06:36,496 --> 00:06:38,846
actually allow you to do all the


174
00:06:38,846 --> 00:06:40,266
tasks related to computer


175
00:06:40,266 --> 00:06:40,716
vision.


176
00:06:41,506 --> 00:06:42,976
The best part about computer


177
00:06:42,976 --> 00:06:44,626
vision of Vision framework is


178
00:06:44,626 --> 00:06:46,406
that you don't have to be a


179
00:06:46,406 --> 00:06:49,616
configuration expert for using


180
00:06:49,616 --> 00:06:50,406
Vision framework.


181
00:06:50,906 --> 00:06:53,816
We are also enhancing our NLP


182
00:06:53,816 --> 00:06:56,966
APIs to include more languages


183
00:06:57,466 --> 00:06:58,606
and more functionality.


184
00:06:59,196 --> 00:07:02,486
We are introducing a brand-new


185
00:07:02,486 --> 00:07:04,106
framework called Core ML.


186
00:07:05,076 --> 00:07:06,676
This framework has an exhaustive


187
00:07:06,756 --> 00:07:08,366
support of machine learning


188
00:07:08,366 --> 00:07:10,286
algorithms, more deep learning


189
00:07:10,586 --> 00:07:11,656
as well as a standard.


190
00:07:12,276 --> 00:07:16,266
And finally, all of these


191
00:07:16,266 --> 00:07:18,496
frameworks are built on top of


192
00:07:18,496 --> 00:07:20,596
Accelerate and Metal Performance


193
00:07:20,596 --> 00:07:20,826
Shaders.


194
00:07:21,396 --> 00:07:28,086
To give you a sneak peak, we


195
00:07:28,306 --> 00:07:29,506
have Vision framework.


196
00:07:31,206 --> 00:07:33,526
Vision is our one-stop shop to


197
00:07:33,526 --> 00:07:34,656
do all things related to


198
00:07:34,656 --> 00:07:36,006
computer vision and images.


199
00:07:37,056 --> 00:07:38,966
You can use it to do things like


200
00:07:39,026 --> 00:07:43,316
object tracking or more modern


201
00:07:43,316 --> 00:07:44,506
deep learning-based face


202
00:07:44,506 --> 00:07:45,006
detection.


203
00:07:45,006 --> 00:07:47,266
There is a lot more to Vision


204
00:07:47,266 --> 00:07:47,736
framework.


205
00:07:47,736 --> 00:07:50,306
We are going to discuss Vision


206
00:07:50,576 --> 00:07:52,066
in detail tomorrow afternoon.


207
00:07:52,626 --> 00:07:59,566
NLP, this is our one-stop shop


208
00:07:59,566 --> 00:08:00,836
to do the text processing.


209
00:08:01,366 --> 00:08:04,686
You can use it to do things like


210
00:08:04,686 --> 00:08:05,916
language identification.


211
00:08:07,126 --> 00:08:09,416
And we are also releasing Named


212
00:08:09,416 --> 00:08:10,886
Entity Recognition APIs.


213
00:08:11,956 --> 00:08:14,986
These APIs can actually tell you


214
00:08:14,986 --> 00:08:17,116
in your text if it is a name of


215
00:08:17,116 --> 00:08:18,916
the location, people, and


216
00:08:18,916 --> 00:08:19,626
organization.


217
00:08:20,146 --> 00:08:21,246
We believe this will be very


218
00:08:21,246 --> 00:08:22,856
powerful for many app developers


219
00:08:23,256 --> 00:08:24,276
so please check this out


220
00:08:24,656 --> 00:08:26,086
tomorrow more in the morning


221
00:08:26,156 --> 00:08:27,946
session on NLP.


222
00:08:29,876 --> 00:08:33,606
Core ML, until now, we were


223
00:08:33,606 --> 00:08:35,476
discussing domain specific


224
00:08:35,566 --> 00:08:35,936
framework.


225
00:08:36,395 --> 00:08:37,916
Core ML is our machine learning


226
00:08:37,916 --> 00:08:39,806
framework and is domain


227
00:08:39,806 --> 00:08:40,525
agnostic.


228
00:08:41,756 --> 00:08:44,316
It supports multiple kind of


229
00:08:44,316 --> 00:08:47,136
inputs such as images, text,


230
00:08:48,256 --> 00:08:50,636
dictionaries, raw numbers.


231
00:08:51,566 --> 00:08:53,156
So you can do things like music


232
00:08:53,196 --> 00:08:53,566
tagging.


233
00:08:55,876 --> 00:08:57,346
There's another interesting part


234
00:08:57,346 --> 00:08:59,006
of our Core ML that it has the


235
00:08:59,006 --> 00:09:01,116
ability to deal with mixed


236
00:09:01,116 --> 00:09:02,136
input/output type.


237
00:09:02,446 --> 00:09:04,396
So you can take an image and you


238
00:09:04,396 --> 00:09:06,286
can get a text out.


239
00:09:09,116 --> 00:09:10,916
The best part about all these


240
00:09:11,026 --> 00:09:13,386
frameworks is that they can work


241
00:09:13,386 --> 00:09:13,776
together.


242
00:09:15,086 --> 00:09:17,316
So you take a text, you pass it


243
00:09:17,316 --> 00:09:18,866
through your NLP, you get the


244
00:09:18,866 --> 00:09:20,476
output, and you pass it through


245
00:09:20,476 --> 00:09:22,356
Core ML and do things like


246
00:09:22,436 --> 00:09:23,376
sentiment analysis.


247
00:09:23,376 --> 00:09:25,336
We are going to discuss it in


248
00:09:25,336 --> 00:09:27,666
detail in our in-depth Core ML


249
00:09:27,746 --> 00:09:29,706
session, how you do that exactly


250
00:09:30,166 --> 00:09:32,876
on Thursday.


251
00:09:33,026 --> 00:09:34,556
All of these frameworks are


252
00:09:34,556 --> 00:09:36,546
being powered by Accelerate and


253
00:09:36,856 --> 00:09:37,106
MPS.


254
00:09:38,316 --> 00:09:39,276
These are our engine.


255
00:09:40,746 --> 00:09:42,946
Now you can use Accelerate and


256
00:09:43,566 --> 00:09:45,246
MPS whenever you need some kind


257
00:09:45,246 --> 00:09:46,566
of math functionality.


258
00:09:46,566 --> 00:09:47,876
So it doesn't have to be


259
00:09:48,056 --> 00:09:49,436
necessarily related to machine


260
00:09:49,436 --> 00:09:49,706
learning.


261
00:09:50,116 --> 00:09:53,576
You can also use these


262
00:09:53,956 --> 00:09:57,766
Accelerate and MPS when you are


263
00:09:57,766 --> 00:10:00,876
doing custom ML models.


264
00:10:00,876 --> 00:10:02,156
So if you are using a machine


265
00:10:02,156 --> 00:10:03,746
learning model, you might like


266
00:10:03,746 --> 00:10:05,776
to use Accelerate and MPS.


267
00:10:08,076 --> 00:10:10,836
All these APIs run on user's


268
00:10:10,836 --> 00:10:11,196
device.


269
00:10:11,266 --> 00:10:12,486
And they're very, very highly


270
00:10:12,486 --> 00:10:13,036
performing.


271
00:10:13,146 --> 00:10:14,656
And we work very hard to make


272
00:10:14,656 --> 00:10:15,846
sure you get the best


273
00:10:15,846 --> 00:10:16,856
performance possible.


274
00:10:18,346 --> 00:10:19,576
Now because they learn on user's


275
00:10:19,576 --> 00:10:21,786
device, there are advantages.


276
00:10:22,366 --> 00:10:24,456
First, user privacy.


277
00:10:24,626 --> 00:10:27,676
Your users will be happy to know


278
00:10:27,676 --> 00:10:28,716
that you're not sending their


279
00:10:28,716 --> 00:10:30,896
personal messages, texts, and


280
00:10:30,896 --> 00:10:34,456
images to the server.


281
00:10:34,456 --> 00:10:35,866
Your users don't have to pay


282
00:10:35,866 --> 00:10:38,956
extra data cost just to receive


283
00:10:38,956 --> 00:10:40,556
these -- just to send this data


284
00:10:40,556 --> 00:10:42,686
to the server to get a


285
00:10:44,176 --> 00:10:44,566
prediction.


286
00:10:44,566 --> 00:10:46,496
You also don't have to actually


287
00:10:46,706 --> 00:10:48,136
give huge amount of money to


288
00:10:48,136 --> 00:10:49,516
server companies just to set up


289
00:10:49,516 --> 00:10:50,586
the servers and get a


290
00:10:50,586 --> 00:10:51,176
prediction.


291
00:10:51,726 --> 00:10:53,136
Our devices are very, very


292
00:10:53,136 --> 00:10:53,486
powerful.


293
00:10:53,486 --> 00:10:54,696
You can do a lot here.


294
00:10:55,306 --> 00:11:00,206
And finally, your app is always


295
00:11:00,206 --> 00:11:00,646
available.


296
00:11:01,866 --> 00:11:03,856
Let's just say your user goes to


297
00:11:03,936 --> 00:11:06,896
Yosemite in the wilderness where


298
00:11:06,896 --> 00:11:08,246
there is no network


299
00:11:08,246 --> 00:11:08,966
connectivity.


300
00:11:10,076 --> 00:11:11,186
They can still use your app.


301
00:11:11,186 --> 00:11:13,186
And this is very powerful.


302
00:11:14,476 --> 00:11:17,366
However, perhaps one of the best


303
00:11:17,436 --> 00:11:19,686
things you can do is real-time


304
00:11:19,686 --> 00:11:20,396
machine learning.


305
00:11:21,706 --> 00:11:23,396
Our devices are very powerful.


306
00:11:23,396 --> 00:11:25,136
They can run modern deep neural


307
00:11:25,136 --> 00:11:28,356
networks such as ResNet 15


308
00:11:28,356 --> 00:11:29,656
[inaudible].


309
00:11:29,656 --> 00:11:32,966
You can use our devices to do


310
00:11:32,966 --> 00:11:34,426
real-time image recognition.


311
00:11:34,816 --> 00:11:37,176
In these cases, you really don't


312
00:11:37,226 --> 00:11:39,306
have [inaudible] to actually get


313
00:11:40,076 --> 00:11:42,146
the application.


314
00:11:43,516 --> 00:11:51,296
[ Applause ]


315
00:11:51,796 --> 00:11:53,336
So for any latency sensitive


316
00:11:53,396 --> 00:11:54,716
things, you might like to do


317
00:11:54,896 --> 00:11:56,326
real-time image recognition.


318
00:11:56,326 --> 00:12:01,276
So just to recap here, we are


319
00:12:01,276 --> 00:12:02,556
releasing a series of machine


320
00:12:02,556 --> 00:12:03,286
learning frameworks.


321
00:12:03,816 --> 00:12:05,606
If your app is a vision-based


322
00:12:05,786 --> 00:12:07,066
app, please use Vision


323
00:12:07,066 --> 00:12:07,516
framework.


324
00:12:08,166 --> 00:12:10,326
If your app is a text-based app,


325
00:12:10,386 --> 00:12:11,486
please use NLP.


326
00:12:12,226 --> 00:12:14,426
Now if you think that NLP and


327
00:12:14,426 --> 00:12:16,286
the Vision framework are not


328
00:12:16,776 --> 00:12:18,896
providing the APIs you need, go


329
00:12:18,896 --> 00:12:19,956
down on Core ML.


330
00:12:20,266 --> 00:12:21,996
Core ML provides exhaustive


331
00:12:22,666 --> 00:12:24,366
support of standard machine


332
00:12:24,366 --> 00:12:26,076
learning and deep learning


333
00:12:27,046 --> 00:12:27,586
algorithms.


334
00:12:27,586 --> 00:12:28,816
And let's just say you are


335
00:12:28,956 --> 00:12:30,736
[inaudible] your APIs machine


336
00:12:30,736 --> 00:12:31,906
learning algorithms then you


337
00:12:31,906 --> 00:12:34,346
should use Accelerate and MPS.


338
00:12:34,996 --> 00:12:36,806
We are going to discuss all


339
00:12:36,806 --> 00:12:39,116
these frameworks in detail in


340
00:12:39,116 --> 00:12:41,366
the next couple of days so you


341
00:12:41,896 --> 00:12:43,796
are [inaudible], but today let's


342
00:12:43,796 --> 00:12:44,956
just focus on Core ML.


343
00:12:44,956 --> 00:12:46,806
And to do that, I'll invite my


344
00:12:46,866 --> 00:12:50,586
friend and colleague, Michael.


345
00:12:51,696 --> 00:12:53,356
>> Thanks Gaurav.


346
00:12:53,486 --> 00:12:54,186
Hi everyone.


347
00:12:54,186 --> 00:12:56,056
I'm so excited to talk about


348
00:12:56,056 --> 00:12:58,066
Core ML and its role in helping


349
00:12:58,066 --> 00:13:00,066
you make a great app.


350
00:13:00,996 --> 00:13:02,246
We're going to start out with a


351
00:13:02,246 --> 00:13:04,146
high level overview of Core MLs


352
00:13:04,146 --> 00:13:04,976
main goals.


353
00:13:06,056 --> 00:13:07,106
We'll then talk a little bit


354
00:13:07,106 --> 00:13:08,286
about models and how they're


355
00:13:08,286 --> 00:13:08,976
represented.


356
00:13:09,016 --> 00:13:11,666
And then we'll walk you through


357
00:13:11,666 --> 00:13:13,196
the typical development process


358
00:13:13,196 --> 00:13:16,766
using Core ML.


359
00:13:17,006 --> 00:13:18,766
So the Core ML framework is


360
00:13:18,766 --> 00:13:20,956
available on macOS, iOS,


361
00:13:22,186 --> 00:13:23,716
watchOS, and tvOS.


362
00:13:25,046 --> 00:13:26,676
But Core ML is more than just a


363
00:13:26,676 --> 00:13:28,946
framework and a set of APIs.


364
00:13:29,436 --> 00:13:30,946
It's really a set of development


365
00:13:30,946 --> 00:13:33,016
tools all designed around making


366
00:13:33,016 --> 00:13:35,046
it as easy as possible to take a


367
00:13:35,046 --> 00:13:36,506
machine learned model and


368
00:13:36,506 --> 00:13:38,826
integrate it into your app.


369
00:13:39,556 --> 00:13:41,366
This will let you focus on the


370
00:13:41,366 --> 00:13:42,826
user experience you're trying to


371
00:13:42,826 --> 00:13:44,216
enable rather than the


372
00:13:44,216 --> 00:13:45,546
implementation details.


373
00:13:47,876 --> 00:13:51,446
Core ML is simple to use, will


374
00:13:51,446 --> 00:13:52,476
give you the performance you


375
00:13:52,476 --> 00:13:55,166
need while being compatible with


376
00:13:55,166 --> 00:13:56,316
a wide variety of machine


377
00:13:56,316 --> 00:14:00,066
learning tools out there.


378
00:14:00,266 --> 00:14:02,516
Its simplicity comes from having


379
00:14:02,516 --> 00:14:04,196
a Unified Inference API across


380
00:14:04,196 --> 00:14:05,226
all model types.


381
00:14:06,276 --> 00:14:08,066
Xcode integration will let you


382
00:14:08,066 --> 00:14:09,166
interact with machine learned


383
00:14:09,166 --> 00:14:10,626
models using the same software


384
00:14:10,626 --> 00:14:12,046
development practices you're all


385
00:14:12,166 --> 00:14:13,556
already experts in.


386
00:14:15,626 --> 00:14:18,616
It uses the inference engines


387
00:14:18,746 --> 00:14:20,066
Apple is already shipping to


388
00:14:20,066 --> 00:14:21,526
millions of customers in its


389
00:14:21,526 --> 00:14:23,136
apps and systems services now


390
00:14:23,136 --> 00:14:24,766
being made available to you via


391
00:14:24,766 --> 00:14:25,286
Core ML.


392
00:14:25,946 --> 00:14:27,316
And as mentioned, these are


393
00:14:27,316 --> 00:14:28,466
built on top of Metal and


394
00:14:28,466 --> 00:14:29,836
Accelerate to make the best use


395
00:14:29,836 --> 00:14:31,076
of hardware that your apps are


396
00:14:31,076 --> 00:14:32,396
being deployed on.


397
00:14:35,096 --> 00:14:36,986
Core ML is also designed to work


398
00:14:36,986 --> 00:14:39,266
in this rapidly evolving machine


399
00:14:39,266 --> 00:14:40,126
learning ecosystem.


400
00:14:40,746 --> 00:14:43,086
It defines a new public format


401
00:14:43,086 --> 00:14:45,196
for describing models as well as


402
00:14:45,196 --> 00:14:46,856
a set of tools that will allow


403
00:14:46,856 --> 00:14:48,056
you to convert the output of


404
00:14:48,056 --> 00:14:50,316
popular training libraries into


405
00:14:50,316 --> 00:14:51,506
this format.


406
00:14:53,156 --> 00:14:55,056
So that's Core ML in a nutshell.


407
00:14:55,476 --> 00:14:56,656
It's all about making it super


408
00:14:56,656 --> 00:14:58,836
easy to get a learned model


409
00:14:59,056 --> 00:15:00,566
integrated into your app.


410
00:15:00,756 --> 00:15:02,416
So let's talk a little bit more


411
00:15:02,416 --> 00:15:03,316
about these models.


412
00:15:03,816 --> 00:15:07,726
Since Core ML, a model is simply


413
00:15:07,726 --> 00:15:08,256
a function.


414
00:15:08,976 --> 00:15:10,836
Now the logic for this function


415
00:15:10,966 --> 00:15:12,386
happens to be learned from data


416
00:15:12,696 --> 00:15:13,666
but just like any other


417
00:15:13,666 --> 00:15:15,086
function, it takes in a set of


418
00:15:15,086 --> 00:15:16,756
inputs and produces a set of


419
00:15:16,756 --> 00:15:17,356
outputs.


420
00:15:17,956 --> 00:15:19,046
In this case, shown on the


421
00:15:19,046 --> 00:15:20,756
slide, we have a single input of


422
00:15:20,756 --> 00:15:22,766
an image and an output, perhaps


423
00:15:22,766 --> 00:15:24,006
telling you what type of flower


424
00:15:24,116 --> 00:15:26,196
is present in it.


425
00:15:26,456 --> 00:15:28,966
Now many of the use cases in


426
00:15:28,966 --> 00:15:30,066
which you may want to apply a


427
00:15:30,066 --> 00:15:31,976
machine learning model have some


428
00:15:32,206 --> 00:15:33,806
key function at its core.


429
00:15:34,996 --> 00:15:36,876
A common type of function is


430
00:15:36,876 --> 00:15:37,766
performing some sort of


431
00:15:37,766 --> 00:15:38,636
classification.


432
00:15:39,576 --> 00:15:41,636
It's taking a set of inputs and


433
00:15:41,636 --> 00:15:42,946
assigning some categorical


434
00:15:42,946 --> 00:15:43,316
label.


435
00:15:43,846 --> 00:15:46,466
Take, for example, sentiment


436
00:15:46,466 --> 00:15:47,066
analysis.


437
00:15:47,706 --> 00:15:49,206
Here, you may take in an English


438
00:15:49,206 --> 00:15:51,196
sentence and output whether the


439
00:15:51,196 --> 00:15:52,566
sentiment was positive or


440
00:15:52,566 --> 00:15:55,506
negative, here represented by an


441
00:15:56,836 --> 00:15:57,026
emoji.


442
00:15:57,116 --> 00:15:58,736
Now, in order to encode


443
00:15:59,106 --> 00:16:01,606
functions of this type, Core ML


444
00:16:01,606 --> 00:16:02,836
supports a wide variety of


445
00:16:02,836 --> 00:16:03,366
models.


446
00:16:03,906 --> 00:16:08,866
It has extensive support for


447
00:16:08,866 --> 00:16:10,886
neural networks both feedforward


448
00:16:10,886 --> 00:16:11,946
and recurrent neural networks


449
00:16:11,946 --> 00:16:13,246
with over 30 different layer


450
00:16:13,246 --> 00:16:13,706
types.


451
00:16:14,956 --> 00:16:17,076
It also supports tree ensembles,


452
00:16:17,076 --> 00:16:18,546
support vector machines, and


453
00:16:18,546 --> 00:16:19,886
generalized linear models.


454
00:16:21,176 --> 00:16:22,516
Now each of these model types


455
00:16:22,516 --> 00:16:23,996
are actually a large family of


456
00:16:23,996 --> 00:16:24,536
models.


457
00:16:24,766 --> 00:16:26,046
And we could spend the rest of


458
00:16:26,046 --> 00:16:27,696
this session and the rest of


459
00:16:27,696 --> 00:16:28,956
this conference just talking


460
00:16:28,956 --> 00:16:31,696
about any one of them, but don't


461
00:16:31,696 --> 00:16:32,826
let that intimidate you.


462
00:16:33,106 --> 00:16:34,536
You do not need to be a machine


463
00:16:34,536 --> 00:16:36,036
learning expert to make use of


464
00:16:36,036 --> 00:16:36,796
these models.


465
00:16:37,296 --> 00:16:39,446
You continue to focus on the use


466
00:16:39,446 --> 00:16:41,246
case you're trying to enable and


467
00:16:41,246 --> 00:16:42,936
let Core ML handle those


468
00:16:42,936 --> 00:16:43,966
low-level details.


469
00:16:44,546 --> 00:16:45,896
Core ML represents a model in a


470
00:16:45,896 --> 00:16:46,776
single document.


471
00:16:47,316 --> 00:16:48,946
That is, sharing a model is just


472
00:16:48,946 --> 00:16:50,196
like sharing a file.


473
00:16:50,806 --> 00:16:52,686
This document has the high level


474
00:16:52,686 --> 00:16:53,976
information you, as a developer,


475
00:16:53,976 --> 00:16:54,886
need to program against.


476
00:16:54,886 --> 00:16:55,656
That is the functional


477
00:16:55,656 --> 00:16:57,276
description, its inputs, types,


478
00:16:57,276 --> 00:16:59,566
and outputs as well as those


479
00:16:59,566 --> 00:17:00,846
details that Core ML needs to


480
00:17:00,846 --> 00:17:02,106
actually execute the function.


481
00:17:02,446 --> 00:17:03,656
For a neural network, this may


482
00:17:03,656 --> 00:17:04,685
be the structure of the neural


483
00:17:04,685 --> 00:17:06,486
network along with its trained


484
00:17:06,486 --> 00:17:06,915
parameter.


485
00:17:07,006 --> 00:17:08,616
We encourage you to come to our


486
00:17:08,616 --> 00:17:10,036
Thursday session to learn more


487
00:17:10,036 --> 00:17:11,665
about this format and the set of


488
00:17:11,665 --> 00:17:15,695
models it can encode.


489
00:17:15,695 --> 00:17:16,856
But maybe now in the back of


490
00:17:16,856 --> 00:17:17,715
your head you're starting to


491
00:17:17,715 --> 00:17:20,066
think well, where do I get these


492
00:17:20,066 --> 00:17:20,616
models?


493
00:17:21,116 --> 00:17:24,175
Where do models come from?


494
00:17:24,175 --> 00:17:25,965
Well, a great place to start is


495
00:17:25,965 --> 00:17:27,326
to visit our machine learning


496
00:17:27,326 --> 00:17:28,156
landing page on


497
00:17:28,156 --> 00:17:29,386
developer.apple.com.


498
00:17:30,816 --> 00:17:32,286
There you'll find a small


499
00:17:32,286 --> 00:17:34,276
collection of task specific,


500
00:17:34,766 --> 00:17:36,966
ready to use models already in


501
00:17:36,966 --> 00:17:37,916
the Core ML format.


502
00:17:38,976 --> 00:17:40,186
We'll be expanding the set


503
00:17:40,186 --> 00:17:40,746
overtime.


504
00:17:40,746 --> 00:17:42,586
And we encourage you to explore.


505
00:17:42,826 --> 00:17:43,556
Give it a try.


506
00:17:43,556 --> 00:17:44,586
This is a great way to get


507
00:17:44,586 --> 00:17:45,656
introduced to machine learning


508
00:17:45,656 --> 00:17:47,406
in general and Core ML and how


509
00:17:47,406 --> 00:17:49,126
it can be used in your app.


510
00:17:51,076 --> 00:17:52,586
But we may not have all the


511
00:17:52,586 --> 00:17:53,366
models you need.


512
00:17:53,516 --> 00:17:54,886
And for your application, you


513
00:17:54,886 --> 00:17:55,586
may need to make some


514
00:17:55,586 --> 00:17:56,956
customization to an existing


515
00:17:56,956 --> 00:17:58,376
model or make a whole new model


516
00:17:58,376 --> 00:17:59,086
from scratch.


517
00:18:00,016 --> 00:18:01,856
And for that, we encourage you


518
00:18:01,856 --> 00:18:03,986
and your colleagues to tap into


519
00:18:03,986 --> 00:18:05,256
the machine learning community.


520
00:18:05,776 --> 00:18:07,526
There is a thriving community


521
00:18:07,526 --> 00:18:08,016
out there.


522
00:18:09,396 --> 00:18:11,026
There are tons of libraries and


523
00:18:11,026 --> 00:18:12,486
models already existing for you


524
00:18:12,486 --> 00:18:13,326
to start playing with.


525
00:18:14,096 --> 00:18:15,686
In addition, there are wonderful


526
00:18:15,686 --> 00:18:17,426
online courses and new tutorials


527
00:18:17,426 --> 00:18:18,806
are popping up every week.


528
00:18:19,966 --> 00:18:20,586
You can do it.


529
00:18:20,796 --> 00:18:22,106
There's great resources to


530
00:18:22,106 --> 00:18:22,606
learn.


531
00:18:22,606 --> 00:18:24,036
And I'm confident that all of


532
00:18:24,036 --> 00:18:24,906
you can get started.


533
00:18:24,906 --> 00:18:26,776
It's a wonderful time.


534
00:18:28,316 --> 00:18:30,066
But you'll see that most of


535
00:18:30,066 --> 00:18:32,096
these libraries are focused


536
00:18:32,096 --> 00:18:33,266
around training.


537
00:18:33,986 --> 00:18:35,266
That is, it's a very important


538
00:18:35,266 --> 00:18:36,986
step but they'll spend a lot of


539
00:18:36,986 --> 00:18:38,146
time making sure that you can


540
00:18:38,146 --> 00:18:39,046
train a great model.


541
00:18:39,046 --> 00:18:40,866
What we want you to do is let


542
00:18:40,926 --> 00:18:42,776
Core ML take you that last mile


543
00:18:42,776 --> 00:18:44,686
from taking that machine learned


544
00:18:44,686 --> 00:18:45,916
model and actually deploying it


545
00:18:45,916 --> 00:18:46,986
in your app.


546
00:18:47,736 --> 00:18:49,416
So we're doing this by


547
00:18:49,416 --> 00:18:51,206
introducing the Core ML Tools


548
00:18:51,206 --> 00:18:52,166
Python package.


549
00:18:52,696 --> 00:18:54,546
This is a Python package that is


550
00:18:54,546 --> 00:18:55,686
centered completely around


551
00:18:55,686 --> 00:18:57,826
taking the output of these


552
00:18:57,826 --> 00:18:59,536
machine learning libraries and


553
00:18:59,536 --> 00:19:01,126
getting them into the Core ML


554
00:19:01,126 --> 00:19:01,576
format.


555
00:19:02,646 --> 00:19:04,096
We'll be expanding these tools


556
00:19:04,096 --> 00:19:06,606
over time, but they're also


557
00:19:06,606 --> 00:19:07,976
being made Open Source.


558
00:19:08,516 --> 00:19:15,046
[ Applause ]


559
00:19:15,546 --> 00:19:16,706
So what this means, if you don't


560
00:19:16,706 --> 00:19:17,626
find the converter you need


561
00:19:17,626 --> 00:19:19,466
there, we're super confident you


562
00:19:19,466 --> 00:19:20,606
can already write your own.


563
00:19:21,136 --> 00:19:22,576
All of the primitives are there


564
00:19:22,576 --> 00:19:23,606
ready for you to use.


565
00:19:24,086 --> 00:19:25,316
Now I encourage you, again, to


566
00:19:25,316 --> 00:19:27,306
visit us on Thursday to learn


567
00:19:27,306 --> 00:19:28,836
more about this Python package


568
00:19:28,836 --> 00:19:30,116
and how you can easily convert


569
00:19:30,116 --> 00:19:30,516
models.


570
00:19:30,516 --> 00:19:34,946
OK. Now we have this model.


571
00:19:35,396 --> 00:19:36,626
Let's talk about how we use it


572
00:19:36,626 --> 00:19:37,156
in our apps.


573
00:19:37,156 --> 00:19:40,256
So you're going to start with


574
00:19:40,256 --> 00:19:41,626
the machine learned model in


575
00:19:41,626 --> 00:19:42,616
this Core ML format.


576
00:19:43,986 --> 00:19:45,666
You're going to simply add it to


577
00:19:45,666 --> 00:19:46,656
your Xcode project.


578
00:19:47,416 --> 00:19:49,036
And Xcode will automatically


579
00:19:49,176 --> 00:19:50,636
recognize this model as a


580
00:19:50,636 --> 00:19:51,626
machine learned model, and it


581
00:19:51,626 --> 00:19:53,366
will generate an interface for


582
00:19:53,366 --> 00:19:53,566
you.


583
00:19:54,116 --> 00:19:55,036
That will give you a


584
00:19:55,036 --> 00:19:56,326
programmatic interface to this


585
00:19:56,326 --> 00:19:58,746
model using data types and


586
00:19:58,746 --> 00:19:59,646
structures you're already


587
00:19:59,646 --> 00:20:00,906
familiar programming against.


588
00:20:01,386 --> 00:20:03,536
You'll program using this


589
00:20:03,686 --> 00:20:05,346
interface and compile it into


590
00:20:05,346 --> 00:20:06,446
your app.


591
00:20:07,376 --> 00:20:09,086
But in addition, the model


592
00:20:09,086 --> 00:20:10,866
itself will also get compiled


593
00:20:10,936 --> 00:20:12,496
and bundled into your app.


594
00:20:12,856 --> 00:20:14,636
That is, we'll take this format


595
00:20:14,636 --> 00:20:15,636
that's been optimized for


596
00:20:15,636 --> 00:20:17,856
openness and compatibility and


597
00:20:17,856 --> 00:20:19,176
turn it into one that is


598
00:20:19,176 --> 00:20:20,886
optimized for run time on the


599
00:20:20,886 --> 00:20:21,456
device.


600
00:20:22,996 --> 00:20:24,556
To show you this in action, I'm


601
00:20:24,656 --> 00:20:25,866
going to invite my friend and


602
00:20:25,866 --> 00:20:27,156
colleague, Lizi, to the stage.


603
00:20:28,516 --> 00:20:34,406
[ Applause ]


604
00:20:34,906 --> 00:20:36,436
And I'm an engineer on the Core


605
00:20:36,436 --> 00:20:36,946
ML team.


606
00:20:37,926 --> 00:20:40,316
Today, let's take a look at how


607
00:20:40,316 --> 00:20:42,216
you can use Core ML to integrate


608
00:20:42,216 --> 00:20:43,576
machine learning into your


609
00:20:43,576 --> 00:20:44,106
applications.


610
00:20:45,156 --> 00:20:46,976
Now first some context.


611
00:20:47,796 --> 00:20:49,016
As much as I love machine


612
00:20:49,016 --> 00:20:51,076
learning, I also love to garden.


613
00:20:51,076 --> 00:20:53,096
And if you're anything like me,


614
00:20:53,546 --> 00:20:54,796
you may get really excited


615
00:20:54,796 --> 00:20:56,436
whenever you come across a new


616
00:20:56,436 --> 00:20:58,336
kind of flower and instantly


617
00:20:58,336 --> 00:20:59,276
want to identify it.


618
00:21:00,366 --> 00:21:01,706
So we sought out to build an app


619
00:21:02,296 --> 00:21:03,866
that would take images of


620
00:21:03,866 --> 00:21:05,476
different types of flowers and


621
00:21:05,516 --> 00:21:07,426
classify them by type.


622
00:21:08,346 --> 00:21:09,886
But first, in order to do that,


623
00:21:10,466 --> 00:21:11,296
we needed a model.


624
00:21:12,866 --> 00:21:14,166
So we went ahead and we


625
00:21:14,166 --> 00:21:15,766
collected many different images


626
00:21:15,896 --> 00:21:17,116
of different types of flowers,


627
00:21:17,486 --> 00:21:18,686
and we labeled each one.


628
00:21:20,286 --> 00:21:21,656
I then trained a neural network


629
00:21:21,756 --> 00:21:23,686
classifier using an Open Source


630
00:21:23,756 --> 00:21:25,296
machine learning tool and


631
00:21:25,296 --> 00:21:27,326
converted it into our Core ML


632
00:21:27,416 --> 00:21:27,666
format.


633
00:21:29,256 --> 00:21:31,896
So with this trained model and


634
00:21:31,896 --> 00:21:33,086
with this app shell of this


635
00:21:33,086 --> 00:21:35,006
flower classifier, let's take a


636
00:21:35,006 --> 00:21:36,906
look at how we can put the two


637
00:21:36,906 --> 00:21:37,276
together.


638
00:21:38,946 --> 00:21:40,746
So over in Xcode, you can see


639
00:21:40,746 --> 00:21:42,736
I'm currently running the flower


640
00:21:42,736 --> 00:21:44,616
classifier app and nearing the


641
00:21:44,616 --> 00:21:45,626
display on my device.


642
00:21:46,686 --> 00:21:48,606
And what I can do is I can go


643
00:21:48,606 --> 00:21:50,346
into the photos library and


644
00:21:50,346 --> 00:21:52,626
choose an image, but right now


645
00:21:52,736 --> 00:21:53,906
it doesn't currently predict


646
00:21:53,906 --> 00:21:54,906
anything for the output.


647
00:21:55,396 --> 00:21:57,246
It just displays this string.


648
00:21:58,796 --> 00:22:00,086
So if I move over to the


649
00:22:00,086 --> 00:22:02,606
ViewController, what we see is


650
00:22:02,606 --> 00:22:04,836
it has this caption method that


651
00:22:05,046 --> 00:22:06,226
currently just returns that


652
00:22:06,256 --> 00:22:06,856
blank string.


653
00:22:07,426 --> 00:22:10,746
It doesn't actually do anything.


654
00:22:10,836 --> 00:22:12,636
Over here, I have this flower


655
00:22:12,636 --> 00:22:14,176
classifier in the ML model


656
00:22:14,236 --> 00:22:14,506
format.


657
00:22:15,186 --> 00:22:16,656
And all I need to do is take it


658
00:22:17,306 --> 00:22:18,696
and drag it over into the


659
00:22:18,696 --> 00:22:19,066
project.


660
00:22:19,786 --> 00:22:22,936
And we can see, as soon as I add


661
00:22:23,066 --> 00:22:24,796
it, Xcode automatically


662
00:22:24,796 --> 00:22:26,546
recognizes this file format.


663
00:22:27,406 --> 00:22:28,716
And it populates the name of the


664
00:22:28,716 --> 00:22:30,816
model as well as what type it is


665
00:22:30,816 --> 00:22:31,346
underneath.


666
00:22:31,926 --> 00:22:33,666
We can see that the size is 41


667
00:22:33,666 --> 00:22:35,226
megabytes and also other


668
00:22:35,226 --> 00:22:36,616
information like the author,


669
00:22:36,866 --> 00:22:38,296
license, or description


670
00:22:38,296 --> 00:22:39,226
associated with it.


671
00:22:40,446 --> 00:22:41,876
In the bottom, we can see the


672
00:22:41,916 --> 00:22:43,266
inputs and outputs that this


673
00:22:43,266 --> 00:22:45,736
model takes which, first is


674
00:22:46,126 --> 00:22:48,136
flower image, a type of image


675
00:22:48,476 --> 00:22:49,966
that's ordered red, green, blue,


676
00:22:50,086 --> 00:22:51,096
the following width and height.


677
00:22:52,406 --> 00:22:53,816
We can also see that it outputs


678
00:22:53,816 --> 00:22:55,506
a flower type as a string.


679
00:22:56,166 --> 00:22:57,756
So if give it an image of a


680
00:22:57,756 --> 00:22:59,576
rose, I would expect flower type


681
00:22:59,646 --> 00:23:00,526
to be rose.


682
00:23:01,896 --> 00:23:03,466
There's also a dictionary full


683
00:23:03,466 --> 00:23:05,016
of probabilities for different


684
00:23:05,016 --> 00:23:05,736
types of flowers.


685
00:23:06,306 --> 00:23:08,796
Now the first thing I want to do


686
00:23:10,206 --> 00:23:11,236
is I want to make sure that I


687
00:23:11,436 --> 00:23:12,966
add this to my app.


688
00:23:13,586 --> 00:23:15,876
And you'll see that afterwards


689
00:23:16,396 --> 00:23:18,406
this middle pane will show that


690
00:23:18,406 --> 00:23:20,086
the Swift generated source has


691
00:23:20,366 --> 00:23:21,896
arrived in the application.


692
00:23:22,896 --> 00:23:23,966
What that means is we can


693
00:23:23,966 --> 00:23:25,866
actually use this generated code


694
00:23:26,176 --> 00:23:27,536
to load the model and predict


695
00:23:27,536 --> 00:23:28,046
against it.


696
00:23:28,846 --> 00:23:30,576
To show you, let's go to the


697
00:23:30,576 --> 00:23:31,206
ViewController.


698
00:23:31,816 --> 00:23:36,156
The first thing I want to do is


699
00:23:36,156 --> 00:23:37,526
define this flower model.


700
00:23:38,766 --> 00:23:40,216
And to instantiate it, all I


701
00:23:40,216 --> 00:23:42,086
need to do is use the name of a


702
00:23:42,206 --> 00:23:45,656
model class itself.


703
00:23:45,806 --> 00:23:47,346
We'll see that it's highlighted


704
00:23:47,506 --> 00:23:48,456
because it exists in the


705
00:23:48,456 --> 00:23:48,866
project.


706
00:23:49,906 --> 00:23:51,516
Now next in the caption method,


707
00:23:51,766 --> 00:23:52,866
let's define a prediction.


708
00:23:53,446 --> 00:23:56,886
And we'll actually use the


709
00:23:56,936 --> 00:24:00,206
flower model and look inside.


710
00:24:01,076 --> 00:24:02,546
We'll see using auto complete


711
00:24:02,696 --> 00:24:03,896
that this prediction method is


712
00:24:03,896 --> 00:24:05,156
available that takes a


713
00:24:05,156 --> 00:24:07,056
CVPixelBuffer as input.


714
00:24:07,906 --> 00:24:09,666
We'll use that and pass the


715
00:24:09,666 --> 00:24:12,786
image directly to it.


716
00:24:13,286 --> 00:24:14,726
Now, instead of returning this


717
00:24:14,866 --> 00:24:16,776
preset string, we can use the


718
00:24:16,776 --> 00:24:18,716
prediction object and instead


719
00:24:19,236 --> 00:24:20,966
return the flower type.


720
00:24:21,676 --> 00:24:24,166
Now if I save, let's build and


721
00:24:24,166 --> 00:24:25,266
run the app.


722
00:24:26,026 --> 00:24:27,586
And what's happening while I do


723
00:24:27,586 --> 00:24:29,916
this is the model itself is


724
00:24:29,916 --> 00:24:31,306
actually getting compiled and


725
00:24:31,306 --> 00:24:32,566
bundled with the application.


726
00:24:33,706 --> 00:24:34,936
So if we go back to the device,


727
00:24:35,476 --> 00:24:37,196
if I go to my photos library, I


728
00:24:37,506 --> 00:24:39,136
can choose a rose on the second


729
00:24:39,136 --> 00:24:39,366
row.


730
00:24:40,216 --> 00:24:41,536
And we can see it actually is


731
00:24:41,536 --> 00:24:42,696
able to display rose.


732
00:24:43,516 --> 00:24:49,356
[ Applause ]


733
00:24:49,856 --> 00:24:51,106
We give it another one, maybe


734
00:24:51,106 --> 00:24:52,356
the sunflower on the third.


735
00:24:53,406 --> 00:24:54,746
It's also able to get this one.


736
00:24:55,736 --> 00:24:56,846
Or even something a little bit


737
00:24:56,846 --> 00:24:58,216
more difficult like the passion


738
00:24:58,216 --> 00:24:59,646
flower on the bottom, it can


739
00:24:59,646 --> 00:25:00,196
still do.


740
00:25:01,296 --> 00:25:04,346
So to recap, we were just able


741
00:25:04,576 --> 00:25:06,876
to drag a trained ML model into


742
00:25:07,146 --> 00:25:09,236
Xcode and easily add it into our


743
00:25:09,236 --> 00:25:10,906
application with three lines of


744
00:25:10,946 --> 00:25:11,346
code.


745
00:25:12,006 --> 00:25:13,696
And now we have a full neural


746
00:25:13,696 --> 00:25:15,186
network classifier running on


747
00:25:15,186 --> 00:25:15,616
the device.


748
00:25:16,576 --> 00:25:19,096
But I have some other models


749
00:25:19,096 --> 00:25:20,576
that we might want to try so


750
00:25:21,006 --> 00:25:21,896
let's go through this again.


751
00:25:22,376 --> 00:25:25,016
You may have noticed there's


752
00:25:25,016 --> 00:25:26,506
also this FlowerSqueeze.


753
00:25:27,356 --> 00:25:28,996
And the first thing I'll do is


754
00:25:28,996 --> 00:25:31,346
try to drag it in again.


755
00:25:32,066 --> 00:25:35,476
And then I'll also add this one


756
00:25:35,596 --> 00:25:37,246
to the flower classifier, the


757
00:25:37,386 --> 00:25:39,416
hello flowers target so that it


758
00:25:39,566 --> 00:25:40,686
starts generating the Swift


759
00:25:40,816 --> 00:25:41,786
source in the background.


760
00:25:42,566 --> 00:25:43,956
And we'll see, again if we zoom


761
00:25:44,036 --> 00:25:46,016
in, that the name of this model


762
00:25:46,056 --> 00:25:46,456
is different.


763
00:25:46,606 --> 00:25:48,606
It's called FlowerSqueeze but in


764
00:25:48,606 --> 00:25:50,186
the bottom the inputs and


765
00:25:50,186 --> 00:25:51,706
outputs are exactly the same.


766
00:25:53,086 --> 00:25:54,626
What this means is that if I go


767
00:25:54,626 --> 00:25:55,886
back into the ViewController,


768
00:25:57,786 --> 00:25:59,016
instead of flower -- actually,


769
00:25:59,476 --> 00:26:00,906
let's delete flower classifier.


770
00:26:01,076 --> 00:26:02,006
We don't want that model


771
00:26:02,006 --> 00:26:02,336
anymore.


772
00:26:02,926 --> 00:26:05,656
And in the ViewController,


773
00:26:06,216 --> 00:26:07,436
instead of instantiating this


774
00:26:07,436 --> 00:26:10,146
one, we can use FlowerSqueeze.


775
00:26:10,786 --> 00:26:13,336
And the really neat thing about


776
00:26:13,386 --> 00:26:15,276
this model is its size.


777
00:26:16,136 --> 00:26:19,106
So if we go back, this one is


778
00:26:19,106 --> 00:26:21,146
only 5.4 megabytes, which is a


779
00:26:21,146 --> 00:26:21,806
huge difference.


780
00:26:22,266 --> 00:26:27,086
Back in the device, we can go to


781
00:26:27,086 --> 00:26:28,756
the photos library and choose


782
00:26:28,756 --> 00:26:29,906
another one like love in the


783
00:26:29,906 --> 00:26:32,256
mist and see that this app is


784
00:26:32,256 --> 00:26:34,966
also able to classify this as


785
00:26:35,556 --> 00:26:35,676
well.


786
00:26:35,886 --> 00:26:38,856
Try another passion flower or


787
00:26:38,856 --> 00:26:40,376
even a more difficult photo of a


788
00:26:40,376 --> 00:26:42,696
rose on the side and it's still


789
00:26:42,696 --> 00:26:43,966
able to predict correctly.


790
00:26:44,586 --> 00:26:48,716
So to recap, we just saw that


791
00:26:48,856 --> 00:26:50,126
we're able to run this neural


792
00:26:50,126 --> 00:26:51,676
network classifier on device


793
00:26:51,896 --> 00:26:54,086
using Core ML, all with only a


794
00:26:54,176 --> 00:26:55,886
couple lines of code.


795
00:26:56,516 --> 00:27:04,086
[ Applause ]


796
00:27:04,586 --> 00:27:06,396
Now let's go back and recap what


797
00:27:06,396 --> 00:27:08,616
we just saw.


798
00:27:08,856 --> 00:27:11,556
We saw that in Xcode, as soon as


799
00:27:11,746 --> 00:27:13,586
you drag an ML model that's been


800
00:27:13,586 --> 00:27:15,106
trained into your application,


801
00:27:15,376 --> 00:27:16,446
you get this view that's


802
00:27:16,446 --> 00:27:17,956
populated that gives you


803
00:27:17,956 --> 00:27:19,616
information such as the name of


804
00:27:19,616 --> 00:27:21,836
the model, the type that it is


805
00:27:21,836 --> 00:27:23,316
underneath, and also other


806
00:27:23,316 --> 00:27:25,806
information like the size or any


807
00:27:25,806 --> 00:27:27,526
other information the author put


808
00:27:27,526 --> 00:27:27,676
in.


809
00:27:28,276 --> 00:27:29,996
You can also see in the bottom


810
00:27:30,086 --> 00:27:31,816
that you get information such as


811
00:27:31,816 --> 00:27:33,096
the input and output of the


812
00:27:33,096 --> 00:27:34,946
model which is very helpful if


813
00:27:34,946 --> 00:27:35,946
you're actually trying to use


814
00:27:35,946 --> 00:27:36,106
it.


815
00:27:36,106 --> 00:27:38,276
And once you add it to your app


816
00:27:38,276 --> 00:27:40,166
target, you get generated code


817
00:27:40,166 --> 00:27:42,106
that you can use to load and use


818
00:27:42,106 --> 00:27:45,366
the model to predict.


819
00:27:45,436 --> 00:27:47,076
We also saw in our application


820
00:27:47,076 --> 00:27:48,706
how simple it was to actually


821
00:27:48,706 --> 00:27:49,106
use it.


822
00:27:49,956 --> 00:27:51,826
Again, even though the model was


823
00:27:51,826 --> 00:27:53,746
a neural network classifier, all


824
00:27:53,746 --> 00:27:55,266
we needed to do to instantiate


825
00:27:55,266 --> 00:27:56,626
it was call the name of the


826
00:27:56,626 --> 00:27:57,066
file.


827
00:27:57,736 --> 00:27:58,856
This means that the model type


828
00:27:58,856 --> 00:28:00,686
was completely abstracted so


829
00:28:00,686 --> 00:28:01,856
whether it's a support vector


830
00:28:01,856 --> 00:28:03,536
machine, a linear model, or a


831
00:28:03,536 --> 00:28:05,686
neural network, you all load


832
00:28:05,916 --> 00:28:07,906
them the exact same way.


833
00:28:07,906 --> 00:28:09,206
We also noticed that the


834
00:28:09,206 --> 00:28:11,036
prediction method took a


835
00:28:11,136 --> 00:28:12,106
CVPixelBuffer.


836
00:28:12,776 --> 00:28:14,346
It was strongly typed so you


837
00:28:14,346 --> 00:28:15,406
know you'll be able to check


838
00:28:15,536 --> 00:28:17,496
input errors at compile time


839
00:28:17,646 --> 00:28:18,616
rather than at runtime.


840
00:28:20,026 --> 00:28:22,766
But now let's take a deeper look


841
00:28:23,126 --> 00:28:24,486
at the generated source that's


842
00:28:24,536 --> 00:28:27,416
being created by Xcode.


843
00:28:27,476 --> 00:28:29,706
Here, we see it defines three


844
00:28:29,706 --> 00:28:31,696
classes, first of which is the


845
00:28:31,696 --> 00:28:33,966
input that defines the flower


846
00:28:33,966 --> 00:28:35,456
image as a CVPixelBuffer.


847
00:28:36,456 --> 00:28:38,306
We then see the output with the


848
00:28:38,306 --> 00:28:40,836
same two types that were listed


849
00:28:40,836 --> 00:28:41,976
in the generated interface.


850
00:28:43,176 --> 00:28:44,936
And next, in the model class


851
00:28:44,936 --> 00:28:46,586
itself, we see the convenience


852
00:28:46,586 --> 00:28:48,116
initializer that we use to


853
00:28:48,116 --> 00:28:50,346
actually create the model and


854
00:28:50,346 --> 00:28:53,336
the predict method as well.


855
00:28:53,536 --> 00:28:55,486
But also inside this generated


856
00:28:55,486 --> 00:28:57,326
source, you have access to the


857
00:28:57,386 --> 00:28:58,376
underlying model.


858
00:28:58,986 --> 00:29:00,676
So for more advanced use cases,


859
00:29:01,006 --> 00:29:02,656
you can use it as well to


860
00:29:02,656 --> 00:29:04,096
programmatically interact with


861
00:29:04,096 --> 00:29:04,226
it.


862
00:29:05,556 --> 00:29:06,816
We take a look inside the


863
00:29:06,886 --> 00:29:08,966
MLModel class, we can see that


864
00:29:08,966 --> 00:29:10,186
there's a model description


865
00:29:10,896 --> 00:29:12,626
which gives you access to


866
00:29:12,626 --> 00:29:14,946
anything you saw in the metadata


867
00:29:15,136 --> 00:29:16,066
in Xcode.


868
00:29:16,636 --> 00:29:18,196
There is also an additional


869
00:29:18,196 --> 00:29:19,726
prediction method that takes


870
00:29:19,816 --> 00:29:21,726
protocol conforming input and


871
00:29:21,726 --> 00:29:23,206
returns protocol conforming


872
00:29:23,206 --> 00:29:24,606
output giving you more


873
00:29:24,676 --> 00:29:26,456
flexibility in how the input is


874
00:29:26,456 --> 00:29:26,956
provided.


875
00:29:27,836 --> 00:29:29,366
So we walked through the top


876
00:29:29,366 --> 00:29:31,016
half of this development flow


877
00:29:31,446 --> 00:29:32,946
and saw what happens when you


878
00:29:32,946 --> 00:29:34,826
drag a trained model into Xcode.


879
00:29:35,486 --> 00:29:36,766
It's able to generate Swift


880
00:29:36,856 --> 00:29:38,806
source that you can then use in


881
00:29:39,486 --> 00:29:41,436
your -- to use the model in your


882
00:29:41,526 --> 00:29:42,006
application.


883
00:29:44,096 --> 00:29:46,156
The model itself, as we saw, is


884
00:29:46,156 --> 00:29:47,926
also compiled and bundled in


885
00:29:47,926 --> 00:29:49,066
your app.


886
00:29:51,816 --> 00:29:53,506
To elaborate on this step a bit


887
00:29:53,506 --> 00:29:55,986
more, what we do, we first take


888
00:29:55,986 --> 00:29:58,646
the model from its JSON form,


889
00:29:58,836 --> 00:29:59,756
which is optimized for


890
00:29:59,846 --> 00:30:01,056
portability and ease of


891
00:30:01,056 --> 00:30:02,886
conversion, and compile it so


892
00:30:03,166 --> 00:30:04,606
that it loads quickly on your


893
00:30:04,606 --> 00:30:06,826
device when you initialize it.


894
00:30:08,036 --> 00:30:09,886
We also make sure that the model


895
00:30:09,886 --> 00:30:11,376
is matched to the underlying


896
00:30:11,376 --> 00:30:12,716
inference engines that are


897
00:30:12,716 --> 00:30:14,276
ultimately evaluating it on your


898
00:30:14,276 --> 00:30:15,916
device giving you optimized


899
00:30:16,026 --> 00:30:17,426
prediction when you need it.


900
00:30:17,426 --> 00:30:20,576
And the last thing we saw in the


901
00:30:20,576 --> 00:30:22,806
demo was we swapped out models.


902
00:30:23,646 --> 00:30:25,586
Mostly just to reduce the size


903
00:30:26,426 --> 00:30:28,196
but there are other reasons why


904
00:30:28,196 --> 00:30:29,226
you might want to play around


905
00:30:29,226 --> 00:30:29,936
with different ones.


906
00:30:30,346 --> 00:30:32,406
For example, accuracy or just to


907
00:30:32,406 --> 00:30:34,266
test out different types to see


908
00:30:34,266 --> 00:30:35,096
how they perform.


909
00:30:35,696 --> 00:30:40,636
In summary, today we saw an


910
00:30:40,636 --> 00:30:41,966
overview of some of the machine


911
00:30:41,966 --> 00:30:43,206
learning frameworks that we have


912
00:30:43,206 --> 00:30:43,746
available.


913
00:30:44,686 --> 00:30:46,736
We also introduced Core ML, a


914
00:30:46,736 --> 00:30:48,166
new framework for on-device


915
00:30:48,276 --> 00:30:48,746
inference.


916
00:30:49,956 --> 00:30:51,176
And also we showed you the


917
00:30:51,176 --> 00:30:52,456
development flow in action.


918
00:30:54,536 --> 00:30:55,946
For more information, make sure


919
00:30:55,946 --> 00:30:56,786
you check us out at


920
00:30:56,786 --> 00:30:58,376
developer.apple.com with our


921
00:30:58,376 --> 00:30:59,816
session number 703.


922
00:31:00,696 --> 00:31:02,096
And also, come visit some of the


923
00:31:02,096 --> 00:31:03,976
related sessions such as Vision


924
00:31:03,976 --> 00:31:06,536
and NLP and Core ML in depth on


925
00:31:06,596 --> 00:31:08,076
Thursday if you want to see some


926
00:31:08,076 --> 00:31:09,606
more use cases of how you can


927
00:31:09,606 --> 00:31:11,366
use Core ML or how to actually


928
00:31:11,366 --> 00:31:12,946
convert models into the Core ML


929
00:31:13,006 --> 00:31:13,266
format.


930
00:31:14,276 --> 00:31:15,666
Thank you and enjoy the rest of


931
00:31:15,666 --> 00:31:16,206
the conference.


932
00:31:17,016 --> 00:31:18,000
[ Applause ]

